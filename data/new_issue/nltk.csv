,html_url,number,labels,state,created_at,pull_request,comments,title,body
0,https://github.com/nltk/nltk/issues/1246,1246,[],closed,2016-01-04 19:27:09+00:00,,5,"Add ""ll"" to nltk.corpus.stopwords.word(""english"")","I think that ""ll"" should be added to this corpus, as ""s"" and ""t"" are already there, and when sentences with contractions such as ""they'll"" or ""you'll"" are tokenized, ""ll"" will be added as a token, and if we filter out stopwords, ""ll"" should be removed as well.
"
1,https://github.com/nltk/nltk/issues/1248,1248,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}]",open,2016-01-08 19:25:24+00:00,,8,Turkish language extension,"Hello all,
I'm a Turkish NLP developer and I'd like to add Turkish language libraries including a morphological analyzer and a parser. Would anyone be interested?
"
2,https://github.com/nltk/nltk/issues/1250,1250,[],closed,2016-01-13 10:30:10+00:00,,4,Tokenization error,"There is an error of processing dots in the tokenizer:
The following code

```
    val ptbt = new PTBTokenizer(
      new StringReader(""Tokenization is performed.Parameters can be specified.""),
      new CoreLabelTokenFactory(), """")
    while (ptbt.hasNext()) {
      val label = ptbt.next()
      val w = label.originalText()
      println(w)
    }
```

gives ""performed.Parameters"" as an word. If I put a space after 'performed' it will work, but still, natural text can be messy and omitting spaces after punctuation is very common.
"
3,https://github.com/nltk/nltk/issues/1251,1251,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-01-13 17:16:18+00:00,,1,corpus.reader.knbc misreads annotation for 33 words,"When using the corpus reader for the KNB Corpus, tags are misread / displayed wrongly for 33 words.

`nltk.corpus.reader.knbc.tagged_words()` returns a list of touples in the following form:

```
tagged_word = (word(str), tags(str))
tags = ""reading lemma pos1 posid1 pos2 posid2 pos3 posid3 others ...""
```

(note that this is a different form than what the comments in the code document, on line 39, 40, 41 in  knbc.py says)

This misreading happens for 30 occurrences of '特殊', which appears to be used in the corpus to mark some kind of special non-word entity and is not annotated with _reading_ nor _lemma_.

Word number 14768 '５００', returns a double space between _reading_ and _lemma_, causing the lemma tag to appear empty and the pos1 tag to appear as the lemma.

Word 54546 and 54678 are smileys that have been loaded wrongly by the corpus reader and part of the smileys appear in the string that represents 

Words for which tags are returned with a wrong form are:

[1396, 7219, 8750, 10916, 14445, 14464, 14768, 18538, 18540, 20655, 23116, 23120, 23123, 23128, 26698, 32867, 32869, 32871, 32873, 32875, 32877, 32892, 32894, 44702, 46037, 46490, 46492, 46494, 46496, 46498, 46557, 54546, 54678]
"
4,https://github.com/nltk/nltk/issues/1252,1252,[],closed,2016-01-16 23:08:40+00:00,,0,"Tox fails with ""ERROR: Failure: ImportError (No module named 'six')""","When I try to run the tests with Tox (on Ubuntu) from within a local clone of the repo, it manages to install the dependencies but blows up when trying to import things from within NLTK.

I imagine I can work around this by figuring out how to manually run just the tests I care about, but it's inconvenient.

I'm not sure whether I'm doing something dumb or whether the Tox setup is broken; if the former, the CONTRIBUTING docs should probably mention what needs to be done besides just running Tox; if the latter, it should probably be fixed.

Here's the full output (had to pastebin it due to GitHub's post length limit):

http://pastebin.com/ENuCLnv6
"
5,https://github.com/nltk/nltk/issues/1253,1253,[],closed,2016-01-17 11:25:40+00:00,,30,how to download corpus panlex_lite package in nltk in python,"I am able to download all the packages except the panlex_lite how to download it?
"
6,https://github.com/nltk/nltk/issues/1254,1254,[],closed,2016-01-18 06:09:47+00:00,,4,Loading jars from custom path.,"Hello Team,
 I want to load the stanfor-parser.jar file from my custom defined path. I dont want to set ENV variable for jar locations. Is this possible ? If yes then how?

Thanks,
Rahul 
"
7,https://github.com/nltk/nltk/issues/1255,1255,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 719374994, 'node_id': 'MDU6TGFiZWw3MTkzNzQ5OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/GUI', 'name': 'GUI', 'color': 'f9b3d9', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-01-18 13:00:40+00:00,,3,Printing to postscript error,"I draw a syntax tree with tree.draw(), which is below:

![2016-01-18 8 56 33](https://cloud.githubusercontent.com/assets/5208653/12391905/0683e8a8-be26-11e5-85f7-fba3b98c1626.png)

But after i save it as a ps file using 'File --> Print to Postscript', I get below:

![2016-01-18 8 58 20](https://cloud.githubusercontent.com/assets/5208653/12391934/3f2fa23c-be26-11e5-9494-36cd0e99427d.png)

So what is wrong with it?? can somebody help me?
"
8,https://github.com/nltk/nltk/issues/1256,1256,[],closed,2016-01-18 13:27:37+00:00,,11,Reading wordnet error,"My environment is Python3.5.1 and nltk3.1. When I executed the second command below, it throws an AssertionError.

``` python
from nltk.corpus import wordnet as wn
wn.synsets('dog')
```

I find that there is a bug in class OpenOnDemandZipFile(data.py) about file reference count.
I modified the member function read() and it worked.

``` python
def read(self, name):
　　assert self.fp is None
　　self.fp = open(self.filename, 'rb')
　　self._fileRefCnt += 1 #my modification
　　value = zipfile.ZipFile.read(self, name)
　　self.close()
　　return value
```
"
9,https://github.com/nltk/nltk/issues/1258,1258,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-01-20 10:33:09+00:00,,1,Potential wordnet lemmatization issue,"I am not sure if its an issue with wordnet corpus or with nltk or my way of using it, but I've found one case where I am not getting expected result:
~/nltk_data/corpora/wordnet/noun.exc contain this line:
antae anta

where (according to wikipedia) anta is singular and antae plural.
yet this code:

from nltk.stem import WordNetLemmatizer
wnl = WordNetLemmatizer()
print(wnl.lemmatize('antae'))

prints antae, not anta.
same goes for antalkalies -> antalkali
but antefixa -> antefix works fine.

Is it a bug (and where?) or I am wrong expecting it to be lemmatized this way?
"
10,https://github.com/nltk/nltk/issues/1259,1259,[],closed,2016-01-20 17:22:30+00:00,,3,ImportError: No module named sentiment  for python 2.7 on Windows PC and Anaconda  installation,"I use python 2.7 on Windows PC and Anaconda python installation
Pls help run  example from  http://www.nltk.org/howto/sentiment.html 
can not import nltk.sentiment, error comes from `from nltk.sentiment import SentimentAnalyzer`

It gives the error:

```
Traceback (most recent call last):
  Debug Probe, prompt 1, line 1
ImportError: No module named sentiment
from nltk.corpus import subjectivity
Traceback (most recent call last):
  Debug Probe, prompt 2, line 1
ImportError: cannot import name subjectivity
```

but nltk import is ok 

`import nltk` and `NaiveBayesClassifier` import is ok seems to be everything 

```
from nltk.classify import NaiveBayesClassifier
```

but 

```
from nltk.sentiment fails to import 
```

gives 

```
from nltk.sentiment.vader import SentimentIntensityAnalyzer
Traceback (most recent call last):
  Debug Probe, prompt 6, line 1
ImportError: No module named sentiment.vader
```
"
11,https://github.com/nltk/nltk/issues/1260,1260,[],closed,2016-01-21 20:46:15+00:00,,3,SRL in SENNA,"Just a quick raincheck. Is there a reason why semantic role labeling (SRL) was left out in the [nltk.tag.senna.py](https://github.com/nltk/nltk/blob/develop/nltk/tag/senna.py)? Is someone working on adding it?  
"
12,https://github.com/nltk/nltk/issues/1266,1266,[],closed,2016-01-24 08:23:07+00:00,,1,AttributeError with convert_regexp_to_noncapturing_parsed,"I've got this error working with TextBlob, the traceback is more or less all on the NLTK side though.

Essentially, running `blob.noun_phrases` throws an AttributeError. Here's the end of the dump:

```
/usr/lib/python3.5/site-packages/nltk/__init__.py in compile_regexp_to_noncapturing(pattern, flags)
     54     """"""
     55     global _java_bin, _java_options
---> 56     _java_bin = find_binary('java', bin, env_vars=['JAVAHOME', 'JAVA_HOME'], verbose=verbose, binary_names=['java.exe'])
     57 
     58     if options is not None:

/usr/lib/python3.5/site-packages/nltk/__init__.py in convert_regexp_to_noncapturing_parsed(parsed_pattern)
     50         ``'-Xmx512m'``, which tells Java binary to increase
     51         the maximum heap size to 512 megabytes.  If no options are
---> 52         specified, then do not modify the options list.
     53     :type options: list(str)
     54     """"""

AttributeError: can't set attribute
```

Could this be a problem with the Java install?
"
13,https://github.com/nltk/nltk/issues/1267,1267,[],closed,2016-01-24 17:32:54+00:00,,4,RegexpTokenizer does not handle capturing parens as advertised,"The docstring for `nltk.tokenize.regexp.RegexpTokenizer` states:

```
:param pattern: The pattern used to build this tokenizer.
        (This pattern may safely contain capturing parentheses.)
```

But this does not seem to be the case:

``` python
import nltk
nltk.regexp_tokenize(""foo bar baz-qux"", r""\w+(-\w+)*"")
# returns ['', '', '-qux']
```

Contrast with:

``` python
nltk.regexp_tokenize(""foo bar baz-qux"", r""\w+(?:-\w+)*"")
# returns ['foo', 'bar', 'baz-qux']
```

Tested both with PyPI release (`nltk-3.1`) and current `develop` branch.
"
14,https://github.com/nltk/nltk/issues/1268,1268,[],closed,2016-01-25 02:06:09+00:00,,6,Technical issues in BLEU,"This is an issue with how BLEU works and not exactly the implementation that causes an error with https://nltk.ci.cloudbees.com/job/nltk/TOXENV=py35-jenkins,jdk=jdk8latestOnlineInstall/492/testReport/junit/(root)/bleu_doctest/bleu_doctest/

---

There are many flaws in the original formulation of BLEU in http://www.aclweb.org/anthology/P02-1040.pdf but there's a big problem that we've all overlooked. 

Equation in section 2.3 in the paper:

```
BLEU = BP * exp (sum (w_n * log(p_n)))
```

where `p_n` is the modified precision calculated with the formula from section 2.1.1 and `w_n` is the corresponding weights as per each order of ngram set by the user. 

Consider the case where the numerator in the `p_n` formula of section 2.1.1 is 0 for every order of ngram. So `log(p_n)` becomes invalid because the logarithm curve is asymptotic at 0 (giving a math domain error).

So if the solution was to simply set `log(p_n) = 0 if p_n == 0 else log(p_n)` then we get into another problem when the sum of all `log(p_n)` is 0 and our `exp(0)` returns 1 which gives perfect BLEU scores for 0 precision. 

The assumption is that BLEU is measured at corpus level so having absolute 0 for all hypothesis-reference pairs for all order of ngram seems ridiculous.

So at segment level, there seem to be multiple ways to overcome that according to http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf .

**How should we proceed with the implementation of BLEU in NLTK?**
- Implement the Chen and Cherry (2014) methods and enforce one of the smoothing by default
- If numerator == 0, do the ""stupid"" smoothing i.e. to add 1 to numerator and denominator (name from http://www.aclweb.org/anthology/D07-1090.pdf)
- Leave it as it is, and use the awkward doctest as an example of how BLEU is inadequate at sentence level
"
15,https://github.com/nltk/nltk/issues/1276,1276,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-01-27 17:29:15+00:00,,0,Tgrep label predicates fail without macros tgrep.py line 696,"I'm not a tgrep expert but I was futzing around with nltk's implementation and it seems like tgrep strings of the form:
""NNP=n: =n > NP;"" fail because the NNP predicate is a string and the source code is expecting the lambda function.

The NLTK implementation fails on the above tgrep string because the string 'NNP' isn't a lambda function like the code in tgrep.py line 696 assumes.

The following does work:
""@ NNPM /NNP/; @NNPM=n: =n > NP;"" Correctly returns any NNP that is dominated by an NP.

From what I can tell from the tgrep2 documentation, it _should_ allow non-macro predicates.

If I am correct that tgrep should allow non-macro label predicates, maybe the NLTK implementation could provide some documentation that tgrep node labeling only works with declared macros.
"
16,https://github.com/nltk/nltk/issues/1277,1277,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 718773983, 'node_id': 'MDU6TGFiZWw3MTg3NzM5ODM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/classifier', 'name': 'classifier', 'color': '60d2ff', 'default': False, 'description': None}]",closed,2016-01-28 14:44:56+00:00,,1,Better documentation for HMM unsupervised training,"I had some problems using unsupervised training for hmms.
The docs say:

```
        :param unlabeled_sequences: the unsupervised training data, a set of
            sequences of observations
        :type unlabeled_sequences: list
```

When i read this i thought about a List [ word_1, ..., word_n ] < a unlabeled sequence > but the correct way is [ (word_1, tag_1),...,(word_n,tag_n) ] < a labeled sequence imo > .
I think we should explain this better to avoid confusion.
"
17,https://github.com/nltk/nltk/issues/1278,1278,[],closed,2016-01-29 07:36:44+00:00,,2,use source-code to install fail,"git clone code;
sudo python setup.py install;
a problem:
Traceback (most recent call last):
  File ""setup.py"", line 30, in <module>
    from setuptools import setup, find_packages
ImportError: No module named setuptools
"
18,https://github.com/nltk/nltk/issues/1279,1279,[],closed,2016-01-29 18:56:00+00:00,,8,OSError when downloading/unzipping NLTK data (Python 3.5.1),"I'm getting an OSError when I try to download data via the NLTK command line interface. This occurs when unzipping `corpora/panlex_lite.zip`

Running NLTK 3.1 on Python 3.5.1 (Python installed via Homebrew, NLTK installed via pip) on Mac OS X 10.11.3

Tried running `python3 -m nltk.downloader all` as suggested on http://www.nltk.org/data.html

```
sandip ~> python3 -m nltk.downloader all
[nltk_data] Downloading collection 'all'
[nltk_data]    | 
[nltk_data]    | Downloading package abc to /Users/sandip/nltk_data...
[nltk_data]    |   Package abc is already up-to-date!
[nltk_data]    | Downloading package alpino to
[nltk_data]    |     /Users/sandip/nltk_data...
[nltk_data]    |   Package alpino is already up-to-date!
[nltk_data]    | Downloading package biocreative_ppi to
[nltk_data]    |     /Users/sandip/nltk_data...
[nltk_data]    |   Package biocreative_ppi is already up-to-date!
[nltk_data]    | Downloading package brown to
[nltk_data]    |     /Users/sandip/nltk_data...
[nltk_data]    |   Package brown is already up-to-date!
[nltk_data]    | Downloading package brown_tei to
[nltk_data]    |     /Users/sandip/nltk_data...
[nltk_data]    |   Package brown_tei is already up-to-date!
[nltk_data]    | Downloading package cess_cat to
[nltk_data]    |     /Users/sandip/nltk_data...
[nltk_data]    |   Package cess_cat is already up-to-date!
[nltk_data]    | Downloading package cess_esp to
[nltk_data]    |     /Users/sandip/nltk_data...
[nltk_data]    |   Package cess_esp is already up-to-date!
[nltk_data]    | Downloading package chat80 to
[nltk_data]    |     /Users/sandip/nltk_data...
...
[nltk_data]    | Downloading package panlex_lite to
[nltk_data]    |     /Users/sandip/nltk_data...
[nltk_data]    |   Unzipping corpora/panlex_lite.zip.
Traceback (most recent call last):
  File ""/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py"", line 170, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 2267, in <module>
    halt_on_error=options.halt_on_error)
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 664, in download
    for msg in self.incr_download(info_or_id, download_dir, force):
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 543, in incr_download
    for msg in self.incr_download(info.children, download_dir, force):
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 529, in incr_download
    for msg in self._download_list(info_or_id, download_dir, force):
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 572, in _download_list
    for msg in self.incr_download(item, download_dir, force):
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 549, in incr_download
    for msg in self._download_package(info, download_dir, force):
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 638, in _download_package
    for msg in _unzip_iter(filepath, zipdir, verbose=False):
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 2039, in _unzip_iter
    outfile.write(contents)
OSError: [Errno 22] Invalid argument
```
"
19,https://github.com/nltk/nltk/issues/1283,1283,[],closed,2016-02-05 11:08:45+00:00,,12,panlex is offline (404),"```
>>> nltk.download('panlex_lite')
[nltk_data] Downloading package panlex_lite to
[nltk_data]     /Users/username/nltk_data...
[nltk_data] Error downloading u'panlex_lite' from
[nltk_data]     <https://raw.githubusercontent.com/nltk/nltk_data/gh-
[nltk_data]     pages/packages/corpora/panlex_lite.zip>:   HTTP Error
[nltk_data]     404: Not Found
False
```
"
20,https://github.com/nltk/nltk/issues/1285,1285,[],closed,2016-02-16 14:45:23+00:00,,21,Weird issue with bleu scores,"I discovered a weird issue when trying to calculate bleu scores for individual sentences with evaluation code [https://github.com/nltk/nltk/blob/develop/nltk/translate/bleu_score.py](here).
The method that contains the weird behaviour is : `sentence_bleu`
The problem occurs whenever the pn value becomes 1.

When for example calculating the bleu 1 score for the following candidate sentence:
_[this,is,an,example]_
When given two reference sentences:
_[this,is,something]
[this,is,an,example]_

`_modified_precision` correctly gives 1 as an answer, since the sentence is exactly the reference sentence. Then the logarithm gets taken which results in a zero value. This still seems correct. Next all the different p_n's gets summed which in this case is just one zero value. So the sum is still 0.

Then however the following code seems rather odd:
`if sum_s == 0:
        return 0`

This way the resulting bleu-score is 0. I don't understand why this needs to be here.
Without this if-clause, the zero-value correctly gets put in the rest of the formula and a correct 1 is returned.
"
21,https://github.com/nltk/nltk/issues/1288,1288,[],closed,2016-02-17 16:04:38+00:00,,6,BLEU test failing,"The CI server reports the following failure for the BLEU doctest.

```
File ""/scratch/jenkins/workspace/nltk/TOXENV/py34-jenkins/jdk/jdk8latestOnlineInstall/nltk/test/bleu.doctest"", line 9, in bleu.doctest
Failed example:
    bleu(
        ['The candidate has no alignment to any of the references'.split()],
        'John loves Mary'.split(),
        [1],
    )
Expected:
    0
Got:
    0.09697196786440505
```
"
22,https://github.com/nltk/nltk/issues/1289,1289,[],closed,2016-02-18 12:09:11+00:00,,1,"Module not found sentiment_analyzer (Python3.5.2, NLTK3.1)","This piece of code should, according to my best knowledge, work:

``` python
import nltk.sentiment
nltk.sentiment.util.demo_sent_subjectivity('This is great')
```

c:\users\user\appdata\local\programs\python\python35-32\lib\site-packages\nltk\sentiment\util.py in demo_sent_subjectivity(text)
    598         print('Cannot find the sentiment analyzer you want to load.')
    599         print('Training a new one using NaiveBayesClassifier.')
--> 600         sentim_analyzer = demo_subjectivity(NaiveBayesClassifier.train, True)
    601 
    602     # Tokenize and convert to lower case

c:\users\user\appdata\local\programs\python\python35-32\lib\site-packages\nltk\sentiment\util.py in demo_subjectivity(trainer, save_analyzer, n_instances, output)
    537     :param output: the output file where results have to be reported.
    538     """"""
--> 539     from sentiment_analyzer import SentimentAnalyzer
    540     from nltk.corpus import subjectivity
    541 

ImportError: No module named 'sentiment_analyzer'
"
23,https://github.com/nltk/nltk/issues/1291,1291,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2016-02-19 12:52:41+00:00,,1,NLTK relation extraction,"I am trying to extract relations from a text 
the example that is provided in the book uses ieer corpus 
I tried to make it on a single text but it's not working , I am not getting anything
Can anyone help ?

Here is my code :

import nltk
import re
sent = ""Barack Obama is the president of America""
tokens = nltk.word_tokenize(sent)
tagged = nltk.pos_tag(tokens)
grammar = r""""""
    NP : {<DT|PP\$>?<JJ>_<NN>}
         {<NNP>+}
         {<NN>+}
         }<VBD|IN>+{
  """"""
cp = nltk.RegexpParser(grammar)
result = cp.parse(tagged)
IN = re.compile(r'._\bin\b(?!\b.+ing)')
headline=[""test headline for sentence""]
for i,sent in enumerate(tagged):
    text = nltk.ne_chunk(sent)
    for rel in nltk.sem.relextract.extract_rels('ORG', 'LOC', sent, corpus='ace', pattern=IN):
        print(nltk.sem.rtuple(rel) )
"
24,https://github.com/nltk/nltk/issues/1293,1293,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}]",closed,2016-02-22 05:15:32+00:00,,4,Gale-Church aligner raises TypeError,"Running

```
nltk.translate.gale_church.align_blocks([10] * 20, [200, 400])
```

raises

```
TypeError: 'NoneType' object is not iterable
```

---

Example:

``` python
>>> from nltk.translate import gale_church
>>> gale_church.align_blocks([5,5,5], [7,7,7])
[(0, 0), (1, 1), (2, 2)]
>>> gale_church.align_blocks([10,] * 20, [5])
[(18, 0), (19, 0)]
>>> gale_church.align_blocks([10,] * 20, [5, 10])
[(16, 0), (17, 0), (18, 1), (19, 1)]
>>> gale_church.align_blocks([10] * 20, [5, 10])
[(16, 0), (17, 0), (18, 1), (19, 1)]
>>> gale_church.align_blocks([10] * 20, [200, 400])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""nltk/translate/gale_church.py"", line 168, in align_blocks
    return trace(backlinks, source_sents, target_sents)
  File ""nltk/translate/gale_church.py"", line 84, in trace
    s, t = backlinks[pos]
TypeError: 'NoneType' object is not iterable
```
"
25,https://github.com/nltk/nltk/issues/1294,1294,[],closed,2016-02-22 12:49:35+00:00,,38,Malt parser not parsing sentences,"whenever I parse a sentence using malt parser it gives me the following exception:
mp.parse_one(token)

Exception: MaltParser parsing (java -cp D:/Python Files/maltparser-1.8.1\maltparser-1.8.1.jar:D:/Python Files/maltparser-1.8.1\lib\liblinear-1.8.jar:D:/Python Files/maltparser-1.8.1\lib\libsvm.jar:D:/Python Files/maltparser-1.8.1\lib\log4j.jar org.maltparser.Malt -c engmalt.poly-1.7.mco -i C:\Users\MUSTUF~1\AppData\Local\Temp\malt_input.conll.9ck59rmy -o C:\Users\MUSTUF~1\AppData\Local\Temp\malt_output.conll.1j7w_xvw -m parse) failed with exit code 1
"
26,https://github.com/nltk/nltk/issues/1296,1296,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}]",closed,2016-02-23 10:19:29+00:00,,4,Fail to load vader_lexicon.txt,"I'd like to use `SentimentIntensityAnalyzer`, but got such a error:

```
[Errno 2] No such file or directory: '/Users/yobichi/mc2/lib/python3.5/site-packages/nltk/sentiment/vader_lexicon.txt'
```

How can I fix it? Or where can I download it manually?
"
27,https://github.com/nltk/nltk/issues/1297,1297,[],closed,2016-02-23 14:36:50+00:00,,1,panlex_lite 404 ,"I am still get error, refer below log.
Any update documents or something else? Thanks 
/usr/local/bin/python2.7 -m nltk.downloader -q all -d /usr/local/share/nltk_data
[nltk_data] Error downloading u'panlex_lite' from
[nltk_data] [nltk_data] pages/packages/corpora/panlex_lite.zip>: HTTP Error
[nltk_data] 404: Not Found
"
28,https://github.com/nltk/nltk/issues/1298,1298,[],closed,2016-02-23 23:34:55+00:00,,2,stanford parser setup error,"Hi,
i am using the following code to parse a sentence using stanford parser and for setting the variables

import os
java_path = ""C:/Program Files/Java/jdk1.8.0_60""
os.environ['JAVAHOME'] = java_path
os.environ['CLASSPATH'] ='C:\Users\ASUS\Downloads\stanford-parser-full-2015-12-09\stanford-parser-full-2015-12-09'
os.environ['STANFORD_MODELS'] = 'C:\Users\ASUS\Downloads\stanford-parser-full-2015-12-09\stanford-parser-full-2015-12-09'

from nltk.parse.stanford  import StanfordDependencyParser 

dep_parser = StanfordDependencyParser(model_path='edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz' )

sentences = dep_parser.raw_parse_sents([""""""

The product was old but it was good.
An excellent product packed very well with a very prompt delivery !
    """"""])
print (str (sentences))

for line in sentences:
    for sentence in line:
        sentence.draw()

But I am getting the following error

Traceback (most recent call last):
  File ""C:/Users/ASUS/PycharmProjects/untitled2/parse.py"", line 11, in <module>
    dep_parser = StanfordDependencyParser(model_path='edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz' )
  File ""C:\Python34\lib\site-packages\nltk\parse\stanford.py"", line 61, in **init**
    key=lambda model_name: re.match(self._MODEL_JAR_PATTERN, model_name)
TypeError: unorderable types: NoneType() > NoneType()

Please help me 

thanks
"
29,https://github.com/nltk/nltk/issues/1299,1299,[],closed,2016-02-24 04:43:02+00:00,,1,zipfile problem for Python 3.5,"The bulk of the CI errors concern zipfiles in Python 3.5:

https://nltk.ci.cloudbees.com/job/nltk/TOXENV=py35-jenkins,jdk=jdk8latestOnlineInstall/lastCompletedBuild/testReport/
"
30,https://github.com/nltk/nltk/issues/1304,1304,[],closed,2016-02-26 15:27:37+00:00,,4,Last stanford-pos-tagger (2015-12-09) jar problem,"Hello,

First I'm really new to nltk and what i learn is amazing, so really thank you for this great piece of software.

I'm trying to use stanford-pos-tagger, and the last version seems to be currently uncompatible with nltk.

The problem is related in this stackoverflow post:
http://stackoverflow.com/questions/34361725/nltk-stanfordnertagger-noclassdeffounderror-org-slf4j-loggerfactory-in-windo/34916721#34916721

This version needs 2 additional jars in the `CLASSPATH`: slf4j-api and slf4j-simple. The problem is that those jars are not loaded and it is impossible the load those jars without modifying nltk. Why ? because when launching the stanford jar, nltk resets the classpath to keep only one jar: the stanford one, it removes all other jars, even the jars specified in the classpath. Consequently there is no way to add slf4j-api and slf4j-simple to the classpath. 

Is there any particular reason to always reset the classpath? I suppose, classpath is cleared to avoid dependency problems, but since stanford jars used to have no dependencies at all would it be possible to disable this feature. Like proposed here: http://stackoverflow.com/a/34916721/3256942 ?

Have a good day !!!
"
31,https://github.com/nltk/nltk/issues/1305,1305,"[{'id': 718773983, 'node_id': 'MDU6TGFiZWw3MTg3NzM5ODM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/classifier', 'name': 'classifier', 'color': '60d2ff', 'default': False, 'description': None}]",open,2016-02-26 16:45:01+00:00,,0,confusing maxent formula.,"I dont understand this formula from  [ nltk.classify.maxent](http://www.nltk.org/_modules/nltk/classify/maxent.html)

> ```
>                     dotprod(weights, encode(fs,label))
>   prob(fs|label) = ---------------------------------------------------
>                  sum(dotprod(weights, encode(fs,l)) for l in labels)
> ```

Isin't maxent same as logistic regression classifier? So the correct formula for p(fs| label) should be:

> ```
>    p(fs|label) =         exp( dotprod(weights, encode(fs,label))
>                          --------------------------------------------------------------
>                    sum( exp( dotprod(weights, encode(f,label)) for f in all_fs)
> ```

There seems to be 2 mistakes in the original formula, is should have been p(label|fs) and the dot product should be exponentiated.

Am I missing something?
"
32,https://github.com/nltk/nltk/issues/1307,1307,[],closed,2016-02-26 20:57:50+00:00,,1,Unicode support in CCG.doctest fails when using Python 2.7,"The result looks like this:

```
Expected:                                                                                                                                                                               [6/797]
       el    ministro    anunció              pero              el    presidente   desmintió     la    nueva  ley
     (NP/N)     N      ((S\NP)/NP)  (((S/NP)\(S/NP))/(S/NP))  (NP/N)      N       ((S\NP)/NP)  (NP/N)  (N/N)   N
    --------Leaf
     (NP/N)
            ----------Leaf
                N
 ... something long...
    -------------------------------------------------------------------------------------------------------------->
                                                          S
Got nothing
```

It produces nothing. 

I have figured out that it is because of the word `panadería` (https://github.com/nltk/nltk/blob/develop/nltk/test/ccg.doctest#L306). When deleting that line, a parse is produced. But the test still fails because of some other issues (see #1306).

I'm guessing that `í` causes a failure in the lexicon.

Please note that the problem only occurs in Python 2.7.
"
33,https://github.com/nltk/nltk/issues/1308,1308,[],closed,2016-02-26 22:22:27+00:00,,0,BufferedGzipFile in Python3.5,"On the [CI tests for Python 3.5](https://nltk.ci.cloudbees.com/job/nltk/TOXENV=py35-jenkins,jdk=jdk8latestOnlineInstall/lastCompletedBuild/testReport/%28root%29/data_doctest/data_doctest/), `BufferedGzipFile` test is breaking at https://github.com/nltk/nltk/blob/develop/nltk/test/data.doctest#L327

This is because of the change in how buffer is read when code was modified from Python3.4 -> 3.5:
- **Python3.4**: https://github.com/python/cpython/blob/3.4/Lib/gzip.py#L189
- **Python3.5**: https://github.com/python/cpython/blob/3.5/Lib/gzip.py#L174

There's a new `GzipFile._buffer` variable in Python3.5 that clashes with `nltk.data.BufferedGzipFile._buffer`, so at https://github.com/nltk/nltk/blob/develop/nltk/data.py#L372 , 
- [When initializing a `nltk.data.BufferedGzipFile`](https://github.com/nltk/nltk/blob/develop/nltk/data.py#L370), it uses the `gzip.GzipFile.__init__` that [sets the `._buffer` variable to the file object](https://github.com/python/cpython/blob/3.5/Lib/gzip.py#L173)
- And [when `nltk.data.BufferedGzipFile` sets `self._buffer = BytesIO()`](https://github.com/nltk/nltk/blob/develop/nltk/data.py#L372), the super-class' buffer was overwritten and unloaded the file object within the `self._buffer` and it reads an empty file causing `test.read()` in the doctest to be None.

Since by default Python3.5 uses a buffer reader, it should have the same functionality as `nltk.data.BufferedGzipFile`. Assuming that the core Python team don't backport this functionality, we should consider deprecating this when we start dropping `python2.7` support.

Meanwhile I think the easiest fix is to refactor `BufferedGzipFile._buffer` to do something like `BufferedGzipFile._nltk_buffer`. And that should resolve Python3.4 -> 3.5 related issues for now =)
"
34,https://github.com/nltk/nltk/issues/1309,1309,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}]",closed,2016-02-26 22:35:28+00:00,,2,Example in SentiWordNet doctest is outdated. ,"The example in SentiWordNet doctest is outdated given the current version of expanded SentiWordNet in NLTK, see https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/sentiwordnet.py#L19 causing [failed tests on the CI server ](https://nltk.ci.cloudbees.com/job/nltk/TOXENV=py35-jenkins,jdk=jdk8latestOnlineInstall/lastCompletedBuild/testReport/nltk.corpus/reader/sentiwordnet/)

This can be fixed easily by using these output instead of the existing one:

```
>>> from nltk.corpus import sentiwordnet as swn
>>> list(swn.senti_synsets('slow'))
[SentiSynset('decelerate.v.01'), SentiSynset('slow.v.02'), SentiSynset('slow.v.03'), SentiSynset('slow.a.01'), SentiSynset('slow.a.02'), SentiSynset('dense.s.04'), SentiSynset('slow.a.04'), SentiSynset('boring.s.01'), SentiSynset('dull.s.08'), SentiSynset('slowly.r.01'), SentiSynset('behind.r.03')]
>>> happy = swn.senti_synsets('happy', 'a')
>>> happy0 = list(happy)[0]
>>> happy0.pos_score()
0.875
>>> happy0.neg_score()
0.0
>>> happy0.obj_score()
0.125
```

Any brave soul want to try to contribute this `good-first-bug`?
"
35,https://github.com/nltk/nltk/issues/1311,1311,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}]",closed,2016-02-27 00:56:43+00:00,,2,Stanford NER tagging results differ between JAVA GUI and NLTK,"Hi there,

For some reason the NER tagging results for my test paragraph differ between JAVA GUI and NLTK's wrapper, even though I used the same trained classifier (english.conll.4class.distsim.crf.ser.gz).  The test sentence I used was:

The studio is super great. **LOCATION LOCATION LOCATION**. 2 blocks from **Union Square**, 1 block from **Chinatown**, right in the middle of restaurants, cafeterias, bars, retail/designer stores. Accessible to bus, train, railcar, hop-on hop-off tour buses.   The place is excellent, the most comfortable bed I have ever stayed at any hotel, (I told **Heather** personally), clean, with all the amenities stated on the listing. Loved it.  **Heather** is always there to help get around town or any other questions you might have.   I would definitely recommend it to my friends and to you, the reader.

The Java GUI picked out all bold instances, while the NLTK wrapper picked out:
great.
LOCATION
LOCATION
LOCATION.
Union
Heather
personally),
listing.
Loved
Heather

Chinatown was not picked out, and there are a few words mistakenly picked out.  A stackoverflow question brought up [the same issue](http://stackoverflow.com/questions/34626555/result-difference-in-stanford-ner-tagger-nltk-python-vs-java).  What is the reason for the difference?  Is there a model parameter that I need to turn on?  

Thank you for any help!
"
36,https://github.com/nltk/nltk/issues/1313,1313,[],closed,2016-02-27 19:03:11+00:00,,1,Semantic predicate for CCG parser,"A little bit of the background: I was one of the @ewan-klein's student that extended the CCG parser in 2009. Sadly, the code was never merged. But I'd like to make the work into the codebase.

The extension included:
- Semantic predicate calculation
- Probability calculation
- Feature value -- right now we have `NP[sg]`. But `NP[num=sg]` or `NP[num=?x]` (variable-value) are better because it fits better with the word `the -> NP[num=?x]/NP[num=?x]`
- Packed chart technique 

We can talk about those in detail later and discuss whether or not to integrate a feature. For example, the packed chart technique is an optimisation that removes some similar parses, so it might not be useful for educational purpose.

I'd like to integrate the above features one-by-one in order to avoid driving reviewers crazy. And it's probably better for code quality maintenance as well.

So, first I'd like to start with semantic predicate calculation.

Here's how the lexicon should look like:

```
    :- S, NP, N
    TransVsg :: (S\NP[sg])/NP {sem=\x y.SYM(y,x)}

    She => NP[sg] {sem=she}
    has => TransVsg  {sym=have}
    a => NP[sg]/N[sg] {sem=\x.one(x)}
    book => N[sg] {sem=book}
```

and here's how the output should look like:

```
       She                         has                                   a                         book       
 NP[sg] {she}  ((S\NP[sg])/NP) {\x y.have(y,x)}  (NP[sg]/N[sg]) {\x.one(x)}  N[sg] {book} 
------------------------>T
(S/(S\NP[sg])) {\T.T(she)}
-------------------------------------------------------------------->B
                      (S/NP) {\x.have(she,x)}
                                                                    ------------------------------------------------------------------>
                                                                                          NP[sg] {one(book)}
-------------------------------------------------------------------------------------------------------------------------------------->
                                                       S {have(she,one(book))}
```

Here are questions I'd like to address:
- Should we integrate the semantic predicate calculation at all? Would it be useful?
- Does the formalisation of semantic predicate look good?
- Should the semantic predicate be optional (on the lexicon level)? I lean toward making it optional because it'll be annoying to force everyone to use the semantic predicate. On another hand, it makes the code more complicated.

Thank you!
"
37,https://github.com/nltk/nltk/issues/1314,1314,[],closed,2016-02-28 08:45:04+00:00,,0,resolve NLTK data issues,"https://github.com/nltk/nltk_data/milestones/3.2
"
38,https://github.com/nltk/nltk/issues/1315,1315,[],closed,2016-02-29 04:55:40+00:00,,5,Dispersion Plot has an incorrectly used Parameter,"`nltk.draw.dispersion.dispersion_plot(text, words, ignore_case=False, title='Lexical Dispersion Plot')[source]¶
`
'title' is seen to be a parameter in the source code and above(documentation), but we cannot modify this parameter.
"
39,https://github.com/nltk/nltk/issues/1317,1317,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 1355143093, 'node_id': 'MDU6TGFiZWwxMzU1MTQzMDkz', 'url': 'https://api.github.com/repos/nltk/nltk/labels/model', 'name': 'model', 'color': '2646af', 'default': False, 'description': ''}]",open,2016-03-02 03:24:40+00:00,,3,Training the default perceptron tagger with bigger corpus,"The default pickle for the `PerceptronTagger` is nice but it's a little too small for realistic usage:

``` python
>>> from nltk import PerceptronTagger
>>> len(PerceptronTagger(load=True).tagdict)
1549
```

Would it be possible to retrain a model on something like the full Penn TreeBank or BNC? Is the model then releasable on NLTK? 
"
40,https://github.com/nltk/nltk/issues/1320,1320,[],closed,2016-03-05 20:50:48+00:00,,3,No module nltk.sentiment,"I'm trying to follow on along with this example http://www.nltk.org/howto/sentiment.html, but there is no nltk.sentiment module. Do I have to install it separately? 
"
41,https://github.com/nltk/nltk/issues/1322,1322,[],closed,2016-03-08 09:49:36+00:00,,2,german stopwords,"the german stopword file contains wrong inflected forms of ""uns""

instead of

```
'unse', 'unsem', 'unsen', 'unser', 'unses'
```

in lines 190-194 of the stopword file `stopwords/german`, it should be 

```
'unsere', 'unserem', 'unseren', 'unser', 'unseres'
```
"
42,https://github.com/nltk/nltk/issues/1323,1323,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-03-08 18:21:43+00:00,,1,Error in BigramAssocMeasures _contingency?,"Hello, 
Before submitting a pull request I would like to know if this was done deliberately for some reason:

The contingency table in BigramAssocMeasures seems incorrect to me.
When I give a set of identical bigrams, i.e. [(X,X), (X,X), (X,X)] what should the table say?
I think:
n_ii = 3, n_io = 0, n_oi = 0, n_oo = 0.
Instead, we get 3, 1, 1, -1.

This causes the likelihood_ratio and chi_sq tests to fail.

The counts also seem wrong for [(X,X), (X,X), (X,Y)]

In this case we should get the following contingencies:
for (X,X):
2, 1, 0, 0
or
for (X,Y):
1, 2, 0, 0

Bug or feature?

The error[?] is due to word_fd.
"
43,https://github.com/nltk/nltk/issues/1324,1324,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}, {'id': 738078305, 'node_id': 'MDU6TGFiZWw3MzgwNzgzMDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python2.7', 'name': 'python2.7', 'color': '7be833', 'default': False, 'description': None}]",open,2016-03-09 00:50:03+00:00,,2,ParentedTree breaks deepcopy,"Can deepcopy Tree but not ParentedTree, not sure if may be related to this: https://github.com/nltk/nltk/issues/130

> > > from nltk.tree import Tree,ParentedTree
> > > t1 = Tree.fromstring(""(TOP (S (NP (NNP Bell,)) (NP (NP (DT a) (NN company)) (SBAR (WHNP (WDT which)) (S (VP (VBZ is) (VP (VBN based) (PP (IN in) (NP (NNP LA,)))))))) (VP (VBZ makes) (CC and) (VBZ distributes) (NP (NN computer))) (. products.)))"")
> > > t2 = copy.deepcopy(t1)
> > > t3 = ParentedTree.fromstring(""(TOP (S (NP (NNP Bell,)) (NP (NP (DT a) (NN company)) (SBAR (WHNP (WDT which)) (S (VP (VBZ is) (VP (VBN based) (PP (IN in) (NP (NNP LA,)))))))) (VP (VBZ makes) (CC and) (VBZ distributes) (NP (NN computer))) (. products.)))"")
> > > import copy
> > > t4 = copy.deepcopy(t3)
> > > Traceback (most recent call last):
> > >   File ""<input>"", line 1, in <module>
> > >   File ""/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/copy.py"", line 190, in deepcopy
> > >     y = _reconstruct(x, rv, 1, memo)
> > >   File ""/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/copy.py"", line 351, in _reconstruct
> > >     item = deepcopy(item, memo)
> > >   File ""/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/copy.py"", line 190, in deepcopy
> > >     y = _reconstruct(x, rv, 1, memo)
> > >   File ""/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/copy.py"", line 351, in _reconstruct
> > >     item = deepcopy(item, memo)
> > >   File ""/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/copy.py"", line 190, in deepcopy
> > >     y = _reconstruct(x, rv, 1, memo)
> > >   File ""/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/copy.py"", line 352, in _reconstruct
> > >     y.append(item)
> > >   File ""/usr/local/lib/python2.7/site-packages/nltk/tree.py"", line 1061, in append
> > >     self._setparent(child, len(self))
> > >   File ""/usr/local/lib/python2.7/site-packages/nltk/tree.py"", line 1221, in _setparent
> > >     raise ValueError('Can not insert a subtree that already '
> > > ValueError: Can not insert a subtree that already has a parent.
"
44,https://github.com/nltk/nltk/issues/1325,1325,[],closed,2016-03-09 17:52:21+00:00,,2,Number text representation,"Within the framework of the project I am currently working on I have to develop a library that will convert a number to its text representation in Russian and English.
For instance, given the number **1235** the library will output ""one thousand two hundred and thirty five"" for English and ""одна тысяча двести тридцать пять"" for Russian.
I can design this library in such a way that it is localizable for any language and language extension grammar files are shipped via nltk_data repository. In the future the reverse process, i.e. parsing of text to number, can also be included within this library which will be a handy tool for discovering numbers in natural language.

Do you think it makes sense to include such a library into NLTK?
"
45,https://github.com/nltk/nltk/issues/1326,1326,[],closed,2016-03-10 15:54:53+00:00,,2,Receiving GetURLError,"When calling function nltk.pos_tag, I get the following error in nltk-3.2:

```
Traceback (most recent call last):
  File ""nltk-tests.py"", line 22, in <module>
    tagged = nltk.pos_tag(tokens)
  File ""C:\Users\Vukan\AppData\Local\Programs\Python\Python35\lib\site-packages\nltk\tag\__init__.py"", line 110, in pos_tag
    tagger = PerceptronTagger()
  File ""C:\Users\Vukan\AppData\Local\Programs\Python\Python35\lib\site-packages\nltk\tag\perceptron.py"", line 141, in __init__
    self.load(AP_MODEL_LOC)
  File ""C:\Users\Vukan\AppData\Local\Programs\Python\Python35\lib\site-packages\nltk\tag\perceptron.py"", line 209, in load
    self.model.weights, self.tagdict, self.classes = load(loc)
  File ""C:\Users\Vukan\AppData\Local\Programs\Python\Python35\lib\site-packages\nltk\data.py"", line 801, in load
    opened_resource = _open(resource_url)
  File ""C:\Users\Vukan\AppData\Local\Programs\Python\Python35\lib\site-packages\nltk\data.py"", line 924, in _open
    return urlopen(resource_url)
  File ""C:\Users\Vukan\AppData\Local\Programs\Python\Python35\lib\urllib\request.py"", line 162, in urlopen
    return opener.open(url, data, timeout)
  File ""C:\Users\Vukan\AppData\Local\Programs\Python\Python35\lib\urllib\request.py"", line 465, in open
    response = self._open(req, data)
  File ""C:\Users\Vukan\AppData\Local\Programs\Python\Python35\lib\urllib\request.py"", line 488, in _open
    'unknown_open', req)
  File ""C:\Users\Vukan\AppData\Local\Programs\Python\Python35\lib\urllib\request.py"", line 443, in _call_chain
    result = func(*args)
  File ""C:\Users\Vukan\AppData\Local\Programs\Python\Python35\lib\urllib\request.py"", line 1310, in unknown_open
    raise URLError('unknown url type: %s' % type)
urllib.error.URLError: <urlopen error unknown url type: c>
```

The error does not happen in nltk-3.1. I am using Windows 7, python 3.5.1
"
46,https://github.com/nltk/nltk/issues/1327,1327,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-03-10 18:15:23+00:00,,0,Proposal for a CorpusReader for presidential debate texts,"Hi all,

I wrote a CorpusReader for reading presidential debates as formatted here: http://www.presidency.ucsb.edu/debates.php

My implementation is here: https://github.com/ELind77/Candidate_Classifier/blob/master/candidate_classifier/debate_corpus_reader.py

Tests are here: https://github.com/ELind77/Candidate_Classifier/blob/master/tests/test_debate_corpus_reader.py

It would need a bit of work to be up to the nltk coding standards, but if there is any interest in this please let me know and I will do my best to bring it up to the project coding standards and submit a pull request.

-- Eric
"
47,https://github.com/nltk/nltk/issues/1328,1328,[],closed,2016-03-12 06:14:37+00:00,,6,Regression in BLEU code,"It looks like #1319 has caused a new set of regressions.
@josiahwang, @alvations – any input would be appreciated, thanks.
"
48,https://github.com/nltk/nltk/issues/1330,1330,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}]",closed,2016-03-13 19:44:26+00:00,,9,BLEU Issues,"The BLEU implementation has sort of generated quite a lot of ""wack-a-mole"" situations where the fringe cases of using BLEU cause errors or unexpected output values from the following issues: #1328 #1288 #1285 #1268 #775 #789 #922 #926 #989 #1172 #1241

To resolve the issues, possibly our best chance is to emulate [`mteval-13a.perl`](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl) from @moses-smt. 

As a best practice, it should accompanied with a lot more test cases. 

**Any takers on writing/thinking of test cases?**

Contributors can also try to writing wrapper code or port code to NLTK from the following libraries:
- https://github.com/odashi/mteval
- https://github.com/moses-smt/mosesdecoder/blob/master/mert/sentence-bleu.cpp
"
49,https://github.com/nltk/nltk/issues/1334,1334,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}]",closed,2016-03-14 16:02:12+00:00,,2,'Text' object has no attribute 'generate',"On page 7 of the hard copy of the book ""Natural Language Processing with Python"", there is an example of the command:

`>>> text3.generate()`

Using nltk version 3.2 I get the following error:

`AttributeError: 'Text' object has no attribute 'generate'`

The online book [states that this feature will be reinstated in a subsequent version](http://www.nltk.org/book/ch01.html#searching-text).

Which version will this feature be in?

Thanks!
"
50,https://github.com/nltk/nltk/issues/1335,1335,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}]",closed,2016-03-14 23:11:29+00:00,,3,pkg_resources.DistributionNotFound: nltk-contrib==3.1,"`Traceback (most recent call last):
  File ""/usr/local/bin/nltk-server"", line 5, in <module>
    from pkg_resources import load_entry_point
  File ""/usr/lib/python3/dist-packages/pkg_resources.py"", line 2749, in <module>
    working_set = WorkingSet._build_master()
  File ""/usr/lib/python3/dist-packages/pkg_resources.py"", line 446, in _build_master
    return cls._build_from_requirements(__requires__)
  File ""/usr/lib/python3/dist-packages/pkg_resources.py"", line 459, in _build_from_requirements
    dists = ws.resolve(reqs, Environment())
  File ""/usr/lib/python3/dist-packages/pkg_resources.py"", line 628, in resolve
    raise DistributionNotFound(req)
pkg_resources.DistributionNotFound: nltk-contrib==3.1
`
While starting the nltk-server with command nltk-server -v 8881 , I get the above error. I have nltk3.1 installed on my ubuntu. Please suggest a solution
"
51,https://github.com/nltk/nltk/issues/1336,1336,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2016-03-16 16:39:15+00:00,,2,ZeroDivisionError when computing Krippendorff's alpha,"(1) Steps to reproduce:

``` python
    from nltk.metrics.agreement import AnnotationTask
    atask = AnnotationTask(data=[('c1', '1', 1),('c2', '1', 1)])
    atask.alpha()
```

(2) Error:

``` python
    Traceback (most recent call last):
      File ""<stdin>"", line 1, in <module>
      File ""/home/sidorenko/.local/lib/python2.7/site-packages/nltk/metrics/agreement.py"", line 316, in alpha
        ret = 1.0 - (self.Do_alpha() / De)
    ZeroDivisionError: float division by zero
```

(3) Expected return value:

```
    1.
```

(4) System information:

```
    nltk.__version__: '3.0.5'
    python --version: Python 2.7.6
    uname -srm: Linux 3.19.0-42-generic x86_64
```
"
52,https://github.com/nltk/nltk/issues/1337,1337,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-03-19 15:01:27+00:00,,0,log_probability bug and inconsistent documentation for HMM tagger,"The log_probability function doc in HMM tagger says:

> :param sequence: the sequence of symbols which must contain the TEXT
>             property, and optionally the TAG property
>         :type sequence:  Token

This is vague and also inconsistent with what a labeled and unlabeled sequence are defined in other functions of the same class (a list of tuples and a list of symbols respectively). In another function such as ""train"", unlabeled_sequence is defined as a list of lists, presumably a list of unlabeled sequences. I think it should be plural for clarity, as it correctly is in some other functions of the same class.
This issue in log_probability causes a bug (more often than not) if you pass an unlabeled sequence of symbols as the ""sequence"" parameter: 

`HMMTagger.log_probability(['a', 'test'])`

will result in:

```
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""...\nltk\tag\hmm.py"", line 247, in log_probability
    state = sequence[t][_TAG]
IndexError: string index out of range
```

because it attempts to look at position _TAG which is 1 which does not exist in an unlabeled sequence. Well, it would work if all the symbols had more than one character, but the result will be incorrect. 
"
53,https://github.com/nltk/nltk/issues/1338,1338,[],closed,2016-03-22 01:37:40+00:00,,32,panlex_lite installation via nltk.download() appears to fail,"Platform: Python 3.5 on Mac OS X 10.11.2
Steps to reproduce:
1. $ python3
2. >>> import nltk; nltk.download('all', halt_on_error=False)

Symptoms:
 # Partial console write:
[nltk_data]    | Downloading package panlex_lite to
[nltk_data]    |     /Users/beng/nltk_data...
[nltk_data]    |   Unzipping corpora/panlex_lite.zip.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 664, in download
    for msg in self.incr_download(info_or_id, download_dir, force):
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 543, in incr_download
    for msg in self.incr_download(info.children, download_dir, force):
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 529, in incr_download
    for msg in self._download_list(info_or_id, download_dir, force):
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 572, in _download_list
    for msg in self.incr_download(item, download_dir, force):
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 549, in incr_download
    for msg in self._download_package(info, download_dir, force):
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 638, in _download_package
    for msg in _unzip_iter(filepath, zipdir, verbose=False):
  File ""/usr/local/lib/python3.5/site-packages/nltk/downloader.py"", line 2039, in _unzip_iter
    outfile.write(contents)
OSError: [Errno 22] Invalid argument
"
54,https://github.com/nltk/nltk/issues/1339,1339,[],closed,2016-03-22 11:58:24+00:00,,1,dispersion_plot incorrectly imported in nltk/text.py,"The issue is around line 444
`from nltk.draw import dispersion_plot`
The function is actually defined in nltk/draw/dispersion.py
The above line causes a failure to import name error in a NLTK book chapter one example
The correct import should be:
`from nltk.draw.dispersion import dispersion_plot`
"
55,https://github.com/nltk/nltk/issues/1340,1340,[],closed,2016-03-23 17:00:09+00:00,,0,Problem with lazy corpus loading in CHILDES?,"I would expect the following to display the beginning of the list fairly quickly:

``` py
>>> from nltk.corpus.reader.childes import CHILDESCorpusReader
>>> childes = CHILDESCorpusReader('/Users/nschneid/nltk_data/corpora/childes/data-xml/Eng-USA', '.*.xml')
>>> childes.tagged_words()
```

But it hangs, suggesting that it's trying to load the entire corpus. Is there a more efficient way to implement this?
"
56,https://github.com/nltk/nltk/issues/1342,1342,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 81645781, 'node_id': 'MDU6TGFiZWw4MTY0NTc4MQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/language-model', 'name': 'language-model', 'color': 'd4c5f9', 'default': False, 'description': ''}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",closed,2016-03-25 07:53:30+00:00,,21,Discussion: Resurrecting the Ngram Model,"Hi folks!

I'm working on making sure the Ngram Model module could be added back into NLTK and would like to bring up a couple of issues for discussion.

**Issue 1**
[Here](https://github.com/nltk/nltk/issues/367#issuecomment-15716162) @afourney said it would be nice to add interpolation as an alternative to the default Katz backoff as a way of handling unseen ngrams. I've been thinking about this and I might have an idea how this could work. I'd like to run it by all interested parties.

The current class structure of the `model` module is as follows:
- `model.api.ModelI` -> this is supposed to be an Abstract class or an Interface, I guess.
- `model.ngram.NgramModel` -> extends above class, contains current implementation of the ngram model.

Here's what I propose:
- `model.api.Model` -> I'm honestly not sure I see the point of this, ambivalent on whether to keep it or ditch it.
- `model.ngram.BasicNgramModel` -> This is the same as `NgramModel`, **minus** everything that has to do with backoff. Basically, it can't handle ngrams unseen in training. ""Why have this?"" - you might ask. I think this would be a great demo of the need for backoff/interpolation. Students can simply try this out and see how badly it performs to convince themselves to use the other classes.
- `model.ngram.BackoffNgramModel` -> Inherits from `BasicNgramModel` to yield the current implementation of `NgramModel`, except that it's more explicit about the backoff part.
- `model.ngram.InterpolatedNgramModel` -> Also inherits from `BasicNgramModel`, but uses interpolation instead of backoff.

The long-term goals here are:

a) to allow any `ProbDist` class to be used as a probability estimator since interpolation/backoff are (mostly) agnostic of the smoothing algorithm being used.
b) to allow anyone who wants to _optimize_ an NgramModel for their own purposes to be able to easily inherit some useful defaults from the classes in NLTK.

**Issue 2**
Unfortunately the probability module has it's own problems (eg. #602 and (my) Kneser-Ney implementation is wonky). So for now I'm only testing correctness with `LidstoneProbDist`, since it is easy to compute by hand. Should I be worried about the lack of support for the more advanced smoothing methods? Or do we want to maybe proceed this way to ensure at least that Ngram Model works, and _then_ tackle the problematic probability classes separately?

**Python 3 `super()`**
When calling `super()`, do I need to worry about supporting python 2? See [this](http://stackoverflow.com/a/10483204/4501212) for context.
"
57,https://github.com/nltk/nltk/issues/1344,1344,[],closed,2016-03-25 22:58:47+00:00,,2,Return Type of WordNetLemmatizer,"```
>>>words = 'I love dogs and cats and all animals'.split() # type is ascii
>>>wnl = WordNetLemmatizer()
>>>lemmas = [wnl.lemmatize(w) for w in words]
>>>lemmas
['I', 'love', u'dog', 'and', u'cat', 'and', 'all', 'other', u'animal'] 
```

I don't think we should have mixed types for a return value. This is a minor error that could be fixed. I've had some confusion on a number of occasions with NLTK due to 'str' vs 'unicode'. I think that it might be best if all NLTK modules return utf-8 for type safety/to avoid headaches.

What does everyone think?
"
58,https://github.com/nltk/nltk/issues/1345,1345,[],closed,2016-03-27 03:14:09+00:00,,0,Problem with parsing Chinese models output from StanfordPOSTagger,"The parser for the StanfordPOSTagger relies on a Separator to receive the output.

This is usually an underscore ('_'), but the output for the Chinese models for some reason uses '#'. This can be fixed by resetting the _SEPARATOR attribute before using the tagger.

```
> english_postagger.tag('What is the airspeed of an unladen swallow ?'.split())

[('What', 'WP'),
 ('is', 'VBZ'),
 ('the', 'DT'),
 ('airspeed', 'NN'),
 ('of', 'IN'),
 ('an', 'DT'),
 ('unladen', 'JJ'),
 ('swallow', 'VB'),
 ('?', '.')]

> chinese_postagger.tag('骑 自行车 那 个 人 是 谁 ？'.split())

[('', '骑#VV'),
 ('', '自行车#NN'),
 ('', '那#DT'),
 ('', '个#M'),
 ('', '人#NN'),
 ('', '是#VC'),
 ('', '谁#PN'),
 ('', '？#PU')]

> print ('Default separator:', chinese_postagger._SEPARATOR)

Default separator: _

> chinese_postagger._SEPARATOR = '#'
> chinese_postagger.tag('骑 自行车 那 个 人 是 谁 ？'.split())

[('骑', 'VV'),
 ('自行车', 'NN'),
 ('那', 'DT'),
 ('个', 'M'),
 ('人', 'NN'),
 ('是', 'VC'),
 ('谁', 'PN'),
 ('？', 'PU')]
```

Setup like this:

```
from nltk.tag import StanfordPOSTagger as POSTagger
from nltk.internals import find_jars_within_path

def fix_path(path):
    """"""Fix the jar-path for the tagger._stanford_jar.

    See: https://github.com/nltk/nltk/issues/1304
    """"""
    stanford_dir = path.rpartition('/')[0]
    stanford_jars = find_jars_within_path(stanford_dir)
    string_jars = ':'.join(stanford_jars)
    return string_jars

tagger_dir = HOME_DIR + 'nlp/stanford-postagger-full-2015-12-09/'
tagger_jar     = tagger_dir + 'stanford-postagger.jar'
english_tagger = tagger_dir + 'models/english-bidirectional-distsim.tagger'
# Same result for both Chinese models:
chinese_tagger = tagger_dir + 'models/chinese-distsim.tagger'
#chinese_tagger = tagger_dir + 'models/chinese-nodistsim.tagger'

english_postagger = POSTagger(english_tagger, tagger_jar)
chinese_postagger = POSTagger(chinese_tagger, tagger_jar)

english_postagger._stanford_jar = fix_path(english_postagger._stanford_jar)
chinese_postagger._stanford_jar = fix_path(chinese_postagger._stanford_jar)
```
"
59,https://github.com/nltk/nltk/issues/1349,1349,[],closed,2016-03-29 13:56:58+00:00,,0,Stanford Word Segmenter,"Issue #101 Concerned the lack of a Word Segmenter for Chinese Text. This issue relates specifically to the Stanford implementation.

There is some old code (2014) floating around for interfacing with the Stanford Word Segmenter, and it seems that quite a few people have implemented similar solutions over the years, but for some reason it has not yet been included in NLTK.

http://textminingonline.com/dive-into-nltk-part-vi-add-stanford-word-segmenter-interface-for-python-nltk

I see @alvations has already asked permission (in the comments) to include it in NLTK.

There are, however, a few problems, which I suppose result from the updated stanford-segmenter-3.6.0, and the use of abstract base class syntax in tokenize/api.py: https://github.com/nltk/nltk/commit/b40ebee0e1238f44984fb13dd1de6b9f77740b73.
"
60,https://github.com/nltk/nltk/issues/1356,1356,"[{'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-04-03 17:17:04+00:00,,5,A probabilistic parsing for CCG,"I'm thinking about implementing a probabilistic parsing for CCG because it might be wanted. It is mentioned in https://github.com/nltk/nltk/wiki/Semantic-Parsing, https://groups.google.com/forum/#!topic/nltk-users/m5rdxSFExF4, and http://stackoverflow.com/questions/26829904/lambda-calculus-representation-in-nltk-ccg
### The API

With semantics:

```
lex = lexicon.fromstring('''
   :- S, NP, N
   She => NP {she} [1.0]
   has => (S\\NP)/NP {\\x y.have(y, x)} [0.5]
   a => NP/N {\\P.exists z.P(z)} [0.6]
   book => N {book}
   ''',
   True)
```

Without semantics:

```
lex = lexicon.fromstring('''
   :- S, NP, N
   She => NP [1.0]
   has => (S\\NP)/NP [1.0]
   a => NP/N
   book => N
   ''',
   False)
```

If `[NUM]` is missing, it means the probability is `1.0`.
### The implementation

A leaf node has been `Token` since #1321 (https://github.com/nltk/nltk/blob/develop/nltk/ccg/lexicon.py#L48). We can simply add a probability field. The probability computation will happen while retrieving a parse. Please note that semantics computation also happens at that stage. Here it is: https://github.com/nltk/nltk/blob/develop/nltk/ccg/chart.py#L261
### Probability computation

I'm a bit out of date in this area, but I think the logarithm of probabilities might make more sense. Because a product of probabilities for a long sentence might become too low to compute. (This probability computation should be refactored in a way that it can be swapped easily)

The results will be sorted based on parses' probabilities.
### Important notes
- There's no probability on the rules. Therefore, if the category assignment (on words) is the same, the probabilities will be equal. This might not make sense. For example, `the weird beautiful woman` might have multiple parses with identical category assignment, but those parses **might** offer different semantics interpretations. Nevertheless, I can add this later. I'll ask Mark Steedman about it.
- There's no such thing as non-probabilistic parsing anymore. That simply means every token's probability is `1.0`.
- This is a part of my master thesis (mentioned here #1313), which is a bit out of date. The end goal is to make the CCG parser work with CCGBank out of the box. Ideally, a user can choose a method to train models from CCGBank. Then, the user can use the parser to parse (probabilistically) an input sentence.

@stevenbird @ewan-klein Would this go along with NLTK's vision/mission? Would it be something we want? Does the API part look ok?
"
61,https://github.com/nltk/nltk/issues/1357,1357,[],closed,2016-04-10 01:41:30+00:00,,1,ChangeLog date errors,"Your ChangeLog (https://github.com/nltk/nltk/blob/develop/ChangeLog) appears to have the wrong release dates:

Currently: Version 3.2 2015-03-03
Should be: Version 3.2 2016-03-03

Currently: Version 3.2.1 2015-04-09
Should be: Version 3.2.1 2016-04-09
"
62,https://github.com/nltk/nltk/issues/1361,1361,[],closed,2016-04-13 11:26:59+00:00,,5,SyntaxError,"On Travis CI the following error occurs for `Python 3.2.5`
Just by calling `from nltk import download`
Actually it even happens with just `import nltk`

```
Traceback (most recent call last):
  File ""/opt/python/3.2.5/lib/python3.2/runpy.py"", line 161, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/opt/python/3.2.5/lib/python3.2/runpy.py"", line 74, in _run_code
    exec(code, run_globals)
  File ""/home/travis/virtualenv/python3.2.5/lib/python3.2/site-packages/nalaf/download_corpora.py"", line 10, in <module>
    from nltk import download
  File ""/home/travis/virtualenv/python3.2.5/lib/python3.2/site-packages/nltk/__init__.py"", line 177, in <module>
    from nltk import ccg, chunk, classify, collocations
  File ""/home/travis/virtualenv/python3.2.5/lib/python3.2/site-packages/nltk/ccg/__init__.py"", line 21, in <module>
    from nltk.ccg.chart import CCGEdge, CCGLeafEdge, CCGChartParser, CCGChart
  File ""/home/travis/virtualenv/python3.2.5/lib/python3.2/site-packages/nltk/ccg/chart.py"", line 338
    if op == u'Leaf':
                   ^
SyntaxError: invalid syntax
```
"
63,https://github.com/nltk/nltk/issues/1367,1367,[],closed,2016-04-18 22:52:57+00:00,,5,russian stopwords,"A list of russian stopwords need to be expanded by following words:

badwords = [
    u'я', u'а', u'да', u'но', u'тебе', u'мне', u'ты', u'и', u'у', u'на', u'ща', u'ага', 
    u'так', u'там', u'какие', u'который', u'какая', u'туда', u'давай', u'короче', u'кажется', u'вообще',
    u'ну', u'не', u'чет', u'неа', u'свои', u'наше', u'хотя', u'такое', u'например', u'кароч', u'как-то',
    u'нам', u'хм', u'всем', u'нет', u'да', u'оно', u'своем', u'про', u'вы', u'м', u'тд',
    u'вся', u'кто-то', u'что-то', u'вам', u'это', u'эта', u'эти', u'этот', u'прям', u'либо', u'как', u'мы',
    u'просто', u'блин', u'очень', u'самые', u'твоем', u'ваша', u'кстати', u'вроде', u'типа', u'пока', u'ок'

]
"
64,https://github.com/nltk/nltk/issues/1369,1369,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}]",closed,2016-04-21 03:23:18+00:00,,0,FrameNet corpus reader: load annotated exemplar sentences and full-text documents,"Proposed interface:
## General
- `fn.sents(exemplars=True, full_text=True)`
- `fn.annotations([luNameRegex], exemplars=True, full_text=True)`
- `sent.annotationSet[0]` for POS tagging, `sent.annotationSet[>0]` for frame annotations
## Lexicographic exemplars (one annotationSet per sentence; not organized into documents)
- `fn.exemplars([luNameRegex])`
- `lu.exemplars`
- `lu.subCorpus`
## Full-text annotations
- `fn.docs([docNameRegex])`
- `fn.ft_sents([docNameRegex])`
- `doc.sentence`
- `fn.documents()`, which is an index of the documents, is renamed to `fn.docs_metadata()`
"
65,https://github.com/nltk/nltk/issues/1370,1370,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}]",closed,2016-04-21 04:00:08+00:00,,4,FrameNet corpus reader: display sentence with inline target & FE annotations,"Given that lexicographic and full-text annotations are available (#1369), we want a user-friendly way to display them in the console for interactive browsing.

The proposed solution has the following displays:
## Full-text sentence display

Sentences in full-text documents (potentially) contain multiple annotated targets. This display notes which spans are targets and indicates the corresponding annotationSet indices:

```
>>> s
full-text sentence (4097611) in NorthKorea_NuclearCapabilities:


[POS] 14 tags

[POS_tagset] PENN

[text] + [annotationSet]

Therefore , obtaining reliable open source information on such
            ********* ******** ****        ***********
            Getting   Trust    Secr        Information
            [1]       [6]      [5]         [2]

programs is very challenging .
********         ***********
Project          Difficulty
[3] !            [4]
```
- Frame names are truncated to fit the character width of the span.
- Many full-text targets are annotated for the frame only (no FEs; status is `UNANN`). These are indicated with a `!` next to the index.

```
>>> fn.ft_sents()[230]
full-text sentence (4164978) in Fires_1:


[POS] 23 tags

[POS_tagset] PENN

[text] + [annotationSet]

Four people have perished so far in the blazes which the
                 ******** ******        ******
                 Death    Contin        Fire_b
                 [1]      [6]           [2]

government says were started intentionally by arsonists over a
********** ****      *******                  *********
Organizati Stat      Setting                  Arson
[3]        [4]       [7] ?                    [5]

week ago .
```
- Targets annotated with Problem LUs are designated with `?`.
## Frame annotation sentence display

Drilling down in the full-text sentence—or at the top level of a lexicographic exemplar sentence, which only has one target in the first place—we see the full sentence, now with one target and any FEs indicated:

```
>>> s.annotationSet[4]
annotation set (6528787):

[status] MANUAL

[LU] (6573) challenging.a in Difficulty

[frame] (375) Difficulty

[GF] 2 relations

[PT] 2 phrases

[text] + [Target] + [FE] + [Adj]

Therefore , obtaining reliable open source information on such
            ---------------------------------------------------
            Activity

programs is very challenging .
-------- ^^ ---- ***********
         co Degr
 (co=cop, Degr=Degree)
```
- FE names are truncated to fit, with the expansion shown after the sentence in parentheses. The frame name is not shown inline.
- LUs of certain parts-of-speech may be annotated with special FE-extrinsic spans such as `Cop` (copula), `Supp` (support), `Ctrlr` (controller), `Gov` (governor), and `Asp` (aspectual particle). See notes in updated code. For the display, these are lowercased and shown beneath a span indicated with `^` characters to distinguish them from FEs. In the above case, the `Adj` layer of challenging.a contains a `Cop` span, displayed in truncated form as `co`.
  - `X` phrase spans (which generally include the target and its FEs) will not be displayed.
- The underlying information in the display comes from a combination of the `text`, `Target`, `FE`, and `Adj` attributes of the sentence object, and is therefore preceded with `[text] + [Target] + [FE] + [Adj]`.
### Denoted FEs

```
>>> s.annotationSet[2]
annotation set (6528785):

[status] MANUAL

[LU] (12537) information.n in Information

[frame] (1591) Information

[GF] 2 relations

[PT] 2 phrases

[text] + [Target] + [FE]

Therefore , obtaining reliable open source information on such
                               ----------- =========== --------
                               Means_of_ga Information Topic

programs is very challenging .
--------

 (Means_of_ga=Means_of_gathering)
```
- If an FE span includes the target, the overlapping portion is indicated with `=` underlining rather than `*`.
  - There are rare cases in which the FE span and the target span are not identical; these are handled but not shown here. 
### Null instantiations

```
>>> s.annotationSet[1]
annotation set (6528784):

[status] MANUAL

[LU] (7499) obtain.v in Getting

[frame] (179) Getting

[GF] 1 relation

[PT] 1 phrase

[text] + [Target] + [FE]

Therefore , obtaining reliable open source information on such
            ********* -----------------------------------------
                      Theme

programs is very challenging .
--------
         [Recipient:CNI]
```
- NIs are displayed in brackets after the sentence.
### Multiple FE layers

If present, second-layer and even third-layer FE annotations are stored in `.FE2` and `.FE3` attributes, respectively. All FE layers are displayed:

```
>>> fn.ft_sents()[13].annotationSet[6].FE
([(115, 127, 'Place'), (137, 153, 'Product')], {})
>>> fn.ft_sents()[13].annotationSet[6].FE2
([(115, 127, 'Producer')], {})
>>> fn.ft_sents()[13].annotationSet[6]
annotation set (6528894):

[status] MANUAL

[LU] (7523) produce.v in Manufacturing

[frame] (408) Manufacturing

[GF] 2 relations

[PT] 2 phrases

[text] + [Target] + [FE] + [FE2]

However , all countries known to have successfully acquired





nuclear weapons have done so primarily on the basis of





indigenously produced fissile material .
------------ ******** ----------------
Place                 Product
------------
Producer


```
### `UNANN` annotationSets

```
>>> s.annotationSet[3]
annotation set (6528786):

[status] UNANN

[LU] (10724) program.n in Project

[frame] (1158) Project

[GF] 0 relations

[PT] 0 phrases

[text] + [Target] + [FE]

Therefore , obtaining reliable open source information on such



programs is very challenging .
********
```
- Frame annotations with status `UNANN` have a boring display, as they have a target but no FEs.
### Other, Sent layers
- The **Other** layer marks expletive subject spans (`Null`, `Exist`) and relative clause antecedent/relativizer spans (`Ant`, `Rel`). This information can be accessed but is not displayed.
- The **Sent** layer appears to be for flags that help the annotator categorize lexicographic exemplars (e.g., `sense1`, `Metaphor`, `Idiom`, `reexamine`). This information can be accessed but is not displayed.
## POS annotation display

This is always available in `.annotationSet[0]`:

```
>>> s.annotationSet[0]
POS annotation set (6528581) PENN in sentence 4097611:

Therefore , obtaining reliable open source information on such
--------- - --------- -------- ---- ------ ----------- -- ----
rb        , VVG       jj       jj   nn     nn          in jj

programs is  very challenging .
-------- --  ---- ----------- -
nns      VBZ rb   jj          sent
```
"
66,https://github.com/nltk/nltk/issues/1371,1371,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-04-21 04:02:08+00:00,,1,"FrameNet corpus reader: PT, GF display","In addition to #1370, there ought to be a user-friendly display of PT (phrase type) and GF (grammatical function) information on each FE (perhaps with PT.GF notation, e.g. `PP.Dep`). Two obstacles at present:
1. Always showing this in addition to FE names risks cluttering the annotationSet display. Perhaps make the display configurable with a global option? E.g. `fn.set_opt(show_pt_gf=True)` (and the `set_opt()` function would allow for other global options)?
2. How to handle really short FEs? Right now the FE name is truncated, but this could become unwieldy for syntactic info. Have a non-inline way to display it? Set a minimum width of the displayed FE?
"
67,https://github.com/nltk/nltk/issues/1372,1372,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-04-21 04:23:18+00:00,,0,FrameNet corpus reader: deduplicate sentences?,"Esp. when iterating over both exemplars and full-text (as full-text sentences may show up in exemplars)? Do sentences have consistent IDs to make this possible, and is it always clear which version should be provided?
"
68,https://github.com/nltk/nltk/issues/1373,1373,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-04-21 04:25:29+00:00,,1,FrameNet corpus reader: specify experimental splits?,"There are standard splits of FN 1.5 into train/dev/test (e.g., as used in [Das et al. 2014](http://dx.doi.org/10.1162/COLI_a_00163)). Should we provide a simple interface for accessing sentences according to these splits? What to do with data that is changed or added in FN 1.6+?
"
69,https://github.com/nltk/nltk/issues/1374,1374,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-04-21 04:36:10+00:00,,0,FrameNet corpus reader: load valence information?,"As displayed in the tables in of Lexical Entry reports, e.g.: https://framenet2.icsi.berkeley.edu/fnReports/data/lu/lu17486.xml?mode=lexentrytop&banner=
"
70,https://github.com/nltk/nltk/issues/1375,1375,[],closed,2016-04-21 09:40:32+00:00,,2,a bug when using StanfordSegmenter in windows,"nltk version:3.2.1 jdk 1.8.0_77
when using StanfordSegmenter in windows with all arguments set properly,i got an java error message:Could not find or load main class edu.stanford.nlp.ie.crf.CRFClassifier
i manage to figure out the cause:
line 63 in nltk\tokenize\stanford_segmenter.py  
self._stanford_jar = ':'.join(
            [_ for _ in [stanford_segmenter, slf4j] if not _ is None])
should be
self._stanford_jar = ';'.join(
            [_ for _ in [stanford_segmenter, slf4j] if not _ is None])
this is because in linux java -cp option use : to separate while in windos java -cp option use ;
"
71,https://github.com/nltk/nltk/issues/1376,1376,[],closed,2016-04-21 10:16:33+00:00,,0,Chunk grammar usage,"http://stackoverflow.com/questions/36765146/nltk-chunk-grammar-doesnt-read-commas

The topic i created explain the issue can someone help me out ?
"
72,https://github.com/nltk/nltk/issues/1377,1377,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-04-21 16:44:12+00:00,,0,FrameNet corpus reader: smarter memory management?,"With loading annotated sentences (#1369), the potential memory footprint of FrameNet data gets very large. For some use cases there is no need to keep everything in memory—e.g., a script might want to process one sentence at a time, but currently all the sentences are kept in memory.

Should this be avoided with a cache size limit? Using [weakref](https://docs.python.org/2/library/weakref.html)?
"
73,https://github.com/nltk/nltk/issues/1378,1378,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}]",closed,2016-04-21 17:42:55+00:00,,1,FrameNet corpus reader: configurable handling of data integrity warnings,"Add a `_warn()` method to be called if a suspicious piece of data is encountered, e.g., an annotationSet with no target, or a full-text annotationSet whose LU is omitted from the lexicon (it has Problem status; see #1370). By default, these warnings are silent, but they can be enabled to print to a file (by default: stderr):

``` py
>>> from nltk.corpus import framenet as fn
>>> fn.warnings(True)
# warnings are now enabled
```
"
74,https://github.com/nltk/nltk/issues/1379,1379,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-04-21 18:21:11+00:00,,0,FrameNet corpus reader: faster way to access full-text annotations by LU?,"The current implementation of `fn.annotations()`, which can be used to search both exemplars and full-text annotationSets, allows for filtering by LU. However, the way it does this for full-text is simply to iterate over all annotationSets and only return the ones that match the LU.
- Is there a full-text LU index that would make this more efficient?
- This design ignores annotationSets with Problem LUs, because such LUs are not retrievable by name (see #1370). Is this desirable?
"
75,https://github.com/nltk/nltk/issues/1380,1380,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}]",closed,2016-04-21 19:34:45+00:00,,2,LazyIteratorList,"In working on enhancements to the FrameNet corpus reader, I found it would be convenient to define a lazy sequence type that is instantiated with an iterator (such as a generator expression). I came up with the following:

``` py
class LazyIteratorList(AbstractLazySequence):
    """"""
    Wraps an iterator, loading its elements on demand 
    and making them subscriptable.
    __repr__ displays only the first few elements.
    """"""
    def __init__(self, it, cache_limit=None, known_len=None):
        self._it = it
        self._len = known_len
        self._cache = []
        self._cache_limit = cache_limit
        self._i = 0 # Number of items consumed so far

    def __len__(self):
        if self._len:
            return self._len
        for x in self.iterate_from(len(self._cache)):
            pass
        return len(self._cache)

    def iterate_from(self, start):
        while self._i<start:
            v = next(self._it)
            if self._cache_limit is None or len(self._cache)+1<self._cache_limit:
                self._cache.append(v)
            self._i += 1
        i = start
        while i<len(self._cache):
            yield self._cache[i]
            i += 1
        while True:
            v = next(self._it)
            if self._cache_limit is None or len(self._cache)+1<self._cache_limit:
                self._cache.append(v)
            yield v

    def __add__(self, other):
        """"""Return a list concatenating self with other.""""""
        return type(self)(itertools.chain(self, other))

    def __radd__(self, other):
        """"""Return a list concatenating other with self.""""""
        return type(self)(itertools.chain(other, self))
```

which enables things like:

``` py
LazyIteratorList(aset for sent in self.ft_sents() for aset in sent.annotationSet[1:] if luNamePattern is None)
```

Before I check in the changes to the corpus reader, should this class go in https://github.com/nltk/nltk/blob/develop/nltk/util.py? The alternative of using `LazyMap` seems far less intuitive, especially with nested loops and conditions.
"
76,https://github.com/nltk/nltk/issues/1381,1381,[],closed,2016-04-23 12:23:56+00:00,,4,"""attempt"" classified as past verb","The word ""attempt"" is classified by pos_tag as VBD.
According to http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html this is ""Verb, past tense"".
But ""attempt"" is present tense (VB) and attempted is the past tense (VBD). 

```
import nltk
sentence = """"""We first attempt to tackle the will; how exactly we are going to see. Shall we see then?""""""
tokens = nltk.word_tokenize(sentence)
tagged = nltk.pos_tag(tokens)
print tagged
```

> [('We', 'PRP'),
>  ('first', 'RB'),
>  ('attempt', 'VBD'),
>  ('to', 'TO'),
>  ('tackle', 'VB'),
>  ('the', 'DT'),
>  ('will', 'MD'),
>  (';', ':'),
>  ('how', 'WRB'),
>  ('exactly', 'RB'),
>  ('we', 'PRP'),
>  ('are', 'VBP'),
>  ('going', 'VBG'),
>  ('to', 'TO'),
>  ('see', 'VB'),
>  ('.', '.'),
>  ('Shall', 'NNP'),
>  ('we', 'PRP'),
>  ('see', 'VBP'),
>  ('then', 'RB'),
>  ('?', '.')]

Not sure what other words fall in this error class.

It would be nice if ""going to"", ""will"", ""shall"" could be used to indicate future tense in a sentence.
"
77,https://github.com/nltk/nltk/issues/1384,1384,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",open,2016-04-29 20:41:46+00:00,,6,"Japanese Corpus readers do not return properly formatted (word, tag) tuples","There are still problems with how POS Tagging works for these corpora. This afternoon, I loaded up JEITA, and called jeita.tagged_words(). The problem is that the second half of each tuple in JEITA doesn't contain a tag that is easy to test against a POS-tagger. The second half of each tuple contains both orthographic information (each word in the corpus has a spelling for each syllabary in Japanese) and the tag information, so a word tagged as a noun won't have the same tag as another word tagged as a noun. This leads to quite a few problems when testing a tagger against the corpus. 

Open up one of the .chasen files and you'll see what I mean. Here's line 6 of a0010.chasen in the jeita.zip file. 

出る  デル  出る  動詞-自立   一段  基本形

There's four ( or maybe five) elements here. The first three are ways of writing the word /deru/, and the last is the tag (verb, transitive, group 1, plain form.) 

So I wrote this loop: 

for sent in tagged_sents:
    for(word, tag) in sent:
        print(word)
        print(tag)

And here's some sample output: 

出る
デル  出る  動詞-自立   一段  基本形

As you can see, the tag includes two forms of orthography, which throws things off.

(Also, as a side note, it would be really great if we could have a ""simple"" pos tag version of these files, which didn't include some of the additional categories like ""plain form"" or which group (ichidan/godan) the verb belonged too, since I don't think a lot of parsers care too much about which is which, but doing this would probably take help from a Japanese fluent individual.) 

I can check again with KNBC, the other Japanese corpus included in NLTK, but it does even funkier things with tags last I checked.
"
78,https://github.com/nltk/nltk/issues/1385,1385,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-05-02 17:06:21+00:00,,2,CHILDESCorpusReader: should words() behave like sents() under some circumstances?,"Currently, because of [this line](https://github.com/nltk/nltk/blob/609079da96be0c992e7cb64187dc3ef17a8ff368/nltk/corpus/reader/childes.py#L402), if the user calls `.words(relation=True)`, then the result will be a list of _sentences_ with dependency information shown on each token.

``` py
>>> ch.words(speaker='MOT',relation=False)
['big', 'drum', 'horse', 'who', 'is', 'that', 'those', ...]
>>> ch.words(speaker='MOT',relation=True)
[[('big', 'adj', '1|2|MOD'), ('drum', 'n', '2|0|INCROOT')], [('horse', 'n', '1|0|INCROOT')], ...]
>>> ch.sents(speaker='MOT',relation=True)
[[('big', 'adj', '1|2|MOD'), ('drum', 'n', '2|0|INCROOT')], [('horse', 'n', '1|0|INCROOT')], ...]
```

Is this the desired behavior? On the one hand, the dependency indices are per-sentence, so it would be difficult to reconstruct parses from a call to `.words()` unless the results retained sentence boundaries. However, this means that the user probably should have called `.sents()`.

Possible solutions:
1. Have `.words()` and `.tagged_words()` provide the same data structures regardless of whether the relation is shown on each token. This would facilitate, e.g., listing all words that appear as an utterance root.
2. Disallow `relation` option in `.words()` and `.tagged_words()` (it is assumed to be `False`).
3. Disallow `relation` options entirely, and instead provide `.parsed_sents()` which returns tree data structures.
"
79,https://github.com/nltk/nltk/issues/1387,1387,[],closed,2016-05-03 02:22:58+00:00,,7,GhostScript error when displaying inline Tree structure for Jupyter notebook,"I just reinstalled my system including Jupyter notebook and nltk, and found out that the inline display of tree as png file which used to work like a charm is now broken.

First, relevant installation info
nltk (3.2.1)
jupyter (1.0.0)
jupyter-client (4.2.2)
jupyter-console (4.1.1)
jupyter-core (4.1.0)

Also
GPL Ghostscript 9.18 (2015-10-05)

I experimented on both python3.5 and python2.7, but this shouldn't be relevant.

The error message is as follows

```
---------------------------------------------------------------------------
IOError                                   Traceback (most recent call last)
/home/nykh/Documents/cs224u/program/local/lib/python2.7/site-packages/IPython/core/formatters.pyc in __call__(self, obj)
    341             method = _safe_get_formatter_method(obj, self.print_method)
    342             if method is not None:
--> 343                 return method()
    344             return None
    345         else:

/home/nykh/Documents/cs224u/program/local/lib/python2.7/site-packages/nltk/tree.pyc in _repr_png_(self)
    727                             '-q -dEPSCrop -sDEVICE=png16m -r90 -dTextAlphaBits=4 -dGraphicsAlphaBits=4 -dSAFER -dBATCH -dNOPAUSE -sOutputFile={0:} {1:}'
    728                             .format(out_path, in_path).split())
--> 729             with open(out_path, 'rb') as sr:
    730                 res = sr.read()
    731             os.remove(in_path)

IOError: [Errno 2] No such file or directory: u'/tmp/tmppncGPb.png'
```

Apparently, the temp png file is not emitted by GhostScript, which complains

```
Error: /undefinedresult in --stringwidth--
Operand stack:
   --nostringval--   0   12   67   0   0   (test)
Execution stack:
   %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   --nostringval--   false   1   %stopped_push   1977   1   3   %oparray_pop   1976   1   3   %oparray_pop   --nostringval--   1960   1   3   %oparray_pop   1852   1   3   %oparray_pop   --nostringval--   %errorexec_pop   .runexec2   --nostringval--   --nostringval--   --nostringval--   2   %stopped_push   --nostringval--   --nostringval--   --nostringval--   --nostringval--   %array_continue   --nostringval--   --nostringval--   --nostringval--   %array_continue   --nostringval--
Dictionary stack:
   --dict:1201/1684(ro)(G)--   --dict:0/20(G)--   --dict:83/200(L)--   --dict:23/50(L)--
Current allocation mode is local
Current file position is 7312
GPL Ghostscript 9.18: Unrecoverable error, exit code 1
```

I wonder whether it's the way that `_repr_png` calls GS that causes the GhostScript error

``` python
subprocess.call([find_binary('gs', binary_names=['gswin32c.exe', 'gswin64c.exe'], env_vars=['PATH'], verbose=False)] +
                            '-q -dEPSCrop -sDEVICE=png16m -r90 -dTextAlphaBits=4 -dGraphicsAlphaBits=4 -dSAFER -dBATCH -dNOPAUSE -sOutputFile={0:} {1:}'
                            .format(out_path, in_path).split())
```
"
80,https://github.com/nltk/nltk/issues/1390,1390,[],closed,2016-05-03 13:44:39+00:00,,2,Stanford taggers are slow because they launch the JVM whenever they tag something; launching a Java servlet and communicating via TCP would be faster,"As noted at http://stackoverflow.com/questions/23322674/how-to-improve-speed-with-stanford-nlp-tagger-and-nltk, NLTK's wrappers for the Stanford taggers are really slow.

@alvations [suggests](http://stackoverflow.com/a/23322996/1709587) batching operations to get around this, but this isn't always convenient. It would be nice to instead be able to [launch a servlet](http://nlp.stanford.edu/software/crf-faq.shtml#cc) and communicate with it via a socket, like [Dat Hoang's Python wrapper for the NER tagger](https://github.com/dat/stanford-ner) (which doesn't seem to work with the latest version of the Stanford taggers, by the way) used to do.

For my own purposes, I've managed to hack together something that seems to work (and doesn't use NLTK) on the `stanford-ner-2015-04-20` release of the Stanford NER tagger. I need to look into the other Stanford taggers, though, and the differences between versions, before I'll have anything ready to contribute to NLTK. I'll try to get round to that soon.
"
81,https://github.com/nltk/nltk/issues/1391,1391,[],closed,2016-05-04 03:32:35+00:00,,4,Installed nltk with brew on OSX El Capitan and 'word_tokenize' is not found,"I just went through the example on the main page (http://www.nltk.org/).  What am I missing in terms of dependencies?

```
iMe2 nltk $ python nltk.py 
Traceback (most recent call last):
  File ""nltk.py"", line 1, in <module>
    import nltk
  File ""/Users/iMe2/projects/nltk/nltk.py"", line 3, in <module>
    tokens = nltk.word_tokenize(sentence)
AttributeError: 'module' object has no attribute 'word_tokenize'
iMe2 nltk $ 
```

Thanks!
"
82,https://github.com/nltk/nltk/issues/1392,1392,[],closed,2016-05-08 04:03:15+00:00,,3,Doctests for model package,"We need doctests for the new model package, as pointed out by @Copper-Head in #1351.
"
83,https://github.com/nltk/nltk/issues/1394,1394,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-05-10 11:55:15+00:00,,2,nltk.text.Text.dispersion_plot() fails with ImportError: cannot import name 'dispersion_plot',"How to reproduce?

```
from nltk.book import *
text4.dispersion_plot([""citizens"", ""democracy"", ""freedom"", ""duties"", ""America""])
```

Error log in IPython shell:

```
In [15]: text4.dispersion_plot([""citizens"", ""democracy"", ""freedom"", ""duties"", ""America""])
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-15-d98be29a81e2> in <module>()
----> 1 text4.dispersion_plot([""citizens"", ""democracy"", ""freedom"", ""duties"", ""America""])

/usr/lib/python3.5/site-packages/nltk/text.py in dispersion_plot(self, words)
    442         :seealso: nltk.draw.dispersion_plot()
    443         """"""
--> 444         from nltk.draw import dispersion_plot
    445         dispersion_plot(self, words)
    446

ImportError: cannot import name 'dispersion_plot'
```

Versions:

```
In [18]: nltk.__version__
Out[18]: '3.1'

$ python -V
Python 3.5.1
$ uname -a
Linux 4.5.2-1-ARCH #1 SMP PREEMPT Thu Apr 21 18:21:27 CEST 2016 x86_64 GNU/Linux
```
"
84,https://github.com/nltk/nltk/issues/1395,1395,[],closed,2016-05-10 19:28:37+00:00,,5,Are there Stanford text-to-sentence functionality in NLTK?,"Unfortunately have not found one, does it present or not?
"
85,https://github.com/nltk/nltk/issues/1396,1396,[],closed,2016-05-10 19:34:55+00:00,,2,"Add support ""multiple parsing variants"" functionality for Stanford Parser [and custom command-line options]","Stanford Parser is able to do multiple parsing guesses of the same sentence, each parse variant goes along with its log-probability. This is done via _\- printPCFGkBest n_ option in the command line.

I've implemented this by myself for my project by subclassing StanfordParser, adding a key and updated code to build multiple trees from parser raw output.

If NLTK maintainers and users are interested we can generalize and merge this code.

But, more generally I think we should add support for custom command-line options.

What do you think?
"
86,https://github.com/nltk/nltk/issues/1397,1397,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-05-11 07:46:56+00:00,,6,ResolutionProverCommand (and unify) problem,"I have a problem with the `ResolutionProverCommand`. 

```
h = Expression.fromstring(""all x.(P(x) | Q(x))"")
t= Expression.fromstring(""all x.(-P(x) | -Q(x))"")
s = Expression.fromstring(""exists x.R(x)"")
demostracion=ResolutionProverCommand(s,[h,t])
demostracion.prove()
```

Outputs `True`, which shouldn't be the case. If we print the proof with `print(demostracion.proof())`, we get
[1] {-R(z41)}           A 
[2] {P(z43), Q(z43)}    A 
[3] {-P(z45), -Q(z45)}  A 
[4] {}                  (2, 3) 
As you see, the module thinks that the resolvent of [3] and [4] is the empty clause, which is not. The problem seems to be in `unify`. If we define

```
c=Clause([expr(""P(x)""),expr(""Q(x)"")])
cn=Clause([expr(""-P(x)""),expr(""-Q(x)"")])
```

Then `c.unify(cn)` ouputs [{}], the empy clause.

Am I doing something wrong? The problem does not reproduce neither with Tableau nor with Prover9.

Thank you for your great work!
"
87,https://github.com/nltk/nltk/issues/1398,1398,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-05-14 14:58:49+00:00,,0,Version number for nltk corpus data,"Hi,

I'm packaging nltk-data for Arch Linux, but where is the latest version number? I could not find this anywhere.

Here's the package that I'm in the process of updating: [nltk-data](https://www.archlinux.org/packages/community/any/nltk-data/).
Here's the old sourceforge page that is currently down: [nltk.sourceforge.net](http://nltk.sourceforge.net/).

An addition to the FAQ, a release log or change log would be great.
"
88,https://github.com/nltk/nltk/issues/1399,1399,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-05-15 22:49:34+00:00,,0,API for extracting syntactic-semantic features,"@annefried's [SitEnt Syntactic-Semantic Features](https://github.com/annefried/syntSemFeatures) is a tool for English that marks things like countability on NPs and morphological tense (perfect, progressive, etc.) on verbs. It relies on PTB-style POS tags and Stanford dependencies, and bundles some wordlists (e.g. from CELEX). Seems like it would be useful to have some or all of this functionality in NLTK.
"
89,https://github.com/nltk/nltk/issues/1400,1400,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-05-18 09:56:39+00:00,,1,Perceptron data loading issue with UNC paths,"Hi,

We're storing our data under an UNC path (i.e. our NLTK_DATA is on the lines of \\mypath.com\nltkdata), and this causes the following error on data loading for tag/perceptron.py. All other data loading seems to be fine (e.g. chunk/__init__.py has no problems).

We've been tweaking perceptron.py a little to debug, so the line numbers are a little off. We're currently using this alteration which is working:

`AP_MODEL_LOC = 'taggers/averaged_perceptron_tagger/'+PICKLE`

Traceback as follows:

```
Traceback (most recent call last):
 File "".\test_parser.py"", line 20, in <module>
   parsed = parser.parse(data['text'])
 File ""C:\Users\mike\Project\parser.py"", line 50, in parse
   tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]
 File ""C:\Users\mike\Project\lib\site-packages\nltk\tag\__init__.py"", line 110, in pos_tag
   tagger = PerceptronTagger()
 File ""C:\Users\mike\Project\lib\site-packages\nltk\tag\perceptron.py"", line 143, in __init__
   self.load(AP_MODEL_LOC)
 File ""C:\Users\mike\Project\lib\site-packages\nltk\tag\perceptron.py"", line 211, in load
   self.model.weights, self.tagdict, self.classes = load(loc)
 File ""C:\Users\mike\Project\lib\site-packages\nltk\data.py"", line 800, in load
   opened_resource = _open(resource_url)
 File ""C:\Users\mike\Project\lib\site-packages\nltk\data.py"", line 921, in _open
   return find(path_, ['']).open()
 File ""C:\Users\mike\Project\lib\site-packages\nltk\data.py"", line 640, in find
   raise LookupError(resource_not_found)
LookupError:
**********************************************************************
 Resource u'/C:/Users/mike/Project/mypath.com/nltkdata/taggers/averaged
 _perceptron_tagger/averaged_perceptron_tagger.pickle' not found.
 Please use the NLTK Downloader to obtain the resource:  >>>
 nltk.download()
 Searched in:
   - u''
**********************************************************************
```

Cheers,
Mike
"
90,https://github.com/nltk/nltk/issues/1401,1401,[],closed,2016-05-20 15:34:40+00:00,,1,AttributeError: type object 'Tree' has no attribute 'parse' in ConllCorpusReader.parsed_sents(),"`AttributeError                            Traceback (most recent call last)
/opt/STools/python/2.7-x86_64/lib/python2.7/site-packages/IPython/core/formatters.pyc in **call**(self, obj)
    688                 type_pprinters=self.type_printers,
    689                 deferred_pprinters=self.deferred_printers)
--> 690             printer.pretty(obj)
    691             printer.flush()
    692             return stream.getvalue()

/opt/STools/python/2.7-x86_64/lib/python2.7/site-packages/IPython/lib/pretty.pyc in pretty(self, obj)
    407                             if callable(meth):
    408                                 return meth(obj, self, cycle)
--> 409             return _default_pprint(obj, self, cycle)
    410         finally:
    411             self.end_group()

/opt/STools/python/2.7-x86_64/lib/python2.7/site-packages/IPython/lib/pretty.pyc in _default_pprint(obj, p, cycle)
    527     if _safe_getattr(klass, '__repr__', None) not in _baseclass_reprs:
    528         # A user-provided repr. Find newlines and replace them with p.break_()
--> 529         _repr_pprint(obj, p, cycle)
    530         return
    531     p.begin_group(1, '<')

/opt/STools/python/2.7-x86_64/lib/python2.7/site-packages/IPython/lib/pretty.pyc in _repr_pprint(obj, p, cycle)
    709     """"""A pprint that just redirects to the normal repr function.""""""
    710     # Find newlines and replace them with p.break_()
--> 711     output = repr(obj)
    712     for idx,output_line in enumerate(output.splitlines()):
    713         if idx:

/opt/STools/python/2.7-x86_64/lib/python2.7/site-packages/nltk/compat.pyc in wrapper(self)
    496 def _7bit(method):
    497     def wrapper(self):
--> 498         return method(self).encode('ascii', 'backslashreplace')
    499 
    500     functools.update_wrapper(wrapper, method, [""__name__"", ""**doc**""])

/opt/STools/python/2.7-x86_64/lib/python2.7/site-packages/nltk/util.pyc in **repr**(self)
    662         pieces = []
    663         length = 5
--> 664         for elt in self:
    665             pieces.append(repr(elt))
    666             length += len(pieces[-1]) + 2

/opt/STools/python/2.7-x86_64/lib/python2.7/site-packages/nltk/util.pyc in iterate_from(self, index)
    837         if len(self._lists) == 1 and self._all_lazy:
    838             for value in self._lists[0].iterate_from(index):
--> 839                 yield self._func(value)
    840             return
    841 

/opt/STools/python/2.7-x86_64/lib/python2.7/site-packages/nltk/corpus/reader/conll.pyc in get_parsed_sent(grid)
    135         if pos_in_tree is None: pos_in_tree = self._pos_in_tree
    136         def get_parsed_sent(grid): # capture pos_in_tree as local var
--> 137             return self._get_parsed_sent(grid, pos_in_tree, tagset)
    138         return LazyMap(get_parsed_sent, self._grids(fileids))
    139 

/opt/STools/python/2.7-x86_64/lib/python2.7/site-packages/nltk/corpus/reader/conll.pyc in _get_parsed_sent(self, grid, pos_in_tree, tagset)
    281             treestr += '%s (%s %s) %s' % (left, pos_tag, word, right)
    282         try:
--> 283             tree = self._tree_class.parse(treestr)
    284         except (ValueError, IndexError):
    285             tree = self._tree_class.parse('(%s %s)' %

AttributeError: type object 'Tree' has no attribute 'parse'

> /opt/STools/python/2.7-x86_64/lib/python2.7/site-packages/nltk/corpus/reader/conll.py(286)_get_parsed_sent()
>     285             tree = self._tree_class.parse('(%s %s)' %
> --> 286                                           (self._root_label, treestr))
>     287 
> `
"
91,https://github.com/nltk/nltk/issues/1404,1404,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-05-22 01:14:59+00:00,,3,AssertionError: Bins parameter must not be less than freqdist.B(),"Hi, 
I have been testing the model package for text classifying and it seems to work very good. However, I have been using `LidstoneProbDist` until now and I would like to test the classifier using `WittenBellProbDist`. Nonetheless, I am getting errors and I couldn't find any reference implementing this estimator.  In particular, I have been using:

```
estimator = lambda fdist, bins: LidstoneProbDist(fdist,10) 
lm = NgramModel(2, train, estimator=estimator)
```

which worked pretty fine. Now, I just changed it into:

```
estimator = lambda fdist, bins: WittenBellProbDist(fdist,10) 
lm[categ] = NgramModel(2, train, estimator=estimator)
```

and I get the error:

```
Traceback (most recent call last):
  File ""test7.py"", line 43, in <module>
    lm[categ] = NgramModel(N, train, estimator=estimator)
  File ""/veu4/usuaris30/speech00/.local/lib/python2.7/site-packages/nltk-2.0.4-py2.7.egg/nltk/model/ngram.py"", line 107, in __init__
    self._model = ConditionalProbDist(cfd, estimator, len(cfd))
  File ""/veu4/usuaris30/speech00/.local/lib/python2.7/site-packages/nltk-2.0.4-py2.7.egg/nltk/probability.py"", line 1957, in __init__
    *factory_args, **factory_kw_args)
  File ""test7.py"", line 42, in <lambda>
    estimator = lambda fdist, bins: WittenBellProbDist(fdist,10) 
  File ""/veu4/usuaris30/speech00/.local/lib/python2.7/site-packages/nltk-2.0.4-py2.7.egg/nltk/probability.py"", line 1176, in __init__
    'Bins parameter must not be less than freqdist.B()'
AssertionError: Bins parameter must not be less than freqdist.B()
```

Anyone has an idea? Really apreciate your time.

PS: I am using an old version of NLTK (2.0.4), because I was not able to make the package model work with the current version.; Python 2.7.3 and GNU/Linux.
"
92,https://github.com/nltk/nltk/issues/1406,1406,"[{'id': 159235791, 'node_id': 'MDU6TGFiZWwxNTkyMzU3OTE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/dependency%20parsing', 'name': 'dependency parsing', 'color': 'eb6420', 'default': False, 'description': None}, {'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-05-25 00:05:36+00:00,,8,Issues with MaltParser interface with Glue Semantics,"```
>>> from nltk.tag import RegexpTagger
>>> tagger = RegexpTagger(
...     [('^(chases|runs)$', 'VB'),
...      ('^(a)$', 'ex_quant'),
...      ('^(every)$', 'univ_quant'),
...      ('^(dog|boy)$', 'NN'),
...      ('^(He)$', 'PRP')
... ])
>>> rc = nltk.DrtGlueReadingCommand(depparser=nltk.MaltParser(tagger=tagger))
>>> dt = nltk.DiscourseTester(['Every dog chases a boy', 'He runs'], rc)
>>> dt.readings()
```

Even when you specific the malt parser directory, there are more errors. It's inconsistent with the book.
"
93,https://github.com/nltk/nltk/issues/1410,1410,[],closed,2016-06-07 12:33:42+00:00,,0,CorpusReader chokes on empty files,"If there is a completely empty file (size 0) among the files of a corpus, the reader chokes with a thoroughly uninformative `AssertionError`.

```
>>> reader = nltk.corpus.reader.PlaintextCorpusReader(""bad-corpus"", r"".*\.txt"")
>>> reader.words()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../python3.4/site-packages/nltk/util.py"", line 716, in __repr__
    for elt in self:
  File "".../python3.4/site-packages/nltk/corpus/reader/util.py"", line 394, in iterate_from
    for tok in piece.iterate_from(max(0, start_tok-offset)):
  File "".../python3.4/site-packages/nltk/corpus/reader/util.py"", line 336, in iterate_from
    assert self._len is not None
AssertionError
```

I found a [pull request](https://github.com/nltk/nltk/pull/1408) for this issue, but no associated bug report. I don't believe the work-around in the pull request is a good solution. Submitting my own pull request.
"
94,https://github.com/nltk/nltk/issues/1414,1414,[],closed,2016-06-16 16:01:40+00:00,,1,pip download-cache unavailable,"Users that use `pip 8.0.0` and above experience the following issue (especially while running `tox` tests):

```
no such option: --download-cache
ERROR: InvocationError: '/path/nltk/.tox/py34/bin/pip install --download-cache=/path/nltk/.tox/_download scipy scikit-learn'
```

Since `pip 6.0` the `--download-cache` flag used to be deprecated, but could still be used. On the contrary, newer versions of `pip` are not backward compatible with regard to this flag.

**6.0 (2014-12-22)**:

> - **DEPRECATION** `pip install --download-cache` and `pip wheel --download-cache` command line flags have been deprecated and the functionality removed. Since pip now automatically configures and uses it’s internal HTTP cache which supplants the `--download-cache` the existing options have been made non functional but will still be accepted until their removal in pip v8.0. For more information please see https://pip.pypa.io/en/latest/reference/pip_install.html#caching

**8.0.0 (2016-01-19):**

> - **BACKWARD INCOMPATIBLE** Remove the --download-cache which had been deprecated and no-op'd in 6.0.

Since two years passed since the `--download-cache` flag has been deprecated, I suggest to completely remove it from `tox.ini`.
"
95,https://github.com/nltk/nltk/issues/1417,1417,[],closed,2016-06-21 02:22:20+00:00,,0,is_cjk test is broken,"@alvations – it looks like `is_cjk` breaks on our [CI Server](https://nltk.ci.cloudbees.com/job/nltk/lastCompletedBuild/testReport/). Would you please look into this?
"
96,https://github.com/nltk/nltk/issues/1418,1418,[],closed,2016-06-21 02:24:37+00:00,,2,aline breaks in Python 2.7,"There's a character encoding issue for `metrics.aline` in Python 2.7, as reported by our [CI server](https://nltk.ci.cloudbees.com/job/nltk/lastCompletedBuild/testReport/).
"
97,https://github.com/nltk/nltk/issues/1419,1419,[],closed,2016-06-21 02:26:33+00:00,,5,ReppTokenizer breaks on CI Server,"There are errors reported by the [CI Server](https://nltk.ci.cloudbees.com/job/nltk/lastCompletedBuild/testReport/) for `ReppTokenizer`.
"
98,https://github.com/nltk/nltk/issues/1421,1421,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}]",closed,2016-06-21 15:24:00+00:00,,3,Issues with UnicharsCorpusReader and NonbreakingPrefixesCorpusReader,"For `UnicharsCorpusReader` in Python 2.7, the doctests needs handle the unicode characters properly, see https://nltk.ci.cloudbees.com/job/nltk/TOXENV=py27-jenkins,jdk=jdk8latestOnlineInstall/lastCompletedBuild/testReport/nltk.corpus.reader.wordlist/UnicharsCorpusReader/chars/

For `NonbreakingPrefixesCorpusReader` in Python 3.5, something funky is going on with how the paths are read and NLTK is looking for `'corpora/nonbreaking_prefixes.zip/nonbreaking_prefixes/'` instead of `'corpora/nonbreaking_prefixes.zip` or `'corpora/nonbreaking_prefixes/'`, see https://nltk.ci.cloudbees.com/job/nltk/TOXENV=py35-jenkins,jdk=jdk8latestOnlineInstall/lastCompletedBuild/testReport/nltk.corpus.reader.wordlist/NonbreakingPrefixesCorpusReader/words/

Same for `UnicharsCorpusReader`  in Python 3.5.

@alvations, please take a look at this.
"
99,https://github.com/nltk/nltk/issues/1424,1424,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 159235791, 'node_id': 'MDU6TGFiZWwxNTkyMzU3OTE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/dependency%20parsing', 'name': 'dependency parsing', 'color': 'eb6420', 'default': False, 'description': None}]",closed,2016-07-02 14:42:25+00:00,,9,Stanford parser breaks UTF-8 characters,"[This line](https://github.com/nltk/nltk/blob/develop/nltk/parse/stanford.py#L219) `stdout = stdout.replace(b'\xa0',b' ')` breaks 2-byte characters such as 0xc3 0xa0 (à).

To reproduce (python 3):

``` python
stanford.StanfordNeuralDependencyParser().raw_parse(""aàa"")
```

I got:

```
Traceback (most recent call last):
  File ""pipeline_verbal.py"", line 19, in <module>
    parser.raw_parse(""aàa"")
  File ""/home/amit/tools/anaconda3/lib/python3.5/site-packages/nltk/parse/stanford.py"", line 132, in raw_parse
    return next(self.raw_parse_sents([sentence], verbose))
  File ""/home/amit/tools/anaconda3/lib/python3.5/site-packages/nltk/parse/stanford.py"", line 150, in raw_parse_sents
    return self._parse_trees_output(self._execute(cmd, '\n'.join(sentences), verbose))
  File ""/home/amit/tools/anaconda3/lib/python3.5/site-packages/nltk/parse/stanford.py"", line 221, in _execute
    stdout = stdout.decode(encoding)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc3 in position 3: invalid continuation byte
```

I believe a more specific character sequence should be used instead of '\xa0'.
Also, it would be helpful to have a comment above those lines, that explains why they are necessary. That would make debugging easier.

Thanks!
"
100,https://github.com/nltk/nltk/issues/1425,1425,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-07-03 22:54:43+00:00,,8,Test Suite Errors and SSL Certificate Issues,"Hello,

Linux: Mint 17.3
NLTK: 3.2.1
Python: 3.5 (and 3.4)

When I try and run <code>nltk.download()</code>, I get an error stating `SSL: CERTIFICATE_VERIFY_FAILED`

Also, when I attempt to run the `nosetests`, none of the files that should be in `nltk/test/unit/files/` are there and some tests fail with the following traceback:

```
jwolosonovich@LINUX115XFC2 ~/Desktop $ nosetests -v nltk
/usr/local/python3.5i/lib/python3.5/site-packages/IPython/kernel/__init__.py:13: ShimWarning: The `IPython.kernel` package has been deprecated. You should import from ipykernel or jupyter_client instead.
  ""You should import from ipykernel or jupyter_client instead."", ShimWarning)
/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.
  warnings.warn(""The twython library has not been installed. ""
test_full_matches (nltk.test.unit.translate.test_bleu.TestBLEU) ... ok
Examples from the original BLEU paper ... ok
test_partial_matches_hypothesis_longer_than_reference (nltk.test.unit.translate.test_bleu.TestBLEU) ... ok
test_zero_matches (nltk.test.unit.translate.test_bleu.TestBLEU) ... ok
test_brevity_penalty (nltk.test.unit.translate.test_bleu.TestBLEUFringeCases) ... SKIP: Skipping fringe cases for BLEU.
test_case_where_n_is_bigger_than_hypothesis_length (nltk.test.unit.translate.test_bleu.TestBLEUFringeCases) ... SKIP: Skipping fringe cases for BLEU.
test_empty_hypothesis (nltk.test.unit.translate.test_bleu.TestBLEUFringeCases) ... SKIP: Skipping fringe cases for BLEU.
test_empty_references (nltk.test.unit.translate.test_bleu.TestBLEUFringeCases) ... SKIP: Skipping fringe cases for BLEU.
test_prob_t_a_given_s (nltk.test.unit.translate.test_ibm1.TestIBMModel1) ... ok
test_set_uniform_translation_probabilities (nltk.test.unit.translate.test_ibm1.TestIBMModel1) ... ok
test_set_uniform_translation_probabilities_of_non_domain_values (nltk.test.unit.translate.test_ibm1.TestIBMModel1) ... ok
test_prob_t_a_given_s (nltk.test.unit.translate.test_ibm2.TestIBMModel2) ... ok
test_set_uniform_alignment_probabilities (nltk.test.unit.translate.test_ibm2.TestIBMModel2) ... ok
test_set_uniform_alignment_probabilities_of_non_domain_values (nltk.test.unit.translate.test_ibm2.TestIBMModel2) ... ok
test_prob_t_a_given_s (nltk.test.unit.translate.test_ibm3.TestIBMModel3) ... ok
test_set_uniform_distortion_probabilities (nltk.test.unit.translate.test_ibm3.TestIBMModel3) ... ok
test_set_uniform_distortion_probabilities_of_non_domain_values (nltk.test.unit.translate.test_ibm3.TestIBMModel3) ... ok
test_prob_t_a_given_s (nltk.test.unit.translate.test_ibm4.TestIBMModel4) ... ok
test_set_uniform_distortion_probabilities_of_max_displacements (nltk.test.unit.translate.test_ibm4.TestIBMModel4) ... ok
test_set_uniform_distortion_probabilities_of_non_domain_values (nltk.test.unit.translate.test_ibm4.TestIBMModel4) ... ok
test_prob_t_a_given_s (nltk.test.unit.translate.test_ibm5.TestIBMModel5) ... ok
test_prune (nltk.test.unit.translate.test_ibm5.TestIBMModel5) ... ok
test_set_uniform_vacancy_probabilities_of_max_displacements (nltk.test.unit.translate.test_ibm5.TestIBMModel5) ... ok
test_set_uniform_vacancy_probabilities_of_non_domain_values (nltk.test.unit.translate.test_ibm5.TestIBMModel5) ... ok
test_best_model2_alignment (nltk.test.unit.translate.test_ibm_model.TestIBMModel) ... ok
test_best_model2_alignment_does_not_change_pegged_alignment (nltk.test.unit.translate.test_ibm_model.TestIBMModel) ... ok
test_best_model2_alignment_handles_empty_src_sentence (nltk.test.unit.translate.test_ibm_model.TestIBMModel) ... ok
test_best_model2_alignment_handles_empty_trg_sentence (nltk.test.unit.translate.test_ibm_model.TestIBMModel) ... ok
test_best_model2_alignment_handles_fertile_words (nltk.test.unit.translate.test_ibm_model.TestIBMModel) ... ok
test_hillclimb (nltk.test.unit.translate.test_ibm_model.TestIBMModel) ... ok
test_neighboring_finds_neighbor_alignments (nltk.test.unit.translate.test_ibm_model.TestIBMModel) ... ok
test_neighboring_returns_neighbors_with_pegged_alignment (nltk.test.unit.translate.test_ibm_model.TestIBMModel) ... ok
test_neighboring_sets_neighbor_alignment_info (nltk.test.unit.translate.test_ibm_model.TestIBMModel) ... ok
test_sample (nltk.test.unit.translate.test_ibm_model.TestIBMModel) ... ok
test_vocabularies_are_initialized (nltk.test.unit.translate.test_ibm_model.TestIBMModel) ... ok
test_vocabularies_are_initialized_even_with_empty_corpora (nltk.test.unit.translate.test_ibm_model.TestIBMModel) ... ok
test_total_translated_words (nltk.test.unit.translate.test_stack_decoder.TestHypothesis) ... ok
test_translated_positions (nltk.test.unit.translate.test_stack_decoder.TestHypothesis) ... ok
test_translation_so_far (nltk.test.unit.translate.test_stack_decoder.TestHypothesis) ... ok
test_translation_so_far_for_empty_hypothesis (nltk.test.unit.translate.test_stack_decoder.TestHypothesis) ... ok
test_untranslated_spans (nltk.test.unit.translate.test_stack_decoder.TestHypothesis) ... ok
test_untranslated_spans_for_empty_hypothesis (nltk.test.unit.translate.test_stack_decoder.TestHypothesis) ... ok
test_best_returns_none_when_stack_is_empty (nltk.test.unit.translate.test_stack_decoder.TestStack) ... ok
test_best_returns_the_best_hypothesis (nltk.test.unit.translate.test_stack_decoder.TestStack) ... ok
test_push_bumps_off_worst_hypothesis_when_stack_is_full (nltk.test.unit.translate.test_stack_decoder.TestStack) ... ok
test_push_does_not_add_hypothesis_that_falls_below_beam_threshold (nltk.test.unit.translate.test_stack_decoder.TestStack) ... ok
test_push_removes_hypotheses_that_fall_below_beam_threshold (nltk.test.unit.translate.test_stack_decoder.TestStack) ... ok
test_compute_future_costs (nltk.test.unit.translate.test_stack_decoder.TestStackDecoder) ... ok
test_compute_future_costs_for_phrases_not_in_phrase_table (nltk.test.unit.translate.test_stack_decoder.TestStackDecoder) ... ok
test_distortion_score (nltk.test.unit.translate.test_stack_decoder.TestStackDecoder) ... ok
test_distortion_score_of_first_expansion (nltk.test.unit.translate.test_stack_decoder.TestStackDecoder) ... ok
test_find_all_src_phrases (nltk.test.unit.translate.test_stack_decoder.TestStackDecoder) ... ok
test_future_score (nltk.test.unit.translate.test_stack_decoder.TestStackDecoder) ... ok
test_valid_phrases (nltk.test.unit.translate.test_stack_decoder.TestStackDecoder) ... ok
SKIP: test_2x_compat is for testing nltk.compat under Python 2.x
nltk.test.unit.test_classify.test_megam ... SKIP: 

===========================================================================
NLTK was unable to find the megam file!
Use software specific configuration paramaters or set the MEGAM environment variable.

  For more information on megam, see:
    <http://www.umiacs.umd.edu/~hal/megam/index.html>
===========================================================================
nltk.test.unit.test_classify.test_tadm ... SKIP: 

===========================================================================
NLTK was unable to find the tadm file!
Use software specific configuration paramaters or set the TADM environment variable.

  For more information on tadm, see:
    <http://tadm.sf.net>
===========================================================================
test_bigram2 (nltk.test.unit.test_collocations.TestBigram) ... ok
test_bigram3 (nltk.test.unit.test_collocations.TestBigram) ... ok
test_bigram5 (nltk.test.unit.test_collocations.TestBigram) ... ok
test_catalan (nltk.test.unit.test_corpora.TestCess) ... ok
test_esp (nltk.test.unit.test_corpora.TestCess) ... ok
test_parsed_sents (nltk.test.unit.test_corpora.TestCoNLL2007) ... ok
test_sents (nltk.test.unit.test_corpora.TestCoNLL2007) ... ok
test_words (nltk.test.unit.test_corpora.TestFloresta) ... ok
test_tagged_words (nltk.test.unit.test_corpora.TestIndian) ... ok
test_words (nltk.test.unit.test_corpora.TestIndian) ... ok
test_categories (nltk.test.unit.test_corpora.TestPTB) ... SKIP: A full installation of the Penn Treebank is not available
test_category_words (nltk.test.unit.test_corpora.TestPTB) ... SKIP: A full installation of the Penn Treebank is not available
test_fileids (nltk.test.unit.test_corpora.TestPTB) ... SKIP: A full installation of the Penn Treebank is not available
test_news_fileids (nltk.test.unit.test_corpora.TestPTB) ... SKIP: A full installation of the Penn Treebank is not available
test_tagged_words (nltk.test.unit.test_corpora.TestPTB) ... SKIP: A full installation of the Penn Treebank is not available
test_words (nltk.test.unit.test_corpora.TestPTB) ... SKIP: A full installation of the Penn Treebank is not available
test_parsed_sents (nltk.test.unit.test_corpora.TestSinicaTreebank) ... ok
test_sents (nltk.test.unit.test_corpora.TestSinicaTreebank) ... ok
test_raw_unicode (nltk.test.unit.test_corpora.TestUdhr) ... ok
test_words (nltk.test.unit.test_corpora.TestUdhr) ... ok
test_correct_length (nltk.test.unit.test_corpus_views.TestCorpusViews) ... ok
test_correct_values (nltk.test.unit.test_corpus_views.TestCorpusViews) ... ok
nltk.test.unit.test_hmm.test_forward_probability ... ok
nltk.test.unit.test_hmm.test_forward_probability2 ... ok
nltk.test.unit.test_hmm.test_backward_probability ... ok
Sanity check that file comparison is not giving false positives. ... ERROR
test_retweet_original_tweet (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV) ... ERROR
test_textoutput (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV) ... ERROR
test_tweet_hashtag (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV) ... ERROR
test_tweet_media (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV) ... ERROR
test_tweet_metadata (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV) ... ERROR
test_tweet_place (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV) ... ERROR
test_tweet_place_boundingbox (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV) ... ERROR
test_tweet_url (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV) ... ERROR
test_tweet_usermention (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV) ... ERROR
test_user_metadata (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV) ... ERROR
test_userurl (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV) ... ERROR
test_simple (nltk.test.unit.test_naivebayes.NaiveBayesClassifierTest) ... ok
nltk.test.unit.test_seekable_unicode_stream_reader.test_reader('\n    This is a test file.\n    It is fairly short.\n    ', 'ascii') ... ok
nltk.test.unit.test_seekable_unicode_stream_reader.test_reader('\n    This is a test file.\n    It is fairly short.\n    ', 'latin1') ... ok
nltk.test.unit.test_seekable_unicode_stream_reader.test_reader('\n    This is a test file.\n    It is fairly short.\n    ', 'greek') ... ok
nltk.test.unit.test_seekable_unicode_stream_reader.test_reader('\n    This is a test file.\n    It is fairly short.\n    ', 'hebrew') ... ok
nltk.test.unit.test_seekable_unicode_stream_reader.test_reader('\n    This is a test file.\n    It is fairly short.\n    ', 'utf-16') ... ok
nltk.test.unit.test_seekable_unicode_stream_reader.test_reader('\n    This is a test file.\n    It is fairly short.\n    ', 'utf-8') ... ok
nltk.test.unit.test_seekable_unicode_stream_reader.test_reader('This file can be encoded with latin1. \x83', 'latin1') ... ok
nltk.test.unit.test_seekable_unicode_stream_reader.test_reader('This file can be encoded with latin1. \x83', 'greek') ... ok
nltk.test.unit.test_seekable_unicode_stream_reader.test_reader('This file can be encoded with latin1. \x83', 'hebrew') ... ok
nltk.test.unit.test_seekable_unicode_stream_reader.test_reader('This file can be encoded with latin1. \x83', 'utf-16') ... ok
nltk.test.unit.test_seekable_unicode_stream_reader.test_reader('This file can be encoded with latin1. \x83', 'utf-8') ... ok
nltk.test.unit.test_seekable_unicode_stream_reader.test_reader(""    This is a test file.\n    Here's a blank line:\n\n    And here's some unicode: î ģ ￣\n    "", 'utf-16') ... ok
nltk.test.unit.test_seekable_unicode_stream_reader.test_reader(""    This is a test file.\n    Here's a blank line:\n\n    And here's some unicode: î ģ ￣\n    "", 'utf-8') ... ok
nltk.test.unit.test_seekable_unicode_stream_reader.test_reader('    This is a test file.\n    Unicode characters: ó ∢ ㌳䑄 啕\n    ', 'utf-16') ... ok
nltk.test.unit.test_seekable_unicode_stream_reader.test_reader('    This is a test file.\n    Unicode characters: ó ∢ ㌳䑄 啕\n    ', 'utf-8') ... ok
nltk.test.unit.test_seekable_unicode_stream_reader.test_reader_on_large_string('utf-16',) ... ok
nltk.test.unit.test_seekable_unicode_stream_reader.test_reader_on_large_string('utf-8',) ... ok
test_german (nltk.test.unit.test_stem.SnowballTest) ... ok
test_russian (nltk.test.unit.test_stem.SnowballTest) ... ok
test_short_strings_bug (nltk.test.unit.test_stem.SnowballTest) ... ok
test_spanish (nltk.test.unit.test_stem.SnowballTest) ... ok
nltk.test.unit.test_tag.test_basic ... ok
Test error handling of undefined tgrep operators. ... ok
Test that comments are correctly filtered out of tgrep search ... ok
Test the Basic Examples from the TGrep2 manual. ... ok
Test labeled nodes. ... ok
Test that multiple (3 or more) conjunctions of node relations are ... ok
Test that tgrep search strings handles bytes and strs the same ... ok
Test selecting nodes using case insensitive node names. ... ok
Test node name matching with the search_leaves flag set to False. ... ok
Test that the tgrep print operator ' is properly ignored. ... ok
Test selecting nodes using quoted node names. ... ok
Test regex matching on nodes. ... ok
Test regex matching on nodes. ... ok
Test a simple use of tgrep for finding nodes matching a given ... ok
Test matching on nodes based on NLTK tree position. ... ok
Test matching nodes based on precedence relations. ... ok
Test matching sister nodes in a tree. ... ok
Test that tokenization handles bytes and strs the same way. ... ok
Test tokenization of the TGrep2 manual example patterns. ... ok
Test tokenization of basic link types. ... ok
Test tokenization of macro definitions. ... ok
Test tokenization of labeled nodes. ... ok
Test tokenization of node names. ... ok
Test tokenization of quoting. ... ok
Test tokenization of segmented patterns. ... ok
Simple test of tokenization. ... ok
Test that semicolons at the end of a tgrep2 search string won't ... ok
Test defining and using tgrep2 macros. ... ok
Test matching nodes based on dominance relations. ... ok
Test matching nodes based on their index in their parent node. ... ok
Test TweetTokenizer using words with special and accented characters. ... ok
Failure: SkipTest (The twython library has not been installed.) ... SKIP: The twython library has not been installed.

======================================================================
ERROR: Sanity check that file comparison is not giving false positives.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 182, in test_file_is_wrong
    self.assertFalse(are_files_identical(outfn, ref_fn), msg=self.msg)
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 29, in are_files_identical
    with open(filename2, ""rb"") as fileB:
FileNotFoundError: [Errno 2] No such file or directory: '/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/files/tweets.20150430-223406.retweet.csv.ref'

======================================================================
ERROR: test_retweet_original_tweet (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 171, in test_retweet_original_tweet
    self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 29, in are_files_identical
    with open(filename2, ""rb"") as fileB:
FileNotFoundError: [Errno 2] No such file or directory: '/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/files/tweets.20150430-223406.retweet.csv.ref'

======================================================================
ERROR: test_textoutput (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 61, in test_textoutput
    self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 29, in are_files_identical
    with open(filename2, ""rb"") as fileB:
FileNotFoundError: [Errno 2] No such file or directory: '/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/files/tweets.20150430-223406.text.csv.ref'

======================================================================
ERROR: test_tweet_hashtag (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 93, in test_tweet_hashtag
    self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 29, in are_files_identical
    with open(filename2, ""rb"") as fileB:
FileNotFoundError: [Errno 2] No such file or directory: '/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/files/tweets.20150430-223406.hashtag.csv.ref'

======================================================================
ERROR: test_tweet_media (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 114, in test_tweet_media
    self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 29, in are_files_identical
    with open(filename2, ""rb"") as fileB:
FileNotFoundError: [Errno 2] No such file or directory: '/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/files/tweets.20150430-223406.media.csv.ref'

======================================================================
ERROR: test_tweet_metadata (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 73, in test_tweet_metadata
    self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 29, in are_files_identical
    with open(filename2, ""rb"") as fileB:
FileNotFoundError: [Errno 2] No such file or directory: '/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/files/tweets.20150430-223406.tweet.csv.ref'

======================================================================
ERROR: test_tweet_place (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 147, in test_tweet_place
    self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 29, in are_files_identical
    with open(filename2, ""rb"") as fileB:
FileNotFoundError: [Errno 2] No such file or directory: '/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/files/tweets.20150430-223406.place.csv.ref'

======================================================================
ERROR: test_tweet_place_boundingbox (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 158, in test_tweet_place_boundingbox
    self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 29, in are_files_identical
    with open(filename2, ""rb"") as fileB:
FileNotFoundError: [Errno 2] No such file or directory: '/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/files/tweets.20150430-223406.placeboundingbox.csv.ref'

======================================================================
ERROR: test_tweet_url (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 125, in test_tweet_url
    self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 29, in are_files_identical
    with open(filename2, ""rb"") as fileB:
FileNotFoundError: [Errno 2] No such file or directory: '/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/files/tweets.20150430-223406.url.csv.ref'

======================================================================
ERROR: test_tweet_usermention (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 103, in test_tweet_usermention
    self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 29, in are_files_identical
    with open(filename2, ""rb"") as fileB:
FileNotFoundError: [Errno 2] No such file or directory: '/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/files/tweets.20150430-223406.usermention.csv.ref'

======================================================================
ERROR: test_user_metadata (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 83, in test_user_metadata
    self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 29, in are_files_identical
    with open(filename2, ""rb"") as fileB:
FileNotFoundError: [Errno 2] No such file or directory: '/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/files/tweets.20150430-223406.user.csv.ref'

======================================================================
ERROR: test_userurl (nltk.test.unit.test_json2csv_corpus.TestJSON2CSV)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 136, in test_userurl
    self.assertTrue(are_files_identical(outfn, ref_fn), msg=self.msg)
  File ""/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/test_json2csv_corpus.py"", line 29, in are_files_identical
    with open(filename2, ""rb"") as fileB:
FileNotFoundError: [Errno 2] No such file or directory: '/home/jwolosonovich/.virtualenvs/ztrip35i/lib/python3.5/site-packages/nltk/test/unit/files/tweets.20150430-223406.userurl.csv.ref'

----------------------------------------------------------------------
Ran 147 tests in 5.503s

FAILED (SKIP=14, errors=12)
```

This has always worked flawlessly in the past, so I'm thinking it is a system issue, but I'm not sure where to start.
"
101,https://github.com/nltk/nltk/issues/1426,1426,[],closed,2016-07-05 07:24:03+00:00,,1,nltk/parse/stanford.py maxLength support,"It would be nice if we could set the value of the Stanford parser's `maxLength` parameter from nltk. Now, one has to iterate through all the sentences, split them into words and filter out the long ones. Setting one parameter and letting the parser do it instead would be much more convenient... 
"
102,https://github.com/nltk/nltk/issues/1427,1427,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2016-07-07 03:14:04+00:00,,3,not sure bug in tag.perceptron.py,"
I'm not sure whether there are some bugs in def _get_features(self, i, word, context, prev, prev2) , def train(self, sentences, save_loc=None, nr_iter=5) and def tag(self, tokens), i hope you to check them. 
I think prev, prev2 in train(self, sentences, save_loc=None, nr_iter=5) are wrong.
I guess prev is referred to tag of i-2 word and prev2 is referred to tag of i-1 word because of prev, prev2 = self.START, so the codes in def _get_features(self, i, word, context, prev, prev2): add('i-1 tag', prev) should be add('i-1 tag', prev2), add('i-2 tag', prev2) should be add('i-2 tag', prev), add('i tag+i-2 tag', prev, prev2) should be add('i-1 tag+i-2 tag', prev2, prev), add('i-1 tag+i word', prev, context[i]) should be add('i-1 tag+i word', prev2, context[i]),
codes in train(self, sentences, save_loc=None, nr_iter=5): prev2 = prev  prev = guess should be prev = prev2  prev2 = guess
codes in tag(self, tokens): prev2 = prev  prev = tag should be prev = prev2 prev2 = tag
I'm really not sure my wether my opionion is right, please check it, thank you.

"
103,https://github.com/nltk/nltk/issues/1428,1428,[],closed,2016-07-07 10:38:19+00:00,,11,Vectorizer using Skipgrams,"Can we have a vectorizer which uses skipgrams? I often use it in my projects and it would be great if it is implemented already in nltk.util. I would be happy to contribute.
"
104,https://github.com/nltk/nltk/issues/1430,1430,[],closed,2016-07-08 18:39:23+00:00,,3,Update Senna to v3,"Senna was upgraded ... 5 years ago.

It seems like the `senna` modules should use v3 instead of v2.  Or maybe this is unused code?  I'm finding it hard to imagine that people are using this and 5 years go by while using the old version.

Granted, people could simply have Senna v3 in the `/usr/share/senna-v2.0` folder.
"
105,https://github.com/nltk/nltk/issues/1433,1433,[],closed,2016-07-11 08:57:05+00:00,,6,Sentence splitting: Inconsistent end spans with trailing white space.,"I observed the following inconsistency in the PunktSentenceTokenizer, using NLTK 3.1 on Python 3.4:

```
>>> from nltk.tokenize import PunktSentenceTokenizer
>>> pst = PunktSentenceTokenizer()
>>> pst.span_tokenize('See Section 3.)  Or Section 2.)  ')  # Ex. 1
[(0, 15), (17, 31)]
>>> pst.span_tokenize('See Section 3).  Or Section 2).  ')  # Ex. 2
[(0, 15), (17, 33)]
>>> pst.span_tokenize('See Section 33.  Or Section 213  ')  # Ex. 3
[(0, 15), (17, 33)]
```

Apparently, the spans returned by the sentence splitter don't include trailing whitespace for sentences 1.._n_-1, but include them for sentence _n_ (the last sentence identified in the argument string, see the second and third example).
However, if the last sentence ends in a closing parenthesis, the spans don't include whitespace either (as in the first example).

Thus, there are two mutually exclusive strategies, each having pros and cons:
In examples 2 and 3, the end offset of the last sentence always equals the length of the given string; however, the handling of blanks is inconsistent among the non-last sentences vs. the last sentence.
On the other hand, in example 1, the spans exclude trailing blanks for all sentences; however, it might be undesirable that the final whitespace is not represented in the spans.
"
106,https://github.com/nltk/nltk/issues/1434,1434,[],closed,2016-07-13 11:22:12+00:00,,1,"word_tokenize doesn't tokenize ""»""","Hi all,
we tried to use nltk for the part-of-speech tagging of german texts and in some of them the french quotation marks ""»"" and ""«"" are used for quotation instead of "" "" "", the normal (or standard) quotation mark.

As an example consider the following sentence:

»Now that I can do.«

This is tokenized:
nltk.word_tokenize(""»Now that I can do.«"")

and yields:

['»Now', 'that', 'I', 'can', 'do.«']

instead of:

['»' , 'Now', 'that', 'I', 'can', 'do', '.', '«']

Can this be fixed?

Best regards.

Edit: Minor fix.
"
107,https://github.com/nltk/nltk/issues/1438,1438,[],closed,2016-07-22 18:27:56+00:00,,1,"Can I navigate through large amounts of semantic terrain? (May be an enhancement, may be user education, user education is welcome)","I wrote in http://stackoverflow.com/questions/38525854/how-can-i-ask-the-nltk-to-have-synonyms-be-connected-to-nearby-terms-rather-tha :

> I am working on an NLTK project, intended in principle to be like a standard thesaurus but (quasi-)continuous. To take one example, there are dozens of entries connected with books, including both religious classics and ledgers.
> 
> I tried fiddling with some terms, but I seemed to get just a smaller slice of the pie by doing that. (A ""ledger"" result contained ""daybook"" but the substances was a much smaller collection than one would find by reading a book.) The discussion of ""synsets"" in the documentation seem to imply both that you can find terms close to an existing terms, but the synsets are like islands, or see such to me.
> 
> What (if any) means are there to say something like ""I want all words with a high match score above XYZ threshold"" or ""I want to match the n closest related terms."" The documentation looks like this is possible, with a really nice way of calculating a proximity score between two words, but don't see how to adjust the threshold or alternately how to request the n closest matches.
> 
> What are my best bets here?

I've gotten one reply, basically asking what I meant:

> Hm I don't really follow your project. What exactly are you trying to achieve? Are you just looking for (near)synonyms? 

I responded:

> I'm trying to do something like [Visual Thesaurus](https://www.visualthesaurus.com/). In other words, you start with one term, and you can navigate to other terms with increasingly different meanings. The equivalent for roads and terrain is that you can drive on paved roads and go from the state of Washington to Florida, or California to New York, traversing only paved roads (no Jeep 4x4 needed). Imagine a much sparser graph of roads where Illinois roads wouldn't take you out, and there were actually [no fewer than] two or three connected components. (Can you see why I'd like a much denser graph?)

I'd like to request an enhancement of a de-partitioning of synonym terrain, or pointing me to another technology that covers the terrain with a web of roads, or explaining to me what I am missing here and failing to recognize something that NLTK has down cold.

Thanks,
[C.J.S. Hayward](https://CJSHayward.com)
"
108,https://github.com/nltk/nltk/issues/1439,1439,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-07-25 19:47:20+00:00,,1,[SMT] IBM Model 1 Smoothing EM,"State of the art word alignment involves controlling the amount of over-fitting EM does on the training set by heuristically, or through some Bayesian methods initializing prior probabilities on model parameters and doing smoothing.

I would like to work on this if possible.

Relevant papers:
http://www.aclweb.org/anthology/P00-1056
http://www.aclweb.org/anthology/P04-1066
http://www.cs.rochester.edu/~gildea/pubs/riley-gildea-tr10.pdf
"
109,https://github.com/nltk/nltk/issues/1440,1440,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-07-25 19:51:10+00:00,,1,[SMT] HMM Word Alignment,"IBM models 1-5 are implemented. Are there any plans to implement the HMM word alignment model?

I would like to work on this if possible

Related papers:
http://www.aclweb.org/anthology/C96-2141
"
110,https://github.com/nltk/nltk/issues/1441,1441,[],closed,2016-07-25 20:09:53+00:00,,4,NLTK Installation Failure,"Hi all, I am completely new to the use of NLTK and Python, and I would really appreciate some help on the basics.  I currently have Python 2.7 installed on the OSX El Captain.  When I key in 

> sudo pip install -U nltk
> ""SyntaxError: invalid syntax""

as advised by the website.  That is what I kept on getting, which means I could not install NLTK by any means. 

I shall be grateful for any advice/comments.  Thanks in advance guys!
"
111,https://github.com/nltk/nltk/issues/1442,1442,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-07-26 04:11:47+00:00,,0,nltk.sem.boxer Shell Injection,"Boxer._call is vulnerable to Shell Injection. It is called with user input in the main execution of the file. In this context, an attacker that controls input_str can execute arbitrary commands.
[nltk/sem/boxer.py#L215-L216](https://github.com/nltk/nltk/blob/53bf8a6baee87c6210868719a47d28804c8b45fc/nltk/sem/boxer.py#L215-L216)

``` python
            cmd = 'echo ""%s"" | %s %s' % (input_str, binary, ' '.join(args))
            p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
```
"
112,https://github.com/nltk/nltk/issues/1443,1443,[],closed,2016-08-02 04:53:55+00:00,,2,Java issues in using StanfordParser,"Usage in program:

from nltk.parse.stanford import StanfordParser
# Initialize the parser

path_to_jar = project_path+'Parsers/sfcn/stanford-corenlp-3.5.2.jar'
path_to_models_jar = project_path+'Parsers/sfcn/stanford-corenlp-3.5.2-models.jar'
parser = StanfordParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)
message=""This is a test message""
parse = parser.raw_parse(message)

I get the following error when I am using StanfordParser:

Error1:

Exception in thread ""main"" java.lang.UnsupportedClassVersionError: edu/stanford/nlp/parser/lexparser/LexicalizedParser : Unsupported major.minor version 52.0
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
    at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
##     at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:482)

Error 2:
File ""/usr/local/lib/python2.7/dist-packages/nltk/parse/stanford.py"", line 132, in raw_parse
    return next(self.raw_parse_sents([sentence], verbose))
  File ""/usr/local/lib/python2.7/dist-packages/nltk/parse/stanford.py"", line 150, in raw_parse_sents
    return self._parse_trees_output(self._execute(cmd, '\n'.join(sentences), verbose))
  File ""/usr/local/lib/python2.7/dist-packages/nltk/parse/stanford.py"", line 216, in _execute
    stdout=PIPE, stderr=PIPE)
  File ""/usr/local/lib/python2.7/dist-packages/nltk/internals.py"", line 134, in java
    raise OSError('Java command failed : ' + str(cmd))
OSError: Java command failed : [u'/usr/bin/java', u'-mx1000m', '-cp', '/home/ravisekhar/DS/Car diagnosis/Parsers/sfcn/stanford-corenlp-3.5.2-models.jar:/home/ravisekhar/DS/Car diagnosis/Parsers/sfcn/stanford-corenlp-3.5.2.jar:/home/ravisekhar/DS/Car diagnosis/Parsers/sfcn/stanford-parser-3.5.2-sources.jar:/home/ravisekhar/DS/Car diagnosis/Parsers/sfcn/stanford-parser-3.5.2-javadoc.jar:/home/ravisekhar/DS/Car diagnosis/Parsers/sfcn/ejml-0.23.jar:/home/ravisekhar/DS/Car diagnosis/Parsers/sfcn/stanford-corenlp-3.5.2-models.jar:/home/ravisekhar/DS/Car diagnosis/Parsers/sfcn/backup/stanford-corenlp-3.5.2.jar:/home/ravisekhar/DS/Car diagnosis/Parsers/sfcn/backup/stanford-corenlp-3.5.2-models.jar', u'edu.stanford.nlp.parser.lexparser.LexicalizedParser', u'-model', u'edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz', u'-sentences', u'newline', u'-outputFormat', u'penn', u'-encoding', u'utf8', '/tmp/tmpRnLFOZ']
"
113,https://github.com/nltk/nltk/issues/1445,1445,[],closed,2016-08-04 10:00:29+00:00,,1,nltk.cluster.gaac demo error,"I ran this method under python3, but it throws following exception:

``` python
>>> import nltk.cluster.gaac
>>> nltk.cluster.gaac.demo()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/nltk/cluster/gaac.py"", line 150, in demo
    clusters = clusterer.cluster(vectors, True)
  File ""/usr/local/lib/python3.5/dist-packages/nltk/cluster/gaac.py"", line 41, in cluster
    return VectorSpaceClusterer.cluster(self, vectors, assign_clusters, trace)
  File ""/usr/local/lib/python3.5/dist-packages/nltk/cluster/util.py"", line 57, in cluster
    self.cluster_vectorspace(vectors, trace)
  File ""/usr/local/lib/python3.5/dist-packages/nltk/cluster/gaac.py"", line 79, in cluster_vectorspace
    self.update_clusters(self._num_clusters)
  File ""/usr/local/lib/python3.5/dist-packages/nltk/cluster/gaac.py"", line 99, in update_clusters
    clusters = self._dendrogram.groups(num_clusters)
  File ""/usr/local/lib/python3.5/dist-packages/nltk/cluster/util.py"", line 212, in groups
    return root.groups(n)
  File ""/usr/local/lib/python3.5/dist-packages/nltk/cluster/util.py"", line 160, in groups
    queue.sort()
TypeError: unorderable types: _DendrogramNode() < _DendrogramNode()
```
"
114,https://github.com/nltk/nltk/issues/1446,1446,[],closed,2016-08-05 07:48:56+00:00,,1,Contributing stopwords for Slovene,"I've tried to find the proper folder/file to contribute my list of Slovenian stopwords to, but after an extensive search I wasn't able to find anything.
Where is the proper folder to submit the list of stopwords to and what is the desired format?
Thanks!
"
115,https://github.com/nltk/nltk/issues/1448,1448,[],closed,2016-08-07 00:53:05+00:00,,1,Problem installing NLTK data on mac,"Hi,

I am not able to install NLTK data from NLTK Downloader. I have Python 3.5.2 and NLTK 3.2.1.

A screenshot of NLTK Downloader with the error message is attached.

Any help is very much appreciated. Thank you very much.

![screen shot 2016-08-06 at 8 47 09 pm](https://cloud.githubusercontent.com/assets/1740553/17459814/a254a904-5c17-11e6-9e93-52073a28d034.png)
"
116,https://github.com/nltk/nltk/issues/1449,1449,[],closed,2016-08-09 13:37:21+00:00,,2,Where can I download the NLTK data as a zip file?,"Where can I download the NLTK data as a zip file?

Thanks.
"
117,https://github.com/nltk/nltk/issues/1451,1451,[],closed,2016-08-10 02:22:38+00:00,,1,Segfault while trying to learn NLTK package,"I was going through the NLTK book and was testing out plotting on NLTK when I got a strange segfault. Not sure if this is NLTK, or something else...
[py-seg-fault.txt](https://github.com/nltk/nltk/files/410216/py-seg-fault.txt)
"
118,https://github.com/nltk/nltk/issues/1457,1457,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 25396640, 'node_id': 'MDU6TGFiZWwyNTM5NjY0MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/book', 'name': 'book', 'color': 'e102d8', 'default': False, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",closed,2016-08-27 09:05:41+00:00,,6,Axe FreqDist ordering/comparisons?,"This [StackOverflow question](http://stackoverflow.com/q/38215686/4501212) made me aware that the `FreqDist` class implements ordering comparisons. As the question shows, the implementation is buggy so we should do something about it. 
I propose we get rid of it altogether, considering that its parent class `Counter` doesn't support comparisons and that it's not entirely clear what it means for one frequency distribution to be ""greater than"" another.

Thoughts?
"
119,https://github.com/nltk/nltk/issues/1458,1458,[],closed,2016-08-29 05:21:39+00:00,,7,blue_score:: modified_precision() zero division exception ,"Issue:
In modified_precision(), there is no check if the Counter has 0 results - if there are no ngrams in the hypothesis, a zero division exception is thrown.

Code to reproduce:

```
from nltk.translate import bleu as bleu_score

hypothesis = ['1', '2', '3']
references = [['1', '2', '3', '4']]
bleu_score(references=references, hypothesis=hypothesis)
```

Exception output:

```
Traceback (most recent call last):
  File ""bug.py"", line 6, in <module>
    bleu_score(references=references, hypothesis=hypothesis)
  File ""/home/roei/anaconda2/lib/python2.7/site-packages/nltk/translate/bleu_score.py"", line 79, in sentence_bleu
    return corpus_bleu([references], [hypothesis], weights, smoothing_function)
  File ""/home/roei/anaconda2/lib/python2.7/site-packages/nltk/translate/bleu_score.py"", line 146, in corpus_bleu
    p_i = modified_precision(references, hypothesis, i)
  File ""/home/roei/anaconda2/lib/python2.7/site-packages/nltk/translate/bleu_score.py"", line 287, in modified_precision
    return Fraction(numerator, denominator, _normalize=False)  
  File ""/home/roei/anaconda2/lib/python2.7/site-packages/nltk/compat.py"", line 700, in __new__
    cls = super(Fraction, cls).__new__(cls, numerator, denominator)
  File ""/home/roei/anaconda2/lib/python2.7/fractions.py"", line 162, in __new__
    raise ZeroDivisionError('Fraction(%s, 0)' % numerator)
ZeroDivisionError: Fraction(0, 0)

```
"
120,https://github.com/nltk/nltk/issues/1463,1463,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",closed,2016-09-01 11:35:05+00:00,,1,NLTK 3.2.1: Deprecated Warning with Python 3.5.2,"Hi,
I'm learning Python.
For my first program in Python, I downloaded Chatterbot 0.4.6 that contained nltk module. 

This program doesn't work because of nltk/decorators.py generates a Deprecated Warning (inspect.getargspec). I installed last version of NLTK with the process explained in the website www.nltk.org but I have the same problem.
This is the result when I try to run the program Chatterbot 0.4.6:

Warning (from warnings module):
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/decorators.py"", line 59
    regargs, varargs, varkwargs, defaults = inspect.getargspec(func)
DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead

Can you help me?
"
121,https://github.com/nltk/nltk/issues/1465,1465,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-09-10 18:23:03+00:00,,2,Python 3. WordNet region_domains() not always returns results.  ,"> > > from nltk.corpus import wordnet as wn
> > > wn.synset('center_of_gravity.n.01').region_domains()
> > > []
> > > wn.synset('pukka.s.01').region_domains()
> > > [Synset('india.n.01')]

Expected: to return a list on Center of Gravity
Canada, Britain

http://wordnetweb.princeton.edu/perl/webwn?o2=1&o0=1&o8=1&o1=1&o7=1&o5=1&o9=&o6=1&o3=1&o4=1&s=center+of+gravity&i=3&h=1000#c
"
122,https://github.com/nltk/nltk/issues/1466,1466,"[{'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",closed,2016-09-10 20:47:04+00:00,,7,Better document how to run the tests,"When I run `tox` on my Ubuntu 14.04 machine, before giving up I get:
- a bunch of failures
- a bunch of `LookupError`s
- loads of tracebacks with errors saying `NameError: global name 'pyparsing' is not defined`
- a bunch of other NameErrors
- tracebacks with errors saying `OSError: Senna executable expected at /usr/share/senna-v3.0/senna-linux64 but not found`
- multiple screens of `nltk_data` downloads that still haven't finished after many hours

I've consequently given up ensuring that the tests in [my PR](https://github.com/nltk/nltk/pull/1261) can run as part of the `tox` run, and will just try to run them individually. But shouldn't the process for running the tests be less painful than this? The CONTRIBUTING.md guidelines say

> just execute `tox`

and that's what I'd expect to have to do; the reality is that the tests seem basically unrunnable. Do the core contributors have development setups that let them run the full test suite easily? If so, what're they doing differently to me? And what output should I expect to see when I run `tox`?
"
123,https://github.com/nltk/nltk/issues/1471,1471,[],closed,2016-09-17 19:40:29+00:00,,1,how to run  example program from cloned repository,"unable to find runnable file of any of examples program
"
124,https://github.com/nltk/nltk/issues/1472,1472,[],closed,2016-09-20 09:56:50+00:00,,2,The sent tokenizer deletes end of line characters,"I am not sure whether this is a feature/bug, but the standard nltk.sent_tokenize() deletes characters from the text. 

For instance:

```
s
'In the beginning, God created the heavens and the earth. And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters.'
from nltk import sent_tokenize
sents = sent_tokenize(s)
sents
['In the beginning, God created the heavens and the earth.',
 'And the earth was without form, and void; and darkness was upon the face of the deep.',
 'And the Spirit of God moved upon the face of the waters.']
len(''.join(sents))
197
len(s)
199
```

Is there a way to make sure the tokenizer does not delete characters?
"
125,https://github.com/nltk/nltk/issues/1474,1474,[],closed,2016-09-22 10:06:55+00:00,,0,Unused argument in nltk.sem.relextract.conllned,"The changes in the following commit introduced a bug into `conllned()`.

https://github.com/nltk/nltk/commit/3e73834a2f18f9956c3e11466a49a9354c3924c7#diff-96e974114c1e7b1efc5976f2c387deadR409

The arguments to `show_raw_rtuple` are hardcoded to `True`, which makes `lcon` and `rcon` unused, which in turn makes the argument `trace` effectively unused.

It's not clear whether the hardcoded arguments are the intended behaviour (in which case the correct solution would be to remove the unused variables and function argument) or whether that change is a bug and the line should read `print show_raw_rtuple(rel, lcon=lcon, rcon=rcon)` as it was originally written.
"
126,https://github.com/nltk/nltk/issues/1476,1476,[],closed,2016-09-27 05:22:00+00:00,,3,NLTK fails to find the Java file,"when i use stanford word segmenter ,  here is code :
import nltk
from nltk.tokenize.stanford_segmenter import StanfordSegmenter

segmenter = StanfordSegmenter(
    path_to_jar = ""./stanford_segmenter/stanford-segmenter-3.6.0.jar"", 
    path_to_sihan_corpora_dict = ""./stanford_segmenter/data"", 
    path_to_model = ""./stanford_segmenter/data/pku.gz"", 
    path_to_dict = ""./stanford_segmenter/data/dict-chris6.ser.gz"")

sentence = u""这里是斯坦福分词器测试""

result = segmenter.segment(sentence)
print result

the error as follows :
NLTK was unable to find the java file!
Use software specific configuration paramaters or set the JAVAHOME environment variable.

i have set the javehome enviroment , but it also doesn't work
"
127,https://github.com/nltk/nltk/issues/1479,1479,"[{'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-10-12 02:48:29+00:00,,4,Cannot import nltk after installation,"The package is installed in `/Library/Python/2.7/lib/python/site-packages` but I cannot import nltk. Please advice
"
128,https://github.com/nltk/nltk/issues/1481,1481,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-10-16 10:49:44+00:00,,0,Is this a bug? Split on a colon.,"I can make a chunk from a colon but I cannot split at a colon. Am I doing something wrong?

This sees the colon and adds to chunk

```
from nltk import  RegexpParser

grammar = r""""""
  NP: {<DT|PP\$>?<JJ>*<NN>|<NNP.*><\:><VBD>}   # chunk (Rapunzel + : + let) together
    {<NNP>+}                
    <.*>}{<VBD.*>           


""""""
cp = RegexpParser(grammar)
sentence = [(""Rapunzel"", ""NNP""), ("":"","":""), (""let"", ""VBD""), (""down"", ""RP""), (""her"", ""PP$""), (""long"", ""JJ""), (""golden"", ""JJ""), (""hair"", ""NN"")]

print(cp.parse(sentence))
```

When I try and split at colon I get a error:

```
from nltk import  RegexpParser

grammar = r""""""
  NP: {<DT|PP\$>?<JJ>*<NN>|<NNP.*><\:><VBD>}   # chunk (Rapunzel + : + let) together
    {<NNP>+}                
    <.*>}{<\:.*>           


""""""
cp = RegexpParser(grammar)
sentence = [(""Rapunzel"", ""NNP""), ("":"","":""), (""let"", ""VBD""), (""down"", ""RP""), (""her"", ""PP$""), (""long"", ""JJ""), (""golden"", ""JJ""), (""hair"", ""NN"")]

print(cp.parse(sentence))
```

ValueError: Illegal chunk pattern: >
"
129,https://github.com/nltk/nltk/issues/1482,1482,[],closed,2016-10-17 10:05:18+00:00,,1,Text tilling,"Can I use text tilling for searching boundaries of sentences among continuous text without punctuation? What is then supposed to be a paragraph?
"
130,https://github.com/nltk/nltk/issues/1483,1483,[],closed,2016-10-18 21:58:51+00:00,,1,intent parser from sentence,"I want to create a ChatBot, and I'm planning on using the NLTK libs for parsing the messages from the user, but **how can I detect the user's intent.** like below

Hello!                           -> greeting
Hi <chatbot's name>   -> greeting
Tell me the weather      -> weather
Is it raining?                  -> weather

Is there any option to identify intent action by using NLTK?
"
131,https://github.com/nltk/nltk/issues/1484,1484,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",closed,2016-10-20 14:20:36+00:00,,1,ipipan corpus not found and NKJP,"Running `tox -e py35` I get the following error:

```
ERROR: Failure: LookupError (
**********************************************************************
  Resource 'corpora/ipipan' not found.  Please use the NLTK
  Downloader to obtain the resource:  >>> nltk.download()
```

But the `ipipan` corpus is not available on nltk_data:

```
>>> nltk.download('ipipan')
[nltk_data] Error loading ipipan: Package 'ipipan' not found in index
```

I know there are several open issues and references to this corpus and `nkjp`:
- #913 (Issue: ipipan module looks dead)
- #844 (Pull Request: NKJP Corpus Reader)
- nltk/nltk_data#17 (Pull Request: NKJP corpus on nltk_data)
- [National Corpus of Polish](https://groups.google.com/forum/#!topic/nltk-dev/VMMCXejQJMQ) (on NLTK-dev)

@stevenbird 
What should we do with ipipan and NKJP modules? They are currently breaking `python3.5` tests and nltk_data does not have corpora for them.

Also note: I do not know why tests with previous python versions do not automatically break.
"
132,https://github.com/nltk/nltk/issues/1486,1486,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2016-10-24 08:39:25+00:00,,1,Training on PerceptronTagger with alpino throws TypeError,"With `BigramTagger`:

``` python
>>> from nltk.corpus import alpino as alp
>>> from nltk.tag import UnigramTagger, BigramTagger
>>> training_corpus = alp.tagged_sents()
>>> unitagger = UnigramTagger(training_corpus)
>>> bitagger = BigramTagger(training_corpus, backoff=unitagger)
>>> pos_tag = bitagger.tag
>>> sent = 'NLTK is een goeda taal voor NLP'.split()
>>> pos_tag(sent)
[('NLTK', None), ('is', u'verb'), ('een', u'det'), ('goeda', None), ('taal', u'noun'), ('voor', u'prep'), ('NLP', None)]
```

But with `PerceptronTagger`:

``` python
>>> from nltk.tag import PerceptronTagger
>>> from nltk.corpus import alpino as alp
>>> training_corpus = alp.tagged_sents()
>>> tagger = PerceptronTagger(load=False)
>>> tagger.train(training_corpus)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Python/2.7/site-packages/nltk/tag/perceptron.py"", line 194, in train
    random.shuffle(sentences)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/random.py"", line 291, in shuffle
    x[i], x[j] = x[j], x[i]
TypeError: 'StreamBackedCorpusView' object does not support item assignment
```
"
133,https://github.com/nltk/nltk/issues/1487,1487,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-10-24 17:03:28+00:00,,1,Fix Penn Treebank corpus reader please,"Following the [nltk docs on corpus API](http://www.nltk.org/howto/corpus.html), I wanted to use nltk to parse a copy of full Penn Treebank corpus. I unzipped the required directories ""brown"" and ""wsj"" into `/home/tastyminerals/nltk_data/corpora/ptb` dir as was stated in the docs. The unzipped files were all lowercased and I had to rename them because ptb corpus reader expects them to be all in uppercase (this issue was [reported](https://github.com/nltk/nltk/issues/124) btw).

After that I am trying to extract a parse tree from a particular file. Using, again, the example from the docs.

```
from nltk.corpus import treebank, ptb
treebank.parsed_sents('wsj_0003.mrg')[0])   # notice lowecase filename
> Tree('S', [Tree('S-TPC-1', [Tree('NP-SBJ', [Tree('NP', [Tree('NP'...
```

Works well, for the sample treebank included with nltk.
However, trying to perform the same operation with **ptb** fails.

```
ptb.parsed_sents('WSJ_0003.MRG')[0])  # notice the uppercase because we had to rename them
> No such file or directory: '/home/tastyminerals/nltk_data/corpora/ptb/WSJ_0003.MRG'
```

Well of course there is no such file, the file is in `ptb/WSJ/00/WSJ_0003.MRG`.
Why does **ptb** is not aware of the Penn Treebank directory structure? Moreover, why is it case sensitive in the first place? This needs to be fixed. The docs also need to be updated accordingly.
"
134,https://github.com/nltk/nltk/issues/1489,1489,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",open,2016-10-26 09:16:42+00:00,,5,Reimplementing Monolingual Word Aligner in NLTK,"The [Monolingual Word Aligner](https://github.com/ma-sultan/monolingual-word-aligner) is an important tool in the [Semantic Textual Similarity](http://alt.qcri.org/semeval2017/task1/) task.

@amalinovskiy created a fork at https://github.com/amalinovskiy/upf-cobalt but the Stanford parser and JSON RPC is still based on outdated code.

**May I suggest that we try to re-implment it within NLTK?** The version on @ma-sultan repo is rather old and it's based on an older implementation some other Stanford CoreNLP wrapper outside of NLTK and also older NLTK versions for tokenization, etc. 

Should this be under `nltk.sem`?
"
135,https://github.com/nltk/nltk/issues/1490,1490,[],closed,2016-10-31 08:22:08+00:00,,5,"Normalizer in VADER not really bounding output to [-1.0, 1.0]","The normalizer at https://github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py#L107 is not exactly normalizing the inputs to give an output bounded by [-1.0, 1.0] as reported in the docstring. Given the default value of `alpha=15`, the upperbound of the normalized outputs would be somewhere in the between (-0.13, 0.13) 

![](https://i.stack.imgur.com/p8De8.png)

The outputs gets funkier with other `alpha` values, see http://stackoverflow.com/questions/40325980/how-is-the-compound-polarity-score-calculated-in-python-nltk-vader-approach/40337646#40337646

@cjhutto any ideas whether this is the expected output of the normalizer? Usually a normalization function would expect some sort of mean/variance input or a list of inputs. For example, I would expect the normalization to be taken corpus wide such that the value of the `sum_s` will be normalized against all the `sum_s` computed by across sentences in the corpus.  "
136,https://github.com/nltk/nltk/issues/1491,1491,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-11-01 16:27:48+00:00,,7,Bug when drawing trees.,"When I draw and show a fairly large tree, it stays on the screen for a few seconds, then gives an error, which, when running with ""python x.py"" or ""ipython x.py"" in python 2 shows this:

```
libc++abi.dylib: terminating with uncaught exception of type NSException
Abort trap: 6
```

When running with ipython with python 3, it shows this, and sometimes stays on the screen for a while while printing these errors to the terminal:

```
2016-11-01 12:19:09.788 python[9215:152385] CALayer position contains NaN: [0 nan]
```
When using ipython's qtconsole, the kernel quits after a short time.

I have a Macbook Pro running Sierra; this pattern holds with both Python 2.7.12 and Python 3.5.2

A small script to reproduce:

```

from nltk.tree import Tree
s = ""This is a rather long sentence whose tree may or may not stay on the screen for long.""
parse_tree = ""(S1 (S (NP (DT This)) (VP (VBZ is) (NP (NP (DT a) (ADJP (RB rather) (JJ long)) (NN sentence)) (SBAR (WHNP (WP$ whose) (NN tree)) (S (VP (VP (MD may)) (CC or) (VP (MD may) (RB not) (VP (VB stay) (PP (IN on) (NP (DT the) (NN screen))) (PP (IN for) (ADJP (JJ long)))))))))) (. .)))""
the_tree = Tree.fromstring(parse_tree)
the_tree.draw()
```

Run with:
`python file_name.py`
or
`ipython file_name.py`"
137,https://github.com/nltk/nltk/issues/1492,1492,[],closed,2016-11-02 06:01:40+00:00,,1,Nltk Noun Finding Bug,Hai for Noun it considers only if the first letter is Capital letter. Can you fix this please
138,https://github.com/nltk/nltk/issues/1494,1494,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-11-07 13:36:01+00:00,,5,nltk relation extraction returns nothing.,"Hi all:
     I am recently working on using nltk to extract relation from text. so i build a sample text:"" Tom is the cofounder of Microsoft."" and using following program to test and return nothing. I cannot figure out why.

I'm using NLTK version: 3.2.1, python version: 3.5.2.

Here is my code:
```
import re
import nltk
from nltk.sem.relextract import extract_rels, rtuple
from nltk.tokenize import sent_tokenize, word_tokenize


def test():
    with open('sample.txt', 'r') as f:
        sample = f.read()   # ""Tom is the cofounder of Microsoft""

    sentences = sent_tokenize(sample)
    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]
    tagged_sentences = [nltk.tag.pos_tag(sentence) for sentence in tokenized_sentences]

    OF = re.compile(r'.*\bof\b.*')

    for i, sent in enumerate(tagged_sentences):
        sent = nltk.chunk.ne_chunk(sent) # ne_chunk method expects one tagged sentence
        rels = extract_rels('PER', 'GPE', sent, corpus='ace', pattern=OF, window=10) 
        for rel in rels:
            print('{0:<5}{1}'.format(i, rtuple(rel)))

if __name__ == '__main__':
    test()
```
--------------------------------------------------------------------------------------------------------------------------------
## 1.　After some debug, if found that when i changed the input as 
> ""Gates was born in Seattle, Washington on October 28, 1955. ""
## the nltk.chunk.ne_chunk() output is:
> (S
>   (PERSON Gates/NNS)
>   was/VBD
>   born/VBN
>   in/IN
>   (GPE Seattle/NNP)
>   ,/,
>   (GPE Washington/NNP)
>   on/IN
>   October/NNP
>   28/CD
>   ,/,
>   1955/CD
>   ./.)
## The test() returns:
>  [PER: 'Gates/NNS'] 'was/VBD born/VBN in/IN' [GPE: 'Seattle/NNP']

## 2. After i changed the input as:
> ""Gates was born in Seattle on October 28, 1955. ""
## The test() retuns nothing.

## 3. I digged into **nltk/sem/relextract.py** and find this strange output is caused by function:
**semi_rel2reldict(pairs, window=5, trace=False), which returns result only when len(pairs) > 2, and that's why when one sentence with less than three NEs will return None.**

**Is this a bug or i used NLTK in wrong way?**
 
"
139,https://github.com/nltk/nltk/issues/1495,1495,[],closed,2016-11-09 13:20:54+00:00,,6,Add download directory to nltk.data.path,"**Issue:**
If user adds a new path. data not found error is raised while including data later. and user has to explicitly append path with nltk.data.path.append('/path/to/data')

**What does the feature do?**
Add the path entered on NLTK GUI in download directory, if it does not exist already.
"
140,https://github.com/nltk/nltk/issues/1498,1498,[],closed,2016-11-12 15:07:13+00:00,,1,NLTK Jenkins CI unavailable,"I used to be able to reach NLTK Jenkins Continuous Integration using this link: https://nltk.ci.cloudbees.com

However, since a few days I'm getting this error when I visit the page:
> Jenkins is unavailable
> Please see our support page for more information on raising a support request

@stevenbird do you know what could be the cause of this problem?"
141,https://github.com/nltk/nltk/issues/1504,1504,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}]",closed,2016-11-13 15:27:24+00:00,,4,Bleu tests failing,"BLEU tests are currently failing

```
tox -e py34 nltk/test/unit/translate/test_bleu.py
```
```
py34 runtests: commands[1] | python runtests.py unit/translate/test_bleu.py
.........E
======================================================================
ERROR: test_corpus_bleu (nltk.test.unit.translate.test_bleu.TestBLEUvsMteval13a)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""nltk/test/unit/translate/test_bleu.py"", line 187, in test_corpus_bleu
    nltk_bleu = corpus_bleu(references, hypothesis, weights=(1.0/i,)*i)
  File ""nltk/translate/bleu_score.py"", line 139, in corpus_bleu
    assert len(list_of_references) == len(hypotheses), ""The number of hypotheses and their reference(s) should be the same""
TypeError: object of type 'map' has no len()

----------------------------------------------------------------------
Ran 10 tests in 0.006s

FAILED (errors=1)
```

@alvations Could you please check this issue, if you have time?

P.S. If other test failures or import errors get in the way, you might find it useful to create a new branch and merge the following pending PRs: #1497, #1499, #1500, #1501"
142,https://github.com/nltk/nltk/issues/1506,1506,[],closed,2016-11-14 19:49:12+00:00,,2,Is NLTK FrameNet support frozen at FN version 1.5?,"In #719 there was a question regarding support for newer versions of FrameNet beyond 1.5, yet there didn't appear to be any resolution although the issue is closed.  Is there a way to load newer versions of FrameNet (I have 1.7 burning a hole on my hard drive)?

Thanks!"
143,https://github.com/nltk/nltk/issues/1507,1507,[],closed,2016-11-14 22:44:25+00:00,,3,nltk.data.path not working,"```
import nltk
nltk.data.path.append('nltk_data')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk.classify.util
from nltk.classify import NaiveBayesClassifier
```

I am trying to change path but i continues to search at home location of nltk data... any idea why?"
144,https://github.com/nltk/nltk/issues/1509,1509,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-11-18 01:28:42+00:00,,0,default verbosity,90+% of verbose flags in the codebase default to `False` while less than 10% default to `True`. I think they should all default to `False` except in demonstration code.
145,https://github.com/nltk/nltk/issues/1510,1510,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 159235791, 'node_id': 'MDU6TGFiZWwxNTkyMzU3OTE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/dependency%20parsing', 'name': 'dependency parsing', 'color': 'eb6420', 'default': False, 'description': None}]",closed,2016-11-18 10:21:47+00:00,,31,"DependencyGraph or Stanford Parser API issues with sentences with ""/""","A user has reported that this sentence throws and `AssertionError` when using Stanford's `DependencyParser` API in NLTK for this sentence:

> for all of its insights into the dream world of teen life , and its electronic expression through cyber culture , the film gives no quarter to anyone seeking to pull a cohesive story out of its 2 1/2-hour running time . 

Code:

```python
>>> from nltk.parse.stanford import StanfordDependencyParser                                                                                       >>> dep_parser=StanfordDependencyParser(model_path=""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"")                                        
>>> sent = 'for all of its insights into the dream world of teen life , and its electronic expression through cyber culture , the film gives no quarter to anyone seeking to pull a cohesive story out of its 2 1/2-hour running time . '
>>> dep_parser.raw_parse(sent)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Python/2.7/site-packages/nltk/parse/stanford.py"", line 132, in raw_parse
    return next(self.raw_parse_sents([sentence], verbose))
  File ""/Library/Python/2.7/site-packages/nltk/parse/stanford.py"", line 150, in raw_parse_sents
    return self._parse_trees_output(self._execute(cmd, '\n'.join(sentences), verbose))
  File ""/Library/Python/2.7/site-packages/nltk/parse/stanford.py"", line 91, in _parse_trees_output
    res.append(iter([self._make_tree('\n'.join(cur_lines))]))
  File ""/Library/Python/2.7/site-packages/nltk/parse/stanford.py"", line 339, in _make_tree
    return DependencyGraph(result, top_relation_label='root')
  File ""/Library/Python/2.7/site-packages/nltk/parse/dependencygraph.py"", line 84, in __init__
    top_relation_label=top_relation_label,
  File ""/Library/Python/2.7/site-packages/nltk/parse/dependencygraph.py"", line 328, in _parse
    assert cell_number == len(cells)
AssertionError
```

Possibly, it might be how `DependencyGraph` is reading the output or that the Stanford output is inconsistent.  

More details on the setup for NLTK + Stanford tools is on https://gist.github.com/alvations/e1df0ba227e542955a8a#stanford-parser"
146,https://github.com/nltk/nltk/issues/1515,1515,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",closed,2016-11-20 14:14:56+00:00,,3,ChrF score failing tests,"ChrF score tests are failing with python 3.4 and python 3.5:

```
tox -e py34 nltk/nltk/translate/chrf_score.py

GLOB sdist-make: nltk/setup.py
py34 inst-nodeps: nltk/.tox/dist/nltk-3.2.1.zip
py34 installed: coverage==4.2,nltk==3.2.1,nose==1.3.7,numpy==1.11.2,oauthlib==2.0.0,pyparsing==2.1.10,python-crfsuite==0.8.4,requests==2.12.1,requests-oauthlib==0.7.0,scikit-learn==0.18.1,scipy==0.18.1,six==1.10.0,text-unidecode==1.0,twython==3.4.0
py34 runtests: PYTHONHASHSEED='300012027'
py34 runtests: commands[0] | pip install scipy scikit-learn
Requirement already satisfied: scipy in nltk/.tox/py34/lib/python3.4/site-packages
Requirement already satisfied: scikit-learn in nltk/.tox/py34/lib/python3.4/site-packages
py34 runtests: commands[1] | python runtests.py ../translate/chrf_score.py
.F
======================================================================
FAIL: Doctest: nltk.translate.chrf_score.sentence_chrf
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/python3.4/doctest.py"", line 2187, in runTest
    raise self.failureException(self.format_failure(new.getvalue()))
AssertionError: Failed doctest test for nltk.translate.chrf_score.sentence_chrf
  File ""nltk/nltk/translate/chrf_score.py"", line 16, in sentence_chrf

----------------------------------------------------------------------
File ""nltk/nltk/translate/chrf_score.py"", line 58, in nltk.translate.chrf_score.sentence_chrf
Failed example:
    type(ref1), type(hyp1)
Expected:
    (<type 'str'>, <type 'str'>)
Got:
    (<class 'str'>, <class 'str'>)


----------------------------------------------------------------------
Ran 2 tests in 0.013s

FAILED (failures=1)
```

@alvations could you please check?"
147,https://github.com/nltk/nltk/issues/1516,1516,[],closed,2016-11-21 03:23:02+00:00,,7,Missing modules from nltk.metrics in Python 3.5 ??,"Hey, I'm trying to compute

nltk.metrics.distance.edit_distance

in v3.5, but I'm seeing ""module 'nltk.translate.metrics' has no attribute 'distance'""

Can you help?
Thx!"
148,https://github.com/nltk/nltk/issues/1517,1517,[],closed,2016-11-21 16:37:21+00:00,,2,panlex_lite is failing,"As has been reported previously I am having panlex_lite fail. 

I downloaded the zip file manually (wget) and unzip manually the error I get from this is:
$ unzip panlex_lite.zip 
Archive:  panlex_lite.zip
warning [panlex_lite.zip]:  76 extra bytes at beginning or within zipfile
  (attempting to process anyway)
error [panlex_lite.zip]:  reported length of central directory is
  -76 bytes too long (Atari STZip zipfile?  J.H.Holm ZIPSPLIT 1.1
  zipfile?).  Compensating...
   skipping: panlex_lite/db.sqlite   need PK compat. v4.5 (can do v2.1)
   creating: panlex_lite/
  inflating: panlex_lite/README.txt  

note:  didn't find end-of-central-dir signature at end of central dir.
  (please check that you have transferred or created the zipfile in the
  appropriate BINARY mode and that you have compiled UnZip properly)

"
149,https://github.com/nltk/nltk/issues/1518,1518,[],closed,2016-11-22 05:13:30+00:00,,5,Coding error in using Stanford parser for Chinese!,"Hi, there! Greetings!

I find that when using nltk import the Stanford parser for Chinese. It returns me an error. Here is my code:
```
# -*- coding:utf-8 -*-
import os
from nltk.parse import stanford 

os.environ['STANFORD_PARSER'] = os.path.join('stanford-parser-full', 'stanford-parser.jar')
os.environ['STANFORD_MODELS'] = os.path.join('stanford-parser-full', 'stanford-parser-3.6.0-models.jar')
java_path = ""C:\\Program Files\\Java\\jdk1.8.0_73\\bin\\java.exe""
os.environ['JAVAHOME'] = java_path

string = '俄罗斯 希望 伊朗 没有 制造 核武器 计划'.decode('utf-8')
parser = stanford.StanfordParser(model_path='edu/stanford/nlp/models/lexparser/chineseFactored.ser.gz')
result = list(parser.parse(string.split()))

print(result)
```

And the return msg:

Traceback (most recent call last):
  File ""E:\workspaces\python\tinker\stanford_parser.py"", line 27, in <module>
    result = list(parser.parse(string.split()))
  File ""C:\Anaconda2\lib\site-packages\nltk\parse\api.py"", line 45, in parse
    return next(self.parse_sents([sent], *args, **kwargs))
  File ""C:\Anaconda2\lib\site-packages\nltk\parse\stanford.py"", line 120, in parse_sents
    cmd, '\n'.join(' '.join(sentence) for sentence in sentences), verbose))
  File ""C:\Anaconda2\lib\site-packages\nltk\parse\stanford.py"", line 224, in _execute
    stdout = stdout.decode(encoding)
  File ""C:\Anaconda2\lib\encodings\utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode bytes in position 161-162: invalid continuation byte

Then, I checked the code. I find the following code in function _execute() in stanford.py:

```
stdout = stdout.replace(b'\xc2\xa0',b' ')
stdout = stdout.replace(b'\xa0',b' ')
```

I thought that the problem is here. b' ' cannot be decoded by utf-8. So, I change the code to:

```
if not encoding == 'utf8':
      stdout = stdout.replace(b'\xc2\xa0',b' ')
      stdout = stdout.replace(b'\xa0',b' ')
```

It works well!"
150,https://github.com/nltk/nltk/issues/1519,1519,"[{'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-11-22 16:21:04+00:00,,0,Support PCFG grammar in RecursiveDescentApp,"I have a PCFG grammar and I would like to use the RecursiveDescentApp here:

https://github.com/nltk/nltk/blob/640d39af8d1195a0349f97efa81f535956c28328/nltk/app/rdparser_app.py

If I try to load this grammar in the app I get the error

```
Traceback (most recent call last):
  File ""wordstree.py"", line 906, in <module>
    app()
  File ""wordstree.py"", line 903, in app
    RecursiveDescentApp(grammar, sent).mainloop()
  File ""wordstree.py"", line 120, in __init__
    self._init_grammar(self._top)
  File ""wordstree.py"", line 166, in _init_grammar
    self._productions = list(self._parser.grammar().productions())
AttributeError: PCFG instance has no attribute 'productions'
```

My PCFG grammar comes from TreeBank learn.
How to adapt the demo to support PCFG rather than CFG grammar?

"
151,https://github.com/nltk/nltk/issues/1520,1520,"[{'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-11-22 16:35:12+00:00,,2,Write tests for warnings,"This issue is to keep track of what has been discussed in #1503.

It would be good to test that warnings are actually displayed in all those cases where NLTK provides users with warnings.

E.g.: since `tox` tests will not install many optional dependencies, we could check that a warning is displayed when some module tries to use an uninstalled package (see for example [`twitter/__init__.py`][1]). 

[1]: https://github.com/nltk/nltk/blob/develop/nltk/twitter/__init__.py#L16"
152,https://github.com/nltk/nltk/issues/1523,1523,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 159235791, 'node_id': 'MDU6TGFiZWwxNTkyMzU3OTE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/dependency%20parsing', 'name': 'dependency parsing', 'color': 'eb6420', 'default': False, 'description': None}]",closed,2016-11-24 09:28:36+00:00,,2,whitespace in corenlp_options in nltk.parse.stanford causes OSError,"I needed to keep punctuation, so I added corenlp_options='-ouputFormatOptions includePunctuationDependencies' when creating an instance of a dependency parser.

Because corenlp_options is appended to cmd, the Popen fails.  Changing `cmd.append(self.corenlp_options)` to `cmd.extend(self.corenlp_options.split())` would fix the issue.

Edit:
The Neural Dependency Parser has a default corelnp_options that contains whitespace, and it works fine.  note that because it uses += to extend, that there is no whitespace inserted between user-defined options and default options.  

I don't know why NDP runs fine with whitespace in corenlp_options and StanfordDependencyParser does not."
153,https://github.com/nltk/nltk/issues/1524,1524,[],closed,2016-11-24 10:38:11+00:00,,4,NoneType() < bool() error when doing pretty format of nltk decision tree classifier in python3,"<classifier>.pretty_format() works fine with Python2. But this is the error I get with python3:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-629-dc64618292e3> in <module>()
----> 1 classifier.pretty_format()

/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/classify/decisiontree.py in pretty_format(self, width, prefix, depth)
     84             return '%s%s %s\n' % (prefix, '.'*n, self._label)
     85         s = ''
---> 86         for i, (fval, result) in enumerate(sorted(self._decisions.items())):
     87             hdr = '%s%s=%s? ' % (prefix, self._fname, fval)
     88             n = width-15-len(hdr)

TypeError: unorderable types: NoneType() < bool()"
154,https://github.com/nltk/nltk/issues/1525,1525,"[{'id': 719374994, 'node_id': 'MDU6TGFiZWw3MTkzNzQ5OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/GUI', 'name': 'GUI', 'color': 'f9b3d9', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-11-26 17:43:24+00:00,,0,named fonts and default fonts for GUI,"Its better to use custom named fonts to make an abstraction of the font and to use the default font when the required font doesnt exists.
The font helvetica is not installed on linux by default. The same happens with the font courier.

I am using linux mint, and the downloader GUI looks very ugly for this reason. "
155,https://github.com/nltk/nltk/issues/1526,1526,[],closed,2016-11-27 21:25:54+00:00,,2,"Circular imports between probability, text and utils modules","I am running into circular import problems while preparing a PR to add some functionality to `nltk/text.py` - specifically, I am trying to write a new class `TextFreqDist` in `nltk/probability.py`, which extends `FreqDist` with some custom features, such as ignoring non-alphanumeric characters. I want to use `nltk.text.Text` in `nltk/probability.py`, but this leads to an import error:

    ---------------------------------------------------------------------------
    ImportError                               Traceback (most recent call last)
    <ipython-input-2-16ae48a68ad9> in <module>()
    ----> 1 import nltk
          2 from nltk.text import Text
          3 from nltk.probability import TextFreqDist
          4 import nltk.corpus

    /Users/srm/Documents/sandeep/cst/dev/nltk/nltk/__init__.py in <module>()
        112 # Import top-level functionality into top-level namespace
        113 
    --> 114 from nltk.collocations import *
        115 from nltk.decorators import decorator, memoize
        116 from nltk.featstruct import *
    
    /Users/srm/Documents/sandeep/cst/dev/nltk/nltk/collocations.py in <module>()
         35 from nltk.compat import iteritems
         36 
    ---> 37 from nltk.probability import FreqDist
         38 from nltk.util import ngrams
     39 from nltk.metrics import ContingencyMeasures, BigramAssocMeasures, TrigramAssocMeasures
    
    /Users/srm/Documents/sandeep/cst/dev/nltk/nltk/probability.py in <module>()
         48 from functools import reduce
         49 from nltk import compat
    ---> 50 from nltk.text import Text
         51 from nltk.compat import Counter
         52 
    
    /Users/srm/Documents/sandeep/cst/dev/nltk/nltk/text.py in <module>()
         22 import re
         23 
    ---> 24 from nltk.probability import FreqDist, LidstoneProbDist
         25 from nltk.probability import ConditionalFreqDist as CFD
         26 from nltk.util import tokenwrap, LazyConcatenation
    
    ImportError: cannot import name 'FreqDist'

Would it be possible to restructure the package `__init__.py` to avoid these issues?
"
156,https://github.com/nltk/nltk/issues/1528,1528,[],closed,2016-11-28 09:41:35+00:00,,1,"Import fails, if userhome contains german umlauts","I'm trying to use the python module for nltk, but while it's working perfectly fine on my desktop PC, my notebook refused to miss out following error:

```
Traceback (most recent call last):
  File ""start.py"", line 6, in <module>
    from Controllers.HomeController import homebottle, homesetclassifiercollection
  File ""D:\Documents\Coding\GithubClassificator\Application\Controllers\HomeController.py"", line 6, in <module>
    import Models.ClassifierCollection
  File ""D:\Documents\Coding\GithubClassificator\Application\Models\ClassifierCollection.py"", line 4, in <module>
    import ClassificationModules.ClassificationModule
  File ""D:\Documents\Coding\GithubClassificator\Application\Models\ClassificationModules\ClassificationModule.py"", line 8, in <module>
    from FeatureProcessing import getLabelIndex
  File ""D:\Documents\Coding\GithubClassificator\Application\Models\ClassificationModules\FeatureProcessing.py"", line 9, in <module>
    from nltk.stem import PorterStemmer
  File ""C:\Program Files\Python27\lib\site-packages\nltk\__init__.py"", line 128, in <module>
    from nltk.chunk import *
  File ""C:\Program Files\Python27\lib\site-packages\nltk\chunk\__init__.py"", line 155, in <module>
    from nltk.data import load
  File ""C:\Program Files\Python27\lib\site-packages\nltk\data.py"", line 77, in <module>
    if 'APPENGINE_RUNTIME' not in os.environ and os.path.expanduser('~/') != '~/':
  File ""C:\Program Files\Python27\lib\ntpath.py"", line 311, in expanduser
    return **userhome + path[i:]**
UnicodeDecodeError: 'ascii' codec can't decode **byte 0xe4 in position 23**: ordinal not in range(128)
```
_I marked the important parts of that output with **_

Tracing it down, I figured out that actualy my username contains a german umlaut (ä) on position 23; not a good choice of mine to be fair. It seems that nltk misses to check the userhome for non-ascii characters.

It shouldn't be that hard to fix, though.

Greetings"
157,https://github.com/nltk/nltk/issues/1530,1530,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",closed,2016-12-02 03:23:19+00:00,,2,DeprecationWarnings for @memoize for Python3.5,"The `memoize` decorator needs some updating such that it's using the new/better `inspect` from Python3.5

```bash
~/git-stuff/nltk$ python3 -W error -m nltk
Traceback (most recent call last):
  File ""/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py"", line 174, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File ""/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py"", line 133, in _get_module_details
    return _get_module_details(pkg_main_name, error)
  File ""/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py"", line 109, in _get_module_details
    __import__(pkg_name)
  File ""/Users/liling.tan/git-stuff/nltk/nltk/__init__.py"", line 115, in <module>
    from nltk.decorators import decorator, memoize
  File ""/Users/liling.tan/git-stuff/nltk/nltk/decorators.py"", line 183, in <module>
    @decorator
  File ""/Users/liling.tan/git-stuff/nltk/nltk/decorators.py"", line 172, in decorator
    return update_wrapper(_decorator, caller)
  File ""/Users/liling.tan/git-stuff/nltk/nltk/decorators.py"", line 83, in update_wrapper
    infodict = infodict or getinfo(model)
  File ""/Users/liling.tan/git-stuff/nltk/nltk/decorators.py"", line 59, in getinfo
    regargs, varargs, varkwargs, defaults = inspect.getargspec(func)
  File ""/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/inspect.py"", line 1041, in getargspec
    stacklevel=2)
DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead
```

Related to #630"
158,https://github.com/nltk/nltk/issues/1531,1531,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}]",closed,2016-12-02 03:35:34+00:00,,13,Dependencies on PyPI needs to be updated. ,"Most users installs `nltk` through:

```
$ pip install -U nltk
```

or 

```
$ conda install nltk
```

but somehow the PyPI requirements seems to be far lacking behind the ones in `pip-req.txt`."
159,https://github.com/nltk/nltk/issues/1537,1537,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-12-07 15:08:27+00:00,,0,nltk.tokenize.texttiling TextTilingTokenizer parameter demo_mode not documented,There's no documentation of the parameter demo_mode for TextTilingTokenizer. It doesn't seem to be just for internal use; it's used in the example code.
160,https://github.com/nltk/nltk/issues/1539,1539,[],closed,2016-12-09 03:38:08+00:00,,5,nltk.translate.bleu_score gives false result when ngram larger than maximum ngrams of given sentence,"Given weight = [0.25, 0.25, 0.25, 0.25] (default value),
sentence_bleu([['a', 'b', 'c']], ['a', 'b', 'c']) = 0 
While sentence_bleu([['a', 'b', 'c']], ['a', 'b', 'd']) = 0.7598
Obviously the previous score should be larger than the latter, or both scores should be 0"
161,https://github.com/nltk/nltk/issues/1541,1541,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}]",closed,2016-12-09 20:11:55+00:00,,2,Can't import nltk in Python 3.6 - AttributeError: module 'inspect' has no attribute 'getargspec',"I'm running this under Mac OS X in a virtualenv.

$ ipython
Python **3.6.0a0** (default, Sep 24 2015, 09:10:51) 
Type ""copyright"", ""credits"" or ""license"" for more information.

IPython 5.1.0 -- An enhanced Interactive Python.
?         -> Introduction and overview of IPython's features.
%quickref -> Quick reference.
help      -> Python's own help system.
object?   -> Details about 'object', use 'object??' for extra details.

In [1]: import nltk

AttributeError                            Traceback (most recent call last)
<ipython-input-1-b06499430ee0> in <module>()
----> 1 import nltk

/Users/test/venv/lib/python3.6/site-packages/nltk/__init__.py in <module>()
    113 
    114 from nltk.collocations import *
--> 115 from nltk.decorators import decorator, memoize
    116 from nltk.featstruct import *
    117 from nltk.grammar import *

/Users/test/venv/lib/python3.6/site-packages/nltk/decorators.py in <module>()
    181         return default
    182 
--> 183 @decorator
    184 def memoize(func, *args):
    185     dic = getattr_(func, ""memoize_dic"", dict)

/Users/test/venv/lib/python3.6/site-packages/nltk/decorators.py in decorator(caller)
    170         dec_func = eval(src, dict(_func_=func, _call_=caller))
    171         return update_wrapper(dec_func, func, infodict)
--> 172     return update_wrapper(_decorator, caller)
    173 
    174 def getattr_(obj, name, default_thunk):

/Users/test/venv/lib/python3.6/site-packages/nltk/decorators.py in update_wrapper(wrapper, model, infodict)
     81 # akin to functools.update_wrapper
     82 def update_wrapper(wrapper, model, infodict=None):
---> 83     infodict = infodict or getinfo(model)
     84     wrapper.__name__ = infodict['name']
     85     wrapper.__doc__ = infodict['doc']

/Users/test/venv/lib/python3.6/site-packages/nltk/decorators.py in getinfo(func)
     57     """"""
     58     assert inspect.ismethod(func) or inspect.isfunction(func)
---> 59     regargs, varargs, varkwargs, defaults = inspect.getargspec(func)
     60     argnames = list(regargs)
     61     if varargs:

AttributeError: module 'inspect' has no attribute 'getargspec'

"
162,https://github.com/nltk/nltk/issues/1542,1542,[],closed,2016-12-11 14:40:40+00:00,,1,How to use Dictionary as training data,"Hi,

I have list of categories and it's synonyms in below format

> ([""nsf"",""insufficent funds"",""nonsufficient funds""],""overdraft fees""),
> ([""atms"",""withdraws""],""atm fees""),
> ([""late payment"",""late fees""],""late payment fees"")
> ([""food"", ""hotel"", ""cafe""], ""restaurant"")

How can i train classifier, so that if i will classify `atms` it should return me `atm fees`
for
nsf -> overfraft fees
food -> restaurant

how to solve this problem ?"
163,https://github.com/nltk/nltk/issues/1543,1543,[],closed,2016-12-13 12:25:35+00:00,,1,Wiki page broken link,"machine-translation page below

https://github.com/nltk/nltk/wiki/Machine-Translation

has a broken link(https://github.com/nltk/nltk/tree/develop/nltk/align)

"
164,https://github.com/nltk/nltk/issues/1544,1544,[],closed,2016-12-13 23:34:47+00:00,,3,"Broken import of nltk.sentiment.vader with Python 2.7.12, NLTK 3.0.0","NLTK 3.0.0
Python 2.7.12 
Mac OS X 10.10.5 

When I type:

```python
from nltk.sentiment.vader import SentimentIntensityAnalyzer
```

or even simply

```python
import nltk.sentiment.vader
```


I get the following error:


```
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-5-fcebcd618999> in <module>()
----> 1 from nltk.sentiment.vader import SentimentIntensityAnalyzer

/usr/local/lib/python2.7/site-packages/nltk/sentiment/__init__.py in <module>()
     11 
     12 """"""
---> 13 from nltk.sentiment.sentiment_analyzer import SentimentAnalyzer
     14 from nltk.sentiment.vader import SentimentIntensityAnalyzer

/usr/local/lib/python2.7/site-packages/nltk/sentiment/sentiment_analyzer.py in <module>()
     24 from nltk.probability import FreqDist
     25 
---> 26 from nltk.sentiment.util import save_file, timer
     27 
     28 class SentimentAnalyzer(object):

/usr/local/lib/python2.7/site-packages/nltk/sentiment/util.py in <module>()
     25 from nltk.corpus import CategorizedPlaintextCorpusReader
     26 from nltk.data import load
---> 27 from nltk.tokenize.casual import EMOTICON_RE
     28 from nltk.twitter.common import outf_writer_compat, extract_fields
     29 

/usr/local/lib/python2.7/site-packages/nltk/tokenize/casual.py in <module>()
     39 from __future__ import unicode_literals
     40 import re
---> 41 from nltk.compat import htmlentitydefs, int2byte, unichr
     42 
     43 

ImportError: cannot import name int2byte
```"
165,https://github.com/nltk/nltk/issues/1548,1548,"[{'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}, {'id': 718746024, 'node_id': 'MDU6TGFiZWw3MTg3NDYwMjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stem/lemma', 'name': 'stem/lemma', 'color': '53bab8', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2016-12-18 14:58:13+00:00,,4,There is no easy way to add new synsets/lemmas to the wordnets,"It would be good if it were possible to e.g., create a new synset named company and add the words into it.

for example: 
Synsets: company_synonyms

words to be added:
tata consultancy ser"
166,https://github.com/nltk/nltk/issues/1551,1551,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",closed,2016-12-20 10:06:57+00:00,,3,nltk.tokenize.moses.py needs quite some re-working and tests,"There are several problems that needs to be fixed for `nltk.tokenize.moses` to work properly:

**Non-well-formed regexes**

E.g. The missing brackets at https://github.com/nltk/nltk/blob/develop/nltk/tokenize/moses.py#L90 will cause the `nltk.tokenize.moses.penn_tokenize()` function to break:

```python
$ python -c 'from nltk.tokenize.moses import MosesTokenizer; m = MosesTokenizer(); m.penn_tokenize(""this aint funny"")'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""nltk/tokenize/moses.py"", line 299, in penn_tokenize
    text = re.sub(regexp, subsitution, text)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/re.py"", line 155, in sub
    return _compile(pattern, flags).sub(repl, string, count)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/re.py"", line 251, in _compile
    raise error, v # invalid expression
sre_constants.error: unbalanced parenthesis
``` 

**Mising global/local self scoping for `MosesTokenizer` object**

At codepoint https://github.com/nltk/nltk/blob/develop/nltk/tokenize/moses.py#L301, local function calling a local object function without `self`, causing: 

```
$ python -c 'from nltk.tokenize.moses import MosesTokenizer; m = MosesTokenizer(); m.penn_tokenize(""this aint funny"")'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""nltk/tokenize/moses.py"", line 301, in penn_tokenize
    text = handles_nonbreaking_prefixes(text)
NameError: global name 'handles_nonbreaking_prefixes' is not defined
```

**Misspelling of variables**

It should be `substitution` not `subsitution`, misspelling sprinkled all about the script.

**Typo in comments** 

It should be ""tokens"" not ""toknes"" at https://github.com/nltk/nltk/blob/develop/nltk/tokenize/moses.py#L256
"
167,https://github.com/nltk/nltk/issues/1552,1552,[],closed,2016-12-20 23:29:46+00:00,,2,Unicode Decode Error when navigating around tkinter GUI  ,"On Python `3.5.2` and iPython `5.1.0`

Basically I just start the download window and then try to move to another tab.

```
In [1]: import nltk

In [2]: nltk.download()
showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml
---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
<ipython-input-2-a1a554e5d735> in <module>()
----> 1 nltk.download()

/usr/local/lib/python3.5/site-packages/nltk/downloader.py in download(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error)
    653             # function should make a new copy of self to use?
    654             if download_dir is not None: self._download_dir = download_dir
--> 655             self._interactive_download()
    656             return True
    657

/usr/local/lib/python3.5/site-packages/nltk/downloader.py in _interactive_download(self)
    972         if TKINTER:
    973             try:
--> 974                 DownloaderGUI(self).mainloop()
    975             except TclError:
    976                 DownloaderShell(self).run()

/usr/local/lib/python3.5/site-packages/nltk/downloader.py in mainloop(self, *args, **kwargs)
   1707
   1708     def mainloop(self, *args, **kwargs):
-> 1709         self.top.mainloop(*args, **kwargs)
   1710
   1711     #/////////////////////////////////////////////////////////////////

/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/tkinter/__init__.py in mainloop(self, n)
   1129     def mainloop(self, n=0):
   1130         """"""Call the mainloop of Tk.""""""
-> 1131         self.tk.mainloop(n)
   1132     def quit(self):
   1133         """"""Quit the Tcl interpreter. All widgets will be destroyed.""""""

UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf5 in position 0: invalid start byte
```"
168,https://github.com/nltk/nltk/issues/1554,1554,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}]",closed,2016-12-23 03:16:09+00:00,,1,How to handle BLEU scores for ngrams where n<4 in NLTK? ,"From #1545, BLEU is buggy for ngrams where the n<4. 

One simple way is to clip the weights distribution such that the `weights=(0.25, 0.25, 0.25, 0.25)` redistributes uniformly with respect to the highest ngram order from the hypothesis or reference. 

But we're not sure whether that's the most appropriate solution to fix that. 

Meanwhile, while smoothing functions work fine when reference length is n>=4, it goes haywire when n<4 too. 

----

Without smoothing, NLTK's BLEU is overly-optimistic with the condition on https://github.com/nltk/nltk/blob/develop/nltk/translate/bleu_score.py#L170:

```
>>> from nltk import bleu
>>> ref = 'let it go'.split()
>>> hyp = 'let go it'.split()
>>> bleu([ref], hyp)
1.0
```

With smoothing, the BLEU goes haywire:

```python
>>> from nltk.translate.bleu_score import SmoothingFunction
>>> chencherry = SmoothingFunction()
>>> bleu([ref], hyp, smoothing_function=chencherry.method0) # No smoothing
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""nltk/translate/bleu_score.py"", line 79, in sentence_bleu
    return corpus_bleu([references], [hypothesis], weights, smoothing_function)
  File ""nltk/translate/bleu_score.py"", line 180, in corpus_bleu
    return bp * math.exp(math.fsum(s))
  File ""nltk/translate/bleu_score.py"", line 178, in <genexpr>
    s = (w * math.log(p_i) for i, (w, p_i) in enumerate(zip(weights, p_n)))
ValueError: math domain error
>>> bleu([ref], hyp, smoothing_function=chencherry.method1) 
0.14953487812212207
>>> bleu([ref], hyp, smoothing_function=chencherry.method2) 
0.537284965911771
>>> bleu([ref], hyp, smoothing_function=chencherry.method3) 
0.2973017787506803
>>> bleu([ref], hyp, smoothing_function=chencherry.method4)
4.876766661577289
>>> bleu([ref], hyp, smoothing_function=chencherry.method5) 
0.19245008972987526
>>> bleu([ref], hyp, smoothing_function=chencherry.method6) 
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""nltk/translate/bleu_score.py"", line 79, in sentence_bleu
    return corpus_bleu([references], [hypothesis], weights, smoothing_function)
  File ""nltk/translate/bleu_score.py"", line 177, in corpus_bleu
    hypothesis=hypothesis, hyp_len=hyp_len)
  File ""nltk/translate/bleu_score.py"", line 540, in method6
    pi0 = 0 if p_n[i-2] == 0 else p_n[i-1]**2 / p_n[i-2]
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/fractions.py"", line 362, in forward
    return monomorphic_operator(a, b)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/fractions.py"", line 412, in _div
    a.denominator * b.numerator)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/fractions.py"", line 162, in __new__
    raise ZeroDivisionError('Fraction(%s, 0)' % numerator)
ZeroDivisionError: Fraction(0, 0)
>>> bleu([ref], hyp, smoothing_function=chencherry.method7) 
5.5913883085947536
```

Method 4 is surely wrong since the output looks like it's in log space. And the error in method 7 follows from method 4 since 7 is combination of method 4 and method 6.
"
169,https://github.com/nltk/nltk/issues/1558,1558,[],closed,2016-12-27 07:10:00+00:00,,5,word_tokenizer and Spanish,"I would like to have a word_tokenizer that works with Spanish. For example, this code:

`import nltk
from nltk.tokenize import word_tokenize
sentences = ""¿Quién eres tú? ¡Hola! ¿Dónde estoy?""
spanish_sentence_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')
sentences = spanish_sentence_tokenizer.tokenize(sentences)
for s in sentences:
    print([s for s in vword_tokenize(s)])`

gives the following:

`['¿Quién', 'eres', 'tú', '?']
['¡Hola', '!']
['¿Dónde', 'estoy', '?']`

 but I would have expected the following instead:

`['¿' ,'Quién', 'eres', 'tú', '?']
['¡' ,'Hola', '!']
['¿' ,'Dónde', 'estoy', '?']`"
170,https://github.com/nltk/nltk/issues/1559,1559,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",closed,2016-12-27 08:39:21+00:00,,0,Small improvement to the ToktokTokenizer,"From #1558, we can easily add the `u'\xa1'` character to the `FUNKY_PUNCT_1` regular expression at https://github.com/nltk/nltk/blob/develop/nltk/tokenize/toktok.py#L51 to cover the inverted exclamation. 

It's an easy improvement to make as a first contribution to NLTK. You can easily add to the examples from #1558 to the doctest at https://github.com/nltk/nltk/blob/develop/nltk/tokenize/toktok.py#L45 to check whether it tokenize the string successfully `¡Hola!` -> `['¡' , 'Hola', '!']`

Please do take a look at https://github.com/nltk/nltk/blob/develop/CONTRIBUTING.md before contributing. 

----

Here's a quick cheatsheet to contributing:

 - First, fork the NLTK repository at https://github.com/nltk/nltk (Click on the ""Fork"" button on the top right)

 - Then go to your fork of NLTK and do a git clone to your machine/laptop, e.g. if your username on github is \<newcontributor\>, you should do something like `git clone https://github.com/<newcontributor>/nltk.git` on the terminal/command line or copy the `.git` address to the git GUI.

 - Follow the instructions from https://github.com/nltk/nltk/blob/develop/CONTRIBUTING.md#forks--github-pull-requests to checkout a new branch

 - Now navigate to the file in `nltk/nltk/tokenize/toktok.py` on your local machine. Add to the line https://github.com/nltk/nltk/blob/develop/nltk/tokenize/toktok.py#L51

 - Then write a test example in the docstring to show that the feature you added works (see https://github.com/nltk/nltk/blob/develop/nltk/tokenize/toktok.py#L45 )

 - Then validate that the tests you wrote work, something along the lines of `python -c doctest nltk/tokenize/toktok.py`, check it for `python3` too. For more information on doctest, see https://pymotw.com/2/doctest/

 - Now git add and commit your changes and update your fork of NLTK

 - Finally, go back to your github.com fork of the NLTK repository, and do a new pull request. 

Detailed instructions to do a git add, commit and doing a pull request can be found on https://github.com/nltk/nltk/blob/develop/CONTRIBUTING.md#forks--github-pull-requests"
171,https://github.com/nltk/nltk/issues/1560,1560,[],closed,2016-12-27 10:10:41+00:00,,4,panlex_lite installation,"Hello All,

I know this issues is raised in other threads as well, sorry for a new threads as i did not got answer of what i am looking for 

System : Mac,
OS        : OS X, 13F1911
Python Version : 3.5.2
Nltk Version : 3.2.1

Downloaded all packages. Command Used
python3
>>>import nltk
>>>nltk.download()

panlex_lite download get stuk

Tried downloading few time
Tried downloading panlex_lite.zip independently

In both of the above cases got a file of 2.34 GB

When i replaced the corrupted file in nltk_data/corpora/ folder

tried >>>nltk.download() again no luck

Tried with zip file and also after unzipping it

output of suggested code 
import zipfile
>>> plzip = '/Users/nishkarsh/nltk_data/corpora/panlex_lite.zip'
>>> [zifo.CRC for zifo in zipfile.ZipFile(plzip).infolist()]
[0, 448887900, 3816634357]

The python popup is showing out of date only.

![screen shot 2016-12-27 at 3 32 19 pm](https://cloud.githubusercontent.com/assets/24789485/21497371/7846273e-cc4a-11e6-9ba6-fd0e077bd4bc.png)

![screen shot 2016-12-27 at 3 38 27 pm](https://cloud.githubusercontent.com/assets/24789485/21497389/8bb7b396-cc4a-11e6-958f-593df41ceedd.png)

Can any one plz help the way forward"
172,https://github.com/nltk/nltk/issues/1574,1574,[],closed,2017-01-02 04:16:33+00:00,,27,Cannot Install on Windows,"I'm having issues installing NLTK on a Windows 7 64-bit machine with Python 3.5 32-bit installed.  I'm using the executable located here :  https://pypi.python.org/pypi/nltk

However, I am prompted with an error after running the executable that reads:

""Python version -32 required, which was not found in the registry"".

I've tried this installation against Windows 7 systems with Python 2.7, 3.4, 3.5 & 3.6 installed and they all fail with the same error.  The 32-bit entry for Python exists in the Current User and Local Machine hives, however, the installation does not process.

Please help, thanks."
173,https://github.com/nltk/nltk/issues/1575,1575,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 25396640, 'node_id': 'MDU6TGFiZWwyNTM5NjY0MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/book', 'name': 'book', 'color': 'e102d8', 'default': False, 'description': None}, {'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-01-03 12:42:21+00:00,,8,stem_word and lower case stem output in v3.2.2,"Hi,

Just wanted to confirm the following things affected by the recent upgrade to v3.2.2:
- There is no `stem_word` function in `PorterStemmer()`. I had to replace it with `stem()`
- Unlike before, `stem` returns lower case of a word e.g. `stem` for `Stemming` (http://text-processing.com/demo/stem/ is the same as before).

Cheers,
Ehsan"
174,https://github.com/nltk/nltk/issues/1576,1576,"[{'id': 719817004, 'node_id': 'MDU6TGFiZWw3MTk4MTcwMDQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/multithread%20/%20multiprocessing', 'name': 'multithread / multiprocessing', 'color': 'bdf486', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-01-03 14:48:00+00:00,,9,cant use nltk functions in parallel programs ,"when I want to use nltk stemmer and lemmatizer in a multithreaded program only one thread works fine and other ones throw this exception : 

Exception in thread 2:
Traceback (most recent call last):
  File ""C:\Users\Danial\AppData\Local\Programs\Python\Python35\lib\threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""C:\Users\Danial\AppData\Local\Programs\Python\Python35\lib\threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""D:/iaun/0_Tez/141095-TFATOwithdiscriminative/01/11_MultiThreadedDoctTermInfo.py"", line 21, in myfunc
    lemma = lemmatizer.lemmatize(stem)
  File ""C:\Users\Danial\AppData\Local\Programs\Python\Python35\lib\site-packages\nltk\stem\wordnet.py"", line 40, in lemmatize
    lemmas = wordnet._morphy(word, pos)
  File ""C:\Users\Danial\AppData\Local\Programs\Python\Python35\lib\site-packages\nltk\corpus\util.py"", line 99, in __getattr__
    self.__load()
  File ""C:\Users\Danial\AppData\Local\Programs\Python\Python35\lib\site-packages\nltk\corpus\util.py"", line 73, in __load
    args, kwargs  = self.__args, self.__kwargs
AttributeError: 'WordNetCorpusReader' object has no attribute '_LazyCorpusLoader__args'
"
175,https://github.com/nltk/nltk/issues/1577,1577,"[{'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",closed,2017-01-04 08:19:10+00:00,,2,Tokenize `xml_escape` tests are failing,"Tests for `tokenize.util.xml_escape` are currently failing, see [Jenkins CI test](https://nltk.ci.cloudbees.com/job/nltk/lastCompletedBuild/TOXENV=py27-jenkins,jdk=jdk8latestOnlineInstall/testReport/nltk.tokenize/util/xml_escape)."
176,https://github.com/nltk/nltk/issues/1579,1579,"[{'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",closed,2017-01-05 16:51:47+00:00,,2,Pl196x corpus failure with python 3.5,"Tests are failing using python 3.5 because of `Pl196xCorpusReader`. How is the `textids.txt` file supposed to be obtained by users?

Command: `tox -e py35`
Output:
```bash
======================================================================
1) ERROR: Failure: FileNotFoundError ([Errno 2] No such file or directory: 'textids.txt')
----------------------------------------------------------------------
   Traceback (most recent call last):
    nltk/.tox/py35/lib/python3.5/site-packages/nose/failure.py line 39 in runTest
      raise self.exc_val.with_traceback(self.tb)
    nltk/.tox/py35/lib/python3.5/site-packages/nose/plugins/manager.py line 154 in generate
      for r in result:
    doctest_nose_plugin.py line 106 in loadTestsFromModule
      for suite in super(DoctestPluginHelper, self).loadTestsFromModule(module):
    nltk/.tox/py35/lib/python3.5/site-packages/nose/plugins/doctests.py line 228 in loadTestsFromModule
      tests = self.finder.find(module)
    /usr/lib/python3.5/doctest.py line 924 in find
      self._find(tests, obj, name, module, source_lines, globs, {})
    /usr/lib/python3.5/doctest.py line 983 in _find
      if ((inspect.isroutine(inspect.unwrap(val))
    /usr/lib/python3.5/inspect.py line 471 in unwrap
      while _is_wrapper(func):
    /usr/lib/python3.5/inspect.py line 465 in _is_wrapper
      return hasattr(f, '__wrapped__')
    nltk/nltk/corpus/util.py line 116 in __getattr__
      self.__load()
    nltk/nltk/corpus/util.py line 84 in __load
      corpus = self.__reader_cls(root, *self.__args, **self.__kwargs)
    nltk/nltk/corpus/reader/pl196x.py line 99 in __init__
      self._init_textids()
    nltk/nltk/corpus/reader/pl196x.py line 105 in _init_textids
      with open(self._textids) as fp:
   FileNotFoundError: [Errno 2] No such file or directory: 'textids.txt'
```"
177,https://github.com/nltk/nltk/issues/1580,1580,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",closed,2017-01-05 17:47:14+00:00,,3,Unavailable corpora are breaking tests,"Tests are failing with python 3.5 because of some corpus readers whose corpora are not distributed within NLTK. For example: [`nkjp`][1], [`ipipan`][2], [`ycoe`][3].

You can reproduce these problems using `tox -e py35 /nltk/nltk/corpus/__init__.py`.

I would like to find an answer the following questions:
1. Why is this problem only arising when we run `tox` using python 3.5, and not with previous versions?
2. How can we solve it?

If we want to keep corpus readers even for those corpora that are not currently shipped with NLTK, I think we need to avoid raising exceptions when these corpus readers are loaded. But where should we fix the problem? I briefly write the problems for each of these corpora:

- [`nkjp`][1]: it is not available, but we get this message if we try to use it:
  > Resource 'corpora/nkjp' not found.  Please use the NLTK
  Downloader to obtain the resource:  >>> `nltk.download()`

  But the download cannot be used to download it, since we do not have `nkjp` on `nltk_data`.

- [`ipipan`][2]: same as before. The corpus is not available on `nltk_data`, but when we try to use it we get this message:
  > Resource 'corpora/ipipan' not found.  Please use the NLTK
  > Downloader to obtain the resource:  >>> `nltk.download()`

- [`ycoe`][3]: we don't have the corpus, and we get this error:
  > OSError: No such file or directory: `nltk_data/corpora/ycoe/psd`

I'm also wondering: how could we test a corpus reader if we do not have the corpus it is supposed to read? At the moment, we are not covered against the possibility that any change made to the syntax of original corpus could make our reader useless. Anyway, this is not what I wanted to discuss here.

Could you please give your opinion about these problems?
  
[1]: https://github.com/fievelk/nltk/blob/develop/nltk/corpus/reader/nkjp.py
[2]:https://github.com/fievelk/nltk/blob/develop/nltk/corpus/reader/ipipan.py
[3]: https://github.com/fievelk/nltk/blob/develop/nltk/corpus/reader/ycoe.py"
178,https://github.com/nltk/nltk/issues/1581,1581,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}]",closed,2017-01-07 05:49:30+00:00,,11,porter stemmer: string index out of range,see the following [stackoverflow post](http://stackoverflow.com/questions/41517595/nltk-stemmer-string-index-out-of-range)
179,https://github.com/nltk/nltk/issues/1584,1584,"[{'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",closed,2017-01-07 23:51:47+00:00,,2,StanfordTokenizer failure,"We are experiencing [`StanfordTokenizer` failures on our CI][1]:
```bash
Failed example:
    StanfordTokenizer().tokenize(s)
Exception raised:
    Traceback (most recent call last):
      File ""/scratch/jenkins/python/python-3.4.2-x86_64/lib/python3.4/doctest.py"", line 1324, in __run
        compileflags, 1), test.globs)
      File ""<doctest nltk.tokenize.stanford.StanfordTokenizer[2]>"", line 1, in <module>
        StanfordTokenizer().tokenize(s)
      File ""/scratch/jenkins/workspace/pull_request_tests/TOXENV/py34-jenkins/jdk/jdk8latestOnlineInstall/nltk/tokenize/stanford.py"", line 68, in tokenize
        return self._parse_tokenized_output(self._execute(cmd, s))
      File ""/scratch/jenkins/workspace/pull_request_tests/TOXENV/py34-jenkins/jdk/jdk8latestOnlineInstall/nltk/tokenize/stanford.py"", line 94, in _execute
        stdout=PIPE, stderr=PIPE)
      File ""/scratch/jenkins/workspace/pull_request_tests/TOXENV/py34-jenkins/jdk/jdk8latestOnlineInstall/nltk/internals.py"", line 134, in java
        raise OSError('Java command failed : ' + str(cmd))
    OSError: Java command failed : ['/opt/jdk/jdk8.latest/bin/java', '-mx1000m', '-cp', '/home/jenkins/third/stanford-postagger/stanford-postagger-3.7.0-sources.jar:/home/jenkins/third/stanford-postagger/stanford-postagger-3.7.0.jar:/home/jenkins/third/stanford-postagger/stanford-postagger-3.7.0-javadoc.jar:/home/jenkins/third/stanford-postagger/stanford-postagger.jar:/home/jenkins/third/stanford-postagger/lib/slf4j-simple.jar:/home/jenkins/third/stanford-postagger/lib/slf4j-api.jar', 'edu.stanford.nlp.process.PTBTokenizer', '-charset', 'utf8', '/tmp/tmptx_7hdjm']
```

@alvations I think that you have had some experience with this kind of problems previously. Could you please verify? I have the feeling that this could be related to some changes in the Stanford jar, but I could be wrong.

[1]:https://nltk.ci.cloudbees.com/job/pull_request_tests/lastCompletedBuild/TOXENV=py34-jenkins,jdk=jdk8latestOnlineInstall/testReport/nltk.tokenize/stanford/StanfordTokenizer/"
180,https://github.com/nltk/nltk/issues/1586,1586,[],closed,2017-01-10 23:36:30+00:00,,1,StanfordNeuralDependencyParser memory issues,"**In brief:** should we raise the amount of memory used by default by `StanfordNeuralDependencyParser`?

---

I am experiencing some problems with `StanfordNeuralDependencyParser`. Steps to reproduce the issue:
```python
from nltk.parse.stanford import StanfordNeuralDependencyParser
dep_parser=StanfordNeuralDependencyParser()
dep_parser.raw_parse(""The quick brown fox jumps over the lazy dog."")
```

First I get this error:
> Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space

Then, if I allocate more heap space using `java_options='-mx2000m'` and try again:
> java.lang.OutOfMemoryError: GC overhead limit exceeded

If I increase it again (e.g. to `java_options='-mx4g'`) I finally get a result:
```python
from nltk.parse.stanford import StanfordNeuralDependencyParser
dep_parser=StanfordNeuralDependencyParser()
result = dep_parser.raw_parse(""The quick brown fox jumps over the lazy dog."")
next(result)
<DependencyGraph with 11 nodes>
```

In fact, [Stanford Parser FAQ][1] contains the following useful advice:
> The parser uses considerable amounts of memory. If you see a java.lang.OutOfMemoryError, you either need to give the parser more memory or to take steps to reduce the memory needed. (You give java more memory at the command line by using the -mx flag, for example -mx500m.

They also provide several examples using `-mx4g` or `-mx12g`, so I suppose that we actually need to expect a lot of memory usage.

I think we therefore need to change our code in order to expect a higher memory usage for `StanfordNeuralDependencyParser`.

[1]: http://nlp.stanford.edu/software/parser-faq.shtml#k"
181,https://github.com/nltk/nltk/issues/1596,1596,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",closed,2017-01-18 16:57:34+00:00,,5,Corpus Reader failures,"@alvations #1593 seems to have broken tests: [Jenkins CI failures][1].

[1]: https://nltk.ci.cloudbees.com/job/nltk/639/testReport/"
182,https://github.com/nltk/nltk/issues/1597,1597,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-01-19 15:23:54+00:00,,0,CHUNK_TAG_PATTERN does not allow curly bracket quantifiers for tags,"Running the supplementary example from http://www.nltk.org/book/ch07.html#exploring-text-corpora (""your turn"" section)

```
cp = nltk.RegexpParser('CHUNK: {<N.*>{4,}}')
brown = nltk.corpus.brown
for sent in brown.tagged_sents():
    tree = cp.parse(sent)
    for subtree in tree.subtrees():
        if subtree.label() == 'CHUNK': print(subtree)
```

yields a ValueError:
```
  File ""nltk/chunk/regexp.py"", line 1130, in __init__
    self._read_grammar(grammar, root_label, trace)
  File ""nltk/chunk/regexp.py"", line 1166, in _read_grammar
    rules.append(RegexpChunkRule.fromstring(line))
  File ""nltk/chunk/regexp.py"", line 381, in fromstring
    raise ValueError('Illegal chunk pattern: %s' % rule)
ValueError: Illegal chunk pattern: {<N.*>{4,}}

```

This is because nltk.chunk.CHUNK_TAG_PATTERN does not permit curly brackets. Simple workaround is setting:
```
nltk.chunk.regexp.CHUNK_TAG_PATTERN = re.compile(r'^((%s|<%s>)*)$' %
                                ('([^\{\}<>]|\{\d+,?\d*\}|\{\d*,?\d+\})+',
                                 '[^\{\}<>]+'))
```

which might be a viable fix as well."
183,https://github.com/nltk/nltk/issues/1600,1600,[],closed,2017-01-20 18:40:40+00:00,,5,"3.2.2 stemming ""oed"" crashes","```
from nltk.stem.porter import *
stemmer = PorterStemmer()
stemmer.stem(""oed"")
```

--> crash

The problem is a vowel suffixed by ""ed"" or ""ing"". It works in 3.2.1

python version: 3.5.2"
184,https://github.com/nltk/nltk/issues/1602,1602,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-01-21 20:40:05+00:00,,0,unicode_literal / python_2_unicode_compatible,The codebase uses these two methods for unicode string compatibility; it would be better to standardise on one of them.
185,https://github.com/nltk/nltk/issues/1603,1603,[],closed,2017-01-25 08:05:41+00:00,,2,problems configuring NLTK 3.2.2 to use Stanford segmenter,"I'm having trouble getting NLTK 3.2.2 configured to use the Stanford NLP segmenter. Most of the problems that I see are CLASSPATH and SLF4J related.

With NLTK, there are three ways to identify where to find the SLF4J and Stanford segmenter jar files:

- CLASSPATH environment variable
- SLF4J and STANFORD_SEGMENTER environment variables
- path_to_slf4j and path_to_jar options with the StanfordSegmenter constructor

I've tried variants of all three approaches (see nltk-ss.py) without success. However, I do have an equivalent shell script that works (see nltk-ss.sh).

BTW, I posted a GitHub issue with the Stanford NLP group and heard back that I should use SLF4J 1.7.12.

[nltk-ss.py.txt](https://github.com/nltk/nltk/files/728960/nltk-ss.py.txt)
[nltk-ss.sh.txt](https://github.com/nltk/nltk/files/728961/nltk-ss.sh.txt)

"
186,https://github.com/nltk/nltk/issues/1605,1605,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 719817004, 'node_id': 'MDU6TGFiZWw3MTk4MTcwMDQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/multithread%20/%20multiprocessing', 'name': 'multithread / multiprocessing', 'color': 'bdf486', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-01-28 19:12:46+00:00,,1,Thread-safe CorpusReader,"I'm using a custom subclassed `CorpusReader` to feed documents to the Word2Vec model in gensim.  Because the reader isn't threadsafe, it breaks.  I can write my own simple reader for my files, but it would be nice if I could just use nltk.  Based on the comments above [iterate_from](https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/util.py#L259) it looks like some locking could be added to make the reader thread-safe.   What exactly what I need to acquire a lock on to make `iterate_from` safe?

-- Eric"
187,https://github.com/nltk/nltk/issues/1606,1606,[],closed,2017-02-03 09:31:04+00:00,,1,MWAPPDB corpus fails build tests,"The CI server reports an [error](https://nltk.ci.cloudbees.com/job/nltk/lastCompletedBuild/TOXENV=py35-jenkins,jdk=jdk8latestOnlineInstall/testReport/nltk.test.unit.test_corpora/TestMWAPPDB/test_entries/) with the `mwa_ppdb` corpus."
188,https://github.com/nltk/nltk/issues/1607,1607,[],closed,2017-02-03 09:34:10+00:00,,0,NKJP corpus fails build tests,"The CI server reports an [error](https://nltk.ci.cloudbees.com/job/nltk/lastCompletedBuild/TOXENV=py35-jenkins,jdk=jdk8latestOnlineInstall/testReport/nose.failure/Failure/runTest/) loading the NKJP corpus."
189,https://github.com/nltk/nltk/issues/1609,1609,[],closed,2017-02-03 19:26:05+00:00,,0,YCOE corpus test fails,"The CI server reports an [error](https://nltk.ci.cloudbees.com/job/nltk/lastCompletedBuild/TOXENV=py35-jenkins,jdk=jdk8latestOnlineInstall/testReport/nose.failure/Failure/runTest/) with accessing the YCOE corpus (not publicly available)."
190,https://github.com/nltk/nltk/issues/1610,1610,[],closed,2017-02-04 06:33:22+00:00,,0,ipipan corpus fails build tests,"The CI server reports an [error](https://nltk.ci.cloudbees.com/job/nltk/lastCompletedBuild/TOXENV=py35-jenkins,jdk=jdk8latestOnlineInstall/testReport/nose.failure/Failure/runTest/) because the ipipan corpus is not available."
191,https://github.com/nltk/nltk/issues/1611,1611,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",open,2017-02-07 15:40:47+00:00,,2,Support extended open multilingual wordnet,"Currently, NLTK supports the [open multilingual wordnet](http://compling.hss.ntu.edu.sg/omw/) (requires `nltk.download('omw')`). However, it doesn't (unless I'm missing something) yet support the [extended open multilingual wordnet](http://compling.hss.ntu.edu.sg/omw/summx.html)."
192,https://github.com/nltk/nltk/issues/1612,1612,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}]",open,2017-02-07 16:52:42+00:00,,8,Link to C&C Tools/Boxer is broken,The link (http://svn.ask.it.usyd.edu.au/trac/candc/wiki/Subversion) provided here (https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software#cc-toolsboxer) is broken. Is there anywhere else to download these tools?
193,https://github.com/nltk/nltk/issues/1613,1613,"[{'id': 718733436, 'node_id': 'MDU6TGFiZWw3MTg3MzM0MzY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/Off-topic', 'name': 'Off-topic', 'color': 'f7b4e1', 'default': False, 'description': None}]",closed,2017-02-08 11:12:56+00:00,,2,Can Apache Sorl be integrated with NLTK?,"If not, is there another way around?"
194,https://github.com/nltk/nltk/issues/1614,1614,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718746024, 'node_id': 'MDU6TGFiZWw3MTg3NDYwMjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stem/lemma', 'name': 'stem/lemma', 'color': '53bab8', 'default': False, 'description': None}]",closed,2017-02-08 19:00:12+00:00,,5,"""IndexError: string index out of range"" on trying to stem the word ""oing""","Easy to reproduce:
```python
>>> from nltk import PorterStemmer
>>> stemmer = PorterStemmer()
>>> stemmer.stem('oing')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/peterbe/virtualenvs/songsearch/lib/python3.5/site-packages/nltk/stem/porter.py"", line 665, in stem
    stem = self._step1b(stem)
  File ""/Users/peterbe/virtualenvs/songsearch/lib/python3.5/site-packages/nltk/stem/porter.py"", line 376, in _step1b
    lambda stem: (self._measure(stem) == 1 and
  File ""/Users/peterbe/virtualenvs/songsearch/lib/python3.5/site-packages/nltk/stem/porter.py"", line 258, in _apply_rule_list
    if suffix == '*d' and self._ends_double_consonant(word):
  File ""/Users/peterbe/virtualenvs/songsearch/lib/python3.5/site-packages/nltk/stem/porter.py"", line 214, in _ends_double_consonant
    word[-1] == word[-2] and
IndexError: string index out of range
```

```
>>> import nltk
>>> nltk.__version__
'3.2.2'
```"
195,https://github.com/nltk/nltk/issues/1616,1616,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}]",closed,2017-02-09 02:10:54+00:00,,5,MosesTokenizer is skipping the final fullstop.,"MosesTokenizer is skipping the final fullstop.

```python
>>> from nltk.tokenize.moses import MosesTokenizer
>>> m = MosesTokenizer()
>>> m.tokenize('abc def.')
[u'abc', u'def.']
>>> m.penn_tokenize('abc def.')
[u'abc', u'def', u'.']
```

But MosesTokenizer's penn_tokenize works because of https://github.com/nltk/nltk/blob/develop/nltk/tokenize/moses.py#L103

While the default tokenizer relies on the `handles_nonbreaking_prefixes()` to handle the final fullstop: https://github.com/nltk/nltk/blob/develop/nltk/tokenize/moses.py#L284

The conditions at https://github.com/nltk/nltk/blob/develop/nltk/tokenize/moses.py#L274 is wrong since it's not checking what Moses is actually doing on https://github.com/alvations/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl#L333

The correct implementation should be:

```python
    def handles_nonbreaking_prefixes(self, text):
        # Splits the text into tokens to check for nonbreaking prefixes.
        tokens = text.split()
        num_tokens = len(tokens)
        for i, token in enumerate(tokens):
            # Checks if token ends with a fullstop.
            token_ends_with_period = re.findall(r'^(\S+)\.$', token)
            if token_ends_with_period:
                prefix = token_ends_with_period.group(1)
                # Checks for 3 conditions if
                # i.   the prefix contains a fullstop and
                #      any char in the prefix is within the IsAlpha charset
                # ii.  the prefix is in the list of nonbreaking prefixes and
                #      does not contain #NUMERIC_ONLY#
                # iii. the token is not the last token and that the
                #      next token contains all lowercase.
                if ( ('.' in prefix and self.isalpha(prefix)) or
                     (prefix in self.NONBREAKING_PREFIXES and
                      prefix not in self.NUMERIC_ONLY_PREFIXES) or
                     (i != num_tokens-1 and self.islower(tokens[i+1])) ):
                    pass # No change to the token.
                # Checks if the prefix is in NUMERIC_ONLY_PREFIXES
                # and ensures that the next word is a digit.
                elif (prefix in self.NUMERIC_ONLY_PREFIXES and
                      re.search(r'^[0-9]+', tokens[i+1])):
                    pass # No change to the token.
                else: # Otherwise, adds a space after the tokens before a dot.
                    tokens[i] = prefix + ' .'
        return "" "".join(tokens) # Stitch the tokens back.
```

After the patch:

```
$ python -c ""from nltk.tokenize.moses import MosesTokenizer; m = MosesTokenizer(); print(m.tokenize('abc def.'))""
[u'abc', u'def', u'.']
```"
196,https://github.com/nltk/nltk/issues/1617,1617,"[{'id': 719817004, 'node_id': 'MDU6TGFiZWw3MTk4MTcwMDQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/multithread%20/%20multiprocessing', 'name': 'multithread / multiprocessing', 'color': 'bdf486', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-02-09 03:39:18+00:00,,0,Use multiprocessing package for ParallelProverBuilder and ParallelProverBuilderCommand,"ParallelProverBuilder and ParallelProverBuilderCommand currently use the threading package to spawn two threads, one with a prover, and one with a model builder. Threads are not stoppable, so when one thread returns first, the other is kept alive until it finishes; it might never finish, as Mace4 can be set to have no maximum number of models to try. By using the multiprocessing package, we can replace the two threads with two processes instead, and once one process finishes, the other process can be terminated.

Does this seem like a good idea? I would be happy to try making this change and opening up a PR."
197,https://github.com/nltk/nltk/issues/1618,1618,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",closed,2017-02-09 14:20:53+00:00,,1,Expose _synset_from_pos_and_offset as a public method,"Two reasons for this:

1. Given that http://stackoverflow.com/a/27145655/1709587 has thousands of views and 6 upvotes, it seems that people are already using it. Pragmatism suggests that it should therefore be exposed publicly; it's not good to have a notionally-private-but-public-in-practice part of an API.
2. The fact that NLTK chooses to expose the `offset()` of synsets via a public method means that there's currently no philosophy of hiding the (ugly! awful! fragile!) offset-based data structure of WordNet, and given that offset information is being exposed, looking up a synset by offset (perhaps the single most basic offset-related task you'd ever want to do, more fundamental than wanting to know a given synset's offset) seems like something that NLTK should support and is an odd gap in the current API.

Precisely because of point 1, the old `_`-prefixed method should remain too to preserve backwards compatibility, at least until the next major release."
198,https://github.com/nltk/nltk/issues/1619,1619,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}]",open,2017-02-09 15:49:27+00:00,,1,Update omw data,"It's definitely at least a little out of date (or perhaps was incomplete at the time of being added into NLTK); per http://compling.hss.ntu.edu.sg/omw/, there is a Romanian (language code 'ron') WordNet that is part of OMW but it's not part of NLTK's OMW corpus:

```
>>> nltk.corpus.wordnet.langs()
['eng', 'als', 'arb', 'bul', 'cat', 'cmn', 'dan', 'ell', 'eus', 'fas', 'fin', 'fra', 'glg', 'heb', 'hrv', 'ind', 'ita', 'jpn', 'nno', 'nob', 'pol', 'por', 'qcn', 'slv', 'spa', 'swe', 'tha', 'zsm']
```"
199,https://github.com/nltk/nltk/issues/1620,1620,[],closed,2017-02-09 15:58:57+00:00,,0,Support custom WordNet .tab files,"This would provide an immediate workaround solution for https://github.com/nltk/nltk/issues/1619 and https://github.com/nltk/nltk/issues/1611, ensure that a workaround exists if the OMW data goes out of date in future, and serve a few other use cases I can imagine:

- Testing a new WordNet intended to replace an existing OMW WordNet
- Allowing wholly 'unofficial' (i.e. unblessed by NLTK, OMW, or EOMW) WordNets to be used
- Allowing an EOMW WordNet to be used for a language that already has an OMW WordNet included in NLTK

I'm envisaging being able to make a function call like

```
nltk.corpus.wordnet.use_tab_file_for_language(
    open('/home/mark/my_custom_tab_file.tab'),
    'fra'
)
```

and having NLTK thereafter disregard the French tab file from OMW and instead use the one that I've given it."
200,https://github.com/nltk/nltk/issues/1622,1622,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 81645781, 'node_id': 'MDU6TGFiZWw4MTY0NTc4MQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/language-model', 'name': 'language-model', 'color': 'd4c5f9', 'default': False, 'description': ''}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",open,2017-02-10 05:34:53+00:00,,0,ZeroDivisionError: In BigramAssocMeasures,"I got an error ""ZeroDivisionError: float division by zero"". In nltk/metrics/association.py at line number 213 in phi_sq method."
201,https://github.com/nltk/nltk/issues/1623,1623,[],closed,2017-02-14 02:49:12+00:00,,0,nltk.set_proxy not wroks,"Hi, I am using a proxy named shadowsocks(very very slow in China, have to use proxy), but unable to download assert through the proxy.

I am in windows 10 X64, python 2.7 latest.

I have tried either `nltk.set_proxy('http://127.0.0.1:1080')` or `nltk.set_proxy('http://127.0.0.1:1080', '', '')`, both not work.




"
202,https://github.com/nltk/nltk/issues/1625,1625,[],closed,2017-02-14 06:23:32+00:00,,3,StanfordNERTagger for Chinese got `java.io.StreamCorruptedException: invalid stream header: 504B0304`,"Hi, I am trying to use ntlk to detect ner in Chinese text.

As https://github.com/stanfordnlp/CoreNLP/issues/358 mentioned , `chinese.misc.distsim.crf.ser.gz` has been merged into 'stanford-chinese-corenlp-2016-10-31-models.jar'

My test code:
```
from nltk.tag import StanfordNERTagger

chi_tagger = StanfordNERTagger('stanford-chinese-corenlp-2016-10-31-models.jar')
sent = u'北海 已 成为 中国 对外开放 中 升起 的 一 颗 明星'
for word, tag in  chi_tagger.tag(sent.split()):
    print word.encode('utf-8'), tag
```

Error:
```
Invoked on Tue Feb 14 06:07:47 UTC 2017 with arguments: -loadClassifier /mnt/nlp/stanford-chinese-model/stanford-chinese-corenlp-2016-10-31-models.jar -textFile /tmp/tmp3mvoLR -outputFormat slashTags -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerOptions ""tokenizeNLs=false"" -encoding utf8
tokenizerFactory=edu.stanford.nlp.process.WhitespaceTokenizer
tokenizerOptions=""tokenizeNLs=false""
loadClassifier=/mnt/nlp/stanford-chinese-model/stanford-chinese-corenlp-2016-10-31-models.jar
encoding=utf8
textFile=/tmp/tmp3mvoLR
outputFormat=slashTags
Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: java.io.StreamCorruptedException: invalid stream header: 504B0304
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1507)
        at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3017)
Caused by: java.io.StreamCorruptedException: invalid stream header: 504B0304
        at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:808)
        at java.io.ObjectInputStream.<init>(ObjectInputStream.java:301)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1462)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1494)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1505)
        ... 1 more

---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
<ipython-input-4-120cdfa045bb> in <module>()
      3 chi_tagger = StanfordNERTagger('stanford-chinese-corenlp-2016-10-31-models.jar')
      4 sent = u'北海 已 成为 中国 对外开放 中 升起 的 一 颗 明星'
----> 5 for word, tag in  chi_tagger.tag(sent.split()):
      6     print word.encode('utf-8'), tag
      7

/usr/local/lib/python2.7/dist-packages/nltk/tag/stanford.pyc in tag(self, tokens)
     69     def tag(self, tokens):
     70         # This function should return list of tuple rather than list of list
---> 71         return sum(self.tag_sents([tokens]), [])
     72
     73     def tag_sents(self, sentences):

/usr/local/lib/python2.7/dist-packages/nltk/tag/stanford.pyc in tag_sents(self, sentences)
     92         # Run the tagger and get the output
     93         stanpos_output, _stderr = java(cmd, classpath=self._stanford_jar,
---> 94                                                        stdout=PIPE, stderr=PIPE)
     95         stanpos_output = stanpos_output.decode(encoding)
     96

/usr/local/lib/python2.7/dist-packages/nltk/__init__.pyc in java(cmd, classpath, stdin, stdout, stderr, blocking)
    132     if p.returncode != 0:
    133         print(_decode_stdoutdata(stderr))
--> 134         raise OSError('Java command failed : ' + str(cmd))
    135
    136     return (stdout, stderr)

OSError: Java command failed : [u'/usr/bin/java', '-mx1000m', '-cp', '/mnt/nlp/stanford-ner-2016-10-31/stanford-ner-3.7.0-javadoc.jar:/mnt/nlp/stanford-ner-2016-10-31/stanford-ner-3.7.0-sources.jar:/mnt/nlp/stanford-ner-2016-10-31/stanford-ner.jar:/mnt/nlp/stanford-ner-2016-10-31/stanford-ner-3.7.0.jar:/mnt/nlp/stanford-ner-2016-10-31/lib/joda-time.jar:/mnt/nlp/stanford-ner-2016-10-31/lib/stanford-ner-resources.jar:/mnt/nlp/stanford-ner-2016-10-31/lib/jollyday-0.4.9.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', '/mnt/nlp/stanford-chinese-model/stanford-chinese-corenlp-2016-10-31-models.jar', '-textFile', '/tmp/tmp3mvoLR', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '""tokenizeNLs=false""', '-encoding', 'utf8']
```

I am not familiar with JAVA stack and don't know if `StanfordNERTagger('stanford-chinese-corenlp-2016-10-31-models.jar')` is correct. 
"
203,https://github.com/nltk/nltk/issues/1628,1628,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-02-15 01:37:24+00:00,,3,Re-organizing and refactoring nltk/test,"I suggest that we do some refactoring/reorganizing of `nltk/test/`. Currently the `*.doctest` files are on the `nltk/test` mixed with the `tox` and CI config files and scripts. 

Possibly, this would be a better organization:

```
nltk
    /test
        /doctests
            *.doctest
        /unittests
            *.py
        /ipynbs
           *.ipynb
        # all other files for tox and CI
```

@stevenbird @fievelk any suggestions?"
204,https://github.com/nltk/nltk/issues/1629,1629,[],closed,2017-02-15 05:13:31+00:00,,1,Unknown XML files in main repo directory,"There are these files that somehow got into the repository in #1429 

 - nosetests.xml
- nosetests_scrubbed.xml
- coverage.xml

@adamn Are these files from your debug log? Or are they necessary for actual tests? "
205,https://github.com/nltk/nltk/issues/1630,1630,[],closed,2017-02-15 10:37:22+00:00,,5,word_tokenize replaces characters,"When using the word_tokenize function the quotation marks get replaced with different quotation marks.

Example (german):

```
import nltk
sentence = ""\""Ja.\"""" # sentence[0] = ""
tokens = nltk.word_tokenize(sentence) #tokens[0] = ``
print(tokens[0] == sentence[0]) # Prints false.
```

Is this a bug or is there a reasoning behind this behaviour?"
206,https://github.com/nltk/nltk/issues/1632,1632,[],closed,2017-02-15 20:55:31+00:00,,3,StanfordPnOSTagger failed to tag the underscore,"Please see the python code below:

from nltk.tag.stanford import StanfordPOSTagger

english_postagger = StanfordPOSTagger('./models/english-left3words-distsim.tagger', './stanford-postagger.jar')
english_postagger.java_options='-mx4096m'

print english_postagger.tag(['_'])


Here is my result:
[(u'', u'SYM')]

The underscore just disappeared. I checked the code, that's because in function ""parse_output(self, text, sentences = None)"", the code ""word_tags = tagged_word.strip().split(self._SEPARATOR)"" would delete all ""_SEPARATOR"" which is underscore based on the result of StandfordTagger.

One suggestion, probably we can use this line of code instead:  
word_tags = tagged_word.strip().rsplit(self._SEPARATOR,1)"
207,https://github.com/nltk/nltk/issues/1634,1634,[],closed,2017-02-19 17:28:07+00:00,,8,corpus_bleu does not match multi-bleu.perl for very poor translations,"
example:

**tokenized reference:**
```
Their tasks include changing a pump on the faulty stokehold .
Likewise , two species that are very similar in morphology were distinguished using genetics .
```

**tokenized hypothesis:**
```
Teo S yb , oe uNb , R , T t , , t
Tue Ar saln S , , 5istsi l , 5oe R ulO sae oR R
```

with the `multi-bleu.perl` script the BLEU score is `0`:

```
BLEU = 0.00, 3.4/0.0/0.0/0.0 (BP=1.000, ratio=1.115, hyp_len=29, ref_len=26)
```

with `corpus_bleu` the BLUE score is `43.092382`, my understanding is that default `corpus_bleu` settings corresponds to the `multi-bleu.perl` script.

---

```py
from nltk.translate.bleu_score import corpus_bleu

references = [
    'Their tasks include changing a pump on the faulty stokehold .',
    'Likewise , two species that are very similar in morphology were distinguished using genetics .'

]

hypothesis = [
    'Teo S yb , oe uNb , R , T t , , t',
    'Tue Ar saln S , , 5istsi l , 5oe R ulO sae oR R'
]

hypothesis_tokens = [line.split(' ') for line in hypothesis]
references_tokens = [[line.split(' ')] for line in references]

# calculate corpus-bleu score
bleu = corpus_bleu(
    references_tokens, hypothesis_tokens
)

print('BLEU: %f' % (bleu * 100)) # 43.092382
```
"
208,https://github.com/nltk/nltk/issues/1640,1640,[],closed,2017-02-27 19:22:49+00:00,,2,FreqDist.N() is extremely inefficient,"I am currently using NLTK to train and apply a model based on conditional probabilities. For this I'm using `ConditionalProbDist` constructed from a `ConditionalFreqDist`. During the use of the trained model I noticed extreme slowness for what should simply be arithmetics on conditional probabilities and started to investigate.

A quick profiling run revealed that most (over 99%) of my program's time was being spent evaluating `FreqDist.N()`. A probability distribution used for a `ConditionalProbDist`, such as `MLEProbDist`, calls `FreqDist.freq(sample)`, which in turn is calling `FreqDist.N()` to calculate the probability. The implementation of `FreqDist.N()` looks like:
```python
    def N(self):
        return sum(self.values())
```

The alarming part in there is the `sum` which iterates over the entire frequency distribution. This causes an innocent `FreqDist.freq(sample)` call, which is supposed to be a trivial division of two numbers that could be an O(1) operation, turn into an O(n) operation. When using conditional probability distributions to model anything sufficiently big (e.g. n-grams), such distribution will contain a massive number of `FreqDist`s, each one itself containing a massive number of values, slowing a program relying on this down by multiple orders of magnitude as every single probability calculation iterates through the entire distribution, a completely unnecessary and wasteful operation.

All of which leads to some questions:

* Is there a reason `FreqDist.N()` is implemented this way?
* Why wouldn't a `FreqDist` simply keep track of its `N` so such lookup would be constant?
"
209,https://github.com/nltk/nltk/issues/1641,1641,[],closed,2017-02-28 18:43:03+00:00,,0,Non-English lemmas containing capital letters cannot be looked up using wordnet.lemmas() or wordnet.synsets(),"This is an existing bug that I stumbled across while using the German WordNet from the EOMW via my custom-WordNet loading code in my unmerged PR at https://github.com/nltk/nltk/pull/1621. (It's dramatically more serious for German, since all nouns in German are capitalised and so a huge fraction of the language doesn't work, but it also affects the existing OMW WordNets with support built into NLTK.)

Consider the synset representing London, England. While the synset name is in lowercase, its lemmas are capitalised in both the English WordNet...

```
>>> london_synset = wn.synset('london.n.01')
>>> london_synset.definition()
'the capital and largest city of England; located on the Thames in southeastern England; financial and industrial and cultural center'
>>> london_synset.lemmas()
[Lemma('london.n.01.London'), Lemma('london.n.01.Greater_London'), Lemma('london.n.01.British_capital'), Lemma('london.n.01.capital_of_the_United_Kingdom')]
```

... and also in the French WordNet:

```
>>> london_synset.lemmas(lang='fra')
[Lemma('london.n.01.Grand_Londres'), Lemma('london.n.01.Hellgate:_London'), Lemma('london.n.01.London'), Lemma('london.n.01.Londres')]
```

But when using the English WordNet, I can look up the synset (or an individual `Lemma`) by lemma by passing in 'London' in whatever capitalisation I like:

```
>>> wn.synsets('London')
[Synset('london.n.01'), Synset('london.n.02')]
>>> wn.synsets('london')
[Synset('london.n.01'), Synset('london.n.02')]
>>> wn.synsets('lOnDoN')
[Synset('london.n.01'), Synset('london.n.02')]
>>> wn.lemmas('london')
[Lemma('london.n.01.London'), Lemma('london.n.02.London')]
>>> wn.lemmas('London')
[Lemma('london.n.01.London'), Lemma('london.n.02.London')]
>>> wn.lemmas('LoNdoN')
[Lemma('london.n.01.London'), Lemma('london.n.02.London')]
```

In non-English, on the other hand, it is impossible to look up this synset by lemma, because the first line of `wn.synsets()` coerces the `lemma` passed in to lowercase, and that lemma is then used as a key to look up the synset in a lemma-to-synset dictionary in which `Londres` is *capitalised*.

```
>>> wn.synsets('Londres', lang='fra')
[]
>>> wn.synsets('londres', lang='fra')
[]
>>> wn.lemmas('londres', lang='fra')
[]
>>> wn.lemmas('Londres', lang='fra')
[]
```

(Contrast this with lemmas that are lowercased in the French WordNet's tab file; they can be looked up regardless of how the `lemma` passed to `synsets()` is capitalised:

```
>>> wn.synsets('calin', lang='fra')
[Synset('cuddlesome.s.01')]
>>> wn.synsets('cAlIn', lang='fra')
[Synset('cuddlesome.s.01')]
```

)

To match the English behaviour, the behaviour of `synsets()` for non-English WordNets should be adjusted so that the lookup is properly case-insensitive. This was probably the intent of coercing the given `lemma` to lowercase before doing the lookup, but fails if the `Lemma` to be looked up is spelt with a capital letter in the actual WordNet data."
210,https://github.com/nltk/nltk/issues/1642,1642,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-03-01 09:45:49+00:00,,4,ConcordanceIndex.print_concordance  make target token highlight,"As below pic , it need a highlight for the target token for better viewing.

![default](https://cloud.githubusercontent.com/assets/3938751/23454272/c464e522-fea6-11e6-8372-7c3dd0bf9c7e.png)
"
211,https://github.com/nltk/nltk/issues/1644,1644,[],closed,2017-03-02 01:01:23+00:00,,8,Adding Github topics ,"Github introduced topic tags to tag repositories: https://github.com/blog/2309-introducing-topics

It'll be great if we add at least `nltk`, `nlp`, `python`  tags to the repository.."
212,https://github.com/nltk/nltk/issues/1646,1646,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-03-07 22:50:41+00:00,,0,Broken function ntlk.sem.evaluate.is_rel,"In nltk.sem.evaluate.is_rel()

the elif statement claims to check that all tuples of set s are same length with 

`len(max(s))==len(min(s))`

This will not succeed.  For instance 

In [52]: foo = nltk.sem.evaluate.is_rel                                                                                
                                                                                                                       
In [53]: s = {('a',), ('b', 'b'), ('c',)}                                                                              
                                                                                                                       
In [54]: foo(s)                                                                                                        
Out[54]: True 

We could replace this check with something like

`len(set(len(x) for x in s)) == 1 `

or 

`max(len(x) for x in s) == min(len(x) for x in s)`

but, unless told otherwise, I'll leave this to others who may want to optimize this.

(Note:  grep shows that that is_rel() is currently only used in an assert statement in nltk.sem.chat80.Concept.close() and a couple times in /test/semantics.doctest.)"
213,https://github.com/nltk/nltk/issues/1647,1647,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-03-09 17:45:58+00:00,,6,nltk data download error 503: first byte time out.,"I'm using `python -m nltk.downloader all` to download the nltk data.  But face a lot of following errors.

> [nltk_data]    | Error downloading u'cmudict' from
> [nltk_data]    |     <https://raw.githubusercontent.com/nltk/nltk_data
> [nltk_data]    |     /gh-pages/packages/corpora/cmudict.zip>:   HTTP
> [nltk_data]    |     Error 503: first byte timeout

> [nltk_data]    | Error downloading u'moses_sample' from
> [nltk_data]    |     <https://raw.githubusercontent.com/nltk/nltk_data
> [nltk_data]    |     /gh-pages/packages/models/moses_sample.zip>:
> [nltk_data]    |     HTTP Error 503: first byte timeout

Python version is 2.7.12. nltk version is 3.2.2. On Ubuntu 16.04"
214,https://github.com/nltk/nltk/issues/1648,1648,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-03-10 05:18:04+00:00,,4,Porting several stemming algorithms to NLTK,"`whoosh` has a couple of great things that we can port to NLTK, other than the porter stemmer, there are:

 - [Paice-Husk stemming ](https://bitbucket.org/mchaput/whoosh/src/e344fb64067e45d47ec62dc65a75a50be51264a7/src/whoosh/lang/paicehusk.py?at=default&fileviewer=file-view-default) 
 - [Lovin stemmer](https://bitbucket.org/mchaput/whoosh/src/e344fb64067e45d47ec62dc65a75a50be51264a7/src/whoosh/lang/lovins.py?at=default&fileviewer=file-view-default)

This will be an easy port since the original implementation is in Python. Simply port the code, write some tests and document the functions appropriately, put the module in `nltk/nltk/stem/` and do a pull-request =)

Any takers?"
215,https://github.com/nltk/nltk/issues/1650,1650,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-03-11 21:50:11+00:00,,0,Tokenizer in RTEFeatureExtractor,"(Copying from discussion at pull request resulting in eac9799 at @stevenbird 's request)

Perhaps a deeper question is why we are using the RegexpTokenizer here instead of the much more robust TreebankWordTokenizer.

The documentation in RTEFeatureExtractor says:

""Try to tokenize so that abbreviations, monetary amounts, email                                             
addresses, URLs are single tokens.""

For instance:

In []: tt = nltk.TreebankWordTokenizer()                                                                                                                      
In []: rt = nltk.RegexpTokenizer('[\w.@:/]+|\w+|\$[\d.]+')                                                                               
In []: sent = ""I paid $3.50 for an old U.S. coin at name@domain.com yesterday.""
In []: print tt.tokenize(sent)                                                                                     
['I', 'paid', '$', '3.50', 'for', 'an', 'old', 'U.S.', 'coin', 'at', 'name', '@', 'domain.com', 'yesterday', '.']                                                                         
In []: print rt.tokenize(sent)                                                                                     
['I', 'paid', '$3.50', 'for', 'an', 'old', 'U.S.', 'coin', 'at', 'name@domain.com', 'yesterday.'] 

(Notice that both agree on the abbreviation ""U.S."", but notice the difference in the monetary amount, the email address, and the final period.)

I can understand that one might want to treat URLs/email addresses as a single token (although I'm not sure I fully understand why $3.50 should be one token).  Regardless, given that this RegexpTokenizer will have poor performance in other areas, perhaps the best strategy would be to teach the TreebankWordTokenizer to recognize URLs/email addresses and use this in RTEFeatureExtractor?"
216,https://github.com/nltk/nltk/issues/1652,1652,[],closed,2017-03-12 06:11:13+00:00,,2,Stanford segmenter nltk Could not find SLF4J in your classpath,"I set up a nltk and stanford environment, nltk and stanford jars has downloaded, the program with nltk was ok, but I had a trouble with stanford segmenter. just make a simple program via stanford segmenter, I got a error is Could not find SLF4J in your classpath, although I had export all jars include slf4j-api.jar. Detail as follows

Python3.5 NLTK, 3.2.2, Standford jars 3.7

OS: Centos 

Environment variable:
```
export JAVA_HOME=/usr/java/jdk1.8.0_60
    export NLTK_DATA=/opt/nltk_data
    export STANFORD_SEGMENTER_PATH=/opt/stanford/stanford-segmenter-3.7
    export CLASSPATH=$CLASSPATH:$STANFORD_SEGMENTER_PATH/stanford-segmenter.jar
    export STANFORD_POSTAGGER_PATH=/opt/stanford/stanford-postagger-full-2016-10-31
    export CLASSPATH=$CLASSPATH:$STANFORD_POSTAGGER_PATH/stanford-postagger.jar
    export STANFORD_NER_PATH=/opt/stanford/stanford-ner-2016-10-31
export CLASSPATH=$CLASSPATH:$STANFORD_NER_PATH/stanford-ner.jar
    export STANFORD_MODELS=$STANFORD_NER_PATH/classifiers:$STANFORD_POSTAGGER_PATH/models
    export STANFORD_PARSER_PATH=/opt/stanford/stanford-parser-full-2016-10-31
    export CLASSPATH=$CLASSPATH:$STANFORD_PARSER_PATH/stanford-parser.jar:$STANFORD_PARSER_PATH/stanford-parser-3.6.0-models.jar:$STANFORD_PARSER_PATH/slf4j-api.jar:$STANFORD_PARSER_PATH/ejml-0.23.jar
    export STANFORD_CORENLP_PATH=/opt/stanford/stanford-corenlp-full-2016-10-31
    export CLASSPATH=$CLASSPATH:$STANFORD_CORENLP_PATH/stanford-corenlp-3.7.0.jar:$STANFORD_CORENLP_PATH/stanford-corenlp-3.7.0-models.jar:$STANFORD_CORENLP_PATH/javax.json.jar:$STANFORD_CORENLP_PATH/joda-time.jar:$STANFORD_CORENLP_PATH/jollyday.jar:$STANFORD_CORENLP_PATH/protobuf.jar:$STANFORD_CORENLP_PATH/slf4j-simple.jar:$STANFORD_CORENLP_PATH/xom.jar
    export STANFORD_CORENLP=$STANFORD_CORENLP_PATH
```

The program as follows:
```
from nltk.tokenize import StanfordSegmenter
>>> segmenter = StanfordSegmenter(
    path_to_sihan_corpora_dict=""/opt/stanford/stanford-segmenter-3.7/data/"",
    path_to_model=""/opt/stanford/stanford-segmenter-3.7/data/pku.gz"",
    path_to_dict=""/opt/stanford/stanford-segmenter-3.7/data/dict-chris6.ser.gz""
)
>>> res = segmenter.segment(u""这是斯坦福中文分词器测试"")
```

The error as follows:

```
Exception in thread ""main"" java.lang.ExceptionInInitializerError
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.<clinit>(AbstractSequenceClassifier.java:88)
Caused by: java.lang.IllegalStateException: Could not find SLF4J in your classpath
    at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers.lambda$static$530(RedwoodConfiguration.java:190)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers$7.buildChain(RedwoodConfiguration.java:309)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers$7.apply(RedwoodConfiguration.java:318)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration.lambda$handlers$535(RedwoodConfiguration.java:363)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration.apply(RedwoodConfiguration.java:41)
    at edu.stanford.nlp.util.logging.Redwood.<clinit>(Redwood.java:609)
    ... 1 more
Caused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: java.lang.ClassNotFoundException: edu.stanford.nlp.util.logging.SLF4JHandler
    at edu.stanford.nlp.util.MetaClass.createFactory(MetaClass.java:364)
    at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:381)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers.lambda$static$530(RedwoodConfiguration.java:186)
    ... 6 more
Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.util.logging.SLF4JHandler
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at edu.stanford.nlp.util.MetaClass$ClassFactory.construct(MetaClass.java:135)
    at edu.stanford.nlp.util.MetaClass$ClassFactory.<init>(MetaClass.java:202)
    at edu.stanford.nlp.util.MetaClass$ClassFactory.<init>(MetaClass.java:69)
    at edu.stanford.nlp.util.MetaClass.createFactory(MetaClass.java:360)
    ... 8 more

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/python3/lib/python3.5/site-packages/nltk/tokenize/stanford_segmenter.py"", line 96, in segment
    return self.segment_sents([tokens])
  File ""/usr/local/python3/lib/python3.5/site-packages/nltk/tokenize/stanford_segmenter.py"", line 123, in segment_sents
    stdout = self._execute(cmd)
  File ""/usr/local/python3/lib/python3.5/site-packages/nltk/tokenize/stanford_segmenter.py"", line 143, in _execute
    cmd,classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE)
  File ""/usr/local/python3/lib/python3.5/site-packages/nltk/internals.py"", line 134, in java
    raise OSError('Java command failed : ' + str(cmd))
OSError: Java command failed : ['/usr/java/jdk1.8.0_60/bin/java', '-mx2g', '-cp', '/opt/stanford/stanford-segmenter-3.7/stanford-segmenter.jar:/opt/stanford/stanford-parser-full-2016-10-31/slf4j-api.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-sighanCorporaDict', '/opt/stanford/stanford-segmenter-3.7/data/', '-textFile', '/tmp/tmpkttpldl6', '-sighanPostProcessing', 'true', '-keepAllWhitespaces', 'false', '-loadClassifier', '/opt/stanford/stanford-segmenter-3.7/data/pku.gz', '-serDictionary', '/opt/stanford/stanford-segmenter-3.7/data/dict-chris6.ser.gz', '-inputEncoding', 'UTF-8']
```
"
217,https://github.com/nltk/nltk/issues/1653,1653,[],closed,2017-03-15 21:38:28+00:00,,4,Error training Unigram Tagger with Indian Corpus Data,"```
sents = nltk.corpus.indian.tagged_sents()
# 1280 is the index where the Bengali or Bangla corpus ends. 
>>> train = sents[0:int(1280)*0.9)]
>>> test = sents[int(l1280*0.9):]
>>> ug = nltk.UnigramTagger(train)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/djokester/anaconda3/lib/python3.5/site-packages/nltk/tag/sequential.py"", line 340, in __init__
    backoff, cutoff, verbose)
  File ""/home/djokester/anaconda3/lib/python3.5/site-packages/nltk/tag/sequential.py"", line 287, in __init__
    self._train(train, cutoff, verbose)
  File ""/home/djokester/anaconda3/lib/python3.5/site-packages/nltk/tag/sequential.py"", line 178, in _train
    tokens, tags = zip(*sentence)
ValueError: not enough values to unpack (expected 2, got 0)
```
Usually this is the general pathway we follow while training any Ngram tagger with Brown ortreebank corpus. But this doesn't work with the Indian corpus.Is there an error on my part or is this a bug? 
"
218,https://github.com/nltk/nltk/issues/1656,1656,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}]",open,2017-03-18 12:35:05+00:00,,3,New version of scipy causes fisher test to break,"Our [CI Server](https://nltk.ci.cloudbees.com/job/nltk/lastCompletedBuild/TOXENV=py35-jenkins,jdk=jdk8latestOnlineInstall/testReport/(root)/metrics_doctest/metrics_doctest/) reports this failing test:

```
File ""/scratch/jenkins/workspace/nltk/TOXENV/py35-jenkins/jdk/jdk8latestOnlineInstall/nltk/test/metrics.doctest"", line 242, in metrics.doctest
Failed example:
    bam.fisher(20, (42, 20), N) > bam.fisher(20, (41, 27), N)
Expected:
    False
Got:
    True
```

@pierpaolo points out that this test failure coincided with a new release of scipy. The failure goes away when we modify tox.ini to specify the previous version:

```
pip install -I 'scipy < 0.19'
; pip install -I scipy
```
"
219,https://github.com/nltk/nltk/issues/1658,1658,[],closed,2017-03-19 12:23:41+00:00,,4,Unable to find stanford-postagger.jar on CI server,"The CI server reports that NLTK cannot find the Stanford POS tagger:
https://nltk.ci.cloudbees.com/job/nltk/lastCompletedBuild/TOXENV=py27-jenkins,jdk=jdk8latestOnlineInstall/testReport/nltk.tag/stanford/StanfordPOSTagger/

Cf. http://stackoverflow.com/questions/34726200/nltk-was-unable-to-find-stanford-postagger-jar-set-the-classpath-environment-va"
220,https://github.com/nltk/nltk/issues/1659,1659,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-03-19 12:48:23+00:00,,11,NLTK support for Python 3.6,"Would NLTK be supporting Python3.6? 

Python3.6 had been shipped in Dec 2016 as scheduled on  [PEP 494](https://www.python.org/dev/peps/pep-0494/) . There's also an increase in Py3.6 uptake and the shelf-life should be around 2 years before [Py3.7 reaches the final dev stage](https://www.python.org/dev/peps/pep-0537/).

This might help in the port to Python 3.6 while keeping 2.7 and 3.4/3,5 functionalities: https://docs.python.org/3/whatsnew/3.6.html"
221,https://github.com/nltk/nltk/issues/1664,1664,[],closed,2017-03-21 00:38:38+00:00,,5,Defining utf8 encoding for all `.py` files,"Is it a good idea to add magic encoding ([PEP 263](https://www.python.org/dev/peps/pep-0263/#defining-the-encoding)) on all `.py` files to handle non-ascii systems e.g. #1024 ?

```
# -*- coding: utf-8 -*-
```"
222,https://github.com/nltk/nltk/issues/1665,1665,[],closed,2017-03-23 22:40:47+00:00,,3,Speed up FreqDist.N(),"The function FreqDist.N(), defined in probability.py, performs a sum of all contained values every time it is called. FreqDist inherits from collections.Counter, and instances of it often contain large collections, for instance all distinct unigrams in a text in the case of the TnT tagger. The N() function may thus have to iterate over tens or hundreds of thousands of values to yield its total count. This means that as is, it should be used with care. However, other commonly used functions, such as FreqDist.freq(), call N() - not once but twice per call!

This can easily be optimized and will yield much faster execution. One way to do it is to add a function to FreqDist:

```
def freeze_N(self):
    """""" Freezes the total item count N() at its current value, making its retrieval MUCH faster """"""
    n = self.N()
    self.N = lambda: n

```
...and then call the freeze_N() function for instance after training but before POS tagging new text. In my TnT tagging use case, this simple modification increased tagging speed by 5x.

Other solutions are possible, but in any case, the current behavior is extremely and unnecessarily slow."
223,https://github.com/nltk/nltk/issues/1666,1666,[],closed,2017-03-24 00:08:54+00:00,,2,Speed up TnT tagger with simple modification,"In the TnT tagger, in the function _tagword(), there is this piece of code:

```
        if word in self._wd.conditions():
            self.known += 1
```
self._wd.conditions() returns list(self.keys()), which is a list of all distinct words encountered during training - often tens of thousands, sometimes hundreds of thousands. The if statement searches linearly through this list, taking O(N) time. A simpler, equivalent, much quicker and more readable version would be:

```
        if word in self._wd:
            self.known += 1
```
...since self._wd is derived from a dict, anyway."
224,https://github.com/nltk/nltk/issues/1667,1667,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}]",closed,2017-03-24 22:36:22+00:00,,3,nltk.translate.gleu_score problems,"I see a few possible issues with the gleu_score implementation.

1. It doesn't allow multiple references, despite the [docstring saying it does](https://github.com/nltk/nltk/blob/develop/nltk/translate/gleu_score.py#L72). This is either a code or a doc issue. Can anyone clarify which?

2. I don't see why there is no `corpus_gleu()` function. The Wu et al. 2016 paper states (emphasis added):

   >According to our experiments, GLEU score correlates quite well with the BLEU metric **on a corpus level** but does not have its drawbacks for our per sentence reward objective

   So I would think there's a way to calculate this for the corpus. Maybe just `min(corpus_precision, corpus_recall)`, or alternatively the average of sentence-level GLEU scores?

3. The name GLEU is unfortunate; there are at least 2 other GLEUs that predate the 2016 paper:
   * Mutton et al. (2007) ""GLEU: Automatic Evaluation of Sentence-Level Fluency"" http://anthology.aclweb.org/P/P07/P07-1044.pdf
   * Napoles et al. (2015) ""Ground Truth for Grammatical Error Correction Metrics"" http://www.aclweb.org/anthology/P/P15/P15-2097.pdf

   Maybe the Wu et al. 2016 paper will choose a different name if they publish outside of arXiv? But for NLTK, would `google_bleu` or `gnmt_bleu` be less likely to step on toes?

I'd be happy to submit a PR when the above issues get resolved."
225,https://github.com/nltk/nltk/issues/1672,1672,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-03-27 14:38:14+00:00,,0,nltk.set_proxy not working,I was trying to download the data over a proxy. Tried the set_proxy function. Doesn't seem to make any difference.
226,https://github.com/nltk/nltk/issues/1673,1673,[],closed,2017-03-28 14:54:16+00:00,,4,UTF-8 UnicodeDecodeError and Segfault (with tkinter) on OS X,"`UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte
Segmentation fault: 11`

I get the above error. I believe this has something to do with python3 & tkinter (I am on python 3.5.x). I wanted to know if I am the only one seeing this issue. "
227,https://github.com/nltk/nltk/issues/1675,1675,[],closed,2017-04-02 06:37:26+00:00,,2,u201c and u201d should also be considered as open,"nltk.word_tokenize(""The unicode 201C and 201D “LEFT(RIGHT) DOUBLE QUOTATION MARK” is also OPEN_PUNCT and CLOSE_PUNCT."")

# result:
['The',
 'unicode',
 '201C',
 'and',
 '201D',
 '“LEFT',
 '(',
 'RIGHT',
 ')',
 'DOUBLE',
 'QUOTATION',
 'MARK”',
 'is',
 'also',
 'OPEN_PUNCT',
 'and',
 'CLOSE_PUNCT',
 '.']

note: there's word '“LEFT' and 'MARK”', which is unexpected.
(And also u+2018 and u+2019)"
228,https://github.com/nltk/nltk/issues/1676,1676,[],closed,2017-04-05 00:06:54+00:00,,1,Unable to generate parse trees with ccg parser,"I am trying to generate a parse tree for the below lexicon but the ChartParser doesn't produce any parses. Can anyone throw some light on this?

``` python 
example = '''
:- S, NP, N
jobs => NP
with => (NP\\NP)/NP
red_money => NP/NP
more => S\\NP
than => (NP/NP)\\(S\\NP)
$100 => NP
'''
sentence = ""jobs with red_money more than $100""
lex = lexicon.fromstring(example)
parser = chart.CCGChartParser(lex, ApplicationRuleSet + CompositionRuleSet + SubstitutionRuleSet)
for parse in parser.parse(sentence.split(' ')):
    printCCGDerivation(parse)
```"
229,https://github.com/nltk/nltk/issues/1677,1677,[],closed,2017-04-06 16:23:11+00:00,,0,CFG Error in Thailang,"I write thai cfg in nltk.

```python
import nltk
thaigrammar = nltk.CFG.fromstring(""""""
S -> NP VP
VP -> V NP PP ADV | V NP S1 | V NP | V
PP -> PREP NP
"""""")
rd_parser = nltk.RecursiveDescentParser(thaigrammar)
sent = 'แมว วิ่ง เหมือน วิ่ง'.split()
for tree in rd_parser.parse(sent):
	print(tree)
```
but , it's error. 

```
...
  File ""C:\Anaconda3-new\lib\site-packages\nltk\tree.py"", line 158, in __getitem__
    return self[index[0]][index[1:]]
  File ""C:\Anaconda3-new\lib\site-packages\nltk\tree.py"", line 158, in __getitem__
    return self[index[0]][index[1:]]
  File ""C:\Anaconda3-new\lib\site-packages\nltk\tree.py"", line 158, in __getitem__
    return self[index[0]][index[1:]]
  File ""C:\Anaconda3-new\lib\site-packages\nltk\tree.py"", line 156, in __getitem__
    return self[index[0]]
  File ""C:\Anaconda3-new\lib\site-packages\nltk\tree.py"", line 151, in __getitem__
    return list.__getitem__(self, index)
RecursionError: maximum recursion depth exceeded while calling a Python object
```
NLTK 3.2.2 , Python 3.5.2 in Windows"
230,https://github.com/nltk/nltk/issues/1679,1679,[],closed,2017-04-06 18:50:22+00:00,,1,How to use texttiling with emails?,"I want to tile the email text below into the 5 sections LOGISTICS, INTRO, BODY, OUTRO, POST EMAIL DISCLAIMER

```
email = """"""    From: X
    To: Y                             (LOGISTICS)
    Date: 10/03/2017

    Hello team,                       (INTRO)

    Some text here representing
    the body                          (BODY)
    of the text.

    Regards,                          (OUTRO)
    X

    *****DISCLAIMER*****              (POST EMAIL DISCLAIMER)
    THIS EMAIL IS CONFIDENTIAL
    IF YOU ARE NOT THE INTENDED RECIPIENT PLEASE DELETE THIS EMAIL""""""

tt = TextTilingTokenizer(demo_mode=False)
tiles = tt.tokenize(email)
```

But when I run the code above, I get the same string back in list form, and un-tiled in any way. I have tried using several parameters in TextTilingTokenizer, including (w=2, k=10, similarity_method=0, stopwords=None, smoothing_method=[0], smoothing_width=2, smoothing_rounds=1, cutoff_policy=1, demo_mode=False), but nothing seems to work. Can anyone please advise?
"
231,https://github.com/nltk/nltk/issues/1681,1681,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-04-10 09:17:53+00:00,,3,"Zipfile raises error when ""assert self.fp is None"" in Python3.6","I only get this error on occasion and only in a production-like environment after running some code for several hours. The only thing related to nltk is that I use the `MosesTokenizer`.

I've tried quite a things even setting `self.fp = None` manually after `self.close()` is called, nothing worked. In the end I had to just make `OpenOnDemandZipFile` an alias for `zipfile.ZipFile`.

_I'm using python 3.6 which may be the cause of the error._

```
  File ""/zhome/ff/2/77654/stdpy3/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py"", line 78, in words
    return [line for line in line_tokenize(self.raw(fileids))
  File ""/zhome/ff/2/77654/stdpy3/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py"", line 28, in raw
    return concat([self.open(f).read() for f in fileids])
  File ""/zhome/ff/2/77654/stdpy3/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py"", line 28, in <listcomp>
    return concat([self.open(f).read() for f in fileids])
  File ""/zhome/ff/2/77654/stdpy3/lib/python3.6/site-packages/nltk/corpus/reader/api.py"", line 211, in open
    stream = self._root.join(file).open(encoding)
  File ""/zhome/ff/2/77654/stdpy3/lib/python3.6/site-packages/nltk/data.py"", line 511, in open
    data = self._zipfile.read(self._entry)
  File ""/zhome/ff/2/77654/stdpy3/lib/python3.6/site-packages/nltk/data.py"", line 995, in read
    assert self.fp is None
```

"
232,https://github.com/nltk/nltk/issues/1685,1685,[],closed,2017-04-14 04:45:32+00:00,,2,Urdu corpus,"I want to download complete urdu corpus, ASC II CODE for urdu and urdu wordlist  for python 3.5.2 but i am not able to find it anywhere. can anyone help me please ? send me the links from where i can find both of these."
233,https://github.com/nltk/nltk/issues/1686,1686,[],closed,2017-04-14 12:25:19+00:00,,5,Something wrong with your https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml,"[In xml online validator](http://www.xmlvalidation.com)  tell:
```
An error has been found! 
Click on  to jump to the error. In the document, you can point at  with your mouse to see the error message. 
Errors in file xml-schema: 
	23:	144	Attribute name ""unzipped_size"" associated with an element type ""package"" must be followed by the ' = ' character.
```
When i trying download `stopwords` in python3 
```
import nltk
nltk.download('stopwords')
```
Got error 
```
>>> import nltk
>>> nltk.download('stopwords')

Traceback (most recent call last):
  File ""/usr/lib/python3.5/code.py"", line 91, in runcode
    exec(code, self.locals)
  File ""<input>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/nltk/downloader.py"", line 664, in download
    for msg in self.incr_download(info_or_id, download_dir, force):
  File ""/usr/local/lib/python3.5/dist-packages/nltk/downloader.py"", line 534, in incr_download
    try: info = self._info_or_id(info_or_id)
  File ""/usr/local/lib/python3.5/dist-packages/nltk/downloader.py"", line 508, in _info_or_id
    return self.info(info_or_id)
  File ""/usr/local/lib/python3.5/dist-packages/nltk/downloader.py"", line 875, in info
    self._update_index()
  File ""/usr/local/lib/python3.5/dist-packages/nltk/downloader.py"", line 825, in _update_index
    ElementTree.parse(compat.urlopen(self._url)).getroot())
  File ""/usr/lib/python3.5/xml/etree/ElementTree.py"", line 1184, in parse
    tree.parse(source, parser)
  File ""/usr/lib/python3.5/xml/etree/ElementTree.py"", line 596, in parse
    self._root = parser._parse_whole(source)
xml.etree.ElementTree.ParseError: not well-formed (invalid token): line 23, column 143
```
In your [xml ](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml)
`<package checksum=""6f9c042774b96366c93fd0f9a9adb697"" id=""dolch"" name=""Dolch Word List"" size=""2116"" subdir=""corpora"" unzip=""1"" unzipped_size""1917"" url=""https://en.wikipedia.org/wiki/Dolch_word_list"" />`

**unzipped_size""1917""** must be unzipped_size=""1917"" 
**MISSING EQUALS SIGN**"
234,https://github.com/nltk/nltk/issues/1687,1687,[],closed,2017-04-14 15:01:19+00:00,,2,Can't download punkt anymore,"Starting this morning, `nltk.download('punkt')` results in this error:
```
  File ""/Users/seandodd/code/Sites-API/env/lib/python3.5/site-packages/nltk/downloader.py"", line 664, in download
    for msg in self.incr_download(info_or_id, download_dir, force):
  File ""/Users/seandodd/code/Sites-API/env/lib/python3.5/site-packages/nltk/downloader.py"", line 534, in incr_download
    try: info = self._info_or_id(info_or_id)
  File ""/Users/seandodd/code/Sites-API/env/lib/python3.5/site-packages/nltk/downloader.py"", line 508, in _info_or_id
    return self.info(info_or_id)
  File ""/Users/seandodd/code/Sites-API/env/lib/python3.5/site-packages/nltk/downloader.py"", line 875, in info
    self._update_index()
  File ""/Users/seandodd/code/Sites-API/env/lib/python3.5/site-packages/nltk/downloader.py"", line 825, in _update_index
    ElementTree.parse(compat.urlopen(self._url)).getroot())
  File ""/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/xml/etree/ElementTree.py"", line 1184, in parse
    tree.parse(source, parser)
  File ""/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/xml/etree/ElementTree.py"", line 596, in parse
    self._root = parser._parse_whole(source)
xml.etree.ElementTree.ParseError: not well-formed (invalid token): line 23, column 143
```"
235,https://github.com/nltk/nltk/issues/1688,1688,[],closed,2017-04-14 15:10:38+00:00,,1,Getting Parse error on nltk.download('all'),">>> import nltk
>>> nltk.download('all')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/hi-161/Pictures/unix/local/lib/python2.7/site-packages/nltk/downloader.py"", line 664, in download
    for msg in self.incr_download(info_or_id, download_dir, force):
  File ""/home/hi-161/Pictures/unix/local/lib/python2.7/site-packages/nltk/downloader.py"", line 534, in incr_download
    try: info = self._info_or_id(info_or_id)
  File ""/home/hi-161/Pictures/unix/local/lib/python2.7/site-packages/nltk/downloader.py"", line 508, in _info_or_id
    return self.info(info_or_id)
  File ""/home/hi-161/Pictures/unix/local/lib/python2.7/site-packages/nltk/downloader.py"", line 875, in info
    self._update_index()
  File ""/home/hi-161/Pictures/unix/local/lib/python2.7/site-packages/nltk/downloader.py"", line 825, in _update_index
    ElementTree.parse(compat.urlopen(self._url)).getroot())
  File ""/usr/lib/python2.7/xml/etree/ElementTree.py"", line 1182, in parse
    tree.parse(source, parser)
  File ""/usr/lib/python2.7/xml/etree/ElementTree.py"", line 656, in parse
    parser.feed(data)
  File ""/usr/lib/python2.7/xml/etree/ElementTree.py"", line 1653, in feed
    self._raiseerror(v)
  File ""/usr/lib/python2.7/xml/etree/ElementTree.py"", line 1517, in _raiseerror
    raise err
xml.etree.ElementTree.ParseError: not well-formed (invalid token): line 23, column 143"
236,https://github.com/nltk/nltk/issues/1689,1689,"[{'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",open,2017-04-16 10:47:47+00:00,,1,"wordnet.doctest contains references to bugs by number, but they're not issues from this GitHub repo","See https://github.com/nltk/nltk/blob/3.2.2/nltk/test/wordnet.doctest. We've got references like:

> Bug 284: instance hypernyms not used in similarity calculations

and

> Issue 541: add domains to wordnet

These certainly ain't references to issues opened here at https://github.com/nltk/nltk though; #284 and #541 are both non-WordNet-related issues.

I'm guessing these issue numbers refer to the issue tracker of some other project hosting platform that historically hosted NLTK - although I have no idea where that was and will need someone familiar with the project's history like @stevenbird to point me in the right direction. If those issues are still visible on the internet, then we should replace the issue numbers with full URLs to the issues; otherwise I'll try to get Wayback Machine links."
237,https://github.com/nltk/nltk/issues/1694,1694,"[{'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}]",open,2017-04-17 17:56:30+00:00,,0,Tree pretty_print() fails on a tree containing features,"If I try to call tree.pretty_print() on a tree containing FeatStructNonterminal() built from a feature-grammar, it throws the exception:

    Traceback (most recent call last):
      File ""test.py"", line 22, in <module>
        tree.pretty_print()
      File ""myproject/.env/local/lib/python2.7/site-packages/nltk/tree.py"", line 695, in pretty_print
        print(TreePrettyPrinter(self, sentence, highlight).text(**kwargs),
      File ""myproject/.env/local/lib/python2.7/site-packages/nltk/treeprettyprinter.py"", line 373, in text
        label = label.split('\n')
    AttributeError: 'FeatStructNonterminal' object has no attribute 'split'

I'm using `nltk==3.2.2`."
238,https://github.com/nltk/nltk/issues/1695,1695,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",closed,2017-04-18 06:29:49+00:00,,4,IBM-3 Model is too slow,"Hi,

    Recently, I want to use IBM Model3 in nltk for word alignment. 
    However, the running time is really unaffordable 
    (e.g., for training over 100 aligned sentences, 4 hours are needed per iteration). 
    Is there anybody encounter the same problem?

    p.s: IBM-2 and IBM-1 only cost a few seconds on the same machine.

"
239,https://github.com/nltk/nltk/issues/1696,1696,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-04-21 10:51:25+00:00,,7,OpenOnDemandZipFile fail to open punkt.zip on python3.x,"An exception raises when excuting an simple tutorial in nltk_book as the following command.
`$ python3 -c 'from nltk import word_tokenize; text = word_tokenize(""And now for something completely different"")'`

I think the problem is caused by the decorator @py3_data on ZipFilePathPointer.\_\_init\_\_ and OpenOnDemandZipFile.\_\_init\_\_. This decorator will append '/PY3' to the first arg, which is a a zip filepath as str, like  ""\~/nltk_data/tokenizers/punkt.zip"", but ""~/nltk_data/tokenizers/punkt.zip/PY3"" can't be opened as a zipfile.

```
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/git/nltk/nltk/tokenize/__init__.py"", line 109, in word_tokenize
    return [token for sent in sent_tokenize(text, language)
  File ""/git/nltk/nltk/tokenize/__init__.py"", line 93, in sent_tokenize
    tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))
  File ""/git/nltk/nltk/data.py"", line 808, in load
    opened_resource = _open(resource_url)
  File ""/git/nltk/nltk/data.py"", line 926, in _open
    return find(path_, path + ['']).open()
  File ""/git/nltk/nltk/data.py"", line 648, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource 'tokenizers/punkt/PY3/english.pickle' not found.
  Please use the NLTK Downloader to obtain the resource:  >>>
  nltk.download()
  Searched in:
    - '/home/joybin/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
    - ''
**********************************************************************
```
"
240,https://github.com/nltk/nltk/issues/1697,1697,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-04-21 19:32:37+00:00,,0,SimpleGoodTuring did not find a proper best fit warning in test,"I installed NLTK 3.0 April 15, 2017 (using a pip install since I am running on Windows 8 64 bit and Python 2.7 64 bit). I ran the test suite via nltk/test/runtests.py and had a few errors due to unavailable corpora, which I subsequently downloaded. However, I continue to get a warning from the probability.py SimpleGoodTuringProbDist find_best_fit source line testing if self._slope >= -1. I determined that it is entering that line with self._slope == to 0, causing the warning 'SimpleGoodTuring did not find a proper best fit...probabilty estimates...unreliable'. After adding a simple print self._slope (and wait for ENTER with raw_input so I can record the value) I noted that prior to the 0 value, self._slope had values of -1.1336102829, -1.24421002262, and -1.16221626339, i.e., some other code (HMM I assume) was calling SimpleGoodTuringProbDist interatively trying to achieve an optimal value for the smoothing, scanning up and down before hitting zero.

This is occuring in the probability.doctest sequence where HMM estimators are tested on a subset of the Brown corpus, using supervised training with an estimator supplied as a lambda function. MLEProbDist, Laplace, ELEProbDist, WittenBellProbDist run without error, yielding proper values. When the lambda is assigned to SimpleGoodTuring (with bins=1e5) the train_and_test call produces the correct value (86.93%), but the warning described above occurs.

I ran the probability.py demo (by running probability.py standalone) and it appeared to be credible values compared to the other outcome samplers (particularly after 5000 outcomes, where it hit actual on the money pretty much). However, I still got the warning described above.

I need to read the NLTK book to become more familiar with NLTK and the subject generally, but I wanted to let you know I was encountering this anomaly, in case it impacted accuracy in other areas.  Looking at probability.py source it appears likely that self._slope is being assigned a value of 0 in the protective test of x_var for zero before dividing xy_cov by x_var, i.e., finding x_var is 0 on entry and skipping the division, rather using self._slope == 0, which then hits the next line test for GTE -1 and fires the warning."
241,https://github.com/nltk/nltk/issues/1699,1699,[],closed,2017-04-24 19:33:35+00:00,,3,Nltk word tokenizer strange behaviour,"Given the sentence: 

word = **""B. Young c Moin Khan b Wasim 0""**

if we call nltk.word_tokenize(word) We'll have the following tokens:
    **['B', '.', 'Young', 'c', 'Moin', 'Khan', 'b', 'Wasim', '0']**

But, if we change the sentence to be:
word = **""C. Young c Moin Khan b Wasim 0""**

The tokens will be:
    **['C.', 'Young', 'c', 'Moin', 'Khan', 'b', 'Wasim', '0']**

The char '.' is now joined to the letter 'C' .

What is more strange is that the  **'B'**  splitted from  **'.'** only occurs when the next word is **'Young'** I tried several words and none appears to reproduce the error.


"
242,https://github.com/nltk/nltk/issues/1700,1700,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}]",open,2017-04-24 20:52:14+00:00,,6,sys.platform.startswith('win') is not a reliable way of detecting the OS,"'ve been doing some work with the NLTK using IronPython and Windows, and discovered we'll often run into trouble with file paths. It appears that we mostly use the following to determine whether we're on Windows:

`windows = sys.platform.startswith('win')`

Unfortunately, IronPython has a sys.platform of ""cli"". I haven't been able to dig up historical reasons for this, but I have identified several alternate ways to identify the platform:

```
>>> os.name
'nt'
>>> platform.system()
'Windows'
```

Both of these appear to be more reliable than ""sys.platform"". Does anyone know if there's a particular reason these aren't being used? If there are no objections, I can create a pull request with the changes."
243,https://github.com/nltk/nltk/issues/1701,1701,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-04-26 05:10:40+00:00,,0,skolemize() fails to create skolem constants,"A skolem constant is not created for existential quantifiers. Instead, an IndividualVariableExpression is created as though it had been a universal quantifier. The lack of distinction causes problems in provers.

```
from nltk.sem import Expression, skolemize
print skolemize(Expression.fromstring('exists x. happy(x)'))
print skolemize(Expression.fromstring('all x. happy(x)'))
```

results in:

```
happy(z1)
happy(z2)
```

while it should be something like:

```
happy(S1)
happy(z2)
```

Skolem functions are created properly when the existential is nested in a universal quantifier. Perhaps we need a `skolem_constant()` function similar to the `skolem_function()` function. Then `VariableExpression()` would appropriately return a `ConstantExpression` instead of an `IndividualVariableExpression`. Might have to change `is_funcvar` to ignore `S#`, or, alternatively, use two letters for skolem constants: e.g. `Sk#`. 
"
244,https://github.com/nltk/nltk/issues/1703,1703,[],closed,2017-04-28 09:27:15+00:00,,1,Conditional Random Field implementation in nltk,I have been trying to implement crf for my word prediction algorithm but the pycrfsuite- binding for crfsuite and sklearn-pycrfsuite the wrapper for pycrfsuite are not at all flexible in terms of the data input for training. A complete implementation of the algorithm in nltk will be appreciated. 
245,https://github.com/nltk/nltk/issues/1704,1704,[],closed,2017-04-30 20:11:39+00:00,,2,Debug output in PunktSentenceTokenizer,"In reference to https://github.com/nltk/nltk/commit/b3e2b410eb0031b52ae05e9d35fe275dbe8098cd, there is a print statement in the middle of nowhere, is this intended?"
246,https://github.com/nltk/nltk/issues/1705,1705,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-05-01 18:34:27+00:00,,2,"In nltk.corpus.wordnet, some synsets are not accessible through their parents","In the following example ""european_country.n.01"" is among the hypernyms of ""portugal.n.01"", but ""portugal.n.01"" is not listed as a hyponym of ""european_country.n.01"".

```python
>>> wn.synset(""european_country.n.01"").hyponyms()
[Synset('balkan_country.n.01'), Synset('scandinavian_country.n.01')]

>>> wn.synset('portugal.n.01').hypernym_paths()
[[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('location.n.01'), Synset('region.n.03'), Synset('district.n.01'), Synset('administrative_district.n.01'), Synset('country.n.02'), Synset('european_country.n.01'), Synset('portugal.n.01')]]

>>> wn.synset('portugal.n.01').hypernyms()
[]
```





"
247,https://github.com/nltk/nltk/issues/1706,1706,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}, {'id': 789518993, 'node_id': 'MDU6TGFiZWw3ODk1MTg5OTM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/need-help', 'name': 'need-help', 'color': 'e0609e', 'default': False, 'description': None}]",open,2017-05-02 07:12:33+00:00,,4,Tree.fromstring(): expected u'end-of-string' but got ')' when a node is ')',"Issue:
>>> s = '(S \\))'
>>> Tree.fromstring(s)
ValueError: Tree.read(): expected u'end-of-string' but got ')' at index 4.
""(S \\))""
........^

>>> s = '(S \\()'
>>> Tree.fromstring(s)
ValueError: Tree.read(): expected u')' but got u'end-of-string' at index 5.
""(S \\()""
.......^

Solution:
Escaping '(' and ')' should be allowed.
"
248,https://github.com/nltk/nltk/issues/1708,1708,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}]",open,2017-05-03 14:02:58+00:00,,4,Megam URL incorrect,"The URL given [in the wiki for Megam](https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software#megam-mega-model-optimization-package) is incorrect, the right URL is http://hal3.name/megam/"
249,https://github.com/nltk/nltk/issues/1715,1715,[],closed,2017-05-10 18:36:18+00:00,,1,nltk download 2.x failing on misconfigured www.nltk.org/nltk_data/,"panlex_lite package has been removed from the Packages list, but is still present in the third_party collection.  This is causing downloader to break with:
```python -m nltk.downloader -d /usr/lib/nltk_data all
Traceback (most recent call last):
  File ""/usr/lib64/python2.7/runpy.py"", line 162, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib64/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/usr/lib/python2.7/site-packages/nltk/downloader.py"", line 2225, in <module>
    halt_on_error=options.halt_on_error)
  File ""/usr/lib/python2.7/site-packages/nltk/downloader.py"", line 653, in download
    for msg in self.incr_download(info_or_id, download_dir, force):
  File ""/usr/lib/python2.7/site-packages/nltk/downloader.py"", line 524, in incr_download
    try: info = self._info_or_id(info_or_id)
  File ""/usr/lib/python2.7/site-packages/nltk/downloader.py"", line 498, in _info_or_id
    return self.info(info_or_id)
  File ""/usr/lib/python2.7/site-packages/nltk/downloader.py"", line 861, in info
    self._update_index()
  File ""/usr/lib/python2.7/site-packages/nltk/downloader.py"", line 843, in _update_index
    packages[child.id] = child
AttributeError: 'str' object has no attribute 'id'
```
I've monkey patched https://github.com/nltk/nltk/blob/2.0.4/nltk/downloader.py#L840-L843 with
```
                if isinstance(child, Collection):
                    queue.extend(child.children)
                elif isinstance(child, Package):
                    packages[child.id] = child
                else:
                    pass
```
But I believe the real fix is to correct the published index.  Thank you."
250,https://github.com/nltk/nltk/issues/1717,1717,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}]",closed,2017-05-12 06:05:25+00:00,,0,LazyCorpusLoader doesn't play well with Zipfile in Python >3.5,"c.f. #1716 

```python
~/git-stuff/nltk$ python
Python 2.7.13 (default, Dec 18 2016, 07:03:39) 
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from nltk.corpus import panlex_lite
>>> panlex_lite
<PanLexLiteCorpusReader in u'.../corpora/panlex_lite' (not loaded yet)>
>>> exit()

~/git-stuff/nltk$ python3
Python 3.5.2 (default, Oct 11 2016, 04:59:56) 
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.38)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from nltk.corpus import panlex_lite
>>> panlex_lite
<PanLexLiteCorpusReader in '.../corpora/panlex_lite' (not loaded yet)>
>>> panlex_lite.__getattrs__
corpora/panlex_lite
corpora.zip/corpora/panlex_lite
corpora/panlex_lite.zip/panlex_lite
corpora/panlex_lite.zip/panlex_lite/
Traceback (most recent call last):
  File ""/Users/liling.tan/git-stuff/nltk/nltk/corpus/util.py"", line 80, in __find
    try: root = nltk.data.find('{}/{}'.format(self.subdir, zip_name))
  File ""/Users/liling.tan/git-stuff/nltk/nltk/data.py"", line 649, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource 'corpora/panlex_lite.zip/panlex_lite/' not found.
  Please use the NLTK Downloader to obtain the resource:  >>>
  nltk.download()
  Searched in:
    - '/Users/liling.tan/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/liling.tan/git-stuff/nltk/nltk/corpus/util.py"", line 121, in __getattr__
    self.__load()
  File ""/Users/liling.tan/git-stuff/nltk/nltk/corpus/util.py"", line 86, in __load
    root = self.__find()
  File ""/Users/liling.tan/git-stuff/nltk/nltk/corpus/util.py"", line 81, in __find
    except LookupError: raise e
  File ""/Users/liling.tan/git-stuff/nltk/nltk/corpus/util.py"", line 78, in __find
    root = nltk.data.find('{}/{}'.format(self.subdir, self.__name))
  File ""/Users/liling.tan/git-stuff/nltk/nltk/data.py"", line 649, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource 'corpora/panlex_lite' not found.  Please use the NLTK
  Downloader to obtain the resource:  >>> nltk.download()
  Searched in:
    - '/Users/liling.tan/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************
```"
251,https://github.com/nltk/nltk/issues/1718,1718,[],closed,2017-05-12 10:55:00+00:00,,0,Packages not distributed through nltk_data cannot use LazyCorpusLoader since Py3.5,"c.f. #1716

Since Py >3.5, during the doctest, the `inspect` will check whether a function is wrapped by probing for `hasattr('__wrapped__')` from https://bugs.python.org/issue20691 . Thus the current flow of creating a proxy object and instantiating with `self.__load()` when the `LazyCorpusLoader.__getattr__` is accessed doesn't play well with the new inspect feature.

Thus the problem with `pl196x` and `ipipan` from #1579.

Similarly, the `panlex_lite` LazyCorpusLoader needs to be commented out since `nltk_data`no longer distribute it. Users can use `from nltk.corpus import PanLexLiteCorpusReader` to use `panlex_lite` if they've downloaded it externally."
252,https://github.com/nltk/nltk/issues/1721,1721,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",closed,2017-05-14 12:31:50+00:00,,10,We need to make Verbnet more consumable,"Hello,

As things are today, in Verbnet, one can only access lemmas directly in a list. However, other aspects of verbnet like Frames are merely pretty printed and that is not really usable.

I have written code to generate a list of syntax(es) within a verbnet frame within Verbnet.py, does it make sense for the NLTK community as a whole? If yes, I shall create a pull request.

example output for verbnet class -> 9.1:
`[[{'pos': 'NP', 'modifiers': ['Agent']}, {'pos': 'VERB', 'modifiers': []}, {'pos': 'NP', 'modifiers': ['Theme']}, {'pos': 'PREP', 'modifiers': ['+loc']}, {'pos': 'NP', 'modifiers': ['Destination']}], [{'pos': 'NP', 'modifiers': ['Agent']}, {'pos': 'VERB', 'modifiers': []}, {'pos': 'NP', 'modifiers': ['Theme']}, {'pos': 'NP', 'modifiers': ['Destination', '+adv_loc']}]]`"
253,https://github.com/nltk/nltk/issues/1722,1722,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",closed,2017-05-15 01:25:39+00:00,,5,CKY Algorithm?,"I've noticed that there are a lot of bottom-up parsers, but I don't find the exact CKY parser anywhere or the associated 'inside algorithm' for sentence scoring.

I implemented this a year ago, 
https://github.com/aetilley/pcfg

and I'd be happy to re-write it inside NLTK if people think it would be worthwhile.

I understand many of the other bottom-up- / chart - parsers tend to perform better than CKY, but I believe CKY is often one of the first taught.

Just a thought.

"
254,https://github.com/nltk/nltk/issues/1723,1723,[],closed,2017-05-15 05:22:01+00:00,,0,Unnecessary slow down on DecisionTreeClassifier,"Just a minor unnecessary calculation that might become huge on large trees.

https://github.com/nltk/nltk/blob/develop/nltk/classify/decisiontree.py#L269

Even if verbose is false you are building that description string. Wouldn't it be a good idea to have that string building inside the if condition of verbose?
"
255,https://github.com/nltk/nltk/issues/1724,1724,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",open,2017-05-18 01:03:57+00:00,,3,Better WordNet Reader,"This is a proposal to improve the WordNet interface in terms of code quality, functionalities and speed. Please feel free to add to this issues about current wordnet reader in `nltk` and how and what should be improved. 

-----

Suggestions / Issues
====

- Are there still issues with the [`_morphy()` function](https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L68)?  c.f. http://stackoverflow.com/questions/33594721/why-nltk-lemmatization-has-wrong-output-even-if-verb-exc-has-added-right-value

- What is the best params settings for `lowest_common_subsumers` in `wup_similarity`? See https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L899 

- Should the list of exceptions in the Princeton WordNet be extended? c.f. http://stackoverflow.com/questions/22999273/python-nltk-lemmatization-of-the-word-further-with-wordnet 

- [Add in the option to manually add a new root node; this will be useful for verb similarity as there exist multiple verb taxonomies](https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1946)

- Allow direct offset + pos query, i.e. `wn.synset('dog.n.01') == wn.synset('2084071-n')`.

- Pre-compute similarities for all Synsets. 
  - We know that the Information Content (IC) based similarities scores should have been frozen by now since any new WordNet updates/extensions won't affect it so it's possible to precompute these and save them. That'll speed up the similarity matching quite drastically. 
  - As for the path related similarities, they would change with WordNet versions but its nice to still have them pre-computed respective to the WordNet versions from 3.0 onwards.



Code 
====

- Would `float('inf')` be a better approach than `_INF = 1e300` at https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L68 ? 

- Should encoding be a ""set-able"" parameter for the WordNetCorpusReader? https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1052  

- Should imports be at the top of the script unless it's an `@abstractmethod` accessible from outside of the `CorpusReader` object? e.g.   
  - At `langs()` https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1145
  - At `closure()`  https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L537

- Proper indentation should be preferred for `try-except`/`if-else` and one-liner shortcuts, e.g. https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1165

- `get_version()` should be called once at initialization and not be repeated called at `get_root()` https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L423
"
256,https://github.com/nltk/nltk/issues/1725,1725,[],closed,2017-05-18 14:46:14+00:00,,8,"NLTK depends on corenlp, should not be optional sub dependency","Here is the exception with NLTK 3.2.3

```
.pyenv/lib/python2.7/site-packages/nltk/__init__.py:128: in <module>
    from nltk.chunk import *
.pyenv/lib/python2.7/site-packages/nltk/chunk/__init__.py:157: in <module>
    from nltk.chunk.api import ChunkParserI
.pyenv/lib/python2.7/site-packages/nltk/chunk/api.py:13: in <module>
    from nltk.parse import ParserI
.pyenv/lib/python2.7/site-packages/nltk/parse/__init__.py:81: in <module>
    from nltk.parse.corenlp import CoreNLPParser, CoreNLPDependencyParser
.pyenv/lib/python2.7/site-packages/nltk/parse/corenlp.py:17: in <module>
    import requests
E   ImportError: No module named requests
```"
257,https://github.com/nltk/nltk/issues/1728,1728,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",open,2017-05-19 07:51:16+00:00,,3,Add new language (dv-MV),I would like to request to add Dhivehi (dv-MV) spoken in Maldives to NLTK
258,https://github.com/nltk/nltk/issues/1729,1729,[],closed,2017-05-20 08:46:22+00:00,,1,KeyError in MosesDetokenizer,"Getting a KeyError when trying to detokenize three or more symbols:
```
from nltk.tokenize.moses import MosesDetokenizer
MosesDetokenizer().detokenize(['```'])
```
```
  File ""/usr/local/lib/python3.6/site-packages/nltk/tokenize/moses.py"", line 613, in detokenize
    return self.tokenize(tokens, return_str)
  File ""/usr/local/lib/python3.6/site-packages/nltk/tokenize/moses.py"", line 574, in tokenize
    if quote_counts[normalized_quo] % 2 == 0:
KeyError: '```'
```
In printable ASCII characters, the problem seems to occur for "",  ',  and `.
Using Python 3."
259,https://github.com/nltk/nltk/issues/1732,1732,[],closed,2017-05-21 19:41:29+00:00,,5,“IndexError: string index out of range” with nltk library - to delete,"
I'm using last possible version of nltk library - 3.2.4 with python 2.7+, but the error is still persist, which was first time described [here][1] (SO) and here https://github.com/nltk/nltk/pull/1261


The goal is to apply stemmer to dataframe:
``` python
import pandas as pd
import numpy as np
from sklearn.feature_extraction import text
from nltk.stem.porter import PorterStemmer
from nltk.stem.snowball import SnowballStemmer

import nltk
porter = PorterStemmer()
train = pd.read_csv(""train.csv"")
    
def stem_str(x,stemmer=SnowballStemmer('english')):
      x = text.re.sub(""[^a-zA-Z0-9]"","" "", x)
      x = ("" "").join([stemmer.stem(z) for z in x.split("" "")])
      x = "" "".join(x.split())
      return x
    
train['col2'] = train['col1'].astype(str).apply(lambda x:stem_str(x.lower(),porter))
```


As a result I get such error:

    /home/.../anaconda2/lib/python2.7/site-packages/nltk/stem/porter.pyc in _ends_double_consonant(self, word)
        212         """"""
        213         return (
    --> 214             len(word) >= 2 and
        215             word[-1] == word[-2] and
        216             self._is_consonant(word, len(word)-1)
    
    IndexError: string index out of range


Full stack of the code:

    IndexError                                Traceback (most recent call last)
    <ipython-input-25-58ca95c5b364> in <module>()
    ----> 1 main()

    <ipython-input-24-1a1fab0e5ac4> in main()
         15     print('Generate porter')
         16 
    ---> 17     train['question1_porter'] = train['question1'].astype(str).apply(lambda x:stem_str(x.lower(),porter))
         18     test['question1_porter'] = test['question1'].astype(str).apply(lambda x:stem_str(x.lower(),porter))
         19 

    /home/analyst/anaconda2/lib/python2.7/site-packages/pandas/core/series.pyc in apply(self, func, convert_dtype, args, **kwds)
       2353             else:
       2354                 values = self.asobject
    -> 2355                 mapped = lib.map_infer(values, f, convert=convert_dtype)
       2356 
       2357         if len(mapped) and isinstance(mapped[0], Series):
    
    pandas/_libs/src/inference.pyx in pandas._libs.lib.map_infer (pandas/_libs/lib.c:66440)()
    
    <ipython-input-24-1a1fab0e5ac4> in <lambda>(x)
         15     print('Generate porter')
         16 
    ---> 17     train['question1_porter'] = train['question1'].astype(str).apply(lambda x:stem_str(x.lower(),porter))
         18     test['question1_porter'] = test['question1'].astype(str).apply(lambda x:stem_str(x.lower(),porter))
         19 
    
    <ipython-input-18-3b87bf648e19> in stem_str(x, stemmer)
         37 def stem_str(x,stemmer=SnowballStemmer('english')):
         38         x = text.re.sub(""[^a-zA-Z0-9]"","" "", x)
    ---> 39         x = ("" "").join([stemmer.stem(z) for z in x.split("" "")])
         40         x = "" "".join(x.split())
         41         return x
    
    /home/analyst/anaconda2/lib/python2.7/site-packages/nltk/stem/porter.pyc in stem(self, word)
        663             return word
        664 
    --> 665         stem = self._step1a(stem)
        666         stem = self._step1b(stem)
        667         stem = self._step1c(stem)
    
    /home/analyst/anaconda2/lib/python2.7/site-packages/nltk/stem/porter.pyc in _step1b(self, word)
        374             (
        375                 '',
    --> 376                 'e',
        377                 lambda stem: (self._measure(stem) == 1 and
        378                               self._ends_cvc(stem))
    
    /home/analyst/anaconda2/lib/python2.7/site-packages/nltk/stem/porter.pyc in _apply_rule_list(self, word, rules)
        256         """"""
        257         for rule in rules:
    --> 258             suffix, replacement, condition = rule
        259             if suffix == '*d' and self._ends_double_consonant(word):
        260                 stem = word[:-2]
    
    /home/analyst/anaconda2/lib/python2.7/site-packages/nltk/stem/porter.pyc in _ends_double_consonant(self, word)
        212         """"""
        213         return (
    --> 214             len(word) >= 2 and
        215             word[-1] == word[-2] and
        216             self._is_consonant(word, len(word)-1)
    
    IndexError: string index out of range

  [1]: http://stackoverflow.com/questions/41517595/nltk-stemmer-string-index-out-of-range/41526359#41526359"
260,https://github.com/nltk/nltk/issues/1733,1733,[],closed,2017-05-23 14:48:51+00:00,,8,Stanford Tagger Doesn't Remove File on Failure,"When using NLTK with a Stanford tagger, a temp file is written out and passed on the command line to Java. Only upon successful completion of the tagging does the file get removed. Can we wrap a try: finally: around the call to ensure the file gets removed? If not, can we create a method for specifying the location of the temp file so the user can ensure its removal?

https://github.com/nltk/nltk/blob/develop/nltk/tag/stanford.py#L103"
261,https://github.com/nltk/nltk/issues/1740,1740,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",open,2017-05-31 02:54:30+00:00,,4,Errors in the Norwegian tokenizer,"Hi!

While working on my master in language technology I discovered some errors in the Norwegian tokenizers:

nltk.word_tokenize(“Hello NLTK”, “norwegian”)
nltk.sent_tokenize(“My name. Is bob.”, “norwegian”)

When these errors are fixed the sentence tokenizer fail 7-8 times less, while the word tokenizer will fail 6-7 times less (for Norwegian). I was wondering if this is something I could integrate with NLTK? If yes, how would you want it integrated?

For instance, I could place the patch file in the tokenize folder and call it like this in the tokenize init file: 
```
import patch

def sent_tokenize(text, language='english'):
    if language='norwegian':
        tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))
        return patch.sent_patch(tokenizer.tokenize(text))
    else:
        tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))
        return tokenizer.tokenize(text)
```
However I realize this might become messy if every language wanted a slot. Any other questions, I can elaborate, please ask ^.^

:bob
"
262,https://github.com/nltk/nltk/issues/1741,1741,"[{'id': 718738467, 'node_id': 'MDU6TGFiZWw3MTg3Mzg0Njc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/windows%20related', 'name': 'windows related', 'color': 'fce4ba', 'default': False, 'description': None}]",closed,2017-05-31 14:01:05+00:00,,3,Anyway to install latest version on Windows XP SP3?,"Tried installing via the Windows Installer `nltk-3.2.4.win32.exe` at PyPI, and get this error when I launch the downloaded executable... ""<PATH-TO-FILE>...not a valid win32 application"".

I believe this has to do with the build system or compiler used, So is there any other way?

Windows XP SP3
Python 2.7.13"
263,https://github.com/nltk/nltk/issues/1744,1744,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",closed,2017-06-01 12:01:05+00:00,,1,importerror cannot import name punktwordtokenizer,"My current version of nltk is 3.2.4,, I couldn't import punckt word tokenizer through this,
here is my result.
 `>>> from nltk.tokenize import PunktWordTokenizer
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name PunktWordTokenizer`"
264,https://github.com/nltk/nltk/issues/1745,1745,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",open,2017-06-04 06:39:23+00:00,,2,FrameNet corpus reader: rewrite howto,[This page](http://www.nltk.org/howto/framenet.html) is out of date. Rewrite borrowing examples from https://arxiv.org/abs/1703.07438.
265,https://github.com/nltk/nltk/issues/1746,1746,[],closed,2017-06-05 07:43:12+00:00,,4,Stanford CoreNLP 'originalText' json key might not be reliable,"From stanfordnlp/CoreNLP#453, 'originaText' key from the corenlp server json might not be reliable, esp. for non-English model; maybe adding an backoff yield value at https://github.com/nltk/nltk/blob/develop/nltk/parse/corenlp.py#L302 would be helpful, e.g.:

```python
def tokenize(self, text, properties=None):
        default_properties = {
            'annotators': 'tokenize,ssplit',
        }

        default_properties.update(properties or {})

        result = self.api_call(text, properties=default_properties)
        print(result)
        for sentence in result['sentences']:
            for token in sentence['tokens']:
                yield token['originalText'] or token['word']
```

[Validation]:

```python
>>> '' or u'电脑'
u'\u7535\u8111'
>>> print ('' or u'电脑')
电脑

# The `or` operator would always return the first Truey item.
>>> print ('token' or u'Token')
token
>>> print ('' or u'电脑')
电脑
```

----
Starting the server as such:

```
$ wget http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip

$ unzip stanford-corenlp-full-2016-10-31.zip && cd stanford-corenlp-full-2016-10-31

$ cd tanford-corenlp-full-2016-10-31

~/stanford-corenlp-full-2016-10-31$ wget http://nlp.stanford.edu/software/stanford-chinese-corenlp-2016-10-31-models.jar
java -Xmx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-chinese.properties -preload tokenize,ssplit,pos,lemma,ner,parse  -port 9001 -timeout 15000
```

Before the backoff yield with `CoreNLPTokenizer` from #1735  :

```python
>>> from nltk.tokenize.stanford import CoreNLPTokenizer
>>> sttok = CoreNLPTokenizer('http://localhost:9001')
>>> sttok.tokenize(u'我家没有电脑。')
['', '', '', '']
```

After the backoff:

```python
>>> from nltk.tokenize.stanford import CoreNLPTokenizer
>>> sttok = CoreNLPTokenizer('http://localhost:9001')
>>> sttok.tokenize(u'我家没有电脑。')
['我家', '没有', '电脑', '。']
```"
266,https://github.com/nltk/nltk/issues/1747,1747,"[{'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-06-06 18:14:38+00:00,,0,Unable to find the Ghostscript while using nltk.tree.Tree.fromstring() method,"LookupError                               Traceback (most recent call last)
C:\Users\motongya\AppData\Local\Continuum\Anaconda3\lib\site-packages\IPython\core\formatters.py in __call__(self, obj)
    309             method = get_real_method(obj, self.print_method)
    310             if method is not None:
--> 311                 return method()
    312             return None
    313         else:

C:\Users\motongya\AppData\Local\Continuum\Anaconda3\lib\site-packages\nltk\tree.py in _repr_png_(self)
    724             _canvas_frame.print_to_file(in_path)
    725             _canvas_frame.destroy_widget(widget)
--> 726             subprocess.call([find_binary('gs', binary_names=['gswin32c.exe', 'gswin64c.exe'], env_vars=['PATH'], verbose=False)] +
    727                             '-q -dEPSCrop -sDEVICE=png16m -r90 -dTextAlphaBits=4 -dGraphicsAlphaBits=4 -dSAFER -dBATCH -dNOPAUSE -sOutputFile={0:} {1:}'
    728                             .format(out_path, in_path).split())

C:\Users\motongya\AppData\Local\Continuum\Anaconda3\lib\site-packages\nltk\__init__.py in find_binary(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)
    600                 binary_names=None, url=None, verbose=True):
    601     return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,
--> 602                                  binary_names, url, verbose))
    603 
    604 def find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),

C:\Users\motongya\AppData\Local\Continuum\Anaconda3\lib\site-packages\nltk\__init__.py in find_binary_iter(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)
    594     """"""
    595     for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,
--> 596                      url, verbose):
    597         yield file
    598 

C:\Users\motongya\AppData\Local\Continuum\Anaconda3\lib\site-packages\nltk\__init__.py in find_file_iter(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)
    565                         (filename, url))
    566         div = '='*75
--> 567         raise LookupError('\n\n%s\n%s\n%s' % (div, msg, div))
    568 
    569 

LookupError: 

===========================================================================
NLTK was unable to find the gs file!
Use software specific configuration paramaters or set the PATH environment variable.
===========================================================================
"
267,https://github.com/nltk/nltk/issues/1748,1748,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}]",open,2017-06-07 09:54:13+00:00,,1,Incorrect Part of Speech classification,"I was using NLTK library for one of our NLP based project and where I needed the Part-of-Speech tag of the sentences, string and tokens. So, before using the NLTK library, I went on to understand it's working before implementing it and faced the following error or bug: 

- The pos tag for ""Indemnify"" was recognized correctly as ""VB"" (Verb).
- But, the pos tag for ""indemnify"" (here, the first letter is in small-case) was recognized incorrectly as ""NN"" (Noun). 

Please find the attached piece of code stating the same. Is it a fault from our end or a bug in the library? I would really like to solve the issue and use the library in my project. 

![selection_125](https://user-images.githubusercontent.com/24873215/26872764-52dc3770-4b95-11e7-87b7-71ed782ada46.png)
"
268,https://github.com/nltk/nltk/issues/1750,1750,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}]",open,2017-06-12 08:13:27+00:00,,14,span_tokenize failed when sentence contains double quotation,"If we feed in a sentence with double quotation into TreebankWordTokenizer's span_tokenize function, there will be errors. Probably this is because the function sends the raw string input along with the tokenized string to the align_tokens function, without considering that the tokenize function would replace double quotation marks with something else."
269,https://github.com/nltk/nltk/issues/1752,1752,[],closed,2017-06-13 08:37:40+00:00,,5,ImportError: cannot import name 'format_exception_only',"An error occurred when I import nltk.
Please tell me what I should do, thanks.

``` python
>>> import nltk
Traceback (most recent call last):
  File ""/usr/lib/python3.5/subprocess.py"", line 441, in <module>
    import threading
  File ""/usr/lib/python3.5/threading.py"", line 7, in <module>
    from traceback import format_exc as _format_exc
  File ""/usr/lib/python3.5/traceback.py"", line 5, in <module>
    import linecache
  File ""/usr/lib/python3.5/linecache.py"", line 11, in <module>
    import tokenize
  File ""/home/july/PythonProjects/test/tokenize.py"", line 3, in <module>
    from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize
  File ""/usr/local/lib/python3.5/dist-packages/nltk/tokenize/__init__.py"", line 67, in <module>
    from nltk.tokenize.mwe      import MWETokenizer
  File ""/usr/local/lib/python3.5/dist-packages/nltk/tokenize/mwe.py"", line 31, in <module>
    from nltk.util import Trie
  File ""/usr/local/lib/python3.5/dist-packages/nltk/util.py"", line 15, in <module>
    import pydoc
  File ""/usr/lib/python3.5/pydoc.py"", line 76, in <module>
    from traceback import format_exception_only
ImportError: cannot import name 'format_exception_only'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/nltk/__init__.py"", line 89, in <module>
    from nltk.internals import config_java
  File ""/usr/local/lib/python3.5/dist-packages/nltk/internals.py"", line 11, in <module>
    import subprocess
  File ""/usr/lib/python3.5/subprocess.py"", line 443, in <module>
    import dummy_threading as threading
  File ""/usr/lib/python3.5/dummy_threading.py"", line 45, in <module>
    import threading
  File ""/usr/lib/python3.5/threading.py"", line 7, in <module>
    from traceback import format_exc as _format_exc
  File ""/usr/lib/python3.5/traceback.py"", line 5, in <module>
    import linecache
  File ""/usr/lib/python3.5/linecache.py"", line 11, in <module>
    import tokenize
  File ""/home/july/PythonProjects/test/tokenize.py"", line 3, in <module>
    from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize
  File ""/usr/local/lib/python3.5/dist-packages/nltk/tokenize/__init__.py"", line 67, in <module>
    from nltk.tokenize.mwe      import MWETokenizer
  File ""/usr/local/lib/python3.5/dist-packages/nltk/tokenize/mwe.py"", line 31, in <module>
    from nltk.util import Trie
  File ""/usr/local/lib/python3.5/dist-packages/nltk/util.py"", line 15, in <module>
    import pydoc
  File ""/usr/lib/python3.5/pydoc.py"", line 76, in <module>
    from traceback import format_exception_only
ImportError: cannot import name 'format_exception_only'
Error in sys.excepthook:
Traceback (most recent call last):
  File ""/usr/lib/python3.5/subprocess.py"", line 441, in <module>
    import threading
  File ""/usr/lib/python3.5/threading.py"", line 7, in <module>
    from traceback import format_exc as _format_exc
ImportError: cannot import name 'format_exc'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/apport_python_hook.py"", line 62, in apport_excepthook
    import re, traceback
  File ""/usr/lib/python3.5/traceback.py"", line 5, in <module>
    import linecache
  File ""/usr/lib/python3.5/linecache.py"", line 11, in <module>
    import tokenize
  File ""/home/july/PythonProjects/test/tokenize.py"", line 3, in <module>
    from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize
  File ""/usr/local/lib/python3.5/dist-packages/nltk/__init__.py"", line 89, in <module>
    from nltk.internals import config_java
  File ""/usr/local/lib/python3.5/dist-packages/nltk/internals.py"", line 11, in <module>
    import subprocess
  File ""/usr/lib/python3.5/subprocess.py"", line 443, in <module>
    import dummy_threading as threading
  File ""/usr/lib/python3.5/dummy_threading.py"", line 45, in <module>
    import threading
  File ""/usr/lib/python3.5/threading.py"", line 7, in <module>
    from traceback import format_exc as _format_exc
ImportError: cannot import name 'format_exc'

Original exception was:
Traceback (most recent call last):
  File ""/usr/lib/python3.5/subprocess.py"", line 441, in <module>
    import threading
  File ""/usr/lib/python3.5/threading.py"", line 7, in <module>
    from traceback import format_exc as _format_exc
  File ""/usr/lib/python3.5/traceback.py"", line 5, in <module>
    import linecache
  File ""/usr/lib/python3.5/linecache.py"", line 11, in <module>
    import tokenize
  File ""/home/july/PythonProjects/test/tokenize.py"", line 3, in <module>
    from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize
  File ""/usr/local/lib/python3.5/dist-packages/nltk/tokenize/__init__.py"", line 67, in <module>
    from nltk.tokenize.mwe      import MWETokenizer
  File ""/usr/local/lib/python3.5/dist-packages/nltk/tokenize/mwe.py"", line 31, in <module>
    from nltk.util import Trie
  File ""/usr/local/lib/python3.5/dist-packages/nltk/util.py"", line 15, in <module>
    import pydoc
  File ""/usr/lib/python3.5/pydoc.py"", line 76, in <module>
    from traceback import format_exception_only
ImportError: cannot import name 'format_exception_only'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/nltk/__init__.py"", line 89, in <module>
    from nltk.internals import config_java
  File ""/usr/local/lib/python3.5/dist-packages/nltk/internals.py"", line 11, in <module>
    import subprocess
  File ""/usr/lib/python3.5/subprocess.py"", line 443, in <module>
    import dummy_threading as threading
  File ""/usr/lib/python3.5/dummy_threading.py"", line 45, in <module>
    import threading
  File ""/usr/lib/python3.5/threading.py"", line 7, in <module>
    from traceback import format_exc as _format_exc
  File ""/usr/lib/python3.5/traceback.py"", line 5, in <module>
    import linecache
  File ""/usr/lib/python3.5/linecache.py"", line 11, in <module>
    import tokenize
  File ""/home/july/PythonProjects/test/tokenize.py"", line 3, in <module>
    from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize
  File ""/usr/local/lib/python3.5/dist-packages/nltk/tokenize/__init__.py"", line 67, in <module>
    from nltk.tokenize.mwe      import MWETokenizer
  File ""/usr/local/lib/python3.5/dist-packages/nltk/tokenize/mwe.py"", line 31, in <module>
    from nltk.util import Trie
  File ""/usr/local/lib/python3.5/dist-packages/nltk/util.py"", line 15, in <module>
    import pydoc
  File ""/usr/lib/python3.5/pydoc.py"", line 76, in <module>
    from traceback import format_exception_only
ImportError: cannot import name 'format_exception_only'

```"
270,https://github.com/nltk/nltk/issues/1753,1753,[],closed,2017-06-13 19:38:54+00:00,,0,Deprecation warnings for old Stanford Tool wrappers,"Add deprecation warnings for nltk.tag.StanfordPOSTagger, nltk.tag.StanfordNERTagger and nltk.tokenize.StanfordTokenizer and nltk.tokenize.StanfordSegmenter, cf #1735 "
271,https://github.com/nltk/nltk/issues/1754,1754,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}]",closed,2017-06-14 18:37:29+00:00,,5,NLTK WordNet error with a word look up using synsets,"I am using Python 3.6 with NLTK 3.2.3, and I am getting a ""WordNetError"" only for the word ""escort"". I don't get errors with any other words. Here's the transcript showing success with the word ""dog"" and the error using the word ""escort"":
```
Python 3.6.0 |Anaconda custom (64-bit)| (default, Dec 23 2016, 12:22:00) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from nltk.corpus import wordnet
>>> wordnet.synsets('dog')
[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]
>>> wordnet.synsets('escort')
Traceback (most recent call last):
  File ""/home/user1/.conda/envs/ca/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py"", line 1403, in _synset_from_pos_and_line
    offset = int(_next_token())
ValueError: invalid literal for int() with base 10: '02026433\x00v'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/user1/.conda/envs/ca/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py"", line 1491, in synsets
    for p in pos
  File ""/home/user1/.conda/envs/ca/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py"", line 1493, in <listcomp>
    for offset in index[form].get(p, [])]
  File ""/home/user1/.conda/envs/ca/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py"", line 1335, in synset_from_pos_and_offset
    synset = self._synset_from_pos_and_line(pos, data_file_line)
  File ""/home/user1/.conda/envs/ca/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py"", line 1448, in _synset_from_pos_and_line
    raise WordNetError('line %r: %s' % (data_file_line, e))
nltk.corpus.reader.wordnet.WordNetError: line '02025829 38 v 01 escort 0 006 @ 02025550 v 0000 + 09992538 n 0102 ~ 02026203 v 0000 ~ 02026327 v 0000 ~ 02026433\x00v 0000 ~ 02026712 v 0000 04 + 08 00 + 09 00 + 20 00 + 21 00 | accompany as an escort; ""She asked her older brother to escort her to the ball""  \n': invalid literal for int() with base 10: '02026433\x00v'
However, when I use the online WordNet search tool at http://wordnetweb.princeton.edu/perl/webwn, it performs the lookup as expected. The latest WordNet corpus was downloaded using nltk.download().
```
The error seems to reference a hex value in the WordNet definition for the word when it is expecting to find an integer value.

Any ideas? Please advise if you've run into something like this."
272,https://github.com/nltk/nltk/issues/1755,1755,[],closed,2017-06-19 07:59:43+00:00,,1,File input processing for Stanford CoreNLP wrappers,"Currently, the `nltk.parse.corenlp.GenericCoreNLPParser` has a `raw_parse_sents()` function that iteratively post requests to the `StanfordCoreNLPServer`. 

After the `StanfordCoreNLPServer` is started, it's possible to use the `StanfordCoreNLPClient` to speed up the request by sending in a full file instead of individual sentences (c.f. https://stanfordnlp.github.io/CoreNLP/corenlp-server.html), e.g. 

```
java -cp ""*"" -Xmx1g edu.stanford.nlp.pipeline.StanfordCoreNLPClient \
-annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref \
-file input.txt -outputFormat json -backends localhost:9000
```

It would be good to have this feature before deprecating the old Stanford wrappers (#1753). "
273,https://github.com/nltk/nltk/issues/1756,1756,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",open,2017-06-20 01:37:25+00:00,,5,Fulfilling the requirements of the Treebank Tokenizer License,"I am concerned about the Treebank Tokenizer's use, by its License terms.


The source for the [tokeniser (via waybackmachine)](https://web.archive.org/web/20130804202913/http://www.cis.upenn.edu/~treebank/tokenizer.sed) includes the text:

> by Robert MacIntyre, University of Pennsylvania, late 1995.  
> If this wasn't such a trivial program, I'd include all that stuff about
 no warrantee, free use, etc. from the GNU General Public License.  If you
 want to be picky, assume that all of its terms apply.  Okay?

This is really problematic, AFAICT.
It is an awful crayon license, 
neither its meaning nor its precise intent is clear.
Under a literal interpretation, one is most restricted when not being picky,
and still very restricted if not being picky

I asked about it on [OpenSource StackExchange](https://opensource.stackexchange.com/q/5612/934)

> As I see it, one has two choices:
>  - **Do not be picky:** then this has no license and so can not be incorporated into any open source project. All rights remain with Robert MacIntyre.
>  - **Be picky:** this is *effectively* under *some* version of the GPL, it can be incorporated only into things that can incorporate GPL works. Since this work is from 1995, it means it is either GPL 1, or GPL 2, is intended.




As far as I can tell, that reasoning is correct, and applies to NLTK's usage.
As NLTK is Apache 2 Licensed, incorporating GPL works is not (normally considered) possible. 



I can't find any further reference to the tokenizer or to Robert MacIntyre. [LICENSE](https://github.com/nltk/nltk/blob/develop/LICENSE.txt)  nor in [AUTHORS.md](https://github.com/nltk/nltk/blob/develop/AUTHORS.md).


"
274,https://github.com/nltk/nltk/issues/1757,1757,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}]",closed,2017-06-22 07:03:26+00:00,,2,porter.py returns an error in self._step1b(w) when w=vowel+'ing' (Python 3),"Any word in the form vowel+'ing' makes PorterStemmer to abort.

```
stemmer = PorterStemmer()
word='oing'
stemmer.stem(word)
>>> File ""/usr/local/lib/python3.5/dist-packages/nltk/stem/porter.py"", line 214, in ends_double_consonant
>>>    word[-1] == word[-2] and 
>>> IndexError: string index out of range
```

The same error for aing, eing, iing, and uing. However, no errors if it is  consonant+'ing', e.g., ting, bing, ..."
275,https://github.com/nltk/nltk/issues/1758,1758,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}]",closed,2017-06-22 20:27:37+00:00,,8,Error 403 when connecting to corenlp hosted locally,"Hello,

When I started up a server via my terminal and tried to connect to it with a python client, I got 403 errors. The odd part was that my browser could connect to the server without problems.

I found out that my issue was in my proxy. When my client tried to connect to connect to 'http://localhost:9000' it passed the url to my work's proxy. The proxy didn't recognize url and returned a 403 error.

This issue didn't affect my browser because my browser was using a pac file which excluded localhost from the proxy. Since I was connecting directly to the proxy via environment variables, my terminal was running into issues.

The solution was to set `trust_env` to False. This makes the client connect without using proxy. The client would refuse to import any environment variables.

```
nlp = nltk.parse.corenlp.CoreNLPParser(""http://localhost:9000"")
nlp.session.trust_env = False
```
I am guessing most people setup their corenlp server locally. Maybe there could be some sort of warning. The class could check if there is a proxy url and if the session is pointed to a local host, then warn users that issues may happen. Maybe a more isolated way is to catch 403 exceptions, check the proxy url and localhost, and then raise a exception with advice.

I managed to fix the issue but I am posting an issue to help others in the same boat.

Thanks,
"
276,https://github.com/nltk/nltk/issues/1759,1759,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-06-22 21:21:43+00:00,,21,corenlp.py CoreNLPServer throws TypeError exception,"Hello,

Here's the code:
```
>>> s = nltk.parse.corenlp.CoreNLPServer(path_to_jar='/usr/local/share/stanford/stanford-corenlp-3.8.0.jar', path_to_models_jar='/usr/local/share/stanford/stanford-english-corenlp-2017-06-09-models.jar')
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
    s = nltk.parse.corenlp.CoreNLPServer(path_to_jar='/usr/local/share/stanford/stanford-corenlp-3.8.0.jar', path_to_models_jar='/usr/local/share/stanford/stanford-english-corenlp-2017-06-09-models.jar')
  File ""/Users/adiep/feedback-sentiment/.env/src/nltk/nltk/parse/corenlp.py"", line 69, in __init__
    key=lambda model_name: re.match(self._JAR, model_name)
TypeError: '>' not supported between instances of 'NoneType' and 'NoneType'
```
The `max` function is throwing this exception.

I think what's happening is that `key=lambda model_name: re.match(self._JAR, model_name)` is returning NoneType because it didn't match anything. So it fills the list of `NoneType` and `max` fails to sort it. I found that `self._JAR` and `model_name` evaluated to the following:
```
>>> type(re.match(r'stanford-corenlp-(\d+)\.(\d+)\.(\d+)\.jar', '/usr/local/share/stanford/stanford-corenlp-3.8.0.jar'))
<class 'NoneType'>
```

Thanks,"
277,https://github.com/nltk/nltk/issues/1761,1761,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}]",closed,2017-06-28 15:54:14+00:00,,1,MWE tokenizer doesnt support pickling,"You can reproduce this as follows on python 2.7.11

````
from nltk.tokenize import MWETokenizer
import pickle

tokenizer = MWETokenizer([('a', 'little'), ('a', 'little', 'bit'), ('a', 'lot')])

with open('tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)

with open('tokenizer.pkl', 'rb') as f:
    t = pickle.load(f)

````

Error that i am getting is

````
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-47-205693e12b16> in <module>()
      1 with open('tokenizer.pkl', 'rb') as f:
----> 2     t = pickle.load(f)

/usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.pyc in load(file)
   1382 
   1383 def load(file):
-> 1384     return Unpickler(file).load()
   1385 
   1386 def loads(str):

/usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.pyc in load(self)
    862             while 1:
    863                 key = read(1)
--> 864                 dispatch[key](self)
    865         except _Stop, stopinst:
    866             return stopinst.value

/usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.pyc in load_reduce(self)
   1137         args = stack.pop()
   1138         func = stack[-1]
-> 1139         value = func(*args)
   1140         stack[-1] = value
   1141     dispatch[REDUCE] = load_reduce

/Users/vinay/Sites/nltk_mwe/.env/lib/python2.7/site-packages/nltk/collections.pyc in __init__(self, strings)
    611         defaultdict.__init__(self, Trie)
    612         if strings:
--> 613             for string in strings:
    614                 self.insert(string)
    615 

TypeError: 'type' object is not iterable
````

"
278,https://github.com/nltk/nltk/issues/1762,1762,"[{'id': 718733436, 'node_id': 'MDU6TGFiZWw3MTg3MzM0MzY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/Off-topic', 'name': 'Off-topic', 'color': 'f7b4e1', 'default': False, 'description': None}]",closed,2017-06-29 07:57:31+00:00,,4,Error through training seq2seq model,"in test_batches
    bleu, bleu_log = data_util.corpus_bleu(all_B_predict, all_B_raw)
TypeError: 'int' object is not iterable"
279,https://github.com/nltk/nltk/issues/1763,1763,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-06-30 22:34:53+00:00,,4,"Inconsistent tokenization of ""Mr."" in word_tokenizer","I noticed inconsistent behavior with the nltk.word_tokenize method. Specifically, sometimes the string ""Mr."" gets tokenized as one token, but in other cases it is split into two tokens: ['Mr', '.']. The same holds for the string 'Ms.'.

Here is an example:

```
>>> nltk.word_tokenize('Hello Mr. Rob')
['Hello', 'Mr', '.' 'Rob']
>>> nltk.word_tokenize('Hello Mr. Dan')
['Hello', 'Mr.', 'Dan']
```

I am using nltk version 3.2.2"
280,https://github.com/nltk/nltk/issues/1765,1765,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",closed,2017-07-03 10:14:27+00:00,,7,How could I inline nltk graph to jupyter notebook?,"I have already asked this question on [stackoverflow](https://stackoverflow.com/questions/44880337/use-tkinter-for-nltk-draw-inside-of-jupyter-notebook) without any luck and decide to duplicate it here.

According to [the sources](https://github.com/nltk/nltk/blob/develop/nltk/draw/tree.py#L856) of `nltk` it draws graph by `tkinter (GUI)` but I need to inline this graph to `jupyter notebook`. And I'm trying to do it inside of official docker from [anaconda3](https://github.com/ContinuumIO/docker-images) in other words I don't need any popup GUI here but just image inside of notebook, that should be render on server side by nltk lib.

How could I overcome this by nltk? Maybe there is third party libs which could help there? 

Sources of my try is [here](https://github.com/hyzhak/nltk-experiments/blob/master/main.ipynb) - the last 18th cell.

```python
chunkGram = r""""""Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}""""""
chunkParser = nltk.RegexpParser(chunkGram)

for i in tokenized_text[:5]:
    words = nltk.word_tokenize(i)
    tagged = nltk.pos_tag(words)
    chunked = chunkParser.parse(tagged)
    chunked.draw()
```

PS:
in the same time matplotlib inline by itself works like a charm. Could I use matplotlib for graph rendering?

Thanks!"
281,https://github.com/nltk/nltk/issues/1766,1766,[],closed,2017-07-04 08:03:17+00:00,,0,get OSError: Java command failed,"I'm using stanfordCoreNLP version 2017-6-9 and get the error when I run StanfordSegmenter, I haven't tried the other models yet. Is it due to version conflict?"
282,https://github.com/nltk/nltk/issues/1767,1767,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",open,2017-07-04 10:21:01+00:00,,1,Got error `TypeError: not all arguments converted during string formatting` for tree.pretty_print(),"# Env
- Python 3.6.1 |Anaconda 4.4.0 (64-bit) (official docker from continuumio/anaconda3:4.4.0 https://github.com/ContinuumIO/docker-images)
- Jupyter and IPython 5.3.0
- nltk=3.2.3

# Example:
```python
import nltk
chunkGram = r""""""Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}""""""
chunkParser = nltk.RegexpParser(chunkGram)
tagged = [('Tonight', 'NN'), 
          ('we', 'PRP'), 
          ('are', 'VBP'), 
          ('comforted', 'VBN'), 
          ('by', 'IN'), 
          ('the', 'DT'), 
          ('hope', 'NN'), 
          ('of', 'IN'), 
          ('a', 'DT'), 
          ('glad', 'JJ'), 
          ('reunion', 'NN'), 
          ('with', 'IN'), 
          ('the', 'DT'), 
          ('husband', 'NN'), 
          ('who', 'WP'), 
          ('was', 'VBD'), 
          ('taken', 'VBN'), 
          ('so', 'RB'), 
          ('long', 'RB'), 
          ('ago', 'RB'), 
          (',', ','), 
          ('and', 'CC'), 
          ('we', 'PRP'), 
          ('are', 'VBP'), 
          ('grateful', 'JJ'), 
          ('for', 'IN'), 
          ('the', 'DT'), 
          ('good', 'JJ'), 
          ('life', 'NN'), 
          ('of', 'IN'), 
          ('Coretta', 'NNP'), 
          ('Scott', 'NNP'), 
          ('King', 'NNP'), 
          ('.', '.')]
chunked = chunkParser.parse(tagged)
chunked.pprint()
chunked.pretty_print()
```

# Result

```
(S
  Tonight/NN
  we/PRP
  are/VBP
  comforted/VBN
  by/IN
  the/DT
  hope/NN
  of/IN
  a/DT
  glad/JJ
  reunion/NN
  with/IN
  the/DT
  husband/NN
  who/WP
  was/VBD
  taken/VBN
  so/RB
  long/RB
  ago/RB
  ,/,
  and/CC
  we/PRP
  are/VBP
  grateful/JJ
  for/IN
  the/DT
  good/JJ
  life/NN
  of/IN
  (Chunk Coretta/NNP Scott/NNP King/NNP)
  ./.)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-3-286ecc5b3fd6> in <module>()
     38 chunked = chunkParser.parse(tagged)
     39 chunked.pprint()
---> 40 chunked.pretty_print()

/opt/conda/lib/python3.6/site-packages/nltk/tree.py in pretty_print(self, sentence, highlight, stream, **kwargs)
    697         """"""
    698         from nltk.treeprettyprinter import TreePrettyPrinter
--> 699         print(TreePrettyPrinter(self, sentence, highlight).text(**kwargs),
    700               file=stream)
    701 

/opt/conda/lib/python3.6/site-packages/nltk/treeprettyprinter.py in __init__(self, tree, sentence, highlight)
     95                             if not isinstance(b, Tree):
     96                                 a[n] = len(sentence)
---> 97                                 sentence.append('%s' % b)
     98         self.nodes, self.coords, self.edges, self.highlight = self.nodecoords(
     99                 tree, sentence, highlight)

TypeError: not all arguments converted during string formatting
```"
283,https://github.com/nltk/nltk/issues/1770,1770,"[{'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-07-05 17:42:54+00:00,,9,UnicodeDecodeError: 'utf8' codec can't decode bytes when using Stanford Parser,"This is my original codes, I have declare # coding: utf8 on top of the script, and I'm using the latest stanford nlp package. 
Some sentences can be parsed successfully, but most cannot. I also tried adding encoding='utf8' as parameter in the parser and got the same error. Not sure how to solve it. thanks!

![parser](https://user-images.githubusercontent.com/19523304/27877187-8f5991ae-616e-11e7-8fa8-8a5ed70a621f.jpeg)
![error](https://user-images.githubusercontent.com/19523304/27877190-93a28928-616e-11e7-80f9-e565f0cf7734.jpeg)


 "
284,https://github.com/nltk/nltk/issues/1772,1772,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}]",open,2017-07-06 06:48:42+00:00,,3,Adding Persian language support,"I wonder if there is any Persian NLP tool available in the nltk.
Specifically, I want Persian semantic parser."
285,https://github.com/nltk/nltk/issues/1774,1774,[],closed,2017-07-10 11:46:50+00:00,,3,Link for implemenation details is broken for PerceptronTagger,"Hey,
the link for the implemenation details in the docstring of nltk.tag.perceptron.AveragedPerceptron and  nltk.tag.perceptron.AveragedPerceptron seems to be broken (it is the same link, namely: http://spacy.io/blog/part-of-speech-POS-tagger-in-python/). Can anyone give me a hint to anthoer source with the same contents?

(Link found on http://www.nltk.org/_modules/nltk/tag/perceptron.html)"
286,https://github.com/nltk/nltk/issues/1776,1776,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",open,2017-07-12 03:47:54+00:00,,0,Fixing LGTM alerts ,"#1775 raised a PR from [188 alerts from lgtm.com](https://lgtm.com/projects/g/nltk/nltk/alerts/)

Possibly, it'll be a good thing to look into each one of them. "
287,https://github.com/nltk/nltk/issues/1778,1778,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}]",open,2017-07-13 23:14:41+00:00,,5,"PorterStemmer seems to be stemming ""this"" -> ""thi""","I'm not sure whether it's the expected output but NLTK PorterStemmer is giving different output as compared to https://pypi.python.org/pypi/stemming/1.0

From NLTK:

```python
>>> from nltk.stem import PorterStemmer
>>> porter = PorterStemmer()
>>> porter.stem('this')
u'thi'
```

From `stemming`

```python
>>> from stemming.porter2 import stem
>>> stem('this') 
'this'
```"
288,https://github.com/nltk/nltk/issues/1780,1780,[],closed,2017-07-16 11:21:58+00:00,,1,WordNet jcn (Jiang & Conrath) returns 1e+300 score,"Wordnet `wn.jcn_similarity` method returns `1e+300` similarity score when identical synsets are compared.
For example:
```
from nltk.corpus import wordnet as wn
from nltk.corpus import wordnet_ic as wic
bnc = wic.ic('ic-bnc-add1.dat')
wn.jcn_similarity(Synset('haircloth.n.01'), Synset('haircloth.n.01'), bnc)
``` 
results in `1e+300` score.
"
289,https://github.com/nltk/nltk/issues/1782,1782,[],closed,2017-07-22 04:11:12+00:00,,2,How to not split certain special characters,"Hi guys, in my program I am using nltk.word_tokenize(pattern) (Python), before doing this I have replaced some strings in pattern with their Entity name ie: 

`pattern = ""I like cakes"" becomes ""I like <FOOD>""`

When I tokenise this becomes:

`[u'I', u'like', u'<', u'FOOD', u'>']`

I need the result of tokenizing to be:

`[u'I', u'like', u'<FOOD>']`

Only with those two characters in the instance where there are entities, the rest works perfectly. Is there any way to do this please?

Thanks in advance."
290,https://github.com/nltk/nltk/issues/1783,1783,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}]",open,2017-07-22 07:46:29+00:00,,7,Documentation of how and what of NLTK pre-trained models,"There are several pre-trained models that NLTK provides and it is unclear
 
 - what the models are trained on 
 - how the models are trained

These pre-trained models include:

 - `sent_tokenize`: Punkt sentence tokenizers trained on _____ using  `nltk.tokenize.punkt.PunktSentenceTokenizer` with ____ settings/parameters 

 - `pos_tag`: @honnibal 's perceptron POS tagger trained on ____  using `nltk.tag.perceptron.PerceptronTagger` with ____ settings/parameters 

 - `ne_tag`: Named entity tagger trained on ____ (is it ACE? If so, which ACE?) using `nltk.classify.maxent.MaxentClassifier` with ____ settings/parameters?

It would be great if anyone knows about the ____ information above and help answer this issue. And it'll be awesome if it gets documented somewhere so that we avoid another wave of https://news.ycombinator.com/item?id=10173669 and https://explosion.ai/blog/dead-code-should-be-buried ;P"
291,https://github.com/nltk/nltk/issues/1785,1785,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}]",open,2017-07-26 02:46:50+00:00,,5,Fringe cases in MT eval metrics,"There are several fringe that still bugs the MT evaluation metrics in `nltk.translate`. 

The BLEU related issues are mostly resolved in #1330. But similar issues happens in RIBES and CHRF too:

 - `ribes_score.py`
   - https://github.com/nltk/nltk/blob/develop/nltk/translate/ribes_score.py#L290 and https://github.com/nltk/nltk/blob/develop/nltk/translate/ribes_score.py#L320 are subjected to ZeroDivisionError when the no. of possible ngram pairs is 0

 - `chrf_score.py`
   - The interface for the references for other scores supports multi-reference by default while ChrF score supports single reference. It should be standardize to accommodate multi-reference
      - But in the case of multi-reference score, there's no indication of which reference to choose in ChrF, we might need to contact the author to understand how to handle this.
"
292,https://github.com/nltk/nltk/issues/1786,1786,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}]",open,2017-07-26 02:53:19+00:00,,4,'STANFORD_CORENLP' environment variable showing up twice ,"Hello there,
I am not sure if it was intended to be developed this way, but this is how nltk/nltk/parse/stanford.py looks like now:

`stanford_jar = max(
            find_jar_iter(
                self._JAR, path_to_jar,
                env_vars=('STANFORD_PARSER', 'STANFORD_CORENLP'),
                searchpath=(), url=_stanford_url,
                verbose=verbose, is_regex=True
            ),
            key=lambda model_path: os.path.dirname(model_path)
        )

        model_jar=max(
            find_jar_iter(
                self._MODEL_JAR_PATTERN, path_to_models_jar,
                env_vars=('STANFORD_MODELS', 'STANFORD_CORENLP'),
                searchpath=(), url=_stanford_url,
                verbose=verbose, is_regex=True
            ),
            key=lambda model_path: os.path.dirname(model_path)
        )
`

 Notice how 'STANFORD_CORENLP' shows up twice as 2 paths for 2 different purposes. That is line 48 and 58 in the original file.
Is this an issue ?  'STANFORD_CORENLP' referring to stanford-corenlp.jar and stanford-corenlp-models.jar ?"
293,https://github.com/nltk/nltk/issues/1787,1787,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-07-26 07:43:53+00:00,,47,Failed to download NLTK data: HTTP ERROR 405 / 403,"```
>>> nltk.download(""all"")
[nltk_data] Error loading all: HTTP Error 405: Not allowed.

>>> nltk.version_info
sys.version_info(major=3, minor=5, micro=2, releaselevel='final', serial=0)

```


Also, I tried to visit [https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/cmudict.zip](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/cmudict.zip). Got the same HTTP 405 ERROR.

Find the same problem on stackoverflow: [https://stackoverflow.com/questions/45318066/getting-405-while-trying-to-download-nltk-dta](https://stackoverflow.com/questions/45318066/getting-405-while-trying-to-download-nltk-dta)


Any comments would be appreciated."
294,https://github.com/nltk/nltk/issues/1788,1788,[],closed,2017-07-26 11:41:11+00:00,,1,Broken link on FAQ page,"on [FAQ page](https://github.com/nltk/nltk/wiki/FAQ) 
in section ""13. How did NLTK start?""
link [""this plan""](http://code.google.com/p/nltk/source/browse/trunk/nltk-old/doc/technical/proposal/proposal.tex) leads to a google.code page that says: 

""There was an error getting resource 'source':

401: Anonymous users does not have storage.objects.get access to object google-code-archive/v2/code.google.com/nltk/project.json.""

screenshot:
![screenshot](https://user-images.githubusercontent.com/10132717/28619069-089be71c-7207-11e7-8928-07e3b24c7fac.png)

Best regards."
295,https://github.com/nltk/nltk/issues/1789,1789,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",closed,2017-07-26 16:46:24+00:00,,7,wordnet demo fails,"Using nltk.3.2.4 with phython 3.6:

In [17]: pdb
Automatic pdb calling has been turned ON

In [18]: from nltk.corpus.reader.wordnet import demo

In [19]: demo()
loading wordnet
done loading
getting a synset for go
move.v.15 v verb.competition
['move', 'go']
have a turn; make one's move in a game
['Can I go now?']
[Synset('zap.n.01')]
[Synset('zap.v.01'), Synset('zap.v.02'), Synset('nuke.v.01'), Synset('microwave.v.01')]
Navigations:

<LOTS MORE STUFF, AND THEN>

AttributeError                            Traceback (most recent call last)
<ipython-input-21-00cf94af4371> in <module>()
----> 1 demo()

/Applications/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py in demo()
   2116     print(S('gin.n.1').substance_holonyms())
   2117 
-> 2118     print(L('leader.n.1.leader').antonyms())
   2119     print(L('increase.v.1.increase').antonyms())
   2120 

/Applications/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py in lemma(self, name, lang)
   1244         # cannot simply split on first '.',
   1245         # e.g.: '.45_caliber.a.01..45_caliber'
-> 1246         separator = SENSENUM_RE.search(name).start()
   1247         synset_name, lemma_name = name[:separator+3], name[separator+4:]
   1248         synset = self.synset(synset_name)

AttributeError: 'NoneType' object has no attribute 'start'
> /Applications/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py(1246)lemma()
   1244         # cannot simply split on first '.',
   1245         # e.g.: '.45_caliber.a.01..45_caliber'
-> 1246         separator = SENSENUM_RE.search(name).start()
   1247         synset_name, lemma_name = name[:separator+3], name[separator+4:]
   1248         synset = self.synset(synset_name)

ipdb> SENSENUM_RE
re.compile('\\.\\d\\d\\.')
ipdb> name
'leader.n.1.leader'
ipdb> SENSENUM_RE.search(name)
ipdb> 
"
296,https://github.com/nltk/nltk/issues/1790,1790,[],closed,2017-07-27 04:29:07+00:00,,3,nltk.download(),"403 Forbiden

same result as i  open https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml with browser"
297,https://github.com/nltk/nltk/issues/1791,1791,[],closed,2017-07-27 04:45:49+00:00,,5,NlTK downloading corpus using nltk.download() giving 403 error,"I've tried downloading punkt corpus using nltk.download() as well as manually from http://www.nltk.org/nltk_data/ both are giving me a 403 forbidden, Varnish cache server error."
298,https://github.com/nltk/nltk/issues/1792,1792,[],closed,2017-07-27 08:14:41+00:00,,4,nltk.download() pointing to wrong url,"1. the packages are not loading for the following url: https://raw.githubusercontent.com/nltk/nltk_data
HTTP 403 Error

2. After changing the server index to http://www.nltk.org/nltk_data/
still each package pointing to 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/abc.zip'
which is giving HTTP 403 Error"
299,https://github.com/nltk/nltk/issues/1793,1793,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",closed,2017-07-27 16:37:56+00:00,,2,Adding appropriate message to LazyCorpusLoader for nltk.download,"Following up on #1787, it would be great if the users download specific corpora / models instead of abusing `nltk.download('all')`. 

@alexisdimi gave a good suggestion to warn the user with the appropriate package to download when an error is raised at model/data loading. 

For corpora,  this can be easily patched with specific details to the warning on the `LazyCorpusLoader` object, when loading the corpus at https://github.com/nltk/nltk/blob/develop/nltk/corpus/util.py#L67 "
300,https://github.com/nltk/nltk/issues/1794,1794,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 81645781, 'node_id': 'MDU6TGFiZWw4MTY0NTc4MQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/language-model', 'name': 'language-model', 'color': 'd4c5f9', 'default': False, 'description': ''}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",closed,2017-07-27 16:44:18+00:00,,1,Adding appropriate message for nltk.download when loading pre-trained models,"Following up on #1787, it would be great if the users download specific corpora / models instead of abusing nltk.download('all').

@alexisdimi gave a good suggestion to warn the user with the appropriate package to download when an error is raised at model/data loading.

For the various model, the message would have to be added when the error is raised while loading the model. 

To start off, these should be the more popular models that users might load

 - `punkt` / `sent_tokenize`:  https://github.com/nltk/nltk/blob/develop/nltk/tokenize/__init__.py#L84
 - `pos_tag`: https://github.com/nltk/nltk/blob/develop/nltk/tag/__init__.py#L84
 - `ne_chunk`: https://github.com/nltk/nltk/blob/develop/nltk/chunk/__init__.py

Additionally, a quick search on https://github.com/nltk/nltk/search?utf8=%E2%9C%93&q=%22from+nltk.data+import+load%22&type= reveal several more."
301,https://github.com/nltk/nltk/issues/1796,1796,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}]",closed,2017-07-28 10:16:06+00:00,,2,"Extra blank space in ""No. #NUMERIC#"" returning unexpected tokens from MosesTokenizer","To resolve the nltk/nltk_data#85, stripping the non-breaking prefixes would emulate the Perl chomp from the original Moses tokenizer.perl

This can be patched easily by replacing https://github.com/nltk/nltk/blob/develop/nltk/tokenize/moses.py#L244

```
# Initialize the language specific nonbreaking prefixes.
 self.NONBREAKING_PREFIXES = nonbreaking_prefixes.words(lang)
```

to 

```
# Initialize the language specific nonbreaking prefixes.
 self.NONBREAKING_PREFIXES = [_nbp.strip() for _nbp in nonbreaking_prefixes.words(lang)]
```

This is good first bug to contribute to NLTK too =)"
302,https://github.com/nltk/nltk/issues/1798,1798,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",closed,2017-07-31 16:17:21+00:00,,1,Adding language dependent variables to Punkt Tokenizer,ok
303,https://github.com/nltk/nltk/issues/1799,1799,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",closed,2017-08-01 08:16:35+00:00,,3,TweetTokenizer quirk with long numbers,"When there's a number token that's >8 length, the `TweetTokenizer` tries to iteratively parse the first 8-10 digits as a phone number if it matches the phone number regex on https://github.com/alvations/nltk/blob/develop/nltk/tokenize/casual.py#L125

Maybe, there should be an option to allow users to keep long digits and not try to recognize the first part as a number. 

Or better yet, if there's regex fix that can prevent the number from getting split if it's longer than 10. 

Details on https://stackoverflow.com/questions/45425946/tokenize-in-nltk-tweettokenizer-returning-integers-by-splitting/45431922#45431922"
304,https://github.com/nltk/nltk/issues/1800,1800,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}]",open,2017-08-01 20:23:28+00:00,,5,One Letter Typo in English Stoplist,"The last element returned by 
```
from nltk.corpus import stopwords
stopwords.words('english')
```
is 'wouldn' when I believe it should be wouldn't"
305,https://github.com/nltk/nltk/issues/1801,1801,[],closed,2017-08-02 23:31:10+00:00,,1,'EnglishStemmer' has no attribute 'stematize',"So I'm trying to use the `SnowballStemmer` and I'm getting this error. From the Python 3.6.2 REPL:

```python
>>>from nltk.stem import SnowballStemmer
>>>stemmer = SnowballStemmer('english')

Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""/virtualenvpath/python3.6/site-packages/nltk/stem/snowball.py"", line 93, in __init__
    self.stem = self.stemmer.stematize
AttributeError: 'EnglishStemmer' object has no attribute 'stematize'
```

I've tried with spanish, german and danish at it throws the same error with the corresponding stemmer class.

This is taken right from the NLTK Docs page. Is this a bug or I am missing something? "
306,https://github.com/nltk/nltk/issues/1803,1803,[],closed,2017-08-08 22:22:02+00:00,,3,NLTK looking for Python -32,"First, my apologies if this isn't the correct venue to ask this question -- if it's not, I would appreciate suggestions as to where to ask it.  Also, my apologies if this is a duplicate question....I attempted to search for it in previous dialogs but with no luck.

I've installed Python 3.5, 32-bit since it appears NLTK only interfaces with this version.  When I try to install NLTK (following the link on the NLTK home page),  I get a message that it requires Python -32 and can't find it in the registry.

My understanding from poking around on-line is that the installer is looking for Python with an extension of -32.  In looking in HKEY_CURRENT_USERS in the registry, it appears that, except for the first line in the registry,  references to -32 are there.  Example:
[HKEY_CURRENT_USER\Software\Python\PythonCore]

[HKEY_CURRENT_USER\Software\Python\PythonCore\3.5-32]

[HKEY_CURRENT_USER\Software\Python\PythonCore\3.5-32\Help]

and many more....

I also found some leftover references to Python 3.4 (installed previously and deleted) in the HKEY_LOCAL_MACHINE portion of the registry with no reference to -32:
[HKEY_LOCAL_MACHINE\SOFTWARE\Python]

[HKEY_LOCAL_MACHINE\SOFTWARE\Python\PythonCore]

[HKEY_LOCAL_MACHINE\SOFTWARE\Python\PythonCore\3.4]

[HKEY_LOCAL_MACHINE\SOFTWARE\Python\PythonCore\3.4\Modules]

Python 3.5 is the only version installed on my machine.  Questions:
1.  Are the leftover references to 3.4 in LOCAL_MACHINE causing problems?
2.  If not, are the -32 references in the right place (CURRENT_USER)?

As you might imagine, I don't want to mess with the registry unless I'm sure I'm going the right direction.

Thanks.

Karen 


"
307,https://github.com/nltk/nltk/issues/1805,1805,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-08-12 22:09:51+00:00,,5,Order of lemmas returned by derivationally_related_forms() of WordNet?,"I use `derivationally_related_forms()` to get the derivationally related lemmas of a WordNet lemma. When I upgraded nltk from version 2.0.4 to 3.2.3, I realized that the lemmas returned by this function are no longer sorted as they were. Instead, the order could change from one Python session to another. Of course, I could sort the list afterwards to get a deterministic result, but I want to replicate the same order as I found in nltk 2 (which doesn't work on Python 3).

The following code is an example. Please save it as a script, and run it a few times to see if the output stays the same for each run. (Remember that for nltk 2, you need to change `synset.lemmas()` to `synset.lemmas` on the third line.)

    from nltk.corpus import wordnet as wn
    synset = wn.synsets('study', 'n')[1]
    lemma = synset.lemmas()[0]
    print(lemma.derivationally_related_forms())

The output from nltk 2 is always:
`[Lemma('study.v.02.study'), Lemma('study.v.05.study'), Lemma('bookish.s.01.studious'), Lemma('learn.v.04.study')]`

However, according to my observation, the output from nltk 3 is either:
`[Lemma('bookish.s.01.studious'), Lemma('study.v.05.study'), Lemma('study.v.02.study'), Lemma('learn.v.04.study')]`
Or:
`[Lemma('bookish.s.01.studious'), Lemma('study.v.02.study'), Lemma('learn.v.04.study'), Lemma('study.v.05.study')]`

Does anyone know how was the list ordered in nltk 2? Does the order have any linguistic significance? A temporary workaround that allows me to replicate the same order in nltk 3 would be appreciated."
308,https://github.com/nltk/nltk/issues/1808,1808,[],closed,2017-08-14 08:21:33+00:00,,4,"""TypeError: expected str, bytes or os.PathLike object, not NoneType"" about Stanford NLP","### Environment

System: Mac OS X 10.11.6
Python: 3.6.1
nltk: 3.2.4

### Stanford NLP files

```
$ ls -l
total 1608960
-rwxr-xr-x   1 Scott  staff    2162000 Aug 14 15:29 chinesePCFG.ser.gz
drwxr-xr-x   6 Scott  staff        204 Aug 14 15:01 stanford-chinese-corenlp-2017-06-09-models
-rwxrwxrwx   1 Scott  staff  821613963 Aug 14 14:44 stanford-chinese-corenlp-2017-06-09-models.jar
drwxr-xr-x  22 Scott  staff        748 Aug 14 15:17 stanford-ner-2017-06-09
drwxr-xr-x  33 Scott  staff       1122 Aug 14 15:35 stanford-parser-full-2017-06-09
drwxr-xr-x  20 Scott  staff        680 Aug 14 15:35 stanford-postagger-full-2017-06-09
drwxr-xr-x  15 Scott  staff        510 Aug  6 15:49 stanford-segmenter-2017-06-09
-rw-r--r--   1 Scott  staff        559 Aug 14 16:06 test.py
-rw-r--r--   1 Scott  staff        800 Aug 14 15:41 test2.py
```

`chinese.misc.distsim.crf.ser.gz` and `chinesePCFG.ser.gz` are extracted from `stanford-chinese-corenlp-2017-06-09-models`.

### code

test.py

```
# -*- coding: utf-8 -*-

from nltk.tokenize import StanfordSegmenter

segmenter = StanfordSegmenter(
    path_to_jar='./stanford-segmenter-2017-06-09/stanford-segmenter-3.8.0.jar',
    path_to_slf4j='./stanford-parser-full-2017-06-09/slf4j-api.jar',
    path_to_sihan_corpora_dict='./stanford-segmenter-2017-06-09/data',
    path_to_model='./stanford-segmenter-2017-06-09/data/pku.gz',
    path_to_dict='./stanford-segmenter-2017-06-09/data/dict-chris6.ser.gz')

res = segmenter.segment('北海已成为中国对外开放中升起的一颗明星')
print(res)
```

### Error

```
$ python test.py
Traceback (most recent call last):
  File ""test.py"", line 12, in <module>
    res = segmenter.segment('北海已成为中国对外开放中升起的一颗明星')
  File ""/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/nltk/tokenize/stanford_segmenter.py"", line 164, in segment
    return self.segment_sents([tokens])
  File ""/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/nltk/tokenize/stanford_segmenter.py"", line 192, in segment_sents
    stdout = self._execute(cmd)
  File ""/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/nltk/tokenize/stanford_segmenter.py"", line 211, in _execute
    stdout, _stderr = java(cmd, classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE)
  File ""/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/nltk/internals.py"", line 129, in java
    p = subprocess.Popen(cmd, stdin=stdin, stdout=stdout, stderr=stderr)
  File ""/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/subprocess.py"", line 707, in __init__
    restore_signals, start_new_session)
  File ""/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/subprocess.py"", line 1260, in _execute_child
    restore_signals, start_new_session, preexec_fn)
TypeError: expected str, bytes or os.PathLike object, not NoneType
```
**python2.7.12**

```
$ python test2.py
Traceback (most recent call last):
  File ""test2.py"", line 12, in <module>
    res = segmenter.segment(u'北海已成为中国对外开放中升起的一颗明星')
  File ""/usr/local/var/pyenv/versions/2.7.12/lib/python2.7/site-packages/nltk/tokenize/stanford_segmenter.py"", line 164, in segment
    return self.segment_sents([tokens])
  File ""/usr/local/var/pyenv/versions/2.7.12/lib/python2.7/site-packages/nltk/tokenize/stanford_segmenter.py"", line 192, in segment_sents
    stdout = self._execute(cmd)
  File ""/usr/local/var/pyenv/versions/2.7.12/lib/python2.7/site-packages/nltk/tokenize/stanford_segmenter.py"", line 211, in _execute
    stdout, _stderr = java(cmd, classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE)
  File ""/usr/local/var/pyenv/versions/2.7.12/lib/python2.7/site-packages/nltk/internals.py"", line 129, in java
    p = subprocess.Popen(cmd, stdin=stdin, stdout=stdout, stderr=stderr)
  File ""/usr/local/var/pyenv/versions/2.7.12/lib/python2.7/subprocess.py"", line 711, in __init__
    errread, errwrite)
  File ""/usr/local/var/pyenv/versions/2.7.12/lib/python2.7/subprocess.py"", line 1343, in _execute_child
    raise child_exception
TypeError: execv() arg 2 must contain only strings
```

I don't know what happened, I tested it in python3.6.1 and python2.7.12, both not work. Please help！"
309,https://github.com/nltk/nltk/issues/1810,1810,[],closed,2017-08-14 13:24:41+00:00,,0,Refresh snowball module(add arabicstemmer),"Add arabic stemmer to snowball module

* [arabicstemmer](https://github.com/assem-ch/arabicstemmer).
* [ Snowball arabicstemmer ](https://github.com/snowballstem/snowball/tree/master/algorithms/arabic)."
310,https://github.com/nltk/nltk/issues/1811,1811,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-08-14 15:21:15+00:00,,1,Support optional tokens in PCFG?,"Hi, these quite common syntaxes don't seem to work:

```
bla1 -> optional_token? mandatory_token
bla2 -> optional_token* mandatory_token
bla3 -> optional_token mandatory_token+
```

Is this intended or just not implemented? (Because I do see this functionality in the DependencyGrammar)."
311,https://github.com/nltk/nltk/issues/1812,1812,"[{'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-08-14 16:18:11+00:00,,2,PCFG should infer uniform probability for unspecified probabilities,"If one has 10 RHS elements in a PCFG production rule, it is quite cumbersome to calculate the probabilities for each one to get it to be uniform.

Also it is quite common that I want to make a certain rhs less probable than others, in which case I would have wanted to give only probability for that one rule, and let the others be uniform on the remainder of the probability.

Would such a pull request be merged? Does this make sense? Is something in the works?"
312,https://github.com/nltk/nltk/issues/1814,1814,"[{'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-08-15 13:04:18+00:00,,6,[StanfordNeuralDependencyParser]: I give one sentence and parser gives two dependency tree as output,"HI,

This is a simple sentence : 'C. I. LEWIS: THE GOOD AND THE RIGHT.'

I want to get its dependency tree, but I have tow tree after parsing : '[[<DependencyGraph with 3 nodes>, <DependencyGraph with 10 nodes>]]' 

The first tree is :  'Tree('C.-I', ['.'])'

The second tree is : ' Tree('RIGHT', ['-', 'LEWIS', ':', 'THE', Tree('GOOD', ['AND', 'THE']), '.'])'

I dont want two tree, I want just one tree. So I want to force parser to parse sentence previously tokenized by me. 

I've already tried to use all functions, such as parse, parse_one, parse_all, parse_sents, raw_parse and raw parse_sents. Any of these fonctions solve the problem.

Thank you for help me"
313,https://github.com/nltk/nltk/issues/1817,1817,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-08-22 03:37:44+00:00,,4,Better installation instructions for Windows,"Windows users faces issues when installing NLTK. 
We should have better documentation to install and use NLTK in Windows. 

Anyone up to write some extended documentation and improve the [nltk webpage](https://github.com/nltk/nltk.github.com) ?"
314,https://github.com/nltk/nltk/issues/1819,1819,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}]",open,2017-08-23 08:04:59+00:00,,1,Bug in nltk.tokenize.texttiling,"In the source code `class TextTilingTokenizer`, the bug maybe exist during filting stopword. 

`ts.wrdindex_list = [wi for wi in ts.wrdindex_list
                                if wi[0] not in self.stopwords]`
As shown above, this code intends to filter stopword from ts.wrdindex_list ,but ...wi[0] is the combination of the word and pos, not the word itself. So I'm afraid this code is invalid and can't remove any stopword from ts.wrdindex_list.

Please verify this bug...Thanks : )"
315,https://github.com/nltk/nltk/issues/1822,1822,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 25396640, 'node_id': 'MDU6TGFiZWwyNTM5NjY0MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/book', 'name': 'book', 'color': 'e102d8', 'default': False, 'description': None}]",open,2017-08-28 11:03:37+00:00,,1,problem with discourse tester with maltParser and RegexpTagger,"When trying to apply discourse tester as in:
http://www.nltk.org/book/ch10.html (5.2)

I ran into a problem:

>>> rc = DrtGlueReadingCommand(depparser=MaltParser(tagger=tagger))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: __init__() takes at least 2 arguments (2 given)
>>> 
"
316,https://github.com/nltk/nltk/issues/1823,1823,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-08-29 10:29:47+00:00,,1,Construct nltk.translate.PhraseTable and a language model,"Hi! Thanks for the cool library!

I'm trying to build a toy end-to-end machine translation example with NLTK.

Just wondering if there is a simple way to build a language model and, most importantly, a PhraseTable for feeding into a StackDecoder (given my own parallel corpus)?

If not, could you please provide me with ideas on a fast and simple way to do it?

Thanks in advance."
317,https://github.com/nltk/nltk/issues/1824,1824,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",open,2017-09-04 02:40:26+00:00,,8,punkt.PunktSentenceTokenizer() for Chinese,"I use the following code to train punkt for Chinese, but it doesn't produce desired result:

```python
input_str_cn = ""台湾之所以出现这种危机，是台湾不但长年低薪，且不知远景在哪里。20世纪90年代，台湾的大学毕业生起薪不到新台币3万元（约合人民币6594元），到了今天，依然如此。""

# import punkt
import nltk.tokenize.punkt

# Make a new Tokenizer
tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()

# Read in training corpus 
import codecs

train_file = ""D:/CL/comp_ling/data/dushu_1999_2008/1999.txt""
text = codecs.open(train_file, ""r"", ""gb18030"").read()

# Train tokenizer
tokenizer.sent_end_chars = ('！','？','。','”')
for sent_end in tokenizer.sent_end_chars:
    print sent_end
tokenizer.train(text)

# Dump pickled tokenizer
import pickle
out = open(""chinese.pickle"",""wb"")
pickle.dump(tokenizer, out)
out.close()

# To use the tokenizer
with open(""chinese.pickle"") as infile:
    tokenizer_new = pickle.load(infile)
sents = tokenizer_new.tokenize(input_str_cn)
for s in sents:
    print s
```

The produced result is as follows:


> ""台湾之所以出现这种危机，是台湾不但长年低薪，且不知远景在哪里。20世纪90年代，台湾的大学毕业生起薪不到新台币3万元（约合人民币6594元），到了今天，依然如此。""


It seems that the `sent_end_chars` does not work here. I have checked the encoding. There's no problem with that. Could anyone help with it? Thanks."
318,https://github.com/nltk/nltk/issues/1825,1825,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-09-04 17:38:00+00:00,,4,Skip downloader.py input() prompts when no TTY attached,"In heroku/heroku-buildpack-python#444 an `EOFError: EOF when reading a line` was raised after an nltk_data download failed and downloader.py prompted with ""Error installing package. Retry? [n/y/e]"".

Whilst that prompt can be suppressed by passing `--quiet`:
https://github.com/nltk/nltk/blob/3.2.4/nltk/downloader.py#L679-L688
...doing so means losing useful debugging information.

What would be great is if either:
(a) The `input()` would be skipped if no TTY were attached
(b) There was an additional `--no-prompt` option, in addition to `--quiet`, so users who want the more verbose output can still keep it

Many thanks :-)"
319,https://github.com/nltk/nltk/issues/1828,1828,"[{'id': 25396640, 'node_id': 'MDU6TGFiZWwyNTM5NjY0MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/book', 'name': 'book', 'color': 'e102d8', 'default': False, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-09-08 22:12:15+00:00,,6,Brill Tagger Demo Broken,"I noticed that in [the book](http://www.nltk.org/book/ch05.html#tab-brill-tagging) (section 6, example 6.1), the suggested code is: `nltk.tag.brill.demo()`

When I run this I get the following error: `AttributeError: module 'nltk.tag.brill' has no attribute 'demo'`

Am I missing something?"
320,https://github.com/nltk/nltk/issues/1829,1829,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",closed,2017-09-14 16:33:31+00:00,,4,Word alignment symmetrization is broken,"The module nltk.translate.gdfa, which includes the grow_diag_final_and function and a couple of nested functions, is broken.
It has a few serious bugs; in particular, the main loop of the grow_diag internal function runs forever except in some trivial cases. Possibly this symmetrization algorithm has been ported to NLTK from another language and hasn't been tested.

Although I didn't analyze the algorithm implementation in depth, I think that I have fixed the bugs preventing it from working. I've put the patches in the attached diff file.

[gdfa.py.txt](https://github.com/nltk/nltk/files/1303712/gdfa.py.txt)
"
321,https://github.com/nltk/nltk/issues/1830,1830,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",closed,2017-09-18 02:48:57+00:00,,1,How to set java_home for dockerfile when using nltk,"Hi guys , 
 I just write a chatbot using nltk module. However, one of the modules need Java and need to set JAVA_HOME. Therefore it is not runnable in another new machine.
Anybody knows how to install java and set java_home in the dockerfile ?"
322,https://github.com/nltk/nltk/issues/1831,1831,[],closed,2017-09-21 19:34:48+00:00,,1,A weird bug in MoseTokenizer.tokenize(),"The following code throws `index of range` error
```python
from nltk.tokenize.moses import MosesTokenizer

t = MosesTokenizer()
s = 'Art.'
print(t.tokenize(s))
```"
323,https://github.com/nltk/nltk/issues/1832,1832,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}]",closed,2017-09-22 09:28:40+00:00,,3,nltk.download('stop_words') broken: ParseError: not well-formed,"Hi!

```bash 
$ python
Python 2.7.9 (default, Jun 29 2016, 13:08:31)
[GCC 4.9.2] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import nltk; nltk.download('stop_words')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python2.7/dist-packages/nltk/downloader.py"", line 665, in download
    for msg in self.incr_download(info_or_id, download_dir, force):
  File ""/usr/lib/python2.7/dist-packages/nltk/downloader.py"", line 535, in incr_download
    try: info = self._info_or_id(info_or_id)
  File ""/usr/lib/python2.7/dist-packages/nltk/downloader.py"", line 509, in _info_or_id
    return self.info(info_or_id)
  File ""/usr/lib/python2.7/dist-packages/nltk/downloader.py"", line 873, in info
    self._update_index()
  File ""/usr/lib/python2.7/dist-packages/nltk/downloader.py"", line 826, in _update_index
    ElementTree.parse(compat.urlopen(self._url)).getroot())
  File ""/usr/lib/python2.7/xml/etree/ElementTree.py"", line 1182, in parse
    tree.parse(source, parser)
  File ""/usr/lib/python2.7/xml/etree/ElementTree.py"", line 656, in parse
    parser.feed(data)
  File ""/usr/lib/python2.7/xml/etree/ElementTree.py"", line 1642, in feed
    self._raiseerror(v)
  File ""/usr/lib/python2.7/xml/etree/ElementTree.py"", line 1506, in _raiseerror
    raise err
xml.etree.ElementTree.ParseError: not well-formed (invalid token): line 1, column 0
```

Versions bug submitted: 3.0.0, 3.2.4 "
324,https://github.com/nltk/nltk/issues/1836,1836,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",closed,2017-09-24 10:41:12+00:00,,1,Clean up the RTE classify code,"The `RTEFeatureExtractor` object in `nltk.classify.rte_classify.py` has functions outside the class object, it would be good clean up the code by: 

 - having RTE relation extracting  function as staticmethod within the object
 - porting the demo code into proper unit test in `nltk.test.unit`"
325,https://github.com/nltk/nltk/issues/1838,1838,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-09-25 17:05:36+00:00,,17,A weird edge case for bleu scoring. ,"There is an unintuitive functionality of the bleu_score module. 

```
>>> bleu_score.sentence_bleu([[1,2,3,4]],[5,6,8,1], weights=(0.5, 0.5)) 
0.5
>>> bleu_score.sentence_bleu([[1,2,3,4]],[5,6,1,2], weights=(0.5, 0.5))
0.408248290463863
>>> bleu_score.sentence_bleu([[1,2,3,4]],[5,6,2,3], weights=(0, 0.5))
0.5773502691896257
>>> bleu_score.sentence_bleu([[1,2,3,4]],[5,6,7,3], weights=(0, 0.5))
1.0
>>> bleu_score.sentence_bleu([[1,2,3,4]],[5,6,8,7], weights=(0, 0.5))
0

```
The default `SmoothingFunction.method0` skips any fractions that are equal to zero making bleu scores in these cases quite unintuitive. In my opinion this would be more intuitive to have the log(0) evaluate to -inf resulting in the overall bleu score in these cases evaluating to 0. This along with a clearer warning would make this much more intuitive. 

To clarify, as seen below it gives a higher bleu for a worse match when i believe it should receive a bleu score of 0.
```
>>> bleu_score.sentence_bleu([[1,2,3,4]],[5,6,8,1], weights=(0.5, 0.5))
0.5
>>> bleu_score.sentence_bleu([[1,2,3,4]],[5,6,1,2], weights=(0.5, 0.5))
0.408248290463863
```"
326,https://github.com/nltk/nltk/issues/1839,1839,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 159235791, 'node_id': 'MDU6TGFiZWwxNTkyMzU3OTE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/dependency%20parsing', 'name': 'dependency parsing', 'color': 'eb6420', 'default': False, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2017-09-26 02:24:49+00:00,,4,Deprecating the old Stanford Parser,"We have deprecated the `StanfordTokenizer`/`StanfordSegmenter`, `StanfordPOSTagger` and `StanfordNERTagger`. 

It would be good to also deprecate the old `StanfordParser`, `StanfordDependencyParser` and `StanfordNeuralDependencyParser` by 

1. Adding the appropriate warnings to the old interface

2a. Wrap the duck-types for `CoreNLPParser` that emulates the functions of the old interface

2b.  Write up documentations of how to use the `CoreNLPParser` to use dependency and neural dependency parsing

3. Write tests for the new CoreNLP parser interfaces

Both (2a) and (2b) methods should only affect the `properties` argument of the `api_call`

The current interface for `CoreNLPParser`:

```
>>> from nltk.parse.corenlp import CoreNLPParser
>>> sent = 'The quick brown fox jumps over the lazy dog.'
>>> next(parser.raw_parse(sent)).pretty_print()  # doctest: +NORMALIZE_WHITESPACE
                         ROOT
                          |
                          S
           _______________|__________________________
          |                         VP               |
          |                _________|___             |
          |               |             PP           |
          |               |     ________|___         |
          NP              |    |            NP       |
      ____|__________     |    |     _______|____    |
     DT   JJ    JJ   NN  VBZ   IN   DT      JJ   NN  .
     |    |     |    |    |    |    |       |    |   |
    The quick brown fox jumps over the     lazy dog  .
```

The desired interface might look like this:

```
# Using Duck-types
>>> from nltk.parse.stanford import CoreNLPParser
>>> depparser = CoreNLPDependencyParser('http://localhost:9000')
>>> depparser.parse(sent)
>>> ndepparser = CoreNLPNeuralDependencyParser('http://localhost:9000')
>>> ndepparser.parse(sent)
```

```
# Using arguments to control `properties` for `api_call()` 
>>> from nltk.parse.stanford import CoreNLPParser

>>> depparser = CoreNLPParser('http://localhost:9000', parser_type=""dependency"")
>>> depparser.parse(sent)

>>> ndepparser = CoreNLPNeuralDependencyParser('http://localhost:9000', parser_type=""neural_dependency"")
>>> ndepparser.parse(sent)
```

This would make a good class project or good first challenge ;P

"
327,https://github.com/nltk/nltk/issues/1841,1841,"[{'id': 25396640, 'node_id': 'MDU6TGFiZWwyNTM5NjY0MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/book', 'name': 'book', 'color': 'e102d8', 'default': False, 'description': None}, {'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-09-27 14:28:16+00:00,,2,Error in Online Book - Chapter 7.6,"In the online version of the book, in Chapter 7.6 the final ""Your Turn"" note contains some errors. The book reads:

`print(rtuple(rel, lcon=True, rcon=True))`

But this will throw an error, since `rtuple` is not recognized. Replacing `rtuple` with `nltk.rtuple` fixed the issue.

Also, in the code instead of `rel` we write `r`, so the variable name will also cause an issue.

I believe the following line does the trick (it works for me):

`print(nltk.rtuple(r, lcon=True, rcon=True))`"
328,https://github.com/nltk/nltk/issues/1842,1842,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-09-27 22:16:35+00:00,,3,nist_score vs bleu_score's SmoothingFunction.method3,"The 3.2.5 release introduced the `nltk.translate.nist_score` module, but I was under the impression that the BLEU score using `nltk.translate.bleu_score.SmoothingFunction.method3()` for smoothing was the NIST metric, as its docstring says ""NIST geometric sequence smoothing"". Are these two totally different things? They do give very different scores:

```python
>>> from nltk.translate import nist_score, bleu_score
>>> hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which','ensures', 'that', 'the', 'military', 'always','obeys', 'the', 'commands', 'of', 'the', 'party']
>>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that','ensures', 'that', 'the', 'military', 'will', 'forever','heed', 'Party', 'commands']
>>> nist_score.sentence_nist([reference1], hypothesis1)
0.05876787197888857
>>> bleu_score.sentence_bleu([reference1], hypothesis1, smoothing_function=bleu_score.SmoothingFunction().method3)
0.41180376356915777
```

The docstring for `nist_score.sentence_nist()` says it comes from `mteval-14.pl`, while the Chen & Cherry 2014 paper says their method 3 comes from `mteval-v13a.pl`. Perhaps we could just update the docstrings of the respective functions so it's more clear that the smoothing function and the NIST metric are not the same, so that people who aren't already familiar with the implementation (like me) are not confused."
329,https://github.com/nltk/nltk/issues/1846,1846,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",open,2017-10-04 04:25:39+00:00,,3,Deep Learning + NLTK,"This issue to start a discussion on how we can provide deep learning features to NLTK. The topics can be and are not limited to:

 - What kind of DL features do we want to see in NLTK?
 - Would adding DL features add value to the exist ecology of Python NLP OSS and NLTK users? 
 - What level of education vs practicality should we balance for DL features in NLTK?

Please feel free to discuss on this issue!
"
330,https://github.com/nltk/nltk/issues/1847,1847,[],closed,2017-10-05 14:19:14+00:00,,0,Cannot Import Compat,"hi I'm running NLTK on 2.7 and when I import nltk it gives a error: 


  File ""C:\Anaconda2\lib\site-packages\nltk\parse\chart.py"", line 44, in <module>
    from nltk import compat

ImportError: cannot import name compat

However when I go into ""C:\Anaconda2\lib\site-packages\nltk\"" folder the file compat is in there.  Thanks for your help,"
331,https://github.com/nltk/nltk/issues/1849,1849,[],closed,2017-10-06 02:31:08+00:00,,2,Automating integration tests,"I wanted to make a few contributions to the library a year or two ago but I couldn't get Jenkins working and just gave up. I just made a PR to tensorflow, and jenkins ran automatically! I don't know how they did that, but that seems like a good thing to have!

I don't know if that automation is something I could contribute, or if anyone has given this any thought? I don't even know what would be required to do this? There are definitely all kinds of hairy details about configuration, and probably a server would have to be rented, which would cost money. Maybe its a nonstarter but it seems nice."
332,https://github.com/nltk/nltk/issues/1850,1850,[],closed,2017-10-06 17:18:52+00:00,,1,Bug in NLTK with Python3.5,"I used tutorial from book http://www.nltk.org/book/ch07.html
```
sent = 'Google is best company'
sent = nltk.corpus.treebank.tagged_sents()
r = nltk.ne_chunk(sent)
```
and got

>          183 
>          184 def simplify_pos(s):
>     --> 185     if s.startswith('V'): return ""V""
>          186     else: return s.split('-')[0]
>          187 
> 
> AttributeError: 'tuple' object has no attribute 'startswith'
"
333,https://github.com/nltk/nltk/issues/1851,1851,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",open,2017-10-09 22:37:47+00:00,,1,Missing corpus reader for PIL,"The PIL data was added quite some time ago from https://github.com/nltk/nltk/commit/ef144a94479de5536fc860c00f4f14e0a62787e4 but there's no corpus reader for it. 

Reported on https://stackoverflow.com/q/46650761/610569 

Simply using the XMLCorpusReader is not enough too:

```
>>> from nltk.corpus.reader import XMLCorpusReader
>>> from nltk.corpus.util import LazyCorpusLoader
>>> LazyCorpusLoader('pil',  XMLCorpusReader, r'(?!\.).*\.sgml')
<XMLCorpusReader in '.../corpora/pil' (not loaded yet)>

>>> pil.raw()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/xmldocs.py"", line 84, in raw
    return concat([self.open(f).read() for f in fileids])
  File ""/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/xmldocs.py"", line 84, in <listcomp>
    return concat([self.open(f).read() for f in fileids])
  File ""/usr/local/lib/python3.5/dist-packages/nltk/data.py"", line 1110, in read
    chars = self._read(size)
  File ""/usr/local/lib/python3.5/dist-packages/nltk/data.py"", line 1380, in _read
    chars, bytes_decoded = self._incr_decode(bytes)
  File ""/usr/local/lib/python3.5/dist-packages/nltk/data.py"", line 1411, in _incr_decode
    return self.decode(bytes, 'strict')
  File ""/usr/lib/python3.5/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x92 in position 9209: invalid start byte

>>> pil.words()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/xmldocs.py"", line 66, in words
    elt = self.xml(fileid)
  File ""/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/xmldocs.py"", line 47, in xml
    raise TypeError('Expected a single file identifier string')
TypeError: Expected a single file identifier string

```
"
334,https://github.com/nltk/nltk/issues/1852,1852,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-10-11 11:04:43+00:00,,7,ArabicStemmer AttributeError,"I'm failing to stem certain Arabic terms using the SnowballStemmer. Many terms are stemmed successfully but some terms cause an AttributeError to be raised. Please see below for a minimal example that fails on the term 'from'.

```
(anaconda2-4.4.0) richard-balmer-macbook:~ richardbalmer$ pip freeze | grep nltk
nltk==3.2.5
(anaconda2-4.4.0) richard-balmer-macbook:~ richardbalmer$ ipython
Python 2.7.13 |Anaconda custom (x86_64)| (default, Dec 20 2016, 23:05:08)
Type ""copyright"", ""credits"" or ""license"" for more information.

IPython 5.3.0 -- An enhanced Interactive Python.
?         -> Introduction and overview of IPython's features.
%quickref -> Quick reference.
help      -> Python's own help system.
object?   -> Details about 'object', use 'object??' for extra details.

In [1]: from nltk.stem.snowball import SnowballStemmer

In [2]: stemmer = SnowballStemmer('arabic')

In [3]: stemmer.stem(u'تسدد')
Out[3]: u'\u062a\u0633\u062f\u062f'

In [4]: stemmer.stem(u'من')
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-4-ffa733106049> in <module>()
----> 1 stemmer.stem(u'من')

/Users/richardbalmer/.pyenv/versions/anaconda2-4.4.0/lib/python2.7/site-packages/nltk/stem/snowball.pyc in stem(self, word)
    762                 modified_word = self.__Suffix_Verb_Step2b(modified_word)
    763                 if not self.suffix_verb_step2b_success:
--> 764                     modified_word = self.__Suffix_Verb_Step2a(modified_word)
    765         if self.is_noun:
    766             modified_word = self.__Suffix_Noun_Step2c2(modified_word)

/Users/richardbalmer/.pyenv/versions/anaconda2-4.4.0/lib/python2.7/site-packages/nltk/stem/snowball.pyc in __Suffix_Verb_Step2a(self, token)
    533                     break
    534
--> 535                 if suffix in self.__conjugation_suffix_verb_present and len(token) > 5:
    536                     token = token[:-2]  # present
    537                     self.suffix_verb_step2a_success = True

AttributeError: 'ArabicStemmer' object has no attribute '_ArabicStemmer__conjugation_suffix_verb_present'
```"
335,https://github.com/nltk/nltk/issues/1853,1853,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}]",open,2017-10-11 19:59:36+00:00,,1,Test failures in wordnet.doctest,"From the latest https://github.com/nltk/nltk/commit/0477ceb2a3ab9f4c0abc0f800a827c92e8d48623 of the `develop` branch, I ran tests locally and got three failures from `wordnet.doctest`:

```
1) FAIL: Doctest: wordnet.doctest
----------------------------------------------------------------------
   Traceback (most recent call last):
    /usr/lib/python3.5/doctest.py line 2190 in runTest
      raise self.failureException(self.format_failure(new.getvalue()))
   AssertionError: Failed doctest test for wordnet.doctest
     File ""<path-to-nltk>/nltk/nltk/test/wordnet.doctest"", line 0
   
   ----------------------------------------------------------------------
   File ""<path-to-nltk>/nltk/nltk/test/wordnet.doctest"", line 50, in wordnet.doctest
   Failed example:
       sorted(wn.langs()) # doctest: +NORMALIZE_WHITESPACE
   Expected:
       ['als', 'arb', 'bul', 'cat', 'cmn', 'dan', 'ell', 'eng', 'eus', 'fas', 'fin', 'fra', 'glg', 'heb', 'hrv', 'ind', 
       'ita', 'jpn', 'nno', 'nob', 'pol', 'por', 'qcn', 'slv', 'spa', 'swe', 'tha', 'zsm']
   Got:
       ['als', 'arb', 'cat', 'cmn', 'dan', 'eng', 'eng', 'eus', 'fas', 'fin', 'fra', 'fre', 'glg', 'heb', 'ind', 'ita', 'jpn', 'nno', 'nob', 'pol', 'por', 'spa', 'tha', 'zsm']
   ----------------------------------------------------------------------
   File ""<path-to-nltk>/nltk/nltk/test/wordnet.doctest"", line 64, in wordnet.doctest
   Failed example:
       wn.lemmas('cane', lang='ita') # doctest: +NORMALIZE_WHITESPACE
   Expected:
       [Lemma('dog.n.01.cane'), Lemma('cramp.n.02.cane'), Lemma('hammer.n.01.cane'), Lemma('bad_person.n.01.cane'), 
       Lemma('incompetent.n.01.cane')]
   Got:
       [Lemma('dog.n.01.cane'), Lemma('hammer.n.01.cane'), Lemma('cramp.n.02.cane'), Lemma('bad_person.n.01.cane'), Lemma('incompetent.n.01.cane')]
   ----------------------------------------------------------------------
   File ""<path-to-nltk>/nltk/nltk/test/wordnet.doctest"", line 79, in wordnet.doctest
   Failed example:
       len(wordnet.all_lemma_names(pos='n', lang='jpn'))
   Expected:
       64797
   Got:
       66027
```

The exact same three failures were observed with `tox -e py27` and `tox -e py35` (shown above is from py35).

Two peculiarities:
1. Inconsistency between nltk/nltk and nltk/nltk.github.com: The third failure for `len(wordnet.all_lemma_names(pos='n', lang='jpn'))` shows that [`wordnet.doctest` says 64797](https://github.com/nltk/nltk/blob/0477ceb2a3ab9f4c0abc0f800a827c92e8d48623/nltk/test/wordnet.doctest#L80), but the test got 66027, which is exactly what the NLTK how-to page says from [the underlying HTML](https://github.com/nltk/nltk.github.com/blob/fbd8429d14f9e681b0877c7a3d64e31d6144367b/howto/wordnet.html#L423). This makes me think there's reason to believe what I've observed is real, but...
2. Curiously, these test failures are _not_ observed on the Jenkin CI builds, e.g., in [this recent run](https://nltk.ci.cloudbees.com/job/pull_request_tests/442/TOXENV=py35-jenkins,jdk=jdk8latestOnlineInstall/console) associated with #1848 that I'm working on.

Because of these, I've hesitated to submit a PR to fix what seems to be straightforward, in case there's more story behind and I've missed something."
336,https://github.com/nltk/nltk/issues/1854,1854,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}]",closed,2017-10-11 20:03:47+00:00,,3,Audit codebase for pre-PEP 357 slice handling,"Special handling of slices in getitem methods was not required post PEP 357, cf #1845."
337,https://github.com/nltk/nltk/issues/1855,1855,"[{'id': 718733436, 'node_id': 'MDU6TGFiZWw3MTg3MzM0MzY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/Off-topic', 'name': 'Off-topic', 'color': 'f7b4e1', 'default': False, 'description': None}]",closed,2017-10-12 07:57:01+00:00,,2,How to get the full Penn Treebank?,"Any ideas to get the Penn Treebank?

I've checked the LDC site. The price is too high that I can't afford it. 

I want it for the purpose of Semantic Role Labelling."
338,https://github.com/nltk/nltk/issues/1858,1858,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-10-13 20:24:02+00:00,,6,ElementTree.ParseError: not well-formed ,"I am getting a similar issue to #1832 

```
bash-4.3# /opt/conda/envs/py27/bin/python -m nltk.downloader wordnet
Traceback (most recent call last):
  File ""/opt/conda/envs/py27/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/opt/conda/envs/py27/lib/python2.7/runpy.py"", line 72, in _run_code
...
  File ""/opt/conda/envs/py27/lib/python2.7/xml/etree/ElementTree.py"", line 1659, in feed
    self._raiseerror(v)
  File ""/opt/conda/envs/py27/lib/python2.7/xml/etree/ElementTree.py"", line 1523, in _raiseerror
    raise err
xml.etree.ElementTree.ParseError: not well-formed (invalid token): line 1, column 0
```

The diagnostics you asked for in that ticket
[out.txt](https://github.com/nltk/nltk/files/1383914/out.txt)
 (index.xml) are attached"
339,https://github.com/nltk/nltk/issues/1859,1859,"[{'id': 81645781, 'node_id': 'MDU6TGFiZWw4MTY0NTc4MQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/language-model', 'name': 'language-model', 'color': 'd4c5f9', 'default': False, 'description': ''}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}]",open,2017-10-14 19:18:50+00:00,,3,HiddenMarkovModelTrainer train_unsupervised - TypeError,"Hello, 

I'm trying to use the `train_unsupervised` method from the `nltk.hmm` module, however, it's throwing the following TypeError:
`TypeError: Can't instantiate abstract class RandomProbDist with abstract methods max`

Here is the code I'm using:
```
import nltk

trainer = nltk.hmm.HiddenMarkovModelTrainer()
tagger = trainer.train_unsupervised([
    ('a', ''),
    ('b', ''),
    ('c', '')
])
```

Here's the full error:
`Traceback (most recent call last):
  File ""test.py"", line 10, in <module>
    ('c', '')
  File ""/Library/Python/2.7/site-packages/nltk/tag/hmm.py"", line 932, in train_unsupervised
    priors = RandomProbDist(self._states)
TypeError: Can't instantiate abstract class RandomProbDist with abstract methods max`

NLTK version 3.2.5; tested on Python 2.7.10 and Python 2.7.14.

Am I doing something silly wrong? 

Thank you very much."
340,https://github.com/nltk/nltk/issues/1860,1860,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}, {'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}, {'id': 789518993, 'node_id': 'MDU6TGFiZWw3ODk1MTg5OTM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/need-help', 'name': 'need-help', 'color': 'e0609e', 'default': False, 'description': None}]",closed,2017-10-16 11:20:35+00:00,,8,[Question] Add custom Regex to improve date recognition for tokenizer and POS tagger,"I am trying to recognizing a simple kind of date (""XX/XX/XXXX"") in my Python3 code.
For that, I would like to create a regex, and to add a tag for this one: ""DATE"" (for example). 

This is the code I wrote:
```
# Recognize month/day/year
patterns = [(r'\d{2}/\d{2}/\d{4}', 'DATE')]
# Build a tagger
reg_tagger = nltk.RegexpTagger(patterns)
default_tagger = nltk.data.load(""taggers/maxent_treebank_pos_tagger/english.pickle"")
# Build a tagger that add reg_tagger in an existing tagger (MaxEnt)
tagger = nltk.UnigramTagger(model=reg_tagger, backoff=default_tagger)
# Tag the words in each sentence
tags = [tagger.tag(_s) for _s in sentences]
```

Unfortunately, I got this error:
```
>>> python3.6 scripts.py examples/090003ea802cef84.txt
Traceback (most recent call last):
  File ""scripts.py"", line 202, in <module>
    tags = get_tags_from_sentences(sentences)
  File ""scripts.py"", line 50, in get_tags_from_sentences
    tags = [tagger.tag(_s) for _s in sentences]
  File ""scripts.py"", line 50, in <listcomp>
    tags = [tagger.tag(_s) for _s in sentences]
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tag/sequential.py"", line 63, in tag
    tags.append(self.tag_one(tokens, i, tags))
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tag/sequential.py"", line 83, in tag_one
    tag = tagger.choose_tag(tokens, index, history)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tag/sequential.py"", line 142, in choose_tag
    return self._context_to_tag.get(context)
AttributeError: 'RegexpTagger' object has no attribute 'get'
```

So, my question is: is it possible to add a custom regex (for my case, to recognize dates) in a default tagger, please?

Thanks a lot"
341,https://github.com/nltk/nltk/issues/1861,1861,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}]",closed,2017-10-16 11:23:13+00:00,,4,TextTiling not support python 3,"In line 315 `clip = min(max(len(scores)/10, 2), 5)` you should wrap this value with a `int` because python 3 division type is float.

Thanks"
342,https://github.com/nltk/nltk/issues/1865,1865,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-10-18 04:22:58+00:00,,1,TreebankWordTokenizer.span_tokenize() throws Exceptions for corner case strings,"This comes from one of the sentence in my work. I have created https://github.com/nltk/nltk/pull/1864 as a fix.

to reproduce:

```
>>> from nltk.tokenize import TreebankWordTokenizer
>>> s = '''I said, ""I'd like to buy some ''good muffins"" which cost $3.88\\n each in New (York).""'''
>>> TreebankWordTokenizer().span_tokenize(s)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/site-packages/nltk/tokenize/treebank.py"", line 167, in span_tokenize
    return align_tokens(tokens, text)
  File ""/usr/local/lib/python2.7/site-packages/nltk/tokenize/util.py"", line 232, in align_tokens
    raise ValueError('substring ""{}"" not found in ""{}""'.format(token, sentence))
ValueError: substring ""``"" not found in ""I said, ""I'd like to buy some ''good muffins"" which cost $3.88\n each in New (York).""""
```

Also I have a question - I am not sure why the pull request check didn't run? The error seems to be here: https://nltk.ci.cloudbees.com/job/pull_request_tests/TOXENV=py27-jenkins,jdk=jdk8latestOnlineInstall/lastBuild/console 

It looks like I did something terribly wrong somewhere. This is my first pull request to nltk, could anyone help? Thank you!


"
343,https://github.com/nltk/nltk/issues/1872,1872,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}]",closed,2017-10-24 23:27:30+00:00,,0,Renovating CorpusReader and CorpusView,"@rmalouf has renovated NLTK's CorpusView class in https://github.com/nltk/nltk/pull/1867, which is pulled into the https://github.com/nltk/nltk/tree/new-corpus-view branch.

I'm opening this issue to host feedback.
"
344,https://github.com/nltk/nltk/issues/1874,1874,"[{'id': 718746024, 'node_id': 'MDU6TGFiZWw3MTg3NDYwMjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stem/lemma', 'name': 'stem/lemma', 'color': '53bab8', 'default': False, 'description': None}, {'id': 738293869, 'node_id': 'MDU6TGFiZWw3MzgyOTM4Njk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/duplicate%20issue', 'name': 'duplicate issue', 'color': 'ea385b', 'default': False, 'description': None}]",closed,2017-10-26 10:32:11+00:00,,3,Porter Stemmer Bug,"Token ```aed``` will trigger this bug.

```
  File ""C:\Anaconda3\lib\site-packages\nltk\stem\porter.py"", line 665, in stem
    stem = self._step1b(stem)
  File ""C:\Anaconda3\lib\site-packages\nltk\stem\porter.py"", line 376, in _step1b
    lambda stem: (self._measure(stem) == 1 and
  File ""C:\Anaconda3\lib\site-packages\nltk\stem\porter.py"", line 258, in _apply_rule_list
    if suffix == '*d' and self._ends_double_consonant(word):
  File ""C:\Anaconda3\lib\site-packages\nltk\stem\porter.py"", line 214, in _ends_double_consonant
    word[-1] == word[-2] and
IndexError: string index out of range
```"
345,https://github.com/nltk/nltk/issues/1875,1875,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 25396640, 'node_id': 'MDU6TGFiZWwyNTM5NjY0MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/book', 'name': 'book', 'color': 'e102d8', 'default': False, 'description': None}, {'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}, {'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}]",closed,2017-10-31 18:55:52+00:00,,0,CoreNLP documentation,"Since the CoreNLP client code is more or less stable is actually being tried every now and then it's time to write detailed documentation!

Relevant issues:
 * #1510
 * https://github.com/nltk/nltk/issues/1759#issuecomment-340853721
 * #1758
 * #1876
 * https://github.com/nltk/nltk/issues/1876#issuecomment-343877499

Would it worth extending the NLTK book to include an explanation how to use CoreNLP tools."
346,https://github.com/nltk/nltk/issues/1876,1876,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}]",closed,2017-10-31 22:42:08+00:00,,6,POS tagging after Tokenization using CoreNLP classes,"Hello,

I want to use the `CoreNLPTagger` to tokenize and POS-tag a big corpus.
However, there is no option to specify additional properties to the `raw_tag_sents` method in the `CoreNLPTagger` (in contrary to the `tokenize` method in `CoreNLPTokenizer`, which lets you specify additional properties). Therefore I'm not able to tell the tokenizer to e.g. not normalize the brackets and other stuff.

For example, I want to use the following tokenization options:
```python
additional_properties = {
            'tokenize.options': 'ptb3Escaping=false, unicodeQuotes=true, splitHyphenated=true, normalizeParentheses=false, normalizeOtherBrackets=false',
            'annotators': 'tokenize, ssplit, pos'
        }
```

Using the tokenizer before the tagger does also not work, as this will revert any of the additional options you set in the tokenizer.

I may be doing something wrong here, but I hope you can help me.

Thanks

"
347,https://github.com/nltk/nltk/issues/1877,1877,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 738078305, 'node_id': 'MDU6TGFiZWw3MzgwNzgzMDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python2.7', 'name': 'python2.7', 'color': '7be833', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-11-01 20:21:27+00:00,,5,UnicodeEncodeError when chunking non-ASCII text,"I've encountered a UnicodeEncodeError when chunking text that includes non-ASCII characters, using Python 2.7.12 and NLTK 3.2.5. As a new user of NLTK, there may be some subtleties when working with Unicode that I'm not sensitive to yet, but here's a sample bit of code that demonstrates the problem:

```
import nltk

text = u""You may have heard of Ren\xe9 Descartes.""
tagged = nltk.pos_tag(nltk.word_tokenize(text))
entities = nltk.chunk.ne_chunk(tagged)
print entities
```

When run, this code reports an error at the chunking line above, ending with the report
```
 File ""/Users/jeffgarbers/.virtualenvs/nlplay/lib/python2.7/site-packages/nltk/chunk/named_entity.py"", line 104, in _feature_detector   
  'word+nextpos': '{0}+{1}'.format(word.lower(), nextpos),
UnicodeEncodeError: 'ascii' codec can't encode character u'\xe9' in position 3: ordinal not in range(128)
```

It seems that the error goes away if the format strings in lines 104-106 of _named_entity.py_ are changed to Unicode (_u'word+nextpos'_, for example), although I don't know if such a fix has other implications. 

I'd much appreciate any suggestions anyone might offer. Thanks!

"
348,https://github.com/nltk/nltk/issues/1878,1878,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",closed,2017-11-03 22:33:32+00:00,,5,Identifying chemical entities with OSCAR4,"I'm interested in hearing whether this is something you would be interested in adding.

[OSCAR4](https://bitbucket.org/wwmm/oscar4/wiki/Home) is an open source software toolkit for identifying chemical entities in a text. It also includes a tokeniser. 
The accompanying publication is here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3205045."
349,https://github.com/nltk/nltk/issues/1879,1879,"[{'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}]",closed,2017-11-05 12:01:33+00:00,,11,Fail to install nltk==2.0.5,"```
$ docker run --rm -it python:2.7.8 bash
# pip install  nltk==2.0.5
Downloading/unpacking nltk==2.0.5
  Downloading nltk-2.0.5.tar.gz (954kB): 954kB downloaded
  Running setup.py (path:/tmp/pip_build_root/nltk/setup.py) egg_info for package nltk
    Downloading http://pypi.python.org/packages/source/d/distribute/distribute-0.6.21.tar.gz
    Traceback (most recent call last):
      File ""<string>"", line 17, in <module>
      File ""/tmp/pip_build_root/nltk/setup.py"", line 23, in <module>
        distribute_setup.use_setuptools()
      File ""distribute_setup.py"", line 145, in use_setuptools
        return _do_download(version, download_base, to_dir, download_delay)
      File ""distribute_setup.py"", line 124, in _do_download
        to_dir, download_delay)
      File ""distribute_setup.py"", line 193, in download_setuptools
        src = urlopen(url)
      File ""/usr/local/lib/python2.7/urllib2.py"", line 127, in urlopen
        return _opener.open(url, data, timeout)
      File ""/usr/local/lib/python2.7/urllib2.py"", line 410, in open
        response = meth(req, response)
      File ""/usr/local/lib/python2.7/urllib2.py"", line 523, in http_response
        'http', request, response, code, msg, hdrs)
      File ""/usr/local/lib/python2.7/urllib2.py"", line 448, in error
        return self._call_chain(*args)
      File ""/usr/local/lib/python2.7/urllib2.py"", line 382, in _call_chain
        result = func(*args)
      File ""/usr/local/lib/python2.7/urllib2.py"", line 531, in http_error_default
        raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
    urllib2.HTTPError: HTTP Error 403: SSL is required
    Complete output from command python setup.py egg_info:
    Downloading http://pypi.python.org/packages/source/d/distribute/distribute-0.6.21.tar.gz

Traceback (most recent call last):

  File ""<string>"", line 17, in <module>

  File ""/tmp/pip_build_root/nltk/setup.py"", line 23, in <module>

    distribute_setup.use_setuptools()

  File ""distribute_setup.py"", line 145, in use_setuptools

    return _do_download(version, download_base, to_dir, download_delay)

  File ""distribute_setup.py"", line 124, in _do_download

    to_dir, download_delay)

  File ""distribute_setup.py"", line 193, in download_setuptools

    src = urlopen(url)

  File ""/usr/local/lib/python2.7/urllib2.py"", line 127, in urlopen

    return _opener.open(url, data, timeout)

  File ""/usr/local/lib/python2.7/urllib2.py"", line 410, in open

    response = meth(req, response)

  File ""/usr/local/lib/python2.7/urllib2.py"", line 523, in http_response

    'http', request, response, code, msg, hdrs)

  File ""/usr/local/lib/python2.7/urllib2.py"", line 448, in error

    return self._call_chain(*args)

  File ""/usr/local/lib/python2.7/urllib2.py"", line 382, in _call_chain

    result = func(*args)

  File ""/usr/local/lib/python2.7/urllib2.py"", line 531, in http_error_default

    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)

urllib2.HTTPError: HTTP Error 403: SSL is required

----------------------------------------
Cleaning up...
Command python setup.py egg_info failed with error code 1 in /tmp/pip_build_root/nltk
Storing debug log for failure in /root/.pip/pip.log
```"
350,https://github.com/nltk/nltk/issues/1880,1880,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-11-07 03:56:12+00:00,,4,nltk 2.0.4 get HTTP Error 403 when access pypi api,"https://mail.python.org/pipermail/distutils-sig/2017-October/031712.html
pypi dropped http access of api

https://github.com/nltk/nltk/blob/2.0.4/distribute_setup.py#L50
so there will be an error during install in 2.0.x
could we fix it in the 2.0.5+ version?"
351,https://github.com/nltk/nltk/issues/1883,1883,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}]",open,2017-11-13 15:34:20+00:00,,11,Tree.chomsky_normal_form() throws AttributeError,"More specifically, `AttributeError: 'str' object has no attribute 'label'`. This happens when the grammar doesn't have preterminals, e.g.
```Python
nltk.CFG.fromstring(""""""
S -> '1' | '2' | '3' | '4' | '5' | '6' | '7' | '8' | '9'
S -> S '+' S
S -> S '-' S
S -> S '*' S
S -> S '/' S
S -> '(' S ')'
"""""")
```"
352,https://github.com/nltk/nltk/issues/1884,1884,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}]",open,2017-11-13 15:35:45+00:00,,4,chomsky_normal_form() for grammars,"`nltk.tree.Tree` has a `chomsky_normal_form()` function, but grammars don't. Since CNF is a form of the **grammar**, it should, also."
353,https://github.com/nltk/nltk/issues/1885,1885,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}]",open,2017-11-13 15:38:15+00:00,,4,AlignedSent display broken,"If there is a source word that doesn't appear in the target sentence (or vice versa), `AlignedSent` cannot be displayed at all:
- `_repr_svg_` throws `TypeError: list indices must be integers or slices, not NoneType`
- `__str__` throws `TypeError: %d format: a number is required, not NoneType`"
354,https://github.com/nltk/nltk/issues/1886,1886,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}]",closed,2017-11-13 15:40:19+00:00,,5,__align and __align_all in IBMModel should be public,The `__align` and `__align_all` functions in `IBMModel`s should should be public. What use is an alignment model if it can only be run on the train set?
355,https://github.com/nltk/nltk/issues/1887,1887,"[{'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}, {'id': 719374994, 'node_id': 'MDU6TGFiZWw3MTkzNzQ5OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/GUI', 'name': 'GUI', 'color': 'f9b3d9', 'default': False, 'description': None}, {'id': 739423158, 'node_id': 'MDU6TGFiZWw3Mzk0MjMxNTg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tkinter', 'name': 'tkinter', 'color': '03c4a7', 'default': False, 'description': None}]",open,2017-11-14 11:44:42+00:00,,5,Tree drawing doesn't work on a headless server,"`nltk.tree.Tree` uses the Tcl (tkinter) backend to draw trees. Unfortunately, that doesn't seem to work on a headless server, which is the setup I use: run Jupyter on the server and access it via the browser from another machine.

If `python3-tk` is not installed, I get
```
ImportError: No module named '_tkinter', please install the python3-tk package
```

If it is, I get
```
TclError: no display name and no $DISPLAY environment variable
```

However, `dot` works in such a setup, so it would be preferable if it was used as the backend instead of tk. Also, it could be used to implement `_repr_svg_()` on `Tree`, which would also be a very welcome addition, as svg is a much better format for these kinds of things than png."
356,https://github.com/nltk/nltk/issues/1888,1888,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}]",closed,2017-11-14 17:12:07+00:00,,9,IBM Models mix up target and source,"The documentation IBM models (at least `IBMModel1`) mixes up the source and target languages:
1. The constructor accepts a list of `AlignedSent` objects, which state that `words: source language words ... mots: target language words`.
1. `IBMModel1` just eats it and generates the translation table, as well as generates `Alignment` objects for the input corpus
    - the docstring for `__init__` states that `Translation direction is from ""AlignedSent.mots"" to
        ""AlignedSent.words""`
    - however, `translation_table` is actually a double `defaultdict {words: {mots: probability}}`, so it is definitely `words -> mots`
1. Then comes `IBMModel1.prob_t_a_given_s()`, which actually IS from target to source, and tries to get the probabilities `self.translation_table[trg_word][src_word]`. Which, of course, fails completely because that is not how the table was built."
357,https://github.com/nltk/nltk/issues/1889,1889,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 718738467, 'node_id': 'MDU6TGFiZWw3MTg3Mzg0Njc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/windows%20related', 'name': 'windows related', 'color': 'fce4ba', 'default': False, 'description': None}]",closed,2017-11-16 08:50:01+00:00,,1,nltk.download() error,"Hi, When i am trying to download using nltk.download(), I am getting error: <urlopen error  [WinError 10054] An Existing connection was forcibly closed by remote host>

please suggest.

Thanks,
Saurabh "
358,https://github.com/nltk/nltk/issues/1890,1890,"[{'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}]",open,2017-11-17 10:30:54+00:00,,2,CFG.fromstring cannot handle epsilons,"There are times when one wants to include an empty terminal, such as:
```
S -> A B
A -> 'a'
B -> 'b' | ''
```

However, when trying to read it via `CFG.fromstring`, I get
```
ERROR:root:An unexpected error occurred while tokenizing input
```

Interestingly, `Tree` has no problems with empty terminals:
```Python
tree = Tree('S', [
    Tree('A', ['a']),
    Tree('B', [''])
])
tree.pos()
```
prints
```Python
[('a', 'A'), ('', 'B')]
```"
359,https://github.com/nltk/nltk/issues/1891,1891,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}]",open,2017-11-18 11:38:27+00:00,,0,prob_t_a_given_s() only computes half of what it should,"According to the Jurafsky & Martin book, but also the [original paper](http://www.aclweb.org/anthology/J93-2003) (eq. 5), _P(F,A|E)_ is not just the product of the word probabilities, but it should also be weighted by the alignment probability (_eps / (l + 1)^m_). This is missing from the implementation of `prob_t_a_given_s()`, which, as it is, actually just computes _P(F|A,E)_. So either
- the missing code should be added to it, or
- it should be renamed to `prob_t_given_a_s()`. "
360,https://github.com/nltk/nltk/issues/1892,1892,"[{'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}]",open,2017-11-20 00:30:51+00:00,,1,Deprecating and removing Stanford API support ,"Stanford CoreNLP has released their official Python wrapper https://github.com/stanfordnlp/python-stanford-corenlp 

Should we remove the support for the Stanford API in NLTK and replace it with `NotImplementedError`? "
361,https://github.com/nltk/nltk/issues/1893,1893,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-11-20 13:50:40+00:00,,3,"moses.tokenize(sent, escape), same result when escape =True and escape = False (French data)","#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import csv
import nltk

from nltk.tokenize.moses import MosesTokenizer

moses = MosesTokenizer(lang='fr')
sent = ""Le télétravail n'aura pas d'effet sur ma vie""
print (moses.tokenize(sent, False))

>>>> ['Le', 'télétravail', 'n&apos;', 'aura', 'pas', 'd&apos;', 'effet', 'sur', 'ma', 'vie']

print (moses.tokenize(sent, True))
>>>>>['Le', 'télétravail', 'n&apos;', 'aura', 'pas', 'd&apos;', 'effet', 'sur', 'ma', 'vie']

and I would like to have this result : 
['Le', 'télétravail', 'n' ', 'aura', 'pas', 'd'', 'effet', 'sur', 'ma', 'vie']

(python3)

"
362,https://github.com/nltk/nltk/issues/1895,1895,[],closed,2017-11-21 19:25:08+00:00,,0,AttributeError: 'module' object has no attribute 'gleu_score',"When I'm using BLEU score I'm not getting any error and it works but when I change it to GLEU, it says:
""AttributeError: 'module' object has no attribute 'gleu_score'"", any can give me a clue please?
"
363,https://github.com/nltk/nltk/issues/1897,1897,"[{'id': 718733436, 'node_id': 'MDU6TGFiZWw3MTg3MzM0MzY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/Off-topic', 'name': 'Off-topic', 'color': 'f7b4e1', 'default': False, 'description': None}]",closed,2017-11-23 05:28:33+00:00,,2,Object Score in Sentiment Score Calculation???,"I tried calculating the pos_score(),neg_score() and obj_score() for each word in a sentence.Now my question is to estimate the sentiment score of the sentence do we just consider pos and neg values of all values and based on the avg value we then conclude whether a sentence is positive or negative else should we consider the object value as well???
I couldn't find much resources online to conclude the answer.It would be of great help if someone can help me with this.
Thanks in advance
Cheers"
364,https://github.com/nltk/nltk/issues/1900,1900,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-11-27 09:08:48+00:00,,4,Why does ntlk.tokenize.wordpunct_tokenize not split on `_`,"I am using `nltk.tokenize.wordpunct_tokenize`, but do not see the string containg `_` being split up. 
Test Code
```
from nltk.tokenize import wordpunct_tokenize
wordpunct_tokenize(""Hello_World"")
```
Current Output: `['Hello_world']`
Expected_output: `['Hello', '_', 'World']`
Am I missing something?"
365,https://github.com/nltk/nltk/issues/1901,1901,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-11-27 10:07:17+00:00,,6,Differences between FrameNet in Python3.6 and Python2.7,"Framenet in Python3.6 and Python2.7 seems to behave differently:

From https://github.com/nltk/nltk/blob/develop/nltk/test/framenet.doctest#L119

**Python3.6**

```python
~/git-stuff/nltk$ python3
Python 3.6.1 (default, Apr  4 2017, 09:40:21) 
[GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.38)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from nltk.corpus import framenet as fn
>>> fn.frames_by_lemma(r'(?i)a little') 
[<frame ID=2001 name=Degree>, <frame ID=189 name=Quantified_mass>]
>>> exit()
```

**Python2.7**

```python
~/git-stuff/nltk$ python
Python 2.7.13 (default, Dec 18 2016, 07:03:39) 
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from nltk.corpus import framenet as fn
>>> fn.frames_by_lemma(r'(?i)a little')
[<frame ID=189 name=Quantified_mass>, <frame ID=2001 name=Degree>]
```

Is the order of the frames important?

@nschneid any hints on why this is happening?

----


Another instance is during the doctest at https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/framenet.py#L2250


**Python 3.6**

```
~/git-stuff/nltk$ python3
Python 3.6.1 (default, Apr  4 2017, 09:40:21) 
[GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.38)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from nltk.corpus import framenet as fn
>>> from nltk.corpus.reader.framenet import PrettyDict
>>> frts = list(fn.frame_relation_types())
>>> PrettyDict(frts[0], breakLines=True)
{'ID': 10,
 '_type': 'framerelationtype',
 'frameRelations': [<Causative=Cause_to_move_in_place -- Causative_of -> Inchoative/state=Moving_in_place>, <Causative=Apply_heat -- Causative_of -> Inchoative/state=Absorb_heat>, ...],
 'name': 'Causative_of',
 'subFrameName': 'Inchoative/state',
 'superFrameName': 'Causative'}
```

**Python 2.7**

```
~/git-stuff/nltk$ python2
Python 2.7.13 (default, Dec 18 2016, 07:03:39) 
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from nltk.corpus import framenet as fn
>>> from nltk.corpus.reader.framenet import PrettyDict
>>> frts = list(fn.frame_relation_types())
>>> PrettyDict(frts[0], breakLines=True)
{'ID': 1,
 u'_type': u'framerelationtype',
 u'frameRelations': [<Parent=Event -- Inheritance -> Child=Change_of_consistency>, <Parent=Event -- Inheritance -> Child=Rotting>, ...],
 'name': 'Inheritance',
 'subFrameName': 'Child',
 'superFrameName': 'Parent'}
>>> exit()
```

----

Another instance is at https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/framenet.py#L1914

**Python 3.6**

```
~/git-stuff/nltk$ python3
Python 3.6.1 (default, Apr  4 2017, 09:40:21) 
[GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.38)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from nltk.corpus import framenet as fn
>>> from nltk.corpus.reader.framenet import PrettyList
>>> PrettyList(fn.lus(r'(?i)a little'), maxReprSize=0, breakLines=True)
[<lu ID=14744 name=a little bit.adv>,
 <lu ID=14743 name=a little.adv>,
 <lu ID=14733 name=a little.n>]
```

**Python 2.7**

```
>>> from nltk.corpus import framenet as fn
>>> from nltk.corpus.reader.framenet import PrettyList
>>> PrettyList(fn.lus(r'(?i)a little'), maxReprSize=0, breakLines=True)
[<lu ID=14744 name=a little bit.adv>,
 <lu ID=14733 name=a little.n>,
 <lu ID=14743 name=a little.adv>]
```

Are orders in the frames and lists important?"
366,https://github.com/nltk/nltk/issues/1903,1903,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",open,2017-11-28 10:25:23+00:00,,0,Consider typographical quotes when splitting sentences,"When using various tools that build upon NLTK to split sentences, I keep running into the problem that texts with typographical quotes like chevrons (»«) or guillemets («») are not handled correctly.

E.g. with a text like:

`»Die militärische Welt ist doch manchmal recht unschön. Wie froh bin ich da, dass ich Dich habe, die Du so viel anders bist als die meisten Menschen.« Das hatte Robert Scholl am 1. Februar 1916 an Lina Müller geschrieben, die bei ihren Eltern in Künzelsau Urlaub machte.`

sentences two and three are not split: 

```
»Die militärische Welt ist doch manchmal recht unschön.
Wie froh bin ich da, dass ich Dich habe, die Du so viel anders bist als die meisten Menschen.« Das hatte Robert Scholl am 1. Februar 1916 an Lina Müller geschrieben, die bei ihren Eltern in Künzelsau Urlaub machte.
```

The maintainers of those tools always point back to NLTK as the culprit when I report this issue. (They all seem to use the ""Punkt"" tokenizer for this.)

So it would be great if the NLTK framework could address this issue.

(Tested with NLTK 3.2.5)"
367,https://github.com/nltk/nltk/issues/1905,1905,"[{'id': 718738467, 'node_id': 'MDU6TGFiZWw3MTg3Mzg0Njc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/windows%20related', 'name': 'windows related', 'color': 'fce4ba', 'default': False, 'description': None}]",closed,2017-12-02 23:03:50+00:00,,2,Cannot install on Windows,"Fresh install of Python 3.5.4 (32-bit), no other interpreter versions are installed on my computer, still I cannot install ntlk in any way that is listed on the official webpage. 

When I'm trying to use nltk-3.2.5.win32.exe I'm getting the error 'Python version -32 required, which was not found in the registry.' even though there is a 32-bit version that is listed in the PATH variable.
```
C:\Users\User>pip install ntlk
Collecting ntlk
  Could not find a version that satisfies the requirement ntlk (from versions: )
No matching distribution found for ntlk

C:\Users\User>python -m pip install ntlk
Collecting ntlk
  Could not find a version that satisfies the requirement ntlk (from versions: )
No matching distribution found for ntlk

C:\Users\User>py -m pip install ntlk
Collecting ntlk
  Could not find a version that satisfies the requirement ntlk (from versions: )
No matching distribution found for ntlk

C:\Users\User>python
Python 3.5.4 (v3.5.4:3f56838, Aug  8 2017, 02:07:06) [MSC v.1900 32 bit (Intel)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import struct
>>> print (struct.calcsize(""P"") * 8)
32
>>> import platform
>>> platform.architecture()
('32bit', 'WindowsPE')
>>>
```"
368,https://github.com/nltk/nltk/issues/1912,1912,"[{'id': 718733436, 'node_id': 'MDU6TGFiZWw3MTg3MzM0MzY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/Off-topic', 'name': 'Off-topic', 'color': 'f7b4e1', 'default': False, 'description': None}]",closed,2017-12-10 13:56:44+00:00,,1,"Isuess with ""No such file or directory: ""","I just started learning nltk and tried to followed the book.
 I don't know why but it keep showing No such file or directory: 
Could anyone explain to me why this happens and how to fix it?
Thank you very much!

```python
from nltk.corpus import BracketParseCorpusReader
corpus_root=r""C:\corpora\penntreebank\parsed\mrg\wsj""
file_pattern=r"".*/wsj_.*\.mrg""
ptb=BracketParseCorpusReader(corpus_root,file_pattern)
Traceback (most recent call last):
  File ""C:\Users\Angel\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-97-f0f78aef5a32>"", line 1, in <module>
    ptb=BracketParseCorpusReader(corpus_root,file_pattern)
  File ""C:\Users\Angel\Anaconda3\lib\site-packages\nltk\corpus\reader\bracket_parse.py"", line 49, in __init__
    CorpusReader.__init__(self, root, fileids, encoding)
  File ""C:\Users\Angel\Anaconda3\lib\site-packages\nltk\corpus\reader\api.py"", line 84, in __init__
    root = FileSystemPathPointer(root)
  File ""C:\Users\Angel\Anaconda3\lib\site-packages\nltk\compat.py"", line 221, in _decorator
    return init_func(*args, **kwargs)
  File ""C:\Users\Angel\Anaconda3\lib\site-packages\nltk\data.py"", line 303, in __init__
    raise IOError('No such file or directory: %r' % _path)
OSError: No such file or directory: 'C:\\corpora\\penntreebank\\parsed\\mrg\\wsj'
```

```python
from nltk.corpus import PlaintextCorpusReader
corpus_root='/usr/share/dict'
wordlists=PlaintextCorpusReader(corpus_root,'.*')
Traceback (most recent call last):
  File ""C:\Users\Angel\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-92-913eb73fb808>"", line 1, in <module>
    wordlists=PlaintextCorpusReader(corpus_root,'.*')
  File ""C:\Users\Angel\Anaconda3\lib\site-packages\nltk\corpus\reader\plaintext.py"", line 62, in __init__
    CorpusReader.__init__(self, root, fileids, encoding)
  File ""C:\Users\Angel\Anaconda3\lib\site-packages\nltk\corpus\reader\api.py"", line 84, in __init__
    root = FileSystemPathPointer(root)
  File ""C:\Users\Angel\Anaconda3\lib\site-packages\nltk\compat.py"", line 221, in _decorator
    return init_func(*args, **kwargs)
  File ""C:\Users\Angel\Anaconda3\lib\site-packages\nltk\data.py"", line 303, in __init__
    raise IOError('No such file or directory: %r' % _path)
OSError: No such file or directory: 'C:\\usr\\share\\dict'
```
"
369,https://github.com/nltk/nltk/issues/1913,1913,"[{'id': 789518993, 'node_id': 'MDU6TGFiZWw3ODk1MTg5OTM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/need-help', 'name': 'need-help', 'color': 'e0609e', 'default': False, 'description': None}]",open,2017-12-11 19:21:02+00:00,,1,"TypeError: sequence item 352: expected str instance, NoneType found","i am new in nltk and chunking.i dont know where i am getting this error and why.i tagged the corpus and then i was trying to do chunking in that but i am getting the type error. here is the code:
```
def load_corpus():
    corpus_root = os.path.abspath('../nlp1/dumpfiles')
    mycorpus = nltk.corpus.reader.TaggedCorpusReader(corpus_root,'.*')
    return mycorpus.tagged_sents()

def sents_chunks(tagg_sents, pos_tag_pattern):
    chunk_freq_dict = defaultdict(int)
    chunker = nltk.RegexpParser(pos_tag_pattern)
    for sent in tagg_sents:
        a =filter(bool, sent) 
        print(type(a))
        print(sent)
        for chk in chunker.parse(sent).subtrees():
            if str(chk).startswith('(NP'):
                phrase = chk.__unicode__()[4:-1]
                #print(phrase)
                if '\n' in phrase:
                    phrase = ' '.join(phrase.split())
                    #print(phrase)
                chunk_freq_dict[phrase] += 1
    #print(chunk_freq_dict)
    return chunk_freq_dict
```"
370,https://github.com/nltk/nltk/issues/1915,1915,"[{'id': 159235791, 'node_id': 'MDU6TGFiZWwxNTkyMzU3OTE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/dependency%20parsing', 'name': 'dependency parsing', 'color': 'eb6420', 'default': False, 'description': None}, {'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}]",closed,2017-12-13 19:10:49+00:00,,0,eval() in DependencyEvaluator returns LAS and UAS in wrong order,"According to documentation, return order is Labeled Attachment Score (LAS) and then Unlabeled Attachment Score (UAS). 
UAS is computed based on correct number of HEAD fields only, while LAS is based both on HEAD and DEPREL fields.
Actual code returns UAS as the first item and LAS as the second:
https://github.com/nltk/nltk/blob/8490d9d8ef58482d4c3d17826b4b5224e421714e/nltk/parse/evaluate.py#L121-L127"
371,https://github.com/nltk/nltk/issues/1917,1917,"[{'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}]",open,2017-12-16 07:25:09+00:00,,5,How to modify requests module timeout when using corenlp?,"I want to parse long text using corenlp so the request tooks longer than the default timeout which is 60 seconds. Is it possible to change it?

![image](https://user-images.githubusercontent.com/12383137/34068405-d42d0a40-e274-11e7-9519-679967a1e086.png)
"
372,https://github.com/nltk/nltk/issues/1920,1920,"[{'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}]",open,2017-12-19 07:05:02+00:00,,3,BllipParser: AttributeError: 'str' object has no attribute 'decode' on Python 3.5,"When calling next(BllipParser.parse()) on Python 3.5, I receive the following error:

File ""/home/andrew/.local/lib/python3.5/site-packages/nltk/parse/bllip.py"", line 169, in parse
    _ensure_ascii(sentence)
  File ""/home/andrew/.local/lib/python3.5/site-packages/nltk/parse/bllip.py"", line 101, in _ensure_ascii
    word.decode('ascii')
AttributeError: 'str' object has no attribute 'decode'


Related to https://github.com/nltk/nltk/issues/507

NLTK v3.2.5
Python v3.5.2
OS: Ubuntu (16.04.3) running on WSL (16299.125)"
373,https://github.com/nltk/nltk/issues/1921,1921,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 718746024, 'node_id': 'MDU6TGFiZWw3MTg3NDYwMjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stem/lemma', 'name': 'stem/lemma', 'color': '53bab8', 'default': False, 'description': None}]",open,2017-12-20 11:05:39+00:00,,0,Improving Snowball Arabic stemmer in Nltk,"I've opened this issue to add improvements of snowball ArabicStemmer in [nltk snowball ArabicStemmer](https://github.com/nltk/nltk/blob/e0b882177421498a1a49e42d4b83c51bfa451f3d/nltk/stem/snowball.py#L297) from the [original algorithm](https://github.com/assem-ch/arabicstemmer) 

"
374,https://github.com/nltk/nltk/issues/1923,1923,"[{'id': 718738467, 'node_id': 'MDU6TGFiZWw3MTg3Mzg0Njc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/windows%20related', 'name': 'windows related', 'color': 'fce4ba', 'default': False, 'description': None}, {'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}, {'id': 789518993, 'node_id': 'MDU6TGFiZWw3ODk1MTg5OTM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/need-help', 'name': 'need-help', 'color': 'e0609e', 'default': False, 'description': None}]",closed,2017-12-24 01:10:23+00:00,,12,Windows installation problem,"I have been stuck at the installation stage. Could anybody help please?
The error message I get is ""Python version -32 required which was not found in the registry."" 

When I run the register.py script, I get a ""file not found"" error. No clue which file it is looking for.
And besides, I wonder what NLTK is looking for in the registry?  

My environment is Win7, Python 3.5.0, and NLTK 3.2.5. Installpath and Python path in in the ""environment variables"" and Python can be directly invoked at any directory. 
"
375,https://github.com/nltk/nltk/issues/1926,1926,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2017-12-27 07:35:07+00:00,,3,nist.py regexp breaks in py27,"Our CI server [reports](https://nltk.ci.cloudbees.com/job/nltk/lastCompletedBuild/TOXENV=py27-jenkins,jdk=jdk8latestOnlineInstall/testReport/nose.failure/Failure/runTest/) error `nothing to repeat` on line https://github.com/nltk/nltk/blob/develop/nltk/tokenize/nist.py#L110

@alvations: would you mind having a look at this please?"
376,https://github.com/nltk/nltk/issues/1928,1928,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}]",closed,2018-01-03 14:13:53+00:00,,11,Unclosed file in stopwords corpora,"/Users/kiddo/anaconda/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py:28: ResourceWarning: unclosed file <_io.BufferedReader name='/Users/kiddo/nltk_data/corpora/stopwords/english'>
  return concat([self.open(f).read() for f in fileids])

That's a warning that I found on debugging mode. I thought that maybe you would like to fix that before the next release."
377,https://github.com/nltk/nltk/issues/1929,1929,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}, {'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}]",open,2018-01-05 01:49:41+00:00,,1,Adding popular and book nltk_data during installation process,"It is possible to enforce `nltk_data` downloads inside the `setup.py`. And maybe that'll remove some gotchas when people use functions that requires default models the first time and they would need to do `nltk.download(...)` (e.g. `sent_tokenize`, `word_tokenize`, `pos_tag`). 

Some useful hints to implement this:

 - https://stackoverflow.com/questions/26799894/installing-nltk-data-in-setup-py-script
 - https://stackoverflow.com/questions/21915469/python-setuptools-install-requires-is-ignored-when-overriding-cmdclass
 - https://stackoverflow.com/questions/20194565/running-custom-setuptools-build-during-install
 - https://blog.niteoweb.com/setuptools-run-custom-code-in-setup-py/

"
378,https://github.com/nltk/nltk/issues/1931,1931,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 718746024, 'node_id': 'MDU6TGFiZWw3MTg3NDYwMjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stem/lemma', 'name': 'stem/lemma', 'color': '53bab8', 'default': False, 'description': None}]",open,2018-01-08 03:54:28+00:00,,0,An interface to the 1M sense tagged inventory,"There's a nice corpus of 1M WordNet tagged instances of MultiUN corpus produced in http://www.comp.nus.edu.sg/~kaveh/papers/wsd-conll15.pdf 

If anyone is up for a challenge, It'll be a nice addition to have a reader for this corpus in NLTK.

The data source is on http://www.comp.nus.edu.sg/~nlp/corpora.html"
379,https://github.com/nltk/nltk/issues/1932,1932,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-01-08 15:17:57+00:00,,2,Jenkins is breaking because Stanford and Senna pages are timing out,"It looks like Jenkin is breaking because old Stanford tools pages are timing out https://nltk.ci.cloudbees.com/job/pull_request_tests/564/TOXENV=py34-jenkins,jdk=jdk8latestOnlineInstall/console 

[`Jenkins.sh`](https://github.com/nltk/nltk/blob/develop/jenkins.sh#L34) is trying to curl the timeout page to get the direct link to the tools. 

We're having trouble doing:

```
curl -s 'https://nlp.stanford.edu/software/lex-parser.shtml'
```

The direct links to the Stanford tools still works, but it's extremely slow and it still gets timeout:

 - https://nlp.stanford.edu/software/stanford-parser-full-2017-06-09.zip
 - https://nlp.stanford.edu/software/stanford-postagger-full-2017-06-09.zip


-----

Similarly the [old senna link](https://github.com/nltk/nltk/blob/develop/jenkins.sh#L55) is timing out, we should use this instead:

 - https://ronan.collobert.com/senna/senna-v3.0.tgz
  "
380,https://github.com/nltk/nltk/issues/1934,1934,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",closed,2018-01-10 03:47:54+00:00,,4,Function to access the Synset object using sense key,"I might have missed it but is there a function to access the `Synset` object from the NLTK wordnet interface from the [sense key](https://wordnet.princeton.edu/wordnet/man/senseidx.5WN.html)?

If there isn't could we expose a function that can achieve that in `nltk.corpus.wordnet`? E.g. https://stackoverflow.com/questions/48170666/how-to-get-the-gloss-given-sense-key-using-nltk-wordnet/ 

Ideally, it would be good to have functionalities to access `Synset` objects using:

 1. `offset-pos`, e.g. `1433493-a` -> `Synset('long.a.02')`
 2. `sense_key`, e.g. `long%3:00:02::` ->  `Synset('long.a.02')`

Currently, we have the `synset_from_pos_and_offset()` for (1). 

There's another function (`_synset_from_pos_and_line `) that reads the following line to return `Synset('long.a.02')`:

```
01433493 00 a 01 long 1 016 = 05129201 n 0000 + 05133287 n 0101 ! 01436003 a 0101 & 01434007 a 0000 & 01434218 a 0000 & 01434530 a 0000 & 01434717 a 0000 & 01434841 a 0000 & 01434966 a 0000 & 01435060 a 0000 & 01435189 a 0000 & 01435290 a 0000 & 01435399 a 0000 & 01435507 a 0000 & 01435675 a 0000 & 01435891 a 0000 | primarily spatial sense; of relatively great or greater than average spatial extension or extension as specified; ""a long road""; ""a long distance""; ""contained many long words""; ""ten miles long"" 
``` 

but it's not the sense key."
381,https://github.com/nltk/nltk/issues/1939,1939,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}]",open,2018-01-16 15:33:26+00:00,,1,Translation Probabilities do not sum to 1 in IBM1 Translation Model,"After running IBM1 Translation model in the given code, I've summed the translation probabilites (the probability for a target word given source word), and it doesn't sum to 1:

```python
from nltk.translate import IBMModel1, AlignedSent
import numpy as np

src_sentences = [['a', '.'], ['b', 'b', '.']]
trg_sentences = [['X', '.'], ['Z', '.']]

bitext = [AlignedSent(t, s) for s,t in zip(src_sentences, trg_sentences)]
ibm1 = IBMModel1(bitext, 1)

sum([ibm1.translation_table[t]['b'] for t in ibm1.translation_table])
1.3333333333333333
```

I guess it is because here:
https://github.com/nltk/nltk/blob/7bbc00e908087b60325db30526872306e767de13/nltk/translate/ibm_model.py#L336-L340
The iteration only on the counts, and not all the vocabulary. So (src_word, trg_word) pairs that don't appear in the same sentence pairs, won't get updated and will stay with the initial uniform probabilities"
382,https://github.com/nltk/nltk/issues/1941,1941,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}]",open,2018-01-18 14:56:12+00:00,,3,WordNet unclosed files,"When using wordnet, Python complains about unclosed files, i.e.:
```
venv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py:1107: ResourceWarning: unclosed file <_io.BufferedReader name='venv/nltk_data/corpora/wordnet/lexnames'>
  for i, line in enumerate(self.open('lexnames')):
venv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py:1159: ResourceWarning: unclosed file <_io.BufferedReader name='venv/nltk_data/corpora/wordnet/index.adj'>
  for i, line in enumerate(self.open('index.%s' % suffix)):
venv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py:1159: ResourceWarning: unclosed file <_io.BufferedReader name='venv/nltk_data/corpora/wordnet/index.adv'>
  for i, line in enumerate(self.open('index.%s' % suffix)):
venv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py:1159: ResourceWarning: unclosed file <_io.BufferedReader name='venv/nltk_data/corpora/wordnet/index.noun'>
  for i, line in enumerate(self.open('index.%s' % suffix)):
venv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py:1159: ResourceWarning: unclosed file <_io.BufferedReader name='venv/nltk_data/corpora/wordnet/index.verb'>
  for i, line in enumerate(self.open('index.%s' % suffix)):
venv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py:1209: ResourceWarning: unclosed file <_io.BufferedReader name='venv/nltk_data/corpora/wordnet/adj.exc'>
  for line in self.open('%s.exc' % suffix):
venv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py:1209: ResourceWarning: unclosed file <_io.BufferedReader name='venv/nltk_data/corpora/wordnet/adv.exc'>
  for line in self.open('%s.exc' % suffix):
venv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py:1209: ResourceWarning: unclosed file <_io.BufferedReader name='venv/nltk_data/corpora/wordnet/noun.exc'>
  for line in self.open('%s.exc' % suffix):
venv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py:1209: ResourceWarning: unclosed file <_io.BufferedReader name='venv/nltk_data/corpora/wordnet/verb.exc'>
  for line in self.open('%s.exc' % suffix):
```
"
383,https://github.com/nltk/nltk/issues/1943,1943,[],closed,2018-01-20 21:43:02+00:00,,5,"ValueError: not enough values to unpack (expected 2, got 1)","```
import nltk

train_sents = [('', ""The/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd place/nn ./."")]

unigram_tagger = nltk.UnigramTagger(train_sents)

```
While running this code, got the following error.

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\MachineSense\Anaconda3\lib\site-packages\nltk\tag\sequential.py"", line 345, in __init__
    backoff, cutoff, verbose)
  File ""C:\Users\MachineSense\Anaconda3\lib\site-packages\nltk\tag\sequential.py"", line 292, in __init__
    self._train(train, cutoff, verbose)
  File ""C:\Users\MachineSense\Anaconda3\lib\site-packages\nltk\tag\sequential.py"", line 181, in _train
    tokens, tags = zip(*sentence)
ValueError: not enough values to unpack (expected 2, got 1)
```

Using Windows 10, Python 3.6.2"
384,https://github.com/nltk/nltk/issues/1944,1944,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",open,2018-01-25 10:51:39+00:00,,4,Wordnet: Comparison of  a _WordNetObject fails if other object is not a _WordNetObject.,"Problem: As the title says, the comparison of a _WordNetObject fails if compared to something that doesn't have the attribute `.name`. 

Solution: Catch `AttributeError` and return `False`

See output log:
```bash 
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-217-bacea08cca5b> in <module>()
    107 d = 0
    108 for a,b,c in cleaned_entities:
--> 109     if a == 'INVALID':
    110         d += 1
    111 #         for pos, synset in syn_e1:

.../nltk/corpus/reader/wordnet.py in __eq__(self, other)
    196 
    197     def __eq__(self, other):
--> 198         return self._name == other._name
    199 
    200     def __ne__(self, other):

AttributeError: 'str' object has no attribute '_name'
```"
385,https://github.com/nltk/nltk/issues/1946,1946,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718746024, 'node_id': 'MDU6TGFiZWw3MTg3NDYwMjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stem/lemma', 'name': 'stem/lemma', 'color': '53bab8', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-01-25 17:10:01+00:00,,2,present is spelled wrong - causes issue at least on line 535,https://github.com/nltk/nltk/blob/2d925b8816842630f40dc2ca562131ef1161bfc2/nltk/stem/snowball.py#L406
386,https://github.com/nltk/nltk/issues/1949,1949,"[{'id': 719817004, 'node_id': 'MDU6TGFiZWw3MTk4MTcwMDQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/multithread%20/%20multiprocessing', 'name': 'multithread / multiprocessing', 'color': 'bdf486', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-01-30 21:31:38+00:00,,0,Weird conflict with nltk and google storage api under ProcessPool,"Hello,

I experienced some weird error when using `nltk` and `google.cloud.storage` under `ProcessPool`. 

After `import nltk`, I tried to extract data from google storage from the main process, it works fine. See `first_main_then_pool()` below.

However, when I extract data from a process pool executor directly after `import nltk`, See `only_pool()`, I get the following error:

```
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/yipjustin/tmp/a.py"", line 29, in <module>
    only_pool()
  File ""/Users/yipjustin/tmp/a.py"", line 25, in only_pool
    print(executor.submit(get, 'b.txt').result())
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/concurrent/futures/_base.py"", line 405, in result
    return self.__get_result()
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/concurrent/futures/_base.py"", line 357, in __get_result
    raise self._exception
concurrent.futures.process.BrokenProcessPool: A process in the process pool was terminated abruptly while the future was running or pending.
```

I am curious to see if anyone has experienced similar issue. Thanks!


Code:

```
import concurrent.futures
from google.cloud import storage

def get(path):
	bucket_name = 'yipjustin-a'
	client = storage.Client()
	bucket = client.lookup_bucket(bucket_name)
	blob = bucket.get_blob(path)
	return blob.download_as_string()

def first_main_then_pool():
	# GOOD
	import nltk
	print(get('a.txt'))
	with concurrent.futures.ProcessPoolExecutor() as executor:
		print(executor.submit(get, 'b.txt').result())

def only_pool():
	# BAD
	import nltk
	with concurrent.futures.ProcessPoolExecutor() as executor:
		print(executor.submit(get, 'b.txt').result())
```

"
387,https://github.com/nltk/nltk/issues/1952,1952,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 719395503, 'node_id': 'MDU6TGFiZWw3MTkzOTU1MDM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/twitter', 'name': 'twitter', 'color': '99ecf7', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-02-09 21:48:43+00:00,,1,Tweet Tokenizer - preserve_case does not lowercase always ,When the preserve_case argument of TweetTokenizer is set to True we should expect that it lowercases examples like 'Ok' or 'A' but it doesn't. 
388,https://github.com/nltk/nltk/issues/1953,1953,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 719395503, 'node_id': 'MDU6TGFiZWw3MTkzOTU1MDM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/twitter', 'name': 'twitter', 'color': '99ecf7', 'default': False, 'description': None}]",open,2018-02-10 14:35:28+00:00,,1,tweet tokenizer has US-specific phone number regex,"The [phone number regex](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/casual.py#L125) is ONLY for united states phone numbers. Standard international numbers are not recognised at all.

Further, this regex is (causing problems with long numbers in general)[https://github.com/nltk/nltk/issues/1799].

Creating a regex capable of recognising ALL phone numbers (including other countries and other international prefixes) will likely be problematic, as it will likely match other numbers not intended as phone numbers. 

An immediate band-aid remedy would be to add an option to not process phone numbers and a caveat that it is only for US numbers. 

A more serious attempt could be to (optionally!) use [libphonenumber](https://github.com/googlei18n/libphonenumber) or something similar. Having a relatively ad-hoc regex specific to just one country seems a bit inappropriate here (only [around 1/5 of Twitter users are in the US](https://www.statista.com/statistics/274564/monthly-active-twitter-users-in-the-united-states/))."
389,https://github.com/nltk/nltk/issues/1954,1954,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 719395503, 'node_id': 'MDU6TGFiZWw3MTkzOTU1MDM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/twitter', 'name': 'twitter', 'color': '99ecf7', 'default': False, 'description': None}]",open,2018-02-12 11:48:00+00:00,,2,Casual tokeniser allows newline in elipses and phone number tokens,"This may be intended behaviour, but if so, it would be good to point it out in the docs as it is ONLY possible in these two cases and it is a common assumption that tokens do not contain newline characters.

It may be appropriate to disallow newlines in these tokens. This can be done by, [for example](https://stackoverflow.com/a/3469155/420867), using `[^\S\r\n]` in place of `\s` in the relevant regexes.

It is due to `\s` appearing in the regexes for [elipses](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/casual.py#L168) and [phone numbers](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/casual.py#L130)."
390,https://github.com/nltk/nltk/issues/1955,1955,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-02-14 19:21:38+00:00,,4,Misspelled function parameter for MosesTokenizer: agressive_dash_splits,"Just a minor issue. The `agressive_dash_splits` is misspelled. It should be `aggressive_dash_splits`. Or maybe use `hyphen` instead of `dash` to be consistent with both the `AGGRESSIVE_HYPHEN_SPLIT` class member and with `tokenizer.perl`.

http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.moses.MosesTokenizer.tokenize

Also this functionality does not appear to be tested."
391,https://github.com/nltk/nltk/issues/1959,1959,"[{'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-02-17 03:21:54+00:00,,4,NLTK installation error,"I am trying to install nltk on my work computer. I have admin access, but I keep getting the following error. Clearly I am doing something wrong or there is a permission issue I need to resolve, but I am not sure what. I would be grateful for any tips/advice.
<img width=""698"" alt=""nltk_error"" src=""https://user-images.githubusercontent.com/26263982/36337597-02c3c7a0-1357-11e8-962f-d0fc3d8e40f4.png"">



"
392,https://github.com/nltk/nltk/issues/1961,1961,"[{'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-02-18 02:12:53+00:00,,16,AttributeError: module 'nltk' has no attribute 'download,"```
#code 

import nltk
nltk.download()
```


```
Traceback (most recent call last):
  File ""nltk.py"", line 2, in <module>
    import nltk
  File ""/Users/antonionogueras/Desktop/NLTK/nltk.py"", line 10, in <module>
    nltk.download()
AttributeError: module 'nltk' has no attribute 'download'
```

Specs:
Mac, High Sierra, Python 3.6"
393,https://github.com/nltk/nltk/issues/1962,1962,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",open,2018-02-20 00:52:02+00:00,,5,Better CONLLCorpusReader,"Alexis has an improved version of the CONLL corpus reader on https://stackoverflow.com/a/46875652/610569

It'll be good if we incorporate these changes by either:

 - retaining the name `BetterConllCorpusReader` and inherit from `ConllCorpusReader`
 - refactoring the `ConllCorpusReader` and remove the hard-coded columns and a more flexible `self._get_iob_words`"
394,https://github.com/nltk/nltk/issues/1963,1963,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",open,2018-02-20 15:57:21+00:00,,8,"word_tokenize not tokenizing strings containing ','","I tried
```
from nltk.tokenize import word_tokenize
a=""g, a, b, c, 123, g32,12 123121 {1}""
word_tokenize(a)
```
**Output I am getting:** 
['g', ',', 'a', ',', 'b', ',', 'c', ',', '123', ',', 'g32,12', '123121', '{', '1', '}']
**Output should be**
['g', ',', 'a', ',', 'b', ',', 'c', ',', '123', ',', 'g32',',','12', '123121', '{', '1', '}']"
395,https://github.com/nltk/nltk/issues/1967,1967,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}, {'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}]",open,2018-02-27 02:07:48+00:00,,0,Different results using tag vs. tag_sents,"I was having an issue tagging short sentences and I tracked the issue to this line: https://github.com/nltk/nltk/blob/cdaa7dd4e60251390a82e42456edb359191ea6e8/nltk/tag/stanford.py#L190

The full explanation can be found [here](https://stackoverflow.com/questions/48998936/nltk-tag-tag-sents-give-different-results).  I think that the executable is not recognizing this argument (eg. swap false for true) when run inside of subprocess.  It can be fixed it by removing the `\""`, however I think what we actually want is for this argument to be `true`.  The reason is that if it's `false`, the NER tagger is treating everything as a single line, which means that you will get inconsistent results from tagging single sentences (using `tag(...)`) vs. batch tagging (using `tag_sents(...)`).

If you still think it should be `false`, can I recommend to add a parameter to make this configurable?  I am happy to make this update and commit."
396,https://github.com/nltk/nltk/issues/1968,1968,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",open,2018-02-27 17:54:31+00:00,,8,Incorrect word_tokenize for Dutch,"I have observed the following incorrect word_tokenize for Dutch. The characters `'` and `’` are very often used unbalanced to represent an apostrophe that is part of correct word. It will chopped up words when tokenizing on it. The in the code below `tmp` is the correct tokenization.

    from nltk import word_tokenize
    from pprint import pprint
    
    sentence = ""Test alinea's alinea’s ‘smaak’ 's-Hertogenbosch ’s-Hertogenbosch test.""
    print(""sentence:"", sentence)
    pprint(word_tokenize(sentence, language=""dutch""))
    tmp = """"
    opened = False
    for char in sentence:
    	if char == ""'"":
    		tmp += ""α""
    	elif char == ""‘"":
    		opened = True
    		tmp += char
    	elif char == ""’"":
    		if opened:
    			opened = False
    			tmp += char
    		else:
    			tmp += ""β""
    	else:
    		tmp += char
    print()
    print(""tmp:"", tmp)
    result = []
    for word in word_tokenize(tmp, language=""dutch""):
    	result.append(word.replace(""α"", ""'"").replace(""β"", ""’""))
    pprint(result)

with output:

    sentence: Test alinea's alinea’s ‘smaak’ 's-Hertogenbosch ’s-Hertogenbosch test.
    ['Test',
     'alinea',
     ""'s"",
     'alinea',
     '’',
     's',
     '‘',
     'smaak',
     '’',
     ""'s-Hertogenbosch"",
     '’',
     's-Hertogenbosch',
     'test',
     '.']
    
    tmp: Test alineaαs alineaβs ‘smaak’ αs-Hertogenbosch βs-Hertogenbosch test.
    ['Test',
     ""alinea's"",
     'alinea’s',
     '‘',
     'smaak',
     '’',
     ""'s-Hertogenbosch"",
     '’s-Hertogenbosch',
     'test',
     '.']"
397,https://github.com/nltk/nltk/issues/1969,1969,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-02-27 20:32:25+00:00,,3,Improve word tokenizer on balanced opening double quotes,"The following example:

    #!/usr/bin/env python3
    import nltk
    text = 'He said: ""No!"" He said: ""No!"" and walked away.\nHe said: “No!” and walked away.\n\nHe asked: ""Really?"" He asked: ""Really?"" and walked away.\nHe asked: „Really?” and walked away. He asked: „Really?” and walked away.'
    paragraphs = nltk.LineTokenizer().tokenize(text)
    for paragraph in paragraphs:
        sentences = nltk.tokenize.sent_tokenize(paragraph)
        for sentence in sentences:
            for word in nltk.tokenize.word_tokenize(sentence):
                print(word)

has output:

    He
    said
    :
    ``
    No
    !
    ''
    He
    said
    :
    ``
    No
    !
    ''
    and
    walked
    away
    .
    He
    said
    :
    “
    No
    !
    ”
    and
    walked
    away
    .
    He
    asked
    :
    ``
    Really
    ?
    ''
    He
    asked
    :
    ``
    Really
    ?
    ''
    and
    walked
    away
    .
    He
    asked
    :
    „Really
    ?
    ”
    and
    walked
    away
    .
    He
    asked
    :
    „Really
    ?
    ”
    and
    walked
    away
    .

should split two times `„Really` into `„` and `Really`.

(The current version of nltk has already fixed the issue with “No from https://github.com/nltk/nltk/issues/494 )"
398,https://github.com/nltk/nltk/issues/1970,1970,"[{'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",open,2018-03-02 09:49:26+00:00,,0,WordNet: Synsets' relations are absent from their corresponding lemmas,"In WordNet, it seems to me that the relations relevant for a given synset should be relevant for its corresponding lemmas as well. In the `nltk` implementation, all semantic relations implemented as methods for synsets (e.g., `hypernyms()`) are indeed also available for lemmas. Though for example, the lemma `trick.n.01.trick` `hypernyms` method does not return anything, while its synset's does.

Using the lemma method:
```python
>>> trick_lemma = wnt.get_synset('trick.n.01').lemmas()[0]
>>> print(trick_lemma.hypernyms())
[]
```
Using the the synset method instead:
```python
>>> print(trick_lemma.synset().hypernyms())
[Synset('device.n.03')]
>>> print(trick_lemma.synset().hypernyms()[0].lemmas())
[Lemma('device.n.03.device'), Lemma('device.n.03.gimmick'), Lemma('device.n.03.twist')]
```

Expected behavior of the lemma method:
```python
>>> print(trick_lemma.hypernyms())
[Lemma('device.n.03.device'), Lemma('device.n.03.gimmick'), Lemma('device.n.03.twist')]
```

Versions:
```
nltk 3.2.5
Python 3.6.4
Ubuntu 17.10
```"
399,https://github.com/nltk/nltk/issues/1971,1971,"[{'id': 718746024, 'node_id': 'MDU6TGFiZWw3MTg3NDYwMjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stem/lemma', 'name': 'stem/lemma', 'color': '53bab8', 'default': False, 'description': None}]",open,2018-03-07 07:27:42+00:00,,3,RecursionError in PorterStemmer._is_consonant,"RecursionError occurs in [`PorterStemmer._is_consonant`](https://github.com/nltk/nltk/blob/5a63a62cfc2bdd9d35d863d1c583942e3e6baf6b/nltk/stem/porter.py#L126) when trying to stem a word containing 'yyyyyyy....'.
If the length of 'yyyyyyy....' is larger than the maximum recursion depth, PorterStemmer cannot stem the word.

```python
>> from nltk.stem import PorterStemmer
>>> stemmer = PorterStemmer()
>>> stemmer.stem('y'*10000)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ubuntu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/nltk/stem/porter.py"", line 667, in stem
    stem = self._step1c(stem)
  File ""/home/ubuntu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/nltk/stem/porter.py"", line 421, in _step1c
    else original_condition
  File ""/home/ubuntu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/nltk/stem/porter.py"", line 268, in _apply_rule_list
    if condition is None or condition(stem):
  File ""/home/ubuntu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/nltk/stem/porter.py"", line 411, in nltk_condition
    return len(stem) > 1 and self._is_consonant(stem, len(stem) - 1)
  File ""/home/ubuntu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/nltk/stem/porter.py"", line 144, in _is_consonant
    return (not self._is_consonant(word, i - 1))
  File ""/home/ubuntu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/nltk/stem/porter.py"", line 144, in _is_consonant
    return (not self._is_consonant(word, i - 1))
  File ""/home/ubuntu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/nltk/stem/porter.py"", line 144, in _is_consonant
    return (not self._is_consonant(word, i - 1))
  [Previous line repeated 990 more times]
  File ""/home/ubuntu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/nltk/stem/porter.py"", line 140, in _is_consonant
    if word[i] == 'y':
RecursionError: maximum recursion depth exceeded in comparison
>>>
```
"
400,https://github.com/nltk/nltk/issues/1972,1972,"[{'id': 25396640, 'node_id': 'MDU6TGFiZWwyNTM5NjY0MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/book', 'name': 'book', 'color': 'e102d8', 'default': False, 'description': None}]",closed,2018-03-08 00:11:48+00:00,,2,Trivial typos in book,"All these mistakes are in the book located at the URL: http://www.nltk.org/book/ and the pages under it.

Typo in Preface:
In ch00.html, ""the remaining time of time to"" should be ""the remaining amount of time""

   Chapter 3:
replacet

   Chapter 4:
""ph.d..n.01"" should be ""ph.d.n.01"", because paragraph says it has four periods, not five.

Strange end of paragraph: "".. doctest-ignore:"" 



Chapter 5:  
""a feature of Brown tabs"", shouldn't ""tabs"" be ""tags"".
"", basestring), "" extra trailing comma in code example in chapter 4? I'm not sure if this is a mistake, I might just not understand the comma at the end.

    Section 2.8:
""use"" should be ""to use"" in ""probably more instructive use the tagged_words"". (2.8 , in http://www.nltk.org/book/ch05.html)

    Section 4.1:
Shouldn't they escape the dot in the regular expression ""(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers"" The regular expression seems to general, it will match 3d3, 3Z3 not just the usual 3.3.

In ""(Note that 1 describes a way partially automate such work.)""  ""partially automate"" should be ""to partially automate"".
"
401,https://github.com/nltk/nltk/issues/1973,1973,"[{'id': 25396640, 'node_id': 'MDU6TGFiZWwyNTM5NjY0MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/book', 'name': 'book', 'color': 'e102d8', 'default': False, 'description': None}]",closed,2018-03-08 00:12:55+00:00,,3,missing bibliography info,"Missing bibliography:
http://www.nltk.org/book/bibliography.html
The page is empty except for a heading saying ""Bibliography"".
"
402,https://github.com/nltk/nltk/issues/1975,1975,[],closed,2018-03-10 10:42:31+00:00,,0,missing argument in sentence_bleu smoothing methods,"I do not understand what is going on. So when I try:

    ref = [['a', 'b', 'd', 'e', 'f']]
    c = ['a', 'b', 'c', 'd']
    nltk.translate.bleu_score.sentence_bleu(ref, c,
                                        smoothing_function=nltk.translate.bleu_score.SmoothingFunction.method3)

I get:
```
Traceback (most recent call last):
  File ""D:\Anaconda3\envs\ktg\lib\site-packages\IPython\core\interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-17-be2015bb2bd7>"", line 4, in <module>
    smoothing_function=nltk.translate.bleu_score.SmoothingFunction.method3)
  File ""D:\Anaconda3\envs\ktg\lib\site-packages\nltk\translate\bleu_score.py"", line 89, in sentence_bleu
    emulate_multibleu)
  File ""D:\Anaconda3\envs\ktg\lib\site-packages\nltk\translate\bleu_score.py"", line 199, in corpus_bleu
    hyp_len=hyp_len, emulate_multibleu=emulate_multibleu)
TypeError: method3() missing 1 required positional argument: 'p_n'
```
that line 199 actually looks like:
```
    p_n = smoothing_function(p_n, references=references, hypothesis=hypothesis,
                             hyp_len=hyp_len, emulate_multibleu=emulate_multibleu)
```"
403,https://github.com/nltk/nltk/issues/1977,1977,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-03-14 01:40:44+00:00,,1,word_tokenize doesn't treat backtick as a starting quote,"`word_tokenize` fails to treat backtick as a starting quote, resulting in the ` being attached to the following word, e.g. 

```
>>> from nltk import word_tokenize
>>> text = ""(usually followed by `to') having the necessary means or skill or know-how or authority to do something""
>>> word_tokenize(text)
['(', 'usually', 'followed', 'by', '`to', ""'"", ')', 'having', 'the', 'necessary', 'means', 'or', 'skill', 'or', 'know-how', 'or', 'authority', 'to', 'do', 'something']
```

Note: the token ""**`to**"" token."
404,https://github.com/nltk/nltk/issues/1978,1978,"[{'id': 718746024, 'node_id': 'MDU6TGFiZWw3MTg3NDYwMjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stem/lemma', 'name': 'stem/lemma', 'color': '53bab8', 'default': False, 'description': None}]",open,2018-03-14 02:31:21+00:00,,1,Consistent pos argument between wn.synsets() and WordNetLemmatizer.lemmatize(),"Currently, there's some inconsistency of how POS is treated in `wn.synsets()` and `WordNetLemmatizer.lemmatize()`, e.g. 

```python
>>> from nltk.corpus import wordnet as wn
>>> from nltk.stem import WordNetLemmatizer
>>> wnl = WordNetLemmatizer()

# Accepts None and let pos be underspecified.
>>> wn.synsets('running', pos=None)
[Synset('run.n.05'), Synset('run.n.07'), Synset('running.n.03'), Synset('running.n.04'), Synset('track.n.11'), Synset('run.v.01'), Synset('scat.v.01'), Synset('run.v.03'), Synset('operate.v.01'), Synset('run.v.05'), Synset('run.v.06'), Synset('function.v.01'), Synset('range.v.01'), Synset('campaign.v.01'), Synset('play.v.18'), Synset('run.v.11'), Synset('tend.v.01'), Synset('run.v.13'), Synset('run.v.14'), Synset('run.v.15'), Synset('run.v.16'), Synset('prevail.v.03'), Synset('run.v.18'), Synset('run.v.19'), Synset('carry.v.15'), Synset('run.v.21'), Synset('guide.v.05'), Synset('run.v.23'), Synset('run.v.24'), Synset('run.v.25'), Synset('run.v.26'), Synset('run.v.27'), Synset('run.v.28'), Synset('run.v.29'), Synset('run.v.30'), Synset('run.v.31'), Synset('run.v.32'), Synset('run.v.33'), Synset('run.v.34'), Synset('ply.v.03'), Synset('hunt.v.01'), Synset('race.v.02'), Synset('move.v.13'), Synset('melt.v.01'), Synset('ladder.v.01'), Synset('run.v.41'), Synset('running.a.01'), Synset('running.s.02'), Synset('running.a.03'), Synset('running.a.04'), Synset('linear.s.05'), Synset('running.s.06')]


# Doesn't accept None and raise a KeyError
>>> wnl.lemmatize('running', pos=None)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/stem/wordnet.py"", line 40, in lemmatize
    lemmas = wordnet._morphy(word, pos)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py"", line 1774, in _morphy
    exceptions = self._exception_map[pos]
KeyError: None
```

I'm not sure how to allow `None` to `WordNetLemmatizer.lemmatize()` though.

**What should be the expected behavior of `pos=None`, default to `pos='n'`?** If so, then we can make changes at https://github.com/nltk/nltk/blob/develop/nltk/stem/wordnet.py#L39:

```python
from nltk.corpus.reader.wordnet import NOUN
...

    def lemmatize(self, word, pos=None):
        pos = NOUN if pos == None else pos
        lemmas = wordnet._morphy(word, pos)
        return min(lemmas, key=len) if lemmas else word
```

"
405,https://github.com/nltk/nltk/issues/1979,1979,"[{'id': 718746024, 'node_id': 'MDU6TGFiZWw3MTg3NDYwMjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stem/lemma', 'name': 'stem/lemma', 'color': '53bab8', 'default': False, 'description': None}]",open,2018-03-15 05:52:40+00:00,,0,nltk.stem.arlstem.ARLSTem#pref(token) returns None,Is this a bug or feature? Have you missed a return statement at the end of this function?
406,https://github.com/nltk/nltk/issues/1980,1980,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}]",open,2018-03-15 09:38:46+00:00,,5,span_tokenize rtype should be standardized,"Currently, there are different Tokenizer classes having the `span_tokenize()` function that returns a mix of generator/list outputs:

| Tokenizer | rtype |
|:-|:-|
|[CharTokenizer](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/simple.py#L68)| generator|
|[LineTokenizer](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/simple.py#L123)| generator|
|[StringTokenizer](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/api.py#L75) | generator|
|[RegexpTokenizer](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/regexp.py#L133) | generator |
|[PunktSentenceTokenizer](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/punkt.py#L1273)|  list | 
|[TreebankWordTokenizer](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/treebank.py#L147)| list |

`span_tokenize` rtype should be standardized, either all list or all generator. Related tests should be standardized too. 

Question raised comes from https://stackoverflow.com/questions/49290827/span-tokenize-gives-generator-object-as-output"
407,https://github.com/nltk/nltk/issues/1981,1981,"[{'id': 718738467, 'node_id': 'MDU6TGFiZWw3MTg3Mzg0Njc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/windows%20related', 'name': 'windows related', 'color': 'fce4ba', 'default': False, 'description': None}, {'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}]",closed,2018-03-16 04:41:59+00:00,,17,nltk.download() error,"C:\Users\Shradha\AppData\Local\Programs\Python\Python37\python.exe C:/Users/Shradha/PycharmProjects/untitled/start.py
Traceback (most recent call last):
  File ""C:/Users/Shradha/PycharmProjects/untitled/start.py"", line 2, in <module>
    nltk.download()
  File ""C:\Users\Shradha\AppData\Local\Programs\Python\Python37\lib\site-packages\nltk\downloader.py"", line 661, in download
    self._interactive_download()
  File ""C:\Users\Shradha\AppData\Local\Programs\Python\Python37\lib\site-packages\nltk\downloader.py"", line 982, in _interactive_download
    DownloaderGUI(self).mainloop()
  File ""C:\Users\Shradha\AppData\Local\Programs\Python\Python37\lib\site-packages\nltk\downloader.py"", line 1242, in __init__
    self._fill_table()
  File ""C:\Users\Shradha\AppData\Local\Programs\Python\Python37\lib\site-packages\nltk\downloader.py"", line 1538, in _fill_table
    items = self._ds.collections()
  File ""C:\Users\Shradha\AppData\Local\Programs\Python\Python37\lib\site-packages\nltk\downloader.py"", line 505, in collections
    self._update_index()
  File ""C:\Users\Shradha\AppData\Local\Programs\Python\Python37\lib\site-packages\nltk\downloader.py"", line 831, in _update_index
    ElementTree.parse(urlopen(self._url)).getroot())
  File ""C:\Users\Shradha\AppData\Local\Programs\Python\Python37\lib\xml\etree\ElementTree.py"", line 1197, in parse
    tree.parse(source, parser)
  File ""C:\Users\Shradha\AppData\Local\Programs\Python\Python37\lib\xml\etree\ElementTree.py"", line 598, in parse
    self._root = parser._parse_whole(source)
xml.etree.ElementTree.ParseError: not well-formed (invalid token): line 1, column 0

Process finished with exit code 1
I want to use nltk for my project but it is not downloading that and giving me a traceback error each and every time. please help.
This error occur whenever i run:
import nltk
nltk.download()"
408,https://github.com/nltk/nltk/issues/1984,1984,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-03-19 20:47:10+00:00,,1,Wrong BLEU scores?,"I see strange (and possibly wrong) behaviour of the BLEU implementation in NLTK version 3.2.5 (I am using Ubuntu 16.04, Python 3.5.2). Here are my logs:
```
>>> from nltk.translate.bleu_score import sentence_bleu
>>> sentence_bleu([['a', 'b', 'c', 'd']], ['aa', 'bb', 'cc', 'd'], weights=(0,1))
1.0
>>> sentence_bleu([['how', 'are', 'you', 'x']], ['aaa', 'bbb', 'ccc', 'x'], weights=(0,1))
1.0
>>> sentence_bleu([['how', 'are', 'you', '??']], ['aaa', 'bbb', 'ccc', '??'], weights=(0,1))
1.0
```
Can someone help me understand what's going on here?
Thanks.

EDIT:
Testing with cumulative BLEU scores also yields unexpected results:
```
>>> sentence_bleu([['how', 'are', 'you', '??']], ['aaa', 'bbb', 'ccc', '??'], weights=(0.5,0.5))
0.5
```
It seems that there are bugs when using n-th order BLEU scores with no n-gram overlaps."
409,https://github.com/nltk/nltk/issues/1985,1985,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-03-20 07:19:24+00:00,,7,Makefile error when install ReppTokenizer,"I have followed the steps:

```
mkdir -p /path/to/where/you/wanna/save/repp
svn co http://svn.delph-in.net/repp/trunk /path/to/where/you/wanna/save/repp
cd /path/to/where/you/wanna/save/repp/ 
autoreconf -i
./configure CPPFLAGS=-P
make
```
I have passed all steps except 'make'.

And I encountered errors in ""make step"":

```
kelsey@Darkstar:~/repp$ make
Making all in src
make[1]: Entering directory '/home/kelsey/repp/src'
g++ -DPACKAGE_NAME=\""repp\"" -DPACKAGE_TARNAME=\""repp\"" -DPACKAGE_VERSION=\""beta\"" -DPACKAGE_STRING=\""repp\ beta\"" -DPACKAGE_BUGREPORT=\""rdrid@dridan.com\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""repp\"" -DVERSION=\""beta\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DHAVE_UNICODE_UNISTR_H=1 -DHAVE_ICU=1 -DHAVE_BOOST=1 -DHAVE_BOOST_SYSTEM_ERROR_CODE_HPP=1 -DHAVE_BOOST_FILESYSTEM_PATH_HPP=1 -DHAVE_BOOST_PROGRAM_OPTIONS_HPP=1 -DHAVE_BOOST_REGEX_HPP=1 -DREPP_STANDALONE=1 -I.   -I/home/kelsey/anaconda3/include    -g -O2 -MT main.o -MD -MP -MF .deps/main.Tpo -c -o main.o main.cpp
mv -f .deps/main.Tpo .deps/main.Po
g++ -DPACKAGE_NAME=\""repp\"" -DPACKAGE_TARNAME=\""repp\"" -DPACKAGE_VERSION=\""beta\"" -DPACKAGE_STRING=\""repp\ beta\"" -DPACKAGE_BUGREPORT=\""rdrid@dridan.com\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""repp\"" -DVERSION=\""beta\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DHAVE_UNICODE_UNISTR_H=1 -DHAVE_ICU=1 -DHAVE_BOOST=1 -DHAVE_BOOST_SYSTEM_ERROR_CODE_HPP=1 -DHAVE_BOOST_FILESYSTEM_PATH_HPP=1 -DHAVE_BOOST_PROGRAM_OPTIONS_HPP=1 -DHAVE_BOOST_REGEX_HPP=1 -DREPP_STANDALONE=1 -I.   -I/home/kelsey/anaconda3/include    -g -O2 -MT repp.o -MD -MP -MF .deps/repp.Tpo -c -o repp.o repp.cpp
mv -f .deps/repp.Tpo .deps/repp.Po
g++ -DPACKAGE_NAME=\""repp\"" -DPACKAGE_TARNAME=\""repp\"" -DPACKAGE_VERSION=\""beta\"" -DPACKAGE_STRING=\""repp\ beta\"" -DPACKAGE_BUGREPORT=\""rdrid@dridan.com\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""repp\"" -DVERSION=\""beta\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DHAVE_UNICODE_UNISTR_H=1 -DHAVE_ICU=1 -DHAVE_BOOST=1 -DHAVE_BOOST_SYSTEM_ERROR_CODE_HPP=1 -DHAVE_BOOST_FILESYSTEM_PATH_HPP=1 -DHAVE_BOOST_PROGRAM_OPTIONS_HPP=1 -DHAVE_BOOST_REGEX_HPP=1 -DREPP_STANDALONE=1 -I.   -I/home/kelsey/anaconda3/include    -g -O2 -MT token.o -MD -MP -MF .deps/token.Tpo -c -o token.o token.cpp
mv -f .deps/token.Tpo .deps/token.Po
g++ -DPACKAGE_NAME=\""repp\"" -DPACKAGE_TARNAME=\""repp\"" -DPACKAGE_VERSION=\""beta\"" -DPACKAGE_STRING=\""repp\ beta\"" -DPACKAGE_BUGREPORT=\""rdrid@dridan.com\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""repp\"" -DVERSION=\""beta\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DHAVE_UNICODE_UNISTR_H=1 -DHAVE_ICU=1 -DHAVE_BOOST=1 -DHAVE_BOOST_SYSTEM_ERROR_CODE_HPP=1 -DHAVE_BOOST_FILESYSTEM_PATH_HPP=1 -DHAVE_BOOST_PROGRAM_OPTIONS_HPP=1 -DHAVE_BOOST_REGEX_HPP=1 -DREPP_STANDALONE=1 -I.   -I/home/kelsey/anaconda3/include    -g -O2 -MT tdl_options.o -MD -MP -MF .deps/tdl_options.Tpo -c -o tdl_options.o tdl_options.cpp
mv -f .deps/tdl_options.Tpo .deps/tdl_options.Po
g++ -DPACKAGE_NAME=\""repp\"" -DPACKAGE_TARNAME=\""repp\"" -DPACKAGE_VERSION=\""beta\"" -DPACKAGE_STRING=\""repp\ beta\"" -DPACKAGE_BUGREPORT=\""rdrid@dridan.com\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""repp\"" -DVERSION=\""beta\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DHAVE_UNICODE_UNISTR_H=1 -DHAVE_ICU=1 -DHAVE_BOOST=1 -DHAVE_BOOST_SYSTEM_ERROR_CODE_HPP=1 -DHAVE_BOOST_FILESYSTEM_PATH_HPP=1 -DHAVE_BOOST_PROGRAM_OPTIONS_HPP=1 -DHAVE_BOOST_REGEX_HPP=1 -DREPP_STANDALONE=1 -I.   -I/home/kelsey/anaconda3/include    -g -O2 -MT unicode.o -MD -MP -MF .deps/unicode.Tpo -c -o unicode.o unicode.cpp
mv -f .deps/unicode.Tpo .deps/unicode.Po
/bin/bash ../libtool  --tag=CXX   --mode=link g++  -g -O2  -L/home/kelsey/anaconda3/lib -licui18n -licuuc -licudata    -o repp main.o repp.o token.o tdl_options.o unicode.o  -lboost_filesystem -lboost_program_options -lboost_regex -lboost_system -lpthread -ldl -lm    -licui18n -licuuc -licudata  
libtool: link: g++ -g -O2 -o repp main.o repp.o token.o tdl_options.o unicode.o  -L/home/kelsey/anaconda3/lib -lboost_filesystem -lboost_program_options -lboost_regex -lboost_system -lpthread -ldl -lm -licui18n -licuuc -licudata
/usr/bin/ld: warning: libicuuc.so.52, needed by /usr/lib/gcc/x86_64-linux-gnu/4.9/../../../x86_64-linux-gnu/libboost_regex.so, may conflict with libicuuc.so.58
repp.o: In function `boost::re_detail::icu_regex_traits_implementation::transform_primary(int const*, int const*) const':
/usr/include/boost/regex/icu.hpp:75: undefined reference to `boost::re_detail::icu_regex_traits_implementation::do_transform(int const*, int const*, icu_58::Collator const*) const'
repp.o: In function `boost::re_detail::icu_regex_traits_implementation::transform(int const*, int const*) const':
/usr/include/boost/regex/icu.hpp:71: undefined reference to `boost::re_detail::icu_regex_traits_implementation::do_transform(int const*, int const*, icu_58::Collator const*) const'
collect2: error: ld returned 1 exit status
Makefile:418: recipe for target 'repp' failed
make[1]: *** [repp] Error 1
make[1]: Leaving directory '/home/kelsey/repp/src'
Makefile:397: recipe for target 'all-recursive' failed
make: *** [all-recursive] Error 1
```
Does anyone could help me?"
410,https://github.com/nltk/nltk/issues/1986,1986,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}]",closed,2018-03-20 16:15:14+00:00,,2,Zipfile as corpus raises exception unless subdirectory is appended to filename,"Unable to use a zipfile as a corpus unless a subdirectory is appended to the zipfile name.  A root name like 'twitter_samples.zip' will raise an exception.  But 'twitter_samples.zip/twitter_samples/' will work.


Using python 3.6.4, NLTK 3.2.5, Windows.

## Example using the twitter corpus.  

#### Fails with exception
```
from nltk.corpus.reader import TwitterCorpusReader
TwitterCorpusReader(root='twitter_samples.zip', fileids=""twitter_samples/negative_tweets.json"")
```
#### This works
```
TwitterCorpusReader(root='twitter_samples.zip/twitter_samples/', fileids=""negative_tweets.json"")
```

## Traceback
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Apps\Tools\python\conda3\envs\nlp\lib\site-packages\nltk\corpus\reader\twitter.py"", line 74, in __init__
    CorpusReader.__init__(self, root, fileids, encoding)
  File ""C:\Apps\Tools\python\conda3\envs\nlp\lib\site-packages\nltk\corpus\reader\api.py"", line 82, in __init__
    root = ZipFilePathPointer(zipfile, zipentry)
  File ""C:\Apps\Tools\python\conda3\envs\nlp\lib\site-packages\nltk\compat.py"", line 221, in _decorator
    return init_func(*args, **kwargs)
  File ""C:\Apps\Tools\python\conda3\envs\nlp\lib\site-packages\nltk\data.py"", line 506, in __init__
    (zipfile.filename, entry))
OSError: Zipfile 'C:\\Projects\\mfd\\nlp\\twitter_samples.zip' does not contain '.'
```"
411,https://github.com/nltk/nltk/issues/1987,1987,"[{'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-03-21 12:50:52+00:00,,5,ValueError: unknown locale: UTF-8,"when importing `nltk==3.2.5` I am getting locale unknown error. Traceback is long, but I don't think it matters. the issue is in:

```
    /Users/euphorbium/miniconda3/lib/python3.5/locale.py in _parse_localename(localename)
    484     elif code == 'C':
    485         return None, None
--> 486     raise ValueError('unknown locale: %s' % localename)
```

Python 3.5.2 :: Continuum Analytics, Inc.  Running on mac osx"
412,https://github.com/nltk/nltk/issues/1988,1988,[],closed,2018-03-21 19:06:35+00:00,,1,sentence_bleu crashes with method1 smoothing if the input is one word long,"Using python 3.6.1, nltk 3.2.5

I ran the following:
test = ['hi']
smooth = SmoothingFunction.method1
sentence_bleu([test], test, smoothing_function=smooth)

This gives me the error:
File ""/Users/kiraselby/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py"", line 89, in sentence_bleu
    emulate_multibleu)
  File ""/Users/kiraselby/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py"", line 199, in corpus_bleu
    hyp_len=hyp_len, emulate_multibleu=emulate_multibleu)
TypeError: method1() missing 1 required positional argument: 'p_n'

This works just fine with default smoothing, but it doesnt not work for method1 smoothing - even if I pass in weights (1.0, 0.0, 0.0, 0.0) or (1.0,) to indicate it should only compare unigrams.
"
413,https://github.com/nltk/nltk/issues/1989,1989,[],closed,2018-03-22 18:02:24+00:00,,4,How to run Stanford postagger to listen to port 9000 so we can use it from NLTK?,"How to run Stanford postagger to listen to port 9000 so we can use it from NLTK, without running the Java jar each time (and leave the loaded model in memory, to save time)"
414,https://github.com/nltk/nltk/issues/1990,1990,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}, {'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-03-25 09:48:41+00:00,,5,Using CoreNLPPOSTagger.tag returns list of tuples in a different size than the input,"Using CoreNLPPOSTagger.tag returns list of tuples in a different size than the input

This happens for example when the input contain some *space* elements: ' '

The tagger simply ignores the spaces and don't return then. This creates difficulties in correlating the output to the input..."
415,https://github.com/nltk/nltk/issues/1991,1991,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-03-25 15:43:09+00:00,,4,"Why nltk uses ""CoreNLPTokenizer(url='http://localhost:9000').tokenize(s) == expected","Why it uses url='http://localhost:9000', what does it has any matter with Tokenizer??
It always raises errors when I run it. This code is from /python3.6/site-packages/nltk/tokenize/stanford.py. 
What I want to do is to tokenize a normal Engltish sentence. I follow the codes of ""stanford.py"", but it always always pops up errors. How to fix it ?"
416,https://github.com/nltk/nltk/issues/1992,1992,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-03-26 08:17:08+00:00,,10,ROUGE Score for NLTK not found,"I don't think that there is an inbuilt function in nltk for calculating the ROUGE Score between two different pieces of text. The ROUGE Score has been introduced in this paper :    
                            http://www.aclweb.org/anthology/W04-1013

"
417,https://github.com/nltk/nltk/issues/1993,1993,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-03-26 18:51:15+00:00,,9,Weighted Longest Common Subsequence forROUGE metric in nltk.translate,Probably NLTK doesn't have any inbuilt function for finding the Longest Common Subsequence between two pieces of texts .
418,https://github.com/nltk/nltk/issues/1994,1994,"[{'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}]",closed,2018-03-27 17:38:30+00:00,,3,misalignment in pretty print trees with Chinese,"If the leaves of a tree are Chinese words, the pretty printed tree is misaligned:

```
                         3                 
                  _______|___________       
                 6                   |     
              ___|___________        |      
             6               |       |     
      _______|_______        |       |      
     4               7       |       13    
  ___|___         ___|___    |    ___|___   
 4       7       9       0   7   10      13
 |       |       |       |   |   |       |  
 红色      早       就       会   说   了       . 
```

A minimal example:
```
x = '(X (X (X 小) (X 海)) (X 天边)) '
import nltk
t = nltk.Tree.fromstring(x)
t
Out[5]: Tree('X', [Tree('X', [Tree('X', ['小']), Tree('X', ['海'])]), Tree('X', ['天边'])])
t.pretty_print()
         X     
      ___|___   
     X       | 
  ___|___    |  
 X       X   X 
 |       |   |  
 小       海   天边

```"
419,https://github.com/nltk/nltk/issues/1995,1995,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",open,2018-03-28 02:18:01+00:00,,6,word_tokenize keeps the opening single quotes and doesn't pad it with space,"`word_tokenize` keeps the opening single quotes and doesn't pad it with space, this is to make sure that the clitics get tokenized as `'ll`, `'ve', etc. 

The original treebank tokenizer has the same behavior but Stanford CoreNLP doesn't. It looks like some additional regex was put in to make sure that the opening single quotes get padded with spaces if it isn't followed by clitics. 

There should be a non-capturing regex to catch the non-clitics and pad the space. 

Details on https://stackoverflow.com/questions/49499770/nltk-word-tokenizer-treats-ending-single-quote-as-a-separate-word/49506436#49506436"
420,https://github.com/nltk/nltk/issues/1996,1996,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 888120541, 'node_id': 'MDU6TGFiZWw4ODgxMjA1NDE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/cluster', 'name': 'cluster', 'color': '2b5196', 'default': False, 'description': ''}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-03-29 20:27:33+00:00,,4,[Enhancement] nltk.cluster.kmeans.KMeansClusterer Runs Too Slow,"nltk.cluster.kmeans.KMeansClusterer takes a very very long time to run kmeans on my 5000 points data. This issue is also mentioned in [a stackoverflow comment](https://stackoverflow.com/questions/5529625/is-it-possible-to-specify-your-own-distance-function-using-scikit-learn-k-means#comment70579730_39442355) a year ago. I think the problem lies in overusing for-loop. Since numpy is an optional dependency of NLTK,  I think it would be helpful to provide a much efficient matrix based(numpy) implementation of kmeans."
421,https://github.com/nltk/nltk/issues/1997,1997,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}]",closed,2018-03-29 21:39:21+00:00,,4,"Please move the default download directory ""$HOME/nltk_data"" to a less intrusive location","I really don't like the way that nltk assert its default download location to home directory. I have to change the download path and add 'nltk.data.path.append('/less/intrusive/path/')' every time. In my opinion, a software should never take creating a folder in home directory as default option. I think '$HOME/.local/share/nltk_data'  is a better substitute.

There is similar [discussion](https://bugs.launchpad.net/ubuntu/+source/snapd/+bug/1575053) about these issue in SNAP. "
422,https://github.com/nltk/nltk/issues/2000,2000,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}]",closed,2018-04-10 12:27:46+00:00,,9,Is the Moses Tokenizer in violation of it's license?,"I was looking through the tokenizers,
and I spotted Moses Tokenizer
https://github.com/nltk/nltk/blob/cdaa7dd4e60251390a82e42456edb359191ea6e8/nltk/tokenize/moses.py#L23

which is ported from the perl script:
https://github.com/moses-smt/mosesdecoder/blob/ae7aa6a9d25be49ab4c15ec68515e74490af399b/scripts/tokenizer/tokenizer.perl#L3-L4

Which currently says that it is LGPL 2.1+

As I understand it one can not incorporate LGPL 2 or 3 into Apache 2.
but I am not 100% (if it were full GPL I know you can't incorporate that)"
423,https://github.com/nltk/nltk/issues/2003,2003,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-04-13 10:27:59+00:00,,1,Create a new repo for GPL extensions to NLTK,"cf #2000, #1781."
424,https://github.com/nltk/nltk/issues/2004,2004,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",open,2018-04-18 07:15:25+00:00,,1,`word_tokenize` could handled URL's better,"Here is what it currently does for URLs.

```
In [2]: import nltk

In [3]: nltk.word_tokenize(""http://example.com"")
Out[3]: ['http', ':', '//example.com']

In [4]: nltk.word_tokenize(""http://example.com/dir/file.html"")
Out[4]: ['http', ':', '//example.com/dir/file.html']

In [5]: nltk.word_tokenize(""example@example.com"")
Out[5]: ['example', '@', 'example.com']
```

I'm not sure what it should do.
But I feel like the answer is _not that_

I'm pretty sure one can recognise URLs with a regex.
I think the answer might be to just pick them out at the start,
and preform no splitting on them.


Obs. the treeback tokenizer  that `word_tokenize`, is based on doesn't handle URLs because they were not a thing. But we have to move with the times, no?"
425,https://github.com/nltk/nltk/issues/2005,2005,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",open,2018-04-18 07:26:10+00:00,,8,Neither  `word_tokenize` nor `TreebankWordTokenizer`  matchs the original Penn Word Tokenizer,"Right now,
NLTK has 2 tokenizers that are very similar to  Robert McIntyre's original  treebank tokenizer.
Those are:

  - `nltk.tokenize.treebank.TreebankWordTokenizer()`
     -  a default instance of the type
 - and `nltk.tokenize._treebank_word_tokenizer`   
    - a particular instance of that type **behind** `word_tokenize`.
    - It has modifications for better unicode support given [here](https://github.com/nltk/nltk/blob/081a5307b319ccc641272d2daeb53389598d0828/nltk/tokenize/__init__.py#L108-L110)

According to [this comment](https://github.com/nltk/nltk/pull/1437#issuecomment-234164667),
the reason the modifications were not simple made directly to the `TreebankWordTokenizer`,
was so that a default instance would continue to match the behaviour of McIntyre's original.


However, a default instance already does not match.
This is illustrated by tokenizing: _""""I cannot cannot work under these conditions!""""_

 - McIntyre's original gives: 
      - `[""I"", ""can"", ""not"", ""cannot"", ""work"", ""under"", ""these"", ""conditions"", ""!""]`
 - `TreebankWordTokenizer()` and `word_tokenize` gives:
      -   `[""I"", ""can"", ""not"", ""can"", ""not"", ""work"", ""under"", ""these"", ""conditions"", ""!""]`


As such `TreebankWordTokenizer()` already does not match the behaviour of the original.
So either:

 - Matching the original **does not** matter. In which chase merging the unicode support into the class makes sense.
 - Or: matching the original *does* matter, in which case `TreebankWordTokenizer()` needs to be amended.
"
426,https://github.com/nltk/nltk/issues/2006,2006,"[{'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-04-19 16:59:36+00:00,,3,Jaro-Winkler Distance in nltk.metrics.distance,"Hi, I think it would be awesome to have [Jaro Winkler Distance](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance) implemented in nltk.metrics.distance. I'd love to take a crack at it if there's no one currently working on it. There are a number of projects online implementing this, but NLTK seems like the right larger project that should have this implemented."
427,https://github.com/nltk/nltk/issues/2008,2008,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",open,2018-04-24 01:55:09+00:00,,7,Better PunktTrainer,"While trying to retrain a sentence tokenizer model with `PunktTokenizer`, the NLTK code took up >200GB of RAM and a lot of swap and doesn't seem to end after 2 days of training. 

```python
import pickle
from nltk.tokenize.punkt import PunktSentenceTokenizer

tokenizer = PunktSentenceTokenizer()

with open('wikipedia-en.txt') as fin:
    text = fin.read()

tokenizer.train(text)

with open('wiki-en.pkl') as fout:
    pickle.dump(tokenizer, fout)
```

The code seem inefficient for the massive data set we have now to train better Punkt models. 

Let's keep this issue to track Punkt related issue until we get a better version of the algorithm."
428,https://github.com/nltk/nltk/issues/2010,2010,"[{'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-04-25 15:13:38+00:00,,22,CoreNLPNERTagger throws HTTPError: 500 Server Error: Internal Server Error for url: ......,"Hello,

I'm using nltk v3.2.5 and try to use CoreNLPNERTagger with both Stanford CoreNLP v3.9.1 (the latest version) and v3.8.0. However, they both throw an HTTPError: 500 Server Error.

The code is
""""""
from nltk.tag.stanford import CoreNLPPOSTagger, CoreNLPNERTagger
CoreNLPPOSTagger(url='http://localhost:9000').tag('What is the airspeed of an unladen swallow ?'.split())
CoreNLPNERTagger(url='http://localhost:9000').tag('Rami Eid is studying at Stony Brook University in NY.'.split())
""""""

CoreNLPPOSTagger was able to give the expected result, so I guess I set up the server correctly. The error message for CoreNLPNERTagger is 

""""""
---------------------------------------------------------------------------
HTTPError                                 Traceback (most recent call last)
<ipython-input-17-4eb92df8b722> in <module>()
----> 1 CoreNLPNERTagger(url='http://localhost:9000').tag('Rami Eid is studying at Stony Brook University in NY.'.split())

~\AppData\Local\Continuum\anaconda3\lib\site-packages\nltk\tag\stanford.py in tag(self, sentence)
    229 
    230     def tag(self, sentence):
--> 231         return self.tag_sents([sentence])[0]
    232 
    233     def raw_tag_sents(self, sentences):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\nltk\tag\stanford.py in tag_sents(self, sentences)
    225         # Converting list(list(str)) -> list(str)
    226         sentences = (' '.join(words) for words in sentences)
--> 227         return list(self.raw_tag_sents(sentences))
    228 
    229 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\nltk\tag\stanford.py in raw_tag_sents(self, sentences)
    242         default_properties['annotators'] += self.tagtype
    243         for sentence in sentences:
--> 244             tagged_data = self.api_call(sentence, properties=default_properties)
    245             assert len(tagged_data['sentences']) == 1
    246             # Taggers only need to return 1-best sentence.

~\AppData\Local\Continuum\anaconda3\lib\site-packages\nltk\parse\corenlp.py in api_call(self, data, properties)
    249         )
    250 
--> 251         response.raise_for_status()
    252 
    253         return response.json()

~\AppData\Local\Continuum\anaconda3\lib\site-packages\requests\models.py in raise_for_status(self)
    933 
    934         if http_error_msg:
--> 935             raise HTTPError(http_error_msg, response=self)
    936 
    937     def close(self):

HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:9000/?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cssplit%2Cner%22%2C+%22ssplit.isOneSentence%22%3A+%22true%22%7D
""""""
Could anyone point out what happened here? Thanks!

"
429,https://github.com/nltk/nltk/issues/2011,2011,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 719395503, 'node_id': 'MDU6TGFiZWw3MTkzOTU1MDM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/twitter', 'name': 'twitter', 'color': '99ecf7', 'default': False, 'description': None}]",open,2018-04-26 07:25:28+00:00,,2,TweetTokenizer causes UnicodeEncodeError when input string is valid,"```python
from nltk.tokenize import TweetTokenizer
text = ""and i'm always happy when i saw his smile 😍😂&#55357;😍""
tknz = TweetTokenizer()
print(' '.join(tknz.tokenize(text)))
```
The result will be:
```
UnicodeEncodeError: 'utf-8' codec can't encode character '\ud83d' in position 46: surrogates not allowed
```
The input string is valid, and the output of TweetTokenizer has UnicodeEncodeError. This is not expected.

Testing environments:
- Ubuntu Server 16.04
- python 3.6
- nltk 3.2.5"
430,https://github.com/nltk/nltk/issues/2012,2012,[],closed,2018-04-26 16:04:58+00:00,,3,module 'nltk' has no attribute 'data',"Hello. I am getting this error just from importing stopwords:

    ---------------------------------------------------------------------------
    AttributeError                            Traceback (most recent call last)
    <ipython-input-12-9cf149ae136d> in <module>()
        1 from collections import Counter
        2 from nltk.tokenize import word_tokenize
    ----> 3 from nltk.corpus import stopwords
        4 search_stemmed = [stemmer.stem(x) for x in [""art"",""pen"",""marker""]]
        5 
    
    C:\Anaconda3\lib\site-packages\nltk\__init__.py in <module>()
        135 from nltk.translate import *
        136 from nltk.sem import *
    --> 137 from nltk.stem import *
        138 
        139 # Packages which can be lazily imported
    
    C:\Anaconda3\lib\site-packages\nltk\stem\__init__.py in <module>()
        27 from nltk.stem.isri import ISRIStemmer
        28 from nltk.stem.porter import PorterStemmer
    ---> 29 from nltk.stem.snowball import SnowballStemmer
        30 from nltk.stem.wordnet import WordNetLemmatizer
        31 from nltk.stem.rslp import RSLPStemmer
    
    C:\Anaconda3\lib\site-packages\nltk\stem\snowball.py in <module>()
        30 
        31 from nltk import compat
    ---> 32 from nltk.corpus import stopwords
        33 from nltk.stem import porter
        34 from nltk.stem.util import suffix_replace, prefix_replace
    
    C:\Anaconda3\lib\site-packages\nltk\corpus\__init__.py in <module>()
        64 from nltk.tokenize import RegexpTokenizer
        65 from nltk.corpus.util import LazyCorpusLoader
    ---> 66 from nltk.corpus.reader import *
        67 
        68 abc = LazyCorpusLoader(
    
    C:\Anaconda3\lib\site-packages\nltk\corpus\reader\__init__.py in <module>()
        54 """"""
        55 
    ---> 56 from nltk.corpus.reader.plaintext import *
        57 from nltk.corpus.reader.util import *
        58 from nltk.corpus.reader.api import *
    
    C:\Anaconda3\lib\site-packages\nltk\corpus\reader\plaintext.py in <module>()
        21 from nltk.corpus.reader.api import *
        22 
    ---> 23 class PlaintextCorpusReader(CorpusReader):
        24     """"""
        25     Reader for corpora that consist of plaintext documents.  Paragraphs
    
    C:\Anaconda3\lib\site-packages\nltk\corpus\reader\plaintext.py in PlaintextCorpusReader()
        40     def __init__(self, root, fileids,
        41                  word_tokenizer=WordPunctTokenizer(),
    ---> 42                  sent_tokenizer=nltk.data.LazyLoader(
        43                      'tokenizers/punkt/english.pickle'),
        44                  para_block_reader=read_blankline_block,
    
    AttributeError: module 'nltk' has no attribute 'data'


Removing the import of stopwords gets rid of the error. I have tried reinstalling nltk and redownloading all of the stuff but I still get the error. "
431,https://github.com/nltk/nltk/issues/2014,2014,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 719817004, 'node_id': 'MDU6TGFiZWw3MTk4MTcwMDQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/multithread%20/%20multiprocessing', 'name': 'multithread / multiprocessing', 'color': 'bdf486', 'default': False, 'description': None}]",open,2018-05-03 23:41:22+00:00,,10,Importing NLTK breaks multiprocessing,"Using `multiprocessing.Pool().map` to train Keras models concurrently.

As soon as I add the import for `nltk` the shell freezes with no exceptions, requiring the terminal window to be force closed.

> python 2.7.10 (and 2.7.15)
nltk 3.2.5
keras 2.1.5
tensorflow 1.4.0

The exact point of failure is
```python
from keras.models import Sequential
from keras.layers.recurrent import LSTM

model = Sequential()
print('im printed')
model.add(LSTM(...))     # <----
print('doesnt get here')
```
Works as soon as `import nltk` is removed from the project (a completely different py module), so I'm guessing it's a problem with NLTK and not Keras.

Anyone got any ideas? Possibly related to https://github.com/nltk/nltk/issues/947?

"
432,https://github.com/nltk/nltk/issues/2015,2015,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",open,2018-05-05 01:51:51+00:00,,13,Verbnet corpus is out of date,"The nltk data index (https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml) points `verbnet` to version 2.1. The latest `verbnet` definition is 3.2. 

The latest version has updated frame descriptions that provide much more information about the phrasal structure. For example, the primary description of a frame from class `future_having-13.3` in the latest version is `NP V NP-Dative NP`, describing the frame's structure as (noun-phrase, verb, noun-phrase(dative), noun-phrase) while in version 2.1 it just reads `Dative`. "
433,https://github.com/nltk/nltk/issues/2016,2016,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}]",closed,2018-05-10 11:21:12+00:00,,1,Stanford CoreNLP 'coref' resolution model,Does NLTK have Stanford CoreNLP 'coref' resolution model?
434,https://github.com/nltk/nltk/issues/2017,2017,"[{'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}]",open,2018-05-14 03:23:13+00:00,,0,dataclasses,"[Dataclasses](https://www.python.org/dev/peps/pep-0557/) are good and more powerful than `namedtuples` c.f. https://github.com/nltk/nltk/issues/530

It's pretty apt for most classes we have in `nltk` (not restricted to `nltk.corpus`) and one natural application is to re-write `CorpusReader`. I think after adopting `dataclasses`, creating a custom new corpus reader would be a lot easier for users. 

References:

 - https://hackernoon.com/a-brief-tour-of-python-3-7-data-classes-22ee5e046517
 - https://www.youtube.com/watch?v=T-TwcmT6Rcw "
435,https://github.com/nltk/nltk/issues/2019,2019,[],closed,2018-05-14 17:38:12+00:00,,8,CERTIFICATE_VERIFY_FAILED,"![screen shot 2018-05-14 at 11 06 53 pm](https://user-images.githubusercontent.com/25250253/40013591-8da3d942-57cb-11e8-88ac-1a09c7052910.png)
Hi guys,

Please help me with this. I do not firewall ON on my system."
436,https://github.com/nltk/nltk/issues/2020,2020,"[{'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}]",open,2018-05-15 15:19:05+00:00,,7,Type hinting / annotation (PEP 484)?,"Hello folks,

Recently [numpy](https://github.com/numpy/numpy/issues/7370) added some type hinting for their types. Tools like mypy is suppose to make it easier to test large codebases.  There is a recent [pycon](https://www.youtube.com/watch?v=pMgmKJyWKn8) talk about using mypy in the real world. Since nltk is pretty old and large, I figure it might be useful here.

I tried use tools like Monkeytype to generate some stubs for the codebase. But I find it hard to add type hints to a project's code if you aren't familiar with it.

Is there any interest in exploring this route?

Thanks,"
437,https://github.com/nltk/nltk/issues/2022,2022,[],closed,2018-05-18 16:42:23+00:00,,3,Potential license incompatibility,"The https://github.com/nltk/nltk/blob/79ef1cfb4c5dcc8543c465aba8b34aa0878e738b/nltk/corpus/reader/sinica_treebank.py file says that it is CC-NC-SA. Looking at Apache's License guidance ( https://www.apache.org/legal/resolved.html ) they flag both CC-NC and CC-SA code as being problematic with the Apache license.  [that page is for Apache projects, but may be of value here]

CC-NC-SA is also on the FSF's not-compatible list: https://www.gnu.org/licenses/license-list.en.html"
438,https://github.com/nltk/nltk/issues/2024,2024,[],closed,2018-05-21 11:24:39+00:00,,2,"Text.concordance does not take into account ""width"" argument and shows maximum of 25 lines (no matter ""lines"" argument value)","Just started learning nlp with nltk book.
I don't know why, but i'm having very apparent problems nobody seems to talk about (google search gave me nothing).

From the beginning of the book (1.3 Searching Text):
`text1.concordance(""monstrous"")`
Gives me 
```
Displaying 11 of 11 matches:
ong the former , one was of a most monstrous size . ... This came towards us ,
ON OF THE PSALMS . "" Touching that monstrous bulk of the whale or ork we have r
ll over with a heathenish array of monstrous clubs and spears . Some were thick
d as you gazed , and wondered what monstrous cannibal and savage could ever hav
that has survived the flood ; most monstrous and most mountainous ! That Himmal
they might scout at Moby Dick as a monstrous fable , or still worse and more de
th of Radney .'"" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l
ing Scenes . In connexion with the monstrous pictures of whales , I am strongly
ere to enter upon those still more monstrous stories of them which are to be fo
ght have been rummaged out of this monstrous cabinet there is no telling . But
of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u
```

As it should. But
`text1.concordance(""whale"")`
Gives me:
```
Displaying 25 of 25 matches:
s , and to teach them by what name a whale - fish is to be called in our tongue
t which is not true ."" -- HACKLUYT "" WHALE . ... Sw . and Dan . HVAL . This ani
ulted ."" -- WEBSTER ' S DICTIONARY "" WHALE . ... It is more immediately from th
ISH . WAL , DUTCH . HWAL , SWEDISH . WHALE , ICELANDIC . WHALE , ENGLISH . BALE
HWAL , SWEDISH . WHALE , ICELANDIC . WHALE , ENGLISH . BALEINE , FRENCH . BALLE
least , take the higgledy - piggledy whale statements , however authentic , in
 dreadful gulf of this monster ' s ( whale ' s ) mouth , are immediately lost a
 patient Job ."" -- RABELAIS . "" This whale ' s liver was two cartloads ."" -- ST
 Touching that monstrous bulk of the whale or ork we have received nothing cert
 of oil will be extracted out of one whale ."" -- IBID . "" HISTORY OF LIFE AND D
ise ."" -- KING HENRY . "" Very like a whale ."" -- HAMLET . "" Which to secure , n
restless paine , Like as the wounded whale to shore flies thro ' the maine ."" -
. OF SPERMA CETI AND THE SPERMA CETI WHALE . VIDE HIS V . E . "" Like Spencer '
t had been a sprat in the mouth of a whale ."" -- PILGRIM ' S PROGRESS . "" That
EN ' S ANNUS MIRABILIS . "" While the whale is floating at the stern of the ship
e ship called The Jonas - in - the - Whale . ... Some say the whale can ' t ope
 in - the - Whale . ... Some say the whale can ' t open his mouth , but that is
 masts to see whether they can see a whale , for the first discoverer has a duc
 for his pains . ... I was told of a whale taken near Shetland , that had above
oneers told me that he caught once a whale in Spitzbergen that was white all ov
2 , one eighty feet in length of the whale - bone kind came in , which ( as I w
n master and kill this Sperma - ceti whale , for I could never hear of any of t
 . 1729 . ""... and the breath of the whale is frequendy attended with such an i
ed with hoops and armed with ribs of whale ."" -- RAPE OF THE LOCK . "" If we com
contemptible in the comparison . The whale is doubtless the largest animal in c
```
Does that mean there is only 25 occurrences of word ""whale"" in Moby Dick? That can't be right.
`text1.concordance(""it"")`
Gives me:
`Displaying 25 of 25 matches:`
Let's increase amount of lines shown:
```
text1.concordance(""it"", lines=100)
Displaying 25 of 25 matches:
```
Let's decrease and see what it will output:
```
text1.concordance(""it"", lines=10)
Displaying 10 of 25 matches:
```
So, it actually suggests there is only 25 occurrences of the word ""it""? Well, at least it takes `lines` argument into account. 
How about `width`?
```
 text1.concordance(""monstrous"", width=10)
Displaying 11 of 11 matches:
ong the former , one was of a most monstrous size . ... This came towards us ,
ON OF THE PSALMS . "" Touching that monstrous bulk of the whale or ork we have r
ll over with a heathenish array of monstrous clubs and spears . Some were thick
d as you gazed , and wondered what monstrous cannibal and savage could ever hav
that has survived the flood ; most monstrous and most mountainous ! That Himmal
they might scout at Moby Dick as a monstrous fable , or still worse and more de
th of Radney .'"" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l
ing Scenes . In connexion with the monstrous pictures of whales , I am strongly
ere to enter upon those still more monstrous stories of them which are to be fo
ght have been rummaged out of this monstrous cabinet there is no telling . But
of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u
```
No change.
```
text1.concordance(""monstrous"", width=200)
Displaying 11 of 11 matches:
ong the former , one was of a most monstrous size . ... This came towards us ,
ON OF THE PSALMS . "" Touching that monstrous bulk of the whale or ork we have r
ll over with a heathenish array of monstrous clubs and spears . Some were thick
d as you gazed , and wondered what monstrous cannibal and savage could ever hav
that has survived the flood ; most monstrous and most mountainous ! That Himmal
they might scout at Moby Dick as a monstrous fable , or still worse and more de
th of Radney .'"" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l
ing Scenes . In connexion with the monstrous pictures of whales , I am strongly
ere to enter upon those still more monstrous stories of them which are to be fo
ght have been rummaged out of this monstrous cabinet there is no telling . But
of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u
```
No change.

I mean, obviously if that was a widespread problem, an issue about it would already be here, so it must be something to do with my system.
I have win 10 64 bit. Python 3.6.5 32 bit. 
What else i can tell you so you'll be able to help me?"
439,https://github.com/nltk/nltk/issues/2029,2029,[],closed,2018-05-28 05:20:54+00:00,,8,"when nltk failed to download nltk-data, it outputs error info to stdout?","my nltk version is 3.2.5. and I put a code of 

```python
nltk.download('punkt') # if already exist, it will return True.
```

to let the user automatically download it.

but I found that, it output the error info to stdout, I thought it may a problem.

the error info like:

```
[nltk_data] Error loading punkt: <urlopen error [SSL:
[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed
[nltk_data]     (_ssl.c:590)>
```"
440,https://github.com/nltk/nltk/issues/2030,2030,[],closed,2018-05-28 07:47:53+00:00,,1,503 error,"I'm facing a 503 error whenever I'm trying to download WordList.
![screenshot from 2018-05-28 13-15-31](https://user-images.githubusercontent.com/39028831/40603425-7708b9d8-6279-11e8-98a2-5a8ca847bbd3.png)
I tried downloading the file from the website. The problem still remains the same."
441,https://github.com/nltk/nltk/issues/2032,2032,[],open,2018-05-31 11:14:13+00:00,,5,TrigramCollocationFinder nbest method exception,"While trying to determine trigrams using the TrigramCollocation finder an exception was received with following details:
from nltk.collocations import TrigramCollocationFinder
from nltk.metrics import TrigramAssocMeasures

wordList = ['borrower', 'borrower', 'borrower', 'borrower', 'borrower', 'borrower', 'page']

tcf = TrigramCollocationFinder.from_words(wordList)
tcf.apply_freq_filter(3)
for word1, word2, word3 in tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 8):
    print ( (word1,word2,word3))

Exception Traceback:
Traceback (most recent call last):
  File ""D:/Apps/Python/HelloPython/WorExploreTest.py"", line 19, in <module>
    for word1, word2, word3 in tcf.nbest(TrigramAssocMeasures.likelihood_ratio, 8):
  File ""D:\Apps\Python\HelloPython\venv\lib\site-packages\nltk\collocations.py"", line 125, in nbest
    return [p for p, s in self.score_ngrams(score_fn)[:n]]
  File ""D:\Apps\Python\HelloPython\venv\lib\site-packages\nltk\collocations.py"", line 121, in score_ngrams
    return sorted(self._score_ngrams(score_fn), key=lambda t: (-t[1], t[0]))
  File ""D:\Apps\Python\HelloPython\venv\lib\site-packages\nltk\collocations.py"", line 113, in _score_ngrams
    score = self.score_ngram(score_fn, *tup)
  File ""D:\Apps\Python\HelloPython\venv\lib\site-packages\nltk\collocations.py"", line 255, in score_ngram
    n_all)
  File ""D:\Apps\Python\HelloPython\venv\lib\site-packages\nltk\metrics\association.py"", line 143, in likelihood_ratio
    for obs, exp in zip(cont, cls._expected_values(cont))))
  File ""D:\Apps\Python\HelloPython\venv\lib\site-packages\nltk\metrics\association.py"", line 143, in <genexpr>
    for obs, exp in zip(cont, cls._expected_values(cont))))
ValueError: math domain error

Any assistance would be highly appreciated.
Thanks a lot"
442,https://github.com/nltk/nltk/issues/2033,2033,[],closed,2018-06-04 10:32:52+00:00,,0,nltk cosine_distance error with numpy arrays,"Cosine_distance, a function of nltk.cluster.util, throws an error when working with numpy arrays, even though they are same dimension



```
a = X[0, :]

b = X[1, :]

print a.shape, b.shape
(1, 19164) (1, 19164)

print cosine_distance(a,b)
Traceback (most recent call last):

  File ""<ipython-input-56-8bba75f2139c>"", line 1, in <module>
    print cosine_distance(a,b)

  File ""/anaconda2/lib/python2.7/site-packages/nltk/cluster/util.py"", line 132, in cosine_distance
    return 1 - (numpy.dot(u, v) / (

  File ""/anaconda2/lib/python2.7/site-packages/scipy/sparse/base.py"", line 478, in __mul__
    raise ValueError('dimension mismatch')

ValueError: dimension mismatch
```

However when we apply the same function for arrays, it works fine

```
a = range(10)

b = range(10, 20)

cosine_distance(a,b)
Out[59]: 0.068593662967996294

```

I think it would be a good idea if we can extend its compatibility to numpy arrays too, because we work more often with them ( if at all I am missing something or if this is actually a bug )
"
443,https://github.com/nltk/nltk/issues/2034,2034,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",open,2018-06-06 02:39:34+00:00,,1,Better Panlex Swadesh,"From https://github.com/nltk/nltk_data/issues/117 , we can have a better Panlex swadesh list interface that also reads the language ID files (which might be useful for nlp, e.g. language ID).

Maybe an interface in https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordlist.py#L31 might be:

```python
from __future__ import print_function
from collections import namedtuple, defaultdict
import re
from six import string_types

PanlexLanguage = namedtuple('PanlexLanguage',
                          ['panlex_uid',  # (1) PanLex UID
                           'iso639',      # (2) ISO 639 language code
                           'iso639_type', # (3) ISO 639 language type, see README
                           'script',      # (4) normal scripts of expressions
                           'name',        # (5) PanLex default name
                           'langvar_uid'  # (6) UID of the language variety in which the default name is an expression
                           ])

class PanlexSwadeshCorpusReader(WordListCorpusReader):
    """"""
    This is a class to read the PanLex Swadesh list from

    David Kamholz, Jonathan Pool, and Susan M. Colowick (2014).
    PanLex: Building a Resource for Panlingual Lexical Translation.
    In LREC. http://www.lrec-conf.org/proceedings/lrec2014/pdf/1029_Paper.pdf

    License: CC0 1.0 Universal
    https://creativecommons.org/publicdomain/zero/1.0/legalcode
    """"""
    def __init__(self, *args, **kwargs):
        super(PanlexSwadeshCorpusReader, self).__init__(*args, **kwargs)
        # Find the swadesh size using the fileids' path.
        self.swadesh_size = re.match(r'swadesh([0-9].*)\/', self.fileids()[0]).group(1)
        self._languages = {lang.panlex_uid:lang for lang in self.get_languages()}
        self._macro_langauges = self.get_macrolanguages()

    def license(self):
        print('CC0 1.0 Universal')

    def readme(self):
        print(self.raw('README'))

    def language_codes(self):
        return self._languages.keys()

    def get_languages(self):
        for line in self.raw('langs{}.txt'.format(self.swadesh_size)).split('\n'):
            if not line.strip(): # Skip empty lines.
                continue
            yield PanlexLanguage(*line.strip().split('\t'))

    def get_macrolanguages(self):
        macro_langauges = defaultdict(list)
        for lang in self._languages.values():
            macro_langauges[lang.iso639].append(lang.panlex_uid)
        return macro_langauges

    def words_by_lang(self, lang_code):
        """"""
        :return: a list of list(str)
        """"""
        fileid = 'swadesh{}/{}.txt'.format(self.swadesh_size, lang_code)
        return [concept.split('\t') for concept in self.words(fileid)]

    def words_by_iso639(self, iso63_code):
        """"""
        :return: a list of list(str)
        """"""
        fileids = ['swadesh{}/{}.txt'.format(self.swadesh_size, lang_code)
                   for lang_code in self._macro_langauges[iso63_code]]
        return [concept.split('\t') for fileid in fileids for concept in self.words(fileid)]

    def entries(self, fileids=None):
        """"""
        :return: a tuple of words for the specified fileids.
        """"""
        if not fileids:
            fileids = self.fileids()

        wordlists = [self.words(f) for f in fileids]
        return list(zip(*wordlists))

```

And then on https://github.com/nltk/nltk/blob/develop/nltk/corpus/__init__.py#L199

```python
from nltk.corpus.util import LazyCorpusLoader
from nltk.corpus.reader.wordlist import PanlexSwadeshCorpusReader

swadesh110 = LazyCorpusLoader(
    'panlex_swadesh', PanlexSwadeshCorpusReader, r'swadesh110/.*\.txt', encoding='utf8')
swadesh207 = LazyCorpusLoader(
    'panlex_swadesh', PanlexSwadeshCorpusReader, swadesh207/.*\.txt', encoding='utf8')
```"
444,https://github.com/nltk/nltk/issues/2036,2036,[],closed,2018-06-07 06:48:49+00:00,,0,Hamming Distance for NLTK,Hamming Distance between 2 strings is probably not available as of now in NLTK.  
445,https://github.com/nltk/nltk/issues/2038,2038,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}]",open,2018-06-08 19:46:05+00:00,,1,How to add your own tags for POS tagging over default taggers using nltk?,"import nltk.tag, nltk.data
tagger_path = '/home/amit/nltk_data/taggers/maxent_treebank_pos_tagger/english.pickle'
default_tagger = nltk.data.load(tagger_path)
tagger = nltk.tag.UnigramTagger(model=model, backoff=default_tagger)
tagged=tagger.tag(text)
#model is a dict which has the required tags ,""tagged"" gives tags according to default_tagger but I want to put tags to text from model dict .Please , explain me what is wrong here ?"
446,https://github.com/nltk/nltk/issues/2041,2041,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-06-11 06:42:07+00:00,,10,METEOR score for MT in nltk not present,"Introduction of **METEOR score for MT** could be done in nltk as it is better than most of the available  scoring metrics for MT. This score was introduced in the following paper : 

**https://www.cs.cmu.edu/~alavie/papers/BanerjeeLavie2005-final.pdf**

"
447,https://github.com/nltk/nltk/issues/2042,2042,"[{'id': 718773983, 'node_id': 'MDU6TGFiZWw3MTg3NzM5ODM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/classifier', 'name': 'classifier', 'color': '60d2ff', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}, {'id': 961472163, 'node_id': 'MDU6TGFiZWw5NjE0NzIxNjM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/machine-learning', 'name': 'machine-learning', 'color': 'afffe9', 'default': False, 'description': ''}]",closed,2018-06-11 15:26:14+00:00,,2,NLTK Naive Bayes Classifier train method runs out of memory for considerable train data size ,"I was using the Naive Bayes classifier for training a sample set.
The sample set has 11392 labels and 4109 unique features spread across the labels.

When the train method is called the memory runs out rapidly and Memory error is received.
I have done the following:
1. Moved the code to use 64 Bit Python interpreter
2. Used indices (int instead of string) for both features and labels
But still the memory is running out rapidly and the training never completes
On a 16 GB RAM system if the train method is called when Physical RAM under use is 2.9 GB, in a matter of minutes it shoots up to use the whole physical RAM and after that thrashing starts as disk usage starts for swapping pages. 

Any assistance would be highly appreciated"
448,https://github.com/nltk/nltk/issues/2046,2046,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-06-15 19:04:48+00:00,,2,TER Metric for MT not present ,TER metric for machine translation is not present in nltk. TER Metric has been introduced in [here](https://www.cs.umd.edu/~snover/pub/amta06/ter_amta.pdf)
449,https://github.com/nltk/nltk/issues/2048,2048,"[{'id': 159235791, 'node_id': 'MDU6TGFiZWwxNTkyMzU3OTE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/dependency%20parsing', 'name': 'dependency parsing', 'color': 'eb6420', 'default': False, 'description': None}, {'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-06-24 18:31:26+00:00,,1,Following Stanford parser import warning yields an error,"If I init the parser using this import:
`from nltk.parse.stanford import StanfordDependencyParser`
I get:

> .local/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: The StanfordParser will be deprecated
> Please use nltk.parse.corenlp.StanforCoreNLPParser instead.

But this import:
`from nltk.parse.corenlp import StanfordCoreNLPParser`

Gives the following error:

> ----> 1 from nltk.parse.corenlp import StanfordCoreNLPParser
> 
> ImportError: cannot import name 'StanfordCoreNLPParser'
> 

version: nltk 3.3"
450,https://github.com/nltk/nltk/issues/2049,2049,"[{'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}, {'id': 975531894, 'node_id': 'MDU6TGFiZWw5NzU1MzE4OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/metrics', 'name': 'metrics', 'color': 'f442c8', 'default': False, 'description': ''}]",closed,2018-06-25 08:41:10+00:00,,5,Masi Distance Doc_test Error ,"Running the **doctest** on **masi_distance** in *nltk.metrics.distance* gives the doctest error as follows:

    Failed example:
        masi_distance(set([1, 2]), set([1, 2, 3, 4]))
    Expected:
        0.665
    Got:
        0.335
"
451,https://github.com/nltk/nltk/issues/2051,2051,"[{'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}]",closed,2018-06-26 06:31:20+00:00,,2,TypeError: __init__() got an unexpected keyword argument 'tagtype',"Hello, 
I got ""TypeError: __init__() got an unexpected keyword argument 'tagtype'"" when calling 

`parser = CoreNLPParser(url='http://localhost:9000', tagtype='pos')`

I am using NLTK 3.3 and CORENLP 3.9.1

It seems to work when I have `parser = CoreNLPParser(url='http://localhost:9000')`"
452,https://github.com/nltk/nltk/issues/2053,2053,"[{'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}, {'id': 719395503, 'node_id': 'MDU6TGFiZWw3MTkzOTU1MDM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/twitter', 'name': 'twitter', 'color': '99ecf7', 'default': False, 'description': None}]",closed,2018-06-28 22:14:42+00:00,,6,"Remove the ""twython library has not been installed"" warning?","I'm using nltk for sentiment analysis in a Django project. Every time I run a command with Django's `manage.py`, I get this output:

```
/var/www/example.com/venv-example/lib/python3.5/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.
  warnings.warn(""The twython library has not been installed. ""
```

I also get emailed by cron since cron emails whenever output is generated. Could this warning just be in the nltk documentation or is it important for it to be in the code too?"
453,https://github.com/nltk/nltk/issues/2055,2055,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}]",open,2018-07-02 16:33:32+00:00,,2,List installed language support,"By downloading `punkt`, several languages are support, such as:
* czech
* danish
* dutch
* english
* estonian
* finnish
* french
* german
* greek
* italian
* norwegian
* polish
* portuguese
* slovene
* spanish
* swedish
* turkish

How can, in Python, this list of language names be queried from NLTK? If that is not possible in a generic way, could this please be made into a feature request?"
454,https://github.com/nltk/nltk/issues/2057,2057,"[{'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-07-06 07:25:25+00:00,,3,Help me please! nltk and standford nlp integration,"There was a problem make me confused when i used nltk and standford nlp integration.
My develop environments like this :  
1.   nltk 3.3
2.   standford nlp stanford-segmenter 3.6.0 / 3.9.1
And i try to create an StanfordSegmenter Object like this : 
  standfordNlpPath = self.projectPath + ""\standford-nlp\stanford-segmenter-2015-12-09""
  stanfordSegmenter= StanfordSegmenter(
    path_to_jar=standfordNlpPath + ""\stanford-segmenter-3.6.0.jar"",
    path_to_slf4j=standfordNlpPath + ""\slf4j-api.jar"",
    path_to_sihan_corpora_dict=standfordNlpPath + ""\data-2015"",
    path_to_model=standfordNlpPath + ""\data-2015\pku.gz"",
    path_to_dict=standfordNlpPath + ""\data-2015\dict-chris6.ser.gz"")
then the failure like this as outcome : 
 ===========================================================================
  NLTK was unable to find stanford-segmenter.jar! Set the CLASSPATH
  environment variable.
  For more information, on stanford-segmenter.jar, see:
    <https://nlp.stanford.edu/software>
===========================================================================
All kinds of jars exactly exist there i pretty sure, is there anything wrong with my path or the parameters which i put in the class of StanfordSegmenter? The example were quite easy what i find in nltk 3.3 document, they just put in one parameter that ""path_to_slf4j"".
So, somebody, help me :-( !"
455,https://github.com/nltk/nltk/issues/2058,2058,"[{'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-07-07 09:33:35+00:00,,2,Exception while parsing using stanford parser,"```
from nltk.parse.stanford import StanfordParser

parser = StanfordParser(
    path_to_jar=""./stanford-corenlp-full-2018-02-27/stanford-corenlp-3.9.1.jar"",
    path_to_models_jar=""./stanford-corenlp-full-2018-02-27/stanford-corenlp-3.9.1-models.jar"")

parser.parse(['house', ')', 'is', 'in', 'York', 'Avenue'])
```


> ---------------------------------------------------------------------------
> ValueError                                Traceback (most recent call last)
> <ipython-input-9-cac4347e364b> in <module>()
> ----> 1 parser.parser.parse(['house', ')', 'is', 'in', 'York', 'Avenue'])
> 
> ~/.local/lib/python3.6/site-packages/nltk/parse/api.py in parse(self, sent, *args, **kwargs)
>      43         """"""
>      44         if overridden(self.parse_sents):
> ---> 45             return next(self.parse_sents([sent], *args, **kwargs))
>      46         elif overridden(self.parse_one):
>      47             return (tree for tree in [self.parse_one(sent, *args, **kwargs)] if tree is not None)
> 
> ~/.local/lib/python3.6/site-packages/nltk/parse/stanford.py in parse_sents(self, sentences, verbose)
>     120         ]
>     121         return self._parse_trees_output(self._execute(
> --> 122             cmd, '\n'.join(' '.join(sentence) for sentence in sentences), verbose))
>     123 
>     124     def raw_parse(self, sentence, verbose=False):
> 
> ~/.local/lib/python3.6/site-packages/nltk/parse/stanford.py in _parse_trees_output(self, output_)
>      91                     blank = True
>      92                 else:
> ---> 93                     res.append(iter([self._make_tree('\n'.join(cur_lines))]))
>      94                     cur_lines = []
>      95             else:
> 
> ~/.local/lib/python3.6/site-packages/nltk/parse/stanford.py in _make_tree(self, result)
>     291 
>     292     def _make_tree(self, result):
> --> 293         return Tree.fromstring(result)
>     294 
>     295 
> 
> ~/.local/lib/python3.6/site-packages/nltk/tree.py in fromstring(cls, s, brackets, read_node, read_leaf, node_pattern, leaf_pattern, remove_empty_top_bracketing)
>     626                         cls._parse_error(s, match, open_b)
>     627                     else:
> --> 628                         cls._parse_error(s, match, 'end-of-string')
>     629                 label, children = stack.pop()
>     630                 stack[-1][1].append(cls(label, children))
> 
> ~/.local/lib/python3.6/site-packages/nltk/tree.py in _parse_error(cls, s, match, expecting)
>     677             offset = 13
>     678         msg += '\n%s""%s""\n%s^' % (' '*16, s, ' '*(17+offset))
> --> 679         raise ValueError(msg)
>     680 
>     681     #////////////////////////////////////////////////////////////
> 
> ValueError: Tree.read(): expected 'end-of-string' but got ')'
>             at index 113.
> 
"
456,https://github.com/nltk/nltk/issues/2059,2059,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-07-08 08:07:51+00:00,,0,Support XDG_DATA_HOME,"Sort of following on from https://github.com/nltk/nltk/issues/1997 , allowing the data to be stored in typical locations is great, however it should be customisable using `XDG_DATA_HOME`.

i.e. add `os.environ.get('XDG_DATA_HOME') + '/nltk_data/'` to the list at

https://github.com/nltk/nltk/blob/develop/nltk/data.py#L107

```py
        str('/usr/share/nltk_data'),
        str('/usr/local/share/nltk_data'),
```

Wrt writing the data, `NLTK_DATA_PATH` obviously comes first, any existing found `nltk_data` comes second, and `XDG_DATA_HOME` skipped if it is not writable by the user.  That will typically mean `~/nltk_data/` is the location if `XDG_DATA_HOME` is unset or set to non-writable system directories.  Only when `XDG_DATA_HOME` contains `~/.local/share/` would the data be written there.  That typically happens when the OS or user knows about `.local` and wants it to be used, and OS desktop manager typically knows that it will contain ""user-specific data files"".

One immediate use is for CI, which often cant write to `/usr/`, but it gets a bit annoying to have to override different variables for each package which puts data somewhere.
e.g.
https://gitlab.com/gitlab-org/gitlab-ce/issues/4431#note_86683915

Or use [`appdirs`](https://github.com/ActiveState/appdirs) to expand the list of alternative locations, and also includes Windows logic which is very similar to the custom logic already in that function which looks at envvar `APPDATA`."
457,https://github.com/nltk/nltk/issues/2061,2061,[],closed,2018-07-11 19:27:23+00:00,,7,Not able to import nltk ,"I am trying to import nltk in python2.7 interpreter but it throw this :- ![screenshot from 2018-07-12 00-59-09](https://user-images.githubusercontent.com/13703438/42595117-d2d4fdae-856e-11e8-99e1-07b8b2f9628a.png). I am curious why module ""sklearn"" have a problem while importing nltk. Any help is appreciated. The importing problem present in both version of python i.e 2.7 and 3.4. 
EDIT1:- I have tried reinstalling both ""nltk"" and ""sklearn"" module for both python2 and 3 but it still shows this error. 
"
458,https://github.com/nltk/nltk/issues/2063,2063,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}]",closed,2018-07-12 19:47:00+00:00,,2,ConditionalFreqDist keys are mutated on call to plot,"### Brief
`ConditionalFreqDist` modifies the conditions(keys) when a non-existing key 
is passed as a `conditions` keyword argument to `tabulate()`/`plot()`.

Look at an
### Example  
![nltk-condist](https://user-images.githubusercontent.com/8173131/42654522-7dd6451a-8636-11e8-993a-fd10889dc211.png)

This undocumented behaviour appears a bug to me.

---

### Why is this happening to me ? 
**Note**,  please do correct(my blabbering).
The culprits behind this behaviour hide in [plot](https://github.com/nltk/nltk/blob/ecced11335a8942beeab4ed9020eb93f18457d10/nltk/probability.py#L1859) and [tabulate](https://github.com/nltk/nltk/blob/ecced11335a8942beeab4ed9020eb93f18457d10/nltk/probability.py#L1903) (forgive for stating the obvious). 

As `ConditionalFreqDist` is inherited from `defaultdict`, when we do a `self[c]`(links above), 
new key is created as a side-effect. This can be seen in an example below.

![default-dict](https://user-images.githubusercontent.com/8173131/42655543-1b815644-863a-11e8-89c2-01ee3e29eb8f.png)

__Thank You__"
459,https://github.com/nltk/nltk/issues/2064,2064,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}, {'id': 1004152779, 'node_id': 'MDU6TGFiZWwxMDA0MTUyNzc5', 'url': 'https://api.github.com/repos/nltk/nltk/labels/text', 'name': 'text', 'color': 'bfdbfc', 'default': False, 'description': ''}]",closed,2018-07-12 21:07:38+00:00,,3,Something wrong with nltk.text.Text,"When I'm running the function concordance in the Text object, it returns at most 25 results.

For example:
>>> from nltk.book import text1
>>> text1.concordance(""you"")
Displaying 25 of 25 matches:
minded him of his mortality . "" While you take in hand to school others , and t
aketh the signification of the word , you deliver that which is not true ."" -- 
oever , sacred or profane . Therefore you must not , in every case at least , t
n ."" -- GOLDSMITH , NAT . HIST . "" If you should write a fable for little fishe
uld write a fable for little fishes , you would make them speak like great wale
eady , sir ."" "" Mast - head ahoy ! Do you see that whale now ?"" "" Ay ay , sir !
 ."" BY REV . HENRY T . CHEEVER . "" If you make the least damn bit of noise ,"" r
ise ,"" replied Samuel , "" I will send you to hell ."" -- LIFE OF SAMUEL COMSTOCK
is harpooned to be sure ; but bethink you , how you would manage a powerful unb
ed to be sure ; but bethink you , how you would manage a powerful unbroken colt
f . Right and left , the streets take you waterward . Its extreme downtown is t
 , by Whitehall , northward . What do you see ?-- Posted like silent sentinels 
ttract them thither ? Once more . Say you are in the country ; in some high lan
 land of lakes . Take almost any path you please , and ten to one it carries yo
ou please , and ten to one it carries you down in a dale , and leaves you there
rries you down in a dale , and leaves you there by a pool in the stream . There
- going , and he will infallibly lead you to water , if water there be in all t
 there be in all that region . Should you ever be athirst in the great American
re is an artist . He desires to paint you the dreamiest , shadiest , quietest ,
 , when for scores on scores of miles you wade knee - deep among Tiger - lilies
iagara but a cataract of sand , would you travel your thousand miles to see it 
our first voyage as a passenger , did you yourself feel such a mystical vibrati
ical vibration , when first told that you and your ship were now out of sight o
 passenger . For to go as a passenger you must needs have a purse , and a purse
rse , and a purse is but a rag unless you have something in it . Besides , pass
"
460,https://github.com/nltk/nltk/issues/2066,2066,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}]",closed,2018-07-23 09:39:00+00:00,,7,Too many dependencies in conda-forge feedstock,"Hi, I've already created an issue on https://github.com/conda-forge/nltk-feedstock, but my suspicion is that that repo is not actively monitored by anyone, which is why I post this here too.

It seems that in the latest update of the feedstock too many unnecessary dependencies have been added, I suspect by accident. E.g. all of the following have been added as hard dependencies suddenly:

- gensim
- numpy
- python-crfsuite
- scikit-learn
- scipy
- matplotlib
- pyparsing
- twython
- requests

This leads to huge conda environments, including Qt etc.

The workaround is to install using pip instead, which leads to an environment 500MB smaller.

Just in case nobody's noticed..."
461,https://github.com/nltk/nltk/issues/2068,2068,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",open,2018-07-24 22:24:48+00:00,,1,PunktLanguageVars becomes immutable after methods are invoked,"The instance variables of PunktLanguageVars are initialized with a faulty design pattern that leads to buggy behavior when using the sentence tokenizer. The two culprits are `def word_tokenize(self, s):` and `def period_context_re(self):` in PunktLanguageVars. Both of these methods have the following construction.
```
try:
    return self._some_instance_variable
except:
    <initialize self._some_instance_variable>
    return self._some_instance_variable
```
This is a faulty way of initializing an instance variable because it only allows you to modify the variable before the method is called. After the method is called, the variable cannot be modified, and it's not initially obvious why (modifying a variable is done through changing `PunktLanguageVars.sent_end_chars` via [the documentation](http://wiki.apertium.org/wiki/Sentence_segmenting#NLTK_Punkt)). 

Observe the following usage of `PunktSentenceTokenizer` which uses an internal `PunktLanguageVars` instance (keep in mind that the initial default value for `PunktLanguageVars.sent_end_chars` is `('.', '?', '!')`).
```
#1
>>> from nltk.tokenize.punkt import *; s = 'test test test test test. test test test; test test.'; p = PunktSentenceTokenizer()
>>> PunktLanguageVars.sent_end_chars = ('.', ';')
>>> p.tokenize(s)
['test test test test test.', 'test test test;', 'test test.']

#2
>>> from nltk.tokenize.punkt import *; s = 'test test test test test. test test test; test test.'; p = PunktSentenceTokenizer()
>>> p.tokenize(s)
['test test test test test.', 'test test test; test test.']
>>> PunktLanguageVars.sent_end_chars = ('.', ';')
>>> p.tokenize(s)
['test test test test test.', 'test test test; test test.']
```
Why in the second example does changing `PunktLanguageVars.sent_end_chars` have no effect on recognizing the semi colon as a sentence ending mark, but it does have an effect in the first? Because `p.tokenize()` is called beforehand in the second example. Somewhere in `tokenize()` one of the two culprit methods is called on the internal `PunktLanguageVars` instance, thereby making it immutable.

This could be remedied by making the instance variables of `PunktLanguageVars` public instead of private so that they didn't need to be affected obliquely through setting `PunktLanguageVars.sent_end_chars`. Public access should be necessary anyway to give the users ability to modify the regular expressions in `PunktLanguageVars` for parsing the plethora of languages they are dealing with.

Another remedy is just generating the variables each time one of the methods is called instead of recalling a cached version in the `try` block."
462,https://github.com/nltk/nltk/issues/2070,2070,[],closed,2018-07-29 23:07:13+00:00,,4,AttributeError: module 'nltk' has no attribute 'internals',"```
#code 

import nltk
nltk.download()
```

```
AttributeError                            Traceback (most recent call last)
<ipython-input-47-6e230a00a763> in <module>()
----> 1 nltk.download()

~/anaconda3/lib/python3.6/site-packages/nltk/downloader.py in download(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error)
    659             # function should make a new copy of self to use?
    660             if download_dir is not None: self._download_dir = download_dir
--> 661             self._interactive_download()
    662             return True
    663 

~/anaconda3/lib/python3.6/site-packages/nltk/downloader.py in _interactive_download(self)
    980         if TKINTER:
    981             try:
--> 982                 DownloaderGUI(self).mainloop()
    983             except TclError:
    984                 DownloaderShell(self).run()

~/anaconda3/lib/python3.6/site-packages/nltk/downloader.py in __init__(self, dataserver, use_threads)
   1240         self._init_menu()
   1241         try:
-> 1242             self._fill_table()
   1243         except HTTPError as e:
   1244             showerror('Error reading from server', e)

~/anaconda3/lib/python3.6/site-packages/nltk/downloader.py in _fill_table(self)
   1536             items = self._ds.models()
   1537         elif self._tab == 'collections':
-> 1538             items = self._ds.collections()
   1539         else:
   1540             assert 0, 'bad tab value %r' % self._tab

~/anaconda3/lib/python3.6/site-packages/nltk/downloader.py in collections(self)
    503 
    504     def collections(self):
--> 505         self._update_index()
    506         return self._collections.values()
    507 

~/anaconda3/lib/python3.6/site-packages/nltk/downloader.py in _update_index(self, url)
    828 
    829         # Download the index file.
--> 830         self._index = nltk.internals.ElementWrapper(
    831             ElementTree.parse(urlopen(self._url)).getroot())
    832         self._index_timestamp = time.time()

AttributeError: module 'nltk' has no attribute 'internals'

```

Specs:
Mac, High Sierra Version 10.13.6, Python Python 3.6.3
"
463,https://github.com/nltk/nltk/issues/2071,2071,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-07-31 12:50:24+00:00,,2,Paramaters not used in nltk.text.concordance(),"nltk.text.concordance() call print_concordance() and pass word, width, lines as parameters, but print_concordance() then call find_concordance(word, width=80, lines=25) where parameter width and lines are assigned 80 and 25, rendering the parameters input by the user useless (when printing the concordance results, there are always 25 lines whose width are 80-charater long)."
464,https://github.com/nltk/nltk/issues/2072,2072,"[{'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}, {'id': 1012587336, 'node_id': 'MDU6TGFiZWwxMDEyNTg3MzM2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3.7', 'name': 'python3.7', 'color': 'a4e85c', 'default': False, 'description': ''}]",closed,2018-08-02 07:00:07+00:00,,1,Supporting Python3.7 ,"Python 3.7.0 had been shipped in June 2018, [PEP537](https://www.python.org/dev/peps/pep-0537/), I think it's better to pre-empt the end-of-life for Python 3.6 than to wait for user uptake and rush for a build. 

I think we're safe to have a tentative support for 3.7 later this Dec 2018 when 3.6.8 is shipped. End of life for Python 3.6 is Dec 2012 =)

This should help when porting code https://docs.python.org/3/whatsnew/3.7.html

It would be good if NLTK supports Python3.7 and the CI should be updated accordingly. 

Any brave soul?"
465,https://github.com/nltk/nltk/issues/2075,2075,"[{'id': 718733436, 'node_id': 'MDU6TGFiZWw3MTg3MzM0MzY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/Off-topic', 'name': 'Off-topic', 'color': 'f7b4e1', 'default': False, 'description': None}]",closed,2018-08-04 11:12:03+00:00,,2,issue while executing eliminiting proper noun,"file not found at wordsEng.txt
![screenshot from 2018-08-04 16-40-02](https://user-images.githubusercontent.com/42090227/43675889-457e4bf0-9805-11e8-99bb-cd6af85d212d.png)
"
466,https://github.com/nltk/nltk/issues/2076,2076,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",open,2018-08-05 07:39:31+00:00,,2,TreebankWordTokenizer span_tokenize still throwing exception for some corner-cases,"very similar to #1865, there are still unhandled inputs to TreebankWordTokenizer that will cause span_tokenize to throw:
```
>>> from nltk.tokenize import TreebankWordTokenizer
>>> s = u""\'\'good muffins\""""
>>> list(TreebankWordTokenizer().span_tokenize(s))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""nltk/tokenize/treebank.py"", line 198, in span_tokenize
    for tok in align_tokens(tokens, text):
  File ""nltk/tokenize/util.py"", line 258, in align_tokens
    raise ValueError('substring ""{}"" not found in ""{}""'.format(token, sentence))
ValueError: substring ""''"" not found in ""''good muffins""""
```
a working fix could be:
```
line 190 in tokenize.treebank.py:
matched = [m.group() for m in re.finditer(r""\`\`|(?<!^)\'\'|^\'\'(?=\s)|\"""", text)]
```
i don't like the extra look arounds, but at least this wouldn't change anything about the current implementation. happy to submit a merge req if it's helpful!"
467,https://github.com/nltk/nltk/issues/2079,2079,[],closed,2018-08-07 14:58:21+00:00,,19,Integrating word vector support for NLTK,"Word Vectors are currently not supported by NLTK. 

Integrating them would be a really good step as we often deal with them in our day-to-day jobs. This would then make NLTK a one stop for many more kinds of NLP purposes 

Following are a list of word-vectors that can be integrated with NLTK : 
 [**word2vec**](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit)
[**GloVe**](https://nlp.stanford.edu/projects/glove/)


If this issue is a go, then I can make the PR .


"
468,https://github.com/nltk/nltk/issues/2082,2082,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 1026900607, 'node_id': 'MDU6TGFiZWwxMDI2OTAwNjA3', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wontfix', 'name': 'wontfix', 'color': 'f4b7ca', 'default': True, 'description': ''}]",closed,2018-08-11 04:22:53+00:00,,3,sent_tokenize fails to seperate sentences that have no space after fullstop.,"```
In [2]: nltk.sent_tokenize('Mary had little lamb.Mary had a little lamb')
Out[2]: ['Mary had little lamb.Mary had a little lamb']
```"
469,https://github.com/nltk/nltk/issues/2083,2083,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}]",closed,2018-08-15 02:44:59+00:00,,1,corpus BLEU score have a minor issue when smoothing,[Here](https://github.com/nltk/nltk/blob/ecced11335a8942beeab4ed9020eb93f18457d10/nltk/translate/bleu_score.py#L210) should be `hyp_lengths` rather than `hyp_len`.
470,https://github.com/nltk/nltk/issues/2085,2085,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 1026900607, 'node_id': 'MDU6TGFiZWwxMDI2OTAwNjA3', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wontfix', 'name': 'wontfix', 'color': 'f4b7ca', 'default': True, 'description': ''}]",closed,2018-08-16 17:34:47+00:00,,2,sent_tokenize fails on sentence ending in smart quotes,">In June, Trump’s 2020 campaign manager, Brad Parscale, called for Sessions to be fired and “end the Mueller investigation.”
>U.S. attorneys are prosecuting Manafort on charges of failing to pay taxes on millions he made from his work for a Russia-friendly Ukrainian political party and then lying to get loans when the cash stopped coming in.

![screen shot 2018-08-16 at 1 28 02 pm](https://user-images.githubusercontent.com/2790092/44224644-0d2cc800-a159-11e8-8ee0-9c2f803e887c.png)

resolution for now:

```
text = text.replace('“', '""').replace('”', '""')
```

would be great if this was supported OOB"
471,https://github.com/nltk/nltk/issues/2086,2086,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-08-17 03:48:57+00:00,,3,nltk.corpus.reader.wordnet.WordNetError: line '01593254 v 0104,"While loading the application, im facing the  below error, 
ValueError: invalid literal for int() with base 10: 'v' During handling of the above exception, another exception occurred:
This happens when im trying to load wordnet. Please help me how to resolve this. Thank you"
472,https://github.com/nltk/nltk/issues/2087,2087,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",closed,2018-08-19 16:44:06+00:00,,5,Hindi and Hinglish stop-words,"Hi,
I am working with [Hinglish](https://en.wikipedia.org/wiki/Hinglish) data. While exploring the text corpus, I wanted to remove the stopwords from the data. There were none for the Hinglish. So, I created one big stopwords list for Hinglish. I have basically used the English one from NLTK plus transliterated hindi words. I also have one having just Hindi stopwords. I wanted them to be accessible to others as well

I was wondering whether it's okay if I create a pull having the hindi and hinglish stopwords.
"
473,https://github.com/nltk/nltk/issues/2088,2088,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}]",closed,2018-08-20 11:12:42+00:00,,4,find_concordance() return empty list for left_context,"```
if offsets:
            for i in offsets:
                query_word = self._tokens[i]
                # Find the context of query word.
                left_context = self._tokens[i-context:i]
```

When the first occurrence of the search term is at the beginning of the text (for example at offset 7), suppose the width parameter is set to 20, then [i-context:i] would be evaluated as [-13:7].
In this case, if the text consists more than 20 words, the variable left_context would be an empty list, rather than a list containing the first 7 words of the text.

A simple fix would do:

```
if offsets:
    for i in offsets:
        query_word = self._tokens[i]
        # Find the context of query word.
        if i - context < 0:
            left_context = self._tokens[:i]
        else:
            left_context = self._tokens[i-context:i]
```"
474,https://github.com/nltk/nltk/issues/2089,2089,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-08-21 06:02:51+00:00,,0,`wordnet.ic()` does not support non-English corpus,"I was trying to creates an information content lookup dictionary from a Japanese corpus.(KNBC)
```python
import nltk
nltk.download('wordnet')
nltk.download('knbc')
nltk.download('omw')

from nltk.corpus import wordnet as wn

# I had to HACK the 'lang' param into nltk source code, to support Japanese 
knbc_ic = wn.ic(knbc, False, 0.0, lang='jpn')
```
the bug rooted here (should be `self.synsets(ww, lang=lang)` ):
 
https://github.com/nltk/nltk/blob/378fee689b493edc197282444efe8300a936db79/nltk/corpus/reader/wordnet.py#L1907"
475,https://github.com/nltk/nltk/issues/2090,2090,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}]",closed,2018-08-21 17:44:55+00:00,,2,Zipfile as corpus fails unless a subdirectory is appended to the filename,"I am adding another issue because I could not reopen #1986.

Issue #1986 is still broken.   The fix did remove the exception; however, now the corpus fails to return any data.    Note that this issue applies to all corpus; I am using the Twitter Corpus as an example.

The issue is using a zip file as a corpus fails, unless a subdirectory is appended to the zip's filename.
Example: ```twitter_samples.zip``` will fail; but ```twitter_samples.zip/twitter_samples/``` will work.

Python: 3.7.0
NLTK: 3.3.0
OS: Windows 10 x64


## Example using Twitter Corpus
### Appending a subdirectory to the zip filename
``` python
from nltk.corpus.reader import TwitterCorpusReader
c = TwitterCorpusReader(root='twitter_samples.zip/twitter_samples/', fileids=""negative_tweets.json"")
print(c.abspaths())
print(c.fileids())
```
This correctly reports
 ```
[ZipFilePathPointer('D:\\Projects\\corpus\\twitter_samples.zip', 'twitter_samples/negative_tweets.json')]
['negative_tweets.json']
```

### Using a zipfile as a corpus root produces no data
```
c = TwitterCorpusReader(root='twitter_samples.zip', fileids=""twitter_samples/negative_tweets.json"")
print(c.abspaths())
print(c.fileids())
```
This incorrectly reports an empty set: 
```
[]
[]
```"
476,https://github.com/nltk/nltk/issues/2091,2091,"[{'id': 719374994, 'node_id': 'MDU6TGFiZWw3MTkzNzQ5OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/GUI', 'name': 'GUI', 'color': 'f9b3d9', 'default': False, 'description': None}, {'id': 739423158, 'node_id': 'MDU6TGFiZWw3Mzk0MjMxNTg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tkinter', 'name': 'tkinter', 'color': '03c4a7', 'default': False, 'description': None}]",open,2018-08-22 08:19:43+00:00,,1,nltk.app.concordance() crashes on macOS terminal,"macOS: 10.12.6
Python: 3.6.3
NLTK: 3.3

When calling `nltk.app.concordance()` on Terminal, Python would crash with the exception message:
```
>>> nltk.app.concordance()
2018-08-22 16:13:05.134 Python[11374:5908786] -[NSApplication _setup:]: unrecognized selector sent to instance 0x7fc147d6d460
2018-08-22 16:13:05.139 Python[11374:5908786] *** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '-[NSApplication _setup:]: unrecognized selector sent to instance 0x7fc147d6d460'
*** First throw call stack:
(
	0   CoreFoundation                      0x00007fff945cd2cb __exceptionPreprocess + 171
	1   libobjc.A.dylib                     0x00007fffa93e448d objc_exception_throw + 48
	2   CoreFoundation                      0x00007fff9464ef04 -[NSObject(NSObject) doesNotRecognizeSelector:] + 132
	3   CoreFoundation                      0x00007fff9453f755 ___forwarding___ + 1061
	4   CoreFoundation                      0x00007fff9453f2a8 _CF_forwarding_prep_0 + 120
	5   Tk                                  0x0000000102731c02 TkpInit + 471
	6   Tk                                  0x00000001026ad2a9 Tk_Init + 1794
	7   _tkinter.cpython-36m-darwin.so      0x0000000102588ddf Tcl_AppInit + 82
	8   _tkinter.cpython-36m-darwin.so      0x0000000102584422 _tkinter_create + 1047
	9   Python                              0x0000000100b208b5 _PyCFunction_FastCallDict + 166
	10  Python                              0x0000000100b88a00 call_function + 511
	11  Python                              0x0000000100b81106 _PyEval_EvalFrameDefault + 5177
	12  Python                              0x0000000100b8921b _PyEval_EvalCodeWithName + 1903
	13  Python                              0x0000000100b89b0f _PyFunction_FastCallDict + 448
	14  Python                              0x0000000100ae729c _PyObject_FastCallDict + 214
	15  Python                              0x0000000100ae73c0 _PyObject_Call_Prepend + 156
	16  Python                              0x0000000100ae7109 PyObject_Call + 102
	17  Python                              0x0000000100b344b9 slot_tp_init + 61
	18  Python                              0x0000000100b3121b type_call + 184
	19  Python                              0x0000000100ae7260 _PyObject_FastCallDict + 154
	20  Python                              0x0000000100b889c9 call_function + 456
	21  Python                              0x0000000100b81106 _PyEval_EvalFrameDefault + 5177
	22  Python                              0x0000000100b89be7 _PyFunction_FastCall + 122
	23  Python                              0x0000000100ae729c _PyObject_FastCallDict + 214
	24  Python                              0x0000000100ae73c0 _PyObject_Call_Prepend + 156
	25  Python                              0x0000000100ae7109 PyObject_Call + 102
	26  Python                              0x0000000100b344b9 slot_tp_init + 61
	27  Python                              0x0000000100b3121b type_call + 184
	28  Python                              0x0000000100ae7260 _PyObject_FastCallDict + 154
	29  Python                              0x0000000100b889c9 call_function + 456
	30  Python                              0x0000000100b81106 _PyEval_EvalFrameDefault + 5177
	31  Python                              0x0000000100b89be7 _PyFunction_FastCall + 122
	32  Python                              0x0000000100b889d0 call_function + 463
	33  Python                              0x0000000100b81106 _PyEval_EvalFrameDefault + 5177
	34  Python                              0x0000000100b8921b _PyEval_EvalCodeWithName + 1903
	35  Python                              0x0000000100b7fc53 PyEval_EvalCode + 42
	36  Python                              0x0000000100ba8b84 run_mod + 54
	37  Python                              0x0000000100ba8934 PyRun_InteractiveOneObject + 578
	38  Python                              0x0000000100ba81a4 PyRun_InteractiveLoopFlags + 105
	39  Python                              0x0000000100ba8108 PyRun_AnyFileExFlags + 60
	40  Python                              0x0000000100bbcb8a Py_Main + 3754
	41  Python                              0x0000000100ad7e1b Python + 7707
	42  libdyld.dylib                       0x00007fffa9cca235 start + 1
)
libc++abi.dylib: terminating with uncaught exception of type NSException
Abort trap: 6
```

The same problem seems to apply to IDLE as well: [nltk.app.concordance() crashes IDLE on Mac OS
](https://stackoverflow.com/questions/42882127/nltk-app-concordance-crashes-idle-on-mac-os).
However, nltk.app.concordance() works well on Jupyter Notebook and PyCharm."
477,https://github.com/nltk/nltk/issues/2093,2093,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}, {'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}]",open,2018-08-23 13:19:46+00:00,,9,Compile some processing intensive modules,"We are using NLTK 3.3 for large scale text processing with custom grammars, and found that compiling two of the NLTK modules with Cython speeds up our processing by about 30%, essentially for free. All we had to do was
```
cythonize -i -3 nltk/parse/chart.py nltk/grammar.py
```
after the NLTK installation. Note that this compilation remains entirely optional, we didn't change any code.

Obviously, the little downside for us is now that we need to keep our own binary wheel build of NLTK around in order to keep the faster package pip installable for us, but that is easily worth the hours of processing time that we save every day.

So, would this be something that the NLTK project would want to do as well?

Basically, it would be an option in the `setup.py` script to compile some modules (specifically for CPython), and the NLTK project could start uploading binary wheels for different Python versions to PyPI, which, when used, would speed up the processing on end-user side. Since no code change is involved, choosing to use the wheels or not is entirely up to the users in that case. Specifically, Linux binary wheels would certainly be appreciated for all the CI test runners out there."
478,https://github.com/nltk/nltk/issues/2095,2095,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-08-24 15:24:02+00:00,,1,concordance method - bug?,"Hello,

I noticed that when I use concordance method, it always only searches up to 25.  It was not the case before. Not sure if this has something to do with the new release? To give a concrete example, when I did text2.concordance('affection'), it used to return ""Display 25 of 79 matches"" (I did this in Spring). Now it only returns ""Display 25 of 25 matches""). I tried text2.concordance('the'), text2.concordance('the'), etc (i.e., those stop words), it all gave me ""Display 25 of 25 matches'). Just to be more clear, it is the search, not the display issue I raise here. Could someone look into this? Thank you so much."
479,https://github.com/nltk/nltk/issues/2096,2096,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}, {'id': 910625822, 'node_id': 'MDU6TGFiZWw5MTA2MjU4MjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/CI', 'name': 'CI', 'color': '76dbed', 'default': False, 'description': ''}]",closed,2018-08-28 03:17:13+00:00,,1,Supporting Python 3.7,"Python 3.7.0 had been shipped in June 2018, [PEP537](https://www.python.org/dev/peps/pep-0537/), I think it's better to pre-empt the end-of-life for Python 3.6 than to wait for user uptake and rush for a build. 

I think we're safe to have a tentative support for 3.7 later this Dec 2018 when 3.6.8 is shipped. End of life for Python 3.6 is Dec 2012 =)

This should help when porting code https://docs.python.org/3/whatsnew/3.7.html

Any brave soul?"
480,https://github.com/nltk/nltk/issues/2097,2097,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}]",open,2018-08-28 05:52:51+00:00,,5,How to stabilize NaiveBayesClassifier outputs?,"On sometimes we see that the continuous integration test fails because of some non-deterministic results from the `NgramTagger` i.e. are `NaiveBayesClassifer`, e.g. https://nltk.ci.cloudbees.com/job/pull_request_tests/822/PYV=3.6.4,jdk=jdk8latestOnlineInstall/testReport/junit/(root)/nltk/tag/

Is there someway to set a random seed or something such that classifier's output is consistent and make sure that the model is easily replicable?"
481,https://github.com/nltk/nltk/issues/2098,2098,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 910625822, 'node_id': 'MDU6TGFiZWw5MTA2MjU4MjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/CI', 'name': 'CI', 'color': '76dbed', 'default': False, 'description': ''}]",closed,2018-08-28 06:28:42+00:00,,2,"Moving to Travis, cleaning up setup/installation and better CI",More and updated documentation is necessary for [`continuous-integration` section from the `CONTRIBUTING.md`](https://github.com/nltk/nltk/blob/develop/CONTRIBUTING.md#continuous-integration)
482,https://github.com/nltk/nltk/issues/2100,2100,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-08-28 12:29:06+00:00,,3,nltk.collocations.QuadgramAssocMeasures() is missing,"nltk.collocations only import BigramAssocMeasures and TrigramAssocMeasures from nltk.metrics, and QuadgramAssocMeasures is left out.
"
483,https://github.com/nltk/nltk/issues/2102,2102,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}]",closed,2018-08-31 02:13:29+00:00,,2,TreePrettyPrinter throws TypeError when Tree.leaves() are list of tuples,"When the `Tree.leaves()` are made up of list of tuples, e.g. `[('the', 'DT'), ('cat', 'NN')])]`, `Tree.pretty_print()` that calls the `TreePrettyPrinter` throws a `TypeError`, e.g.

```python
import nltk 
sentence = [(""the"", ""DT""), (""little"", ""JJ""), (""yellow"", ""JJ""), (""dog"", ""NN""), (""barked"",""VBD""), (""at"", ""IN""), (""the"", ""DT""), (""cat"", ""NN"")]

pattern = """"""NP: {<DT>?<JJ>*<NN>}
VBD: {<VBD>}
IN: {<IN>}""""""
NPChunker = nltk.RegexpParser(pattern) 
result = NPChunker.parse(sentence)
result.pretty_print()
```

[out]:

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-32-306ca23c9095> in <module>()
      7 NPChunker = nltk.RegexpParser(pattern)
      8 result = NPChunker.parse(sentence)
----> 9 result.pretty_print()

/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tree.py in pretty_print(self, sentence, highlight, stream, **kwargs)
    697         """"""
    698         from nltk.treeprettyprinter import TreePrettyPrinter
--> 699         print(TreePrettyPrinter(self, sentence, highlight).text(**kwargs),
    700               file=stream)
    701 

/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/treeprettyprinter.py in __init__(self, tree, sentence, highlight)
     95                             if not isinstance(b, Tree):
     96                                 a[n] = len(sentence)
---> 97                                 sentence.append('%s' % b)
     98         self.nodes, self.coords, self.edges, self.highlight = self.nodecoords(
     99                 tree, sentence, highlight)

TypeError: not all arguments converted during string formatting
```

More details on https://stackoverflow.com/questions/52107352/how-can-i-pretty-print-a-nltk-tree-object "
484,https://github.com/nltk/nltk/issues/2104,2104,[],closed,2018-09-03 14:32:02+00:00,,1,CorpusReader find_corpus_fileids regex diacritics problem,"Hey there,
thanks for this cool toolkit but I am facing a issue with the corpus reader.
I am using a CategorizedPlaintextCorpusReader with a
` CAT_PATTERN=r'([\w_\s]+)/.*'`
In Python 3 this regex should work for directories that contain diacritics, like the german Umlaute ä, ö, ü. This Pattern matches when i try it in iPython

```    
for dirpath, names, files in os.walk(""corpus/raw/texts""):
        print(re.match(CAT_PATTERN, dirpath))
```

but it doesnt match any direcorty containing diacritics in its name in find_corpus_fileids util function.
Is this known issue, or did i miss something?

I am running python 3.6 with anaconda on a Mac.
Thanks!"
485,https://github.com/nltk/nltk/issues/2105,2105,[],closed,2018-09-03 19:27:43+00:00,,0,What's the name for nltk.collocation.NgramAssocMeasures.mi_like()?,"In the documentation, it says that nltk.collocation.NgramAssocMeasures.mi_like() uses a variant of mutual information. Could you please tell me in which paper or book is this measure described?"
486,https://github.com/nltk/nltk/issues/2106,2106,"[{'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-09-05 01:19:29+00:00,,1,No module named corenlp,"After following the steps,
when I run 
`from nltk.parse.corenlp import CoreNLPDependencyParser`
I am still getting 
`No module named corenlp`
I downloaded the corenlp 3.9.1 from https://stanfordnlp.github.io/CoreNLP/
Kindly reply"
487,https://github.com/nltk/nltk/issues/2107,2107,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",closed,2018-09-05 12:00:34+00:00,,0,sys.prefix should take precedence over /usr in nltk.data.path,"Currently the /usr/... paths are welded in as taking priority over the sys.prefix/... paths, but with the widespread use of virtual environments (e.g. conda) this is pretty clearly wrong: I can update my venv data packages easily, can't update the /usr ones _at all_, but they shadow my venv ones because of the built-in order.   Since I maintain several venvs for teaching, NLTK_DATA is not a reliable way to get the order correct.

Please consider reversing this, i.e. so it reads

    path += [
        os.path.join(sys.prefix, str('nltk_data')),
        os.path.join(sys.prefix, str('share'), str('nltk_data')),
        os.path.join(sys.prefix, str('lib'), str('nltk_data'))
        str('/usr/share/nltk_data'),
        str('/usr/local/share/nltk_data'),
        str('/usr/lib/nltk_data'),
        str('/usr/local/lib/nltk_data')
    ]
"
488,https://github.com/nltk/nltk/issues/2110,2110,"[{'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-09-07 18:40:15+00:00,,4,How to get all the words under a lexname (lexical filename) from nltk wordnet,"Explanation with the help of Example: 
`lexical_filename = Synset('muscle.n.01').lexname() `
The output is `noun.body`

I want to **extract all the words and synsets** under the lexical filename **noun.body**
"
489,https://github.com/nltk/nltk/issues/2111,2111,[],closed,2018-09-09 18:03:05+00:00,,1,Kabyle tokenization/morphosyntactic processing,"I'm using nltk to generate a morph-syntactic analysis for the Kabyle language. I developed scripts for tokenization and use perceptron algorithm. I wrote a morpho-syntactic corpora and wish to share with a public licence.

Please how can I share algorithms and this corpora within nltk?"
490,https://github.com/nltk/nltk/issues/2112,2112,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}]",open,2018-09-10 07:50:26+00:00,,6,CoreNLPParser tag() should allow properties overloading,"With the current `CoreNLPParser.tag()`, the ""retokenization"" by Stanford CoreNLP is unexpected:

```python
>>> from nltk.parse.corenlp import CoreNLPParser
>>> ner_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
>>> sent = ['my', 'phone', 'number', 'is', '1111', '1111', '1111']
>>> ner_tagger.tag(sent)
[('my', 'O'),
 ('phone', 'O'),
 ('number', 'O'),
 ('is', 'O'),
 ('1111\xa01111\xa01111', 'NUMBER')]
```

The expected behavior should be:

```python
>>> from nltk.parse.corenlp import CoreNLPParser
>>> ner_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
>>> sent = ['my', 'phone', 'number', 'is', '1111', '1111', '1111']
>>> ner_tagger.tag(sent)
[('my', 'O'), ('phone', 'O'), ('number', 'O'), ('is', 'O'), ('1111', 'DATE'), ('1111', 'DATE'), ('1111', 'DATE')]
```

Proposed solution is to allow `properties` arguments overloading for `.tag()` and `.tag_sents()`, i.e. at https://github.com/nltk/nltk/blob/develop/nltk/parse/corenlp.py#L348 and by default use `properties = {'tokenize.whitespace':'true'}` because we are concatenating the tokens by spaces in `tag_sents()`.

```python

    def tag_sents(self, sentences, properties=None):
        """"""
        Tag multiple sentences.

        Takes multiple sentences as a list where each sentence is a list of
        tokens.

        :param sentences: Input sentences to tag
        :type sentences: list(list(str))
        :rtype: list(list(tuple(str, str))
        """"""
        # Converting list(list(str)) -> list(str)
        sentences = (' '.join(words) for words in sentences)
        if properties == None:
            properties = {'tokenize.whitespace':'true'}
        return [sentences[0] for sentences in self.raw_tag_sents(sentences, properties)]

    def tag(self, sentence, properties=None):
        """"""
        Tag a list of tokens.

        :rtype: list(tuple(str, str))

        >>> parser = CoreNLPParser(url='http://localhost:9000', tagtype='ner')
        >>> tokens = 'Rami Eid is studying at Stony Brook University in NY'.split()
        >>> parser.tag(tokens)
        [('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'), ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'),
        ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'O')]

        >>> parser = CoreNLPParser(url='http://localhost:9000', tagtype='pos')
        >>> tokens = ""What is the airspeed of an unladen swallow ?"".split()
        >>> parser.tag(tokens)
        [('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'),
        ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'),
        ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]
        """"""
        return self.tag_sents([sentence], properties)[0]

    def raw_tag_sents(self, sentences, properties=None):
        """"""
        Tag multiple sentences.

        Takes multiple sentences as a list where each sentence is a string.

        :param sentences: Input sentences to tag
        :type sentences: list(str)
        :rtype: list(list(list(tuple(str, str)))
        """"""
        default_properties = {'ssplit.isOneSentence': 'true',
                              'annotators': 'tokenize,ssplit,' }

        default_properties.update(properties or {})

        # Supports only 'pos' or 'ner' tags.
        assert self.tagtype in ['pos', 'ner']
        default_properties['annotators'] += self.tagtype
        for sentence in sentences:
            tagged_data = self.api_call(sentence, properties=default_properties)
            yield [[(token['word'], token[self.tagtype]) for token in tagged_sentence['tokens']]
                    for tagged_sentence in tagged_data['sentences']]
```

That should enforce the list of string tokens input by the users. 

Details on https://stackoverflow.com/questions/52250268/why-do-corenlp-ner-tagger-and-ner-tagger-join-the-separated-numbers-together

If we allow the `.tag()` to overload the properties before the `raw_tag_sents`, that'll also allow users to easily handle cases like #1876 "
491,https://github.com/nltk/nltk/issues/2113,2113,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 789518993, 'node_id': 'MDU6TGFiZWw3ODk1MTg5OTM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/need-help', 'name': 'need-help', 'color': 'e0609e', 'default': False, 'description': None}]",open,2018-09-12 01:29:52+00:00,,2,Cyclic imports in nltk.ccg.lexicon,"Pylint seems to be complaining about `nltk.ccg.lexicon` having cyclic imports but I'm not familiar with R0401 to understand what is the error.

Anyone knows how to resolve this issue? 

```
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.corpus -> nltk.tokenize -> nltk.tokenize.punkt -> nltk.probability)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.corpus -> nltk.tokenize -> nltk.tokenize.texttiling)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.draw.tree -> nltk.tree)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.tree -> nltk.treeprettyprinter)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.grammar -> nltk.parse.pchart)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.stem -> nltk.stem.porter)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.classify.maxent -> nltk.classify.tadm)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.tag -> nltk.tag.brill)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.parse -> nltk.parse.shiftreduce)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.cluster -> nltk.cluster.gaac)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.chunk -> nltk.chunk.regexp)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.translate -> nltk.translate.ibm5)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.data)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.tree -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.mwe -> nltk.util -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.draw.util -> nltk.util -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.downloader -> nltk.draw.util -> nltk.util -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.tree -> nltk.treeprettyprinter -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.tree -> nltk.treetransforms)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.tree -> nltk.grammar -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.downloader)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.downloader -> nltk.draw.table -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.punkt -> nltk.probability -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.collocations -> nltk.probability -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.corpus -> nltk.tokenize -> nltk.data)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.cfg -> nltk.draw.util -> nltk.util -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.cfg -> nltk.tree -> nltk.treetransforms -> nltk.draw.tree -> nltk.draw.util -> nltk.util -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.probability -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.tree)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.regexp -> nltk.tokenize.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.tree -> nltk.grammar -> nltk.parse.pchart -> nltk.tokenize -> nltk.tokenize.regexp -> nltk.tokenize.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.corenlp -> nltk.tokenize.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.wsd -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.regexp -> nltk.tokenize.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.help -> nltk.data)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.tree -> nltk.grammar -> nltk.featstruct -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.misc -> nltk.misc.wordfinder -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.regexp -> nltk.tokenize.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.metrics -> nltk.metrics.confusionmatrix -> nltk.probability -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.metrics -> nltk.metrics.agreement -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.metrics -> nltk.metrics.scores -> nltk.util -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.cluster -> nltk.cluster.gaac -> nltk.cluster.util -> nltk.cluster.api -> nltk.probability -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.cluster -> nltk.cluster.util -> nltk.cluster.api -> nltk.probability -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.cluster -> nltk.cluster.kmeans)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.cluster -> nltk.cluster.em -> nltk.cluster.util -> nltk.cluster.api -> nltk.probability -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.cluster -> nltk.cluster.kmeans -> nltk.cluster.util -> nltk.cluster.api -> nltk.probability -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.tnt -> nltk.tag.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.senna -> nltk.tag.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.naivebayes -> nltk.classify.util -> nltk.util -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.weka -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.naivebayes -> nltk.probability -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.corpus.util -> nltk.corpus.reader.api -> nltk.data)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.rte_classify -> nltk.tokenize -> nltk.tokenize.regexp -> nltk.tokenize.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.naivebayes -> nltk.classify.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.rte_classify -> nltk.classify.maxent -> nltk.probability -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.scikitlearn -> nltk.probability -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.rte_classify -> nltk.classify.maxent -> nltk.classify.megam -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.textcat)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.positivenaivebayes -> nltk.probability -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.decisiontree -> nltk.probability -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.sequential -> nltk.classify -> nltk.classify.rte_classify -> nltk.classify.maxent -> nltk.classify.tadm -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.chunk -> nltk.chunk.regexp -> nltk.chunk.api -> nltk.chunk.util)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.chunk -> nltk.chunk.regexp -> nltk.tree -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.projectivedependencyparser -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.chunk -> nltk.chunk.regexp -> nltk.chunk.api -> nltk.parse -> nltk.parse.projectivedependencyparser -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.tree -> nltk.grammar -> nltk.featstruct -> nltk.sem.logic -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.malt -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.inference -> nltk.inference.discourse -> nltk.inference.mace -> nltk.sem.logic -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tree -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.tnt -> nltk.probability -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.corenlp -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.rslp -> nltk.data)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.hmm -> nltk.probability -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.translate -> nltk.translate.ribes_score -> nltk.util -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.translate -> nltk.translate.bleu_score -> nltk.util -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.translate -> nltk.translate.ibm2)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.translate -> nltk.translate.ibm3)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.translate -> nltk.translate.ibm4)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.translate -> nltk.translate.ibm1)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.sem -> nltk.sem.boxer -> nltk.sem.drt -> nltk.util -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.sem -> nltk.sem.skolemize -> nltk.sem.logic -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.sem -> nltk.sem.util -> nltk.grammar -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.sem -> nltk.sem.boxer -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.sem -> nltk.sem.lfg -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.inference -> nltk.inference.discourse -> nltk.sem.glue -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.sem -> nltk.sem.relextract)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.inference -> nltk.inference.discourse -> nltk.sem.glue -> nltk.sem.linearlogic -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.sem -> nltk.sem.evaluate -> nltk.sem.logic -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.featurechart -> nltk.data)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.downloader -> nltk.corpus.reader.util -> nltk.tokenize -> nltk.tokenize.regexp -> nltk.tokenize.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.projectivedependencyparser -> nltk.parse.dependencygraph -> nltk.tree -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.bllip -> nltk.data)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.tree -> nltk.grammar -> nltk.parse.pchart -> nltk.parse.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.recursivedescent -> nltk.parse.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.util -> nltk.parse.featurechart -> nltk.data)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.tree -> nltk.tree -> nltk.grammar -> nltk.parse.pchart -> nltk.parse.chart -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.shiftreduce -> nltk.parse.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.parse -> nltk.parse.recursivedescent)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.nonprojectivedependencyparser -> nltk.grammar -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.parse -> nltk.parse.transitionparser)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.earleychart -> nltk.parse.featurechart -> nltk.data)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.viterbi -> nltk.tokenize -> nltk.tokenize.regexp -> nltk.tokenize.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.stem -> nltk.stem.snowball -> nltk.stem.porter)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.inference -> nltk.inference.tableau -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.inference -> nltk.inference.discourse -> nltk.sem -> nltk.sem.lfg -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.inference -> nltk.inference.discourse -> nltk.inference.mace -> nltk.inference.prover9)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.inference -> nltk.inference.resolution -> nltk.sem.skolemize -> nltk.sem.logic -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.sentiment -> nltk.sentiment.sentiment_analyzer -> nltk.sentiment.util)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.sentiment -> nltk.sentiment.vader -> nltk.sentiment.util)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tree -> nltk.draw.tree -> nltk.draw.util -> nltk.util -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.table -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.regexp -> nltk.tokenize.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.draw -> nltk.draw.dispersion -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.regexp -> nltk.tokenize.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.perceptron -> nltk.tag.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.hunpos -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.crf -> nltk.tag.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.stanford -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.parse -> nltk.parse.nonprojectivedependencyparser -> nltk.classify -> nltk.classify.naivebayes -> nltk.probability -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk.tag -> nltk.tag.brill_trainer)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.stem -> nltk.stem.wordnet -> nltk.corpus.reader.wordnet -> nltk.corpus.reader -> nltk.corpus.reader.bracket_parse -> nltk.tag -> nltk.tag.mapping -> nltk.data)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.stanford_segmenter -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.texttiling -> nltk.tokenize.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.mwe -> nltk.tokenize.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.repp -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.sexpr -> nltk.tokenize.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.punkt -> nltk.probability -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.treebank -> nltk.tokenize.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.simple -> nltk.tokenize.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.text -> nltk.collocations -> nltk.probability -> nltk.corpus -> nltk.tokenize -> nltk.tokenize.toktok -> nltk.tokenize.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.ccg.lexicon -> nltk.ccg.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.ccg.combinator -> nltk.ccg.api -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.chart -> nltk.ccg.lexicon -> nltk.internals)
nltk/nltk/ccg/lexicon.py:1: [R0401(cyclic-import), ] Cyclic import (nltk -> nltk.ccg -> nltk.ccg.lexicon -> nltk.internals)
```"
492,https://github.com/nltk/nltk/issues/2115,2115,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}]",closed,2018-09-13 00:51:38+00:00,,2,cls vs self in classmethods,"There are several instances of `@classmethod` that doesn't have `cls` as the first argument:

```
nltk/test/unit/lm/test_vocabulary.py:20: [C0202(bad-classmethod-argument), NgramModelVocabularyTests.setUpClass] Class method setUpClass should have 'cls' as first argument
nltk/corpus/reader/mte.py:47: [C0202(bad-classmethod-argument), MTEFileReader._word_elt] Class method _word_elt should have 'cls' as first argument
nltk/corpus/reader/mte.py:51: [C0202(bad-classmethod-argument), MTEFileReader._sent_elt] Class method _sent_elt should have 'cls' as first argument
nltk/corpus/reader/mte.py:55: [C0202(bad-classmethod-argument), MTEFileReader._para_elt] Class method _para_elt should have 'cls' as first argument
nltk/corpus/reader/mte.py:59: [C0202(bad-classmethod-argument), MTEFileReader._tagged_word_elt] Class method _tagged_word_elt should have 'cls' as first argument
nltk/corpus/reader/mte.py:78: [C0202(bad-classmethod-argument), MTEFileReader._tagged_sent_elt] Class method _tagged_sent_elt should have 'cls' as first argument
nltk/corpus/reader/mte.py:82: [C0202(bad-classmethod-argument), MTEFileReader._tagged_para_elt] Class method _tagged_para_elt should have 'cls' as first argument
nltk/corpus/reader/mte.py:86: [C0202(bad-classmethod-argument), MTEFileReader._lemma_word_elt] Class method _lemma_word_elt should have 'cls' as first argument
nltk/corpus/reader/mte.py:93: [C0202(bad-classmethod-argument), MTEFileReader._lemma_sent_elt] Class method _lemma_sent_elt should have 'cls' as first argument
nltk/corpus/reader/mte.py:97: [C0202(bad-classmethod-argument), MTEFileReader._lemma_para_elt] Class method _lemma_para_elt should have 'cls' as first argument
nltk/parse/bllip.py:203: [C0202(bad-classmethod-argument), BllipParser.from_unified_model_dir] Class method from_unified_model_dir should have 'cls' as first argument
```

c.f. [PEP8](https://www.python.org/dev/peps/pep-0008/#function-and-method-arguments) and [What is the 'cls' variable used for in Python classes?](https://stackoverflow.com/questions/4613000/what-is-the-cls-variable-used-for-in-python-classes)

> **Function and Method Arguments**
>
>Always use self for the first argument to instance methods.
>
>Always use cls for the first argument to class methods.
>
>If a function argument's name clashes with a reserved keyword, it is generally better to append a single trailing underscore rather than use an abbreviation or spelling corruption. Thus class_ is better than clss. (Perhaps better is to avoid such clashes by using a synonym.)

It'll take some refactoring to fix these but there are some instance of mixing `@classmethod` and `self` usage but it's not clear which is the idiomatic convention we should follow, e.g. https://stackoverflow.com/questions/52305191/using-self-vs-cls-to-access-variable-in-unittest

Any suggestion? 
"
493,https://github.com/nltk/nltk/issues/2116,2116,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 789518993, 'node_id': 'MDU6TGFiZWw3ODk1MTg5OTM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/need-help', 'name': 'need-help', 'color': 'e0609e', 'default': False, 'description': None}]",open,2018-09-13 03:09:29+00:00,,0,How to handle demos in individual modules?,"Some modules have `demo()` methods that sort of act like doctest / unittest for the respective modules but it's hard to track which module has `demo()` and which doesn't, esp. in the `nltk.corpus.__init__.py`, i.e. https://github.com/nltk/nltk/blob/develop/nltk/corpus/__init__.py#L302

```python
def demo():
    # This is out-of-date:
    abc.demo()
    brown.demo()
#    chat80.demo()
    cmudict.demo()
    conll2000.demo()
    conll2002.demo()
    genesis.demo()
    gutenberg.demo()
    ieer.demo()
```

It also leads to an unfortunate hack to override `demo()` function at the top-most `nltk.__init__.py`, i.e. https://github.com/nltk/nltk/blob/develop/nltk/__init__.py#L183

```python
# override any accidentally imported demo
def demo():
    print(""To run the demo code for a module, type nltk.module.demo()"")
```

The above in `nltk.__init__.py` isn't a good thing. Possibly, we should either:

  - each module should be self contained and only propagate a list of `__all__` up or
  - imports at the top-most `__init__` should be specific or
  - move `demo()` codes from each module into somewhere else, e.g. `nltk.test.demos` "
494,https://github.com/nltk/nltk/issues/2118,2118,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}, {'id': 789518993, 'node_id': 'MDU6TGFiZWw3ODk1MTg5OTM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/need-help', 'name': 'need-help', 'color': 'e0609e', 'default': False, 'description': None}]",open,2018-09-13 04:58:02+00:00,,0,Non parents initialization ,"There are several classes that inherits from `SyntaxCorpusReader` but are calling `CorpusReader` for the `__init__()`:

- KNBCorpusReader
- BracketParseCorpusReader
- DependencyCorpusReader

**Why are they inheritting from `SyntaxCorpusReader` but initializing from `CorpusReader`?** Should they be fixed and initialize using SyntaxCorpusReader's init?

"
495,https://github.com/nltk/nltk/issues/2119,2119,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}, {'id': 738078305, 'node_id': 'MDU6TGFiZWw3MzgwNzgzMDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python2.7', 'name': 'python2.7', 'color': '7be833', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-09-13 05:09:39+00:00,,0,Odd StringIO imports in nltk.compat,"In `nltk.compat`[https://github.com/nltk/nltk/blob/develop/nltk/compat.py], there's a `UnicodeWriter` class that uses `StringIO` but it's unclear why are there multiple `cStringIO` and `StringIO` imported but only `cStringIO` is used by the writer.

Specifically from https://github.com/nltk/nltk/blob/develop/nltk/compat.py#L39

```python
    try:
        from cStringIO import StringIO
    except ImportError:
        from StringIO import StringIO
    BytesIO = StringIO

    from datetime import tzinfo, timedelta

    ZERO = timedelta(0)
    HOUR = timedelta(hours=1)

    # A UTC class for python 2.7
    class UTC(tzinfo):
        """"""UTC""""""

        def utcoffset(self, dt):
            return ZERO

        def tzname(self, dt):
            return ""UTC""

        def dst(self, dt):
            return ZERO

    UTC = UTC()

    import csv
    import codecs
    import cStringIO

    class UnicodeWriter:
        """"""
        A CSV writer which will write rows to CSV file ""f"",
        which is encoded in the given encoding.
        see https://docs.python.org/2/library/csv.html
        """"""

        def __init__(self, f, dialect=csv.excel, encoding=""utf-8"",
                     errors='replace', **kwds):
            # Redirect output to a queue
            self.queue = cStringIO.StringIO()
            self.writer = csv.writer(self.queue, dialect=dialect, **kwds)
            self.stream = f
            encoder_cls = codecs.getincrementalencoder(encoding)
            self.encoder = encoder_cls(errors=errors)
```


**Shouldn't the `UnicodeWriter` just use the StringIO?** Esp. when Python3 default is StringIO.

```
$ python2
Python 2.7.14 (v2.7.14:84471935ed, Sep 16 2017, 12:01:12) 
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import cStringIO

$ python3
Python 3.6.4rc1 (v3.6.4rc1:3398dcb14f, Dec  5 2017, 00:58:30) 
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import cStringIO
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'cStringIO'
```
"
496,https://github.com/nltk/nltk/issues/2120,2120,"[{'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}, {'id': 789518993, 'node_id': 'MDU6TGFiZWw3ODk1MTg5OTM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/need-help', 'name': 'need-help', 'color': 'e0609e', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-09-13 05:17:36+00:00,,1,Artifacts from Python <=2.6  ,"There are some artifacts from Python <=2.6 which is irrelevant today in Python 2.7 and Python 3.x, **should we remove them?**

Pylint caught one of them at https://github.com/nltk/nltk/blob/develop/nltk/decorators.py#L27

> nltk/nltk/decorators.py:30: [E0401(import-error), ] Unable to import 'sets'

```python
try:
    set
except NameError:
    from sets import Set as set
```

See also: https://stackoverflow.com/a/38042101/610569

**Are there other artifacts from Python <=2.6 that anyone has noticed?**"
497,https://github.com/nltk/nltk/issues/2121,2121,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}, {'id': 789518993, 'node_id': 'MDU6TGFiZWw3ODk1MTg5OTM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/need-help', 'name': 'need-help', 'color': 'e0609e', 'default': False, 'description': None}]",open,2018-09-13 05:38:11+00:00,,0,Dangerous default values in arguments,"There are quite some dangerous default dicts and lists in class/functions arguments. 

But in some cases it's unclear whether the purpose is really to keep the values in the list in subsequent calls. See https://stackoverflow.com/questions/26320899/why-is-the-empty-dictionary-a-dangerous-default-value-in-python 

It'll be good to look through each of them and convert the to default to `None` instead of `{}` or `[]` but I'm not exactly sure which of these cases the items in list should be persistent and when it shouldn't.


A list of them are:

```
nltk/tgrep.py:757: [W0102(dangerous-default-value), _tgrep_exprs_action.top_level_pred] Dangerous default value macro_dict (builtins.dict) as argument
nltk/util.py:579: [W0102(dangerous-default-value), binary_search_file] Dangerous default value {} as argument
nltk/cluster/util.py:188: [W0102(dangerous-default-value), Dendrogram.__init__] Dangerous default value [] as argument
nltk/cluster/util.py:225: [W0102(dangerous-default-value), Dendrogram.show] Dangerous default value [] as argument
nltk/app/wordnet_app.py:565: [W0102(dangerous-default-value), _collect_all_synsets] Dangerous default value dict() (builtins.dict) as argument
nltk/app/wordnet_app.py:632: [W0102(dangerous-default-value), Reference.__init__] Dangerous default value dict() (builtins.dict) as argument
nltk/classify/weka.py:180: [W0102(dangerous-default-value), WekaClassifier.train] Dangerous default value [] as argument
nltk/chunk/util.py:521: [W0102(dangerous-default-value), ieerstr2tree] Dangerous default value [] as argument
nltk/chat/util.py:38: [W0102(dangerous-default-value), Chat.__init__] Dangerous default value {} as argument
nltk/sem/drt.py:850: [W0102(dangerous-default-value), resolve_anaphora] Dangerous default value [] as argument
nltk/sem/chat80.py:223: [W0102(dangerous-default-value), Concept.__init__] Dangerous default value [] as argument
nltk/sem/chat80.py:223: [W0102(dangerous-default-value), Concept.__init__] Dangerous default value [] as argument
nltk/sem/chat80.py:223: [W0102(dangerous-default-value), Concept.__init__] Dangerous default value set() (builtins.set) as argument
nltk/sem/chat80.py:334: [W0102(dangerous-default-value), clause2concepts] Dangerous default value [] as argument
nltk/sem/boxer.py:195: [W0102(dangerous-default-value), Boxer._call] Dangerous default value [] as argument
nltk/sem/relextract.py:83: [W0102(dangerous-default-value), descape_entity] Dangerous default value html_entities.entitydefs (builtins.dict) as argument
nltk/corpus/reader/framenet.py:1229: [W0102(dangerous-default-value), FramenetCorpusReader.frame_by_id] Dangerous default value [] as argument
nltk/corpus/reader/framenet.py:1271: [W0102(dangerous-default-value), FramenetCorpusReader.frame_by_name] Dangerous default value [] as argument
nltk/corpus/reader/framenet.py:1336: [W0102(dangerous-default-value), FramenetCorpusReader.frame] Dangerous default value [] as argument
nltk/corpus/reader/framenet.py:1468: [W0102(dangerous-default-value), FramenetCorpusReader.lu] Dangerous default value [] as argument
nltk/corpus/reader/framenet.py:1621: [W0102(dangerous-default-value), FramenetCorpusReader._lu_file] Dangerous default value [] as argument
nltk/corpus/reader/framenet.py:2520: [W0102(dangerous-default-value), FramenetCorpusReader._handle_frame_elt] Dangerous default value [] as argument
nltk/parse/chart.py:1240: [W0102(dangerous-default-value), ChartParser.__init__] Dangerous default value BU_LC_STRATEGY (builtins.list) as argument
nltk/parse/chart.py:1410: [W0102(dangerous-default-value), SteppingChartParser.__init__] Dangerous default value [] as argument
nltk/parse/featurechart.py:430: [W0102(dangerous-default-value), FeatureChartParser.__init__] Dangerous default value BU_LC_FEATURE_STRATEGY (builtins.list) as argument
nltk/parse/earleychart.py:265: [W0102(dangerous-default-value), IncrementalChartParser.__init__] Dangerous default value BU_LC_INCREMENTAL_STRATEGY (builtins.list) as argument
nltk/parse/earleychart.py:381: [W0102(dangerous-default-value), FeatureIncrementalChartParser.__init__] Dangerous default value BU_LC_INCREMENTAL_FEATURE_STRATEGY (builtins.list) as argument
nltk/inference/mace.py:166: [W0102(dangerous-default-value), MaceCommand._call_interpformat] Dangerous default value [] as argument
nltk/inference/mace.py:205: [W0102(dangerous-default-value), Mace._call_mace4] Dangerous default value [] as argument
nltk/inference/prover9.py:168: [W0102(dangerous-default-value), Prover9Parent._call] Dangerous default value [] as argument
nltk/inference/prover9.py:282: [W0102(dangerous-default-value), Prover9._call_prover9] Dangerous default value [] as argument
nltk/inference/prover9.py:315: [W0102(dangerous-default-value), Prover9._call_prooftrans] Dangerous default value [] as argument
nltk/draw/util.py:1814: [W0102(dangerous-default-value), CanvasFrame.pack] Dangerous default value {} as argument
nltk/draw/util.py:2011: [W0102(dangerous-default-value), ColorizedList.__init__] Dangerous default value [] as argument
nltk/draw/util.py:2177: [W0102(dangerous-default-value), ColorizedList.pack] Dangerous default value {} as argument
nltk/draw/util.py:2181: [W0102(dangerous-default-value), ColorizedList.grid] Dangerous default value {} as argument
nltk/draw/table.py:61: [W0102(dangerous-default-value), MultiListbox.__init__] Dangerous default value {} as argument
nltk/draw/table.py:295: [W0102(dangerous-default-value), MultiListbox.configure] Dangerous default value {} as argument
nltk/draw/table.py:322: [W0102(dangerous-default-value), MultiListbox.rowconfigure] Dangerous default value {} as argument
nltk/draw/table.py:330: [W0102(dangerous-default-value), MultiListbox.columnconfigure] Dangerous default value {} as argument
nltk/draw/table.py:575: [W0102(dangerous-default-value), Table.__init__] Dangerous default value {} as argument
nltk/draw/table.py:670: [W0102(dangerous-default-value), Table.rowconfigure] Dangerous default value {} as argument
nltk/draw/table.py:674: [W0102(dangerous-default-value), Table.columnconfigure] Dangerous default value {} as argument
nltk/tag/crf.py:50: [W0102(dangerous-default-value), CRFTagger.__init__] Dangerous default value {} as argument
nltk/tokenize/texttiling.py:64: [W0102(dangerous-default-value), TextTilingTokenizer.__init__] Dangerous default value DEFAULT_SMOOTHING (builtins.list) as argument
nltk/ccg/api.py:257: [W0102(dangerous-default-value), PrimitiveCategory.__init__] Dangerous default value [] as argument
```"
498,https://github.com/nltk/nltk/issues/2122,2122,"[{'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-09-13 14:42:15+00:00,,5,Cannot Import PunktWordTokenizer in nltk 3.3,"How do I use PunktWordTokenizer in nltk 3.3? Has it been deprecated or renamed? 
`>>> nltk.__version__
'3.3'
`
`>>> from nltk.tokenize import PunktWordTokenizer
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name 'PunktWordTokenizer'
`
Any help/suggestion is highly appreciated."
499,https://github.com/nltk/nltk/issues/2124,2124,[],closed,2018-09-18 09:57:55+00:00,,4,Adding Good turing and Add -n Smoothing,"Apart from the existing 2 smoothing algorithms already present, I think the add -n smoothing and the Good turing smoothing can be added"
500,https://github.com/nltk/nltk/issues/2127,2127,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",open,2018-09-21 01:01:29+00:00,,2,Loading NLTK corpus like Wordnet from blobs,"nltk.data.load(""URL"") works fine if there are specific items to be loaded like a pickle file or stopwords, Wordnet is a compiled library and there needs to be some way to load this from a blob storage.

In AWS Lambda, the set of zipped files can contain the wordnet compiled binaries and can be loaded.
But if wordnet needs to be imported on runtime from NLTK from the cloud, the best way to do it is from a blob, (without using nltk.download() and dealing with system paths)

This issue is raised to support corpora loading from blobs."
501,https://github.com/nltk/nltk/issues/2128,2128,"[{'id': 719374994, 'node_id': 'MDU6TGFiZWw3MTkzNzQ5OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/GUI', 'name': 'GUI', 'color': 'f9b3d9', 'default': False, 'description': None}, {'id': 739423158, 'node_id': 'MDU6TGFiZWw3Mzk0MjMxNTg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tkinter', 'name': 'tkinter', 'color': '03c4a7', 'default': False, 'description': None}]",open,2018-09-21 07:51:56+00:00,,2,Replacing Tkinter,"Tkinter had served us well for a while. Esp. when `nltk.download()` is getting heated issues esp. with Jupyter notebook and Mac/Windows users.

I think it's time to move to a more modern GUI framework, perhaps https://electronjs.org/ or http://kivy.org?

Anyone good at GUI wants to take a stab at it?"
502,https://github.com/nltk/nltk/issues/2131,2131,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-09-26 20:34:09+00:00,,0,Add Perplexity metric,This will help in case of comparing the n-gram models for a test set.
503,https://github.com/nltk/nltk/issues/2132,2132,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",closed,2018-09-26 21:21:47+00:00,,3,word_tokenize() fails with a misleading error message if you give it an invalid language,"If you call `word_tokenize()` and pass a language that is not supported by `punkt`, it returns an error message saying that **punkt** could not be found, instead of the language.

`word_tokenize()` should probably fail with a different error that indicates an invalid language or that a language was not found in this case. With the current error message, one could be led on a wild goose chase to find out why nltk is not recognizing a download.

If I filter out the language that is missing a punkt model, everything runs normally otherwise.

I'm running Windows 10, Python 3.7.0, and nltk 3.3

Example:
`>>>nltk.tokenize.word_tokenize('Abc cde def, abc?', language='esperanto')`

[I'll put the full traceback in a comment]

```
LookupError: 
**********************************************************************
  Resource punkt not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('punkt')
  
  Searched in:
    - 'C:\\Users\\roger/nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
    - 'C:\\Users\\roger\\Miniconda3\\nltk_data'
    - 'C:\\Users\\roger\\Miniconda3\\share\\nltk_data'
    - 'C:\\Users\\roger\\Miniconda3\\lib\\nltk_data'
    - 'C:\\Users\\roger\\AppData\\Roaming\\nltk_data'
    - ''
**********************************************************************
```"
504,https://github.com/nltk/nltk/issues/2133,2133,"[{'id': 718733436, 'node_id': 'MDU6TGFiZWw3MTg3MzM0MzY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/Off-topic', 'name': 'Off-topic', 'color': 'f7b4e1', 'default': False, 'description': None}]",closed,2018-09-27 10:02:30+00:00,,1,How to distinguish adverbs of manner from other adverbe categories,"Is theire a way to have more detailed pos tagging that specifies which type of adverbes, adjectives it is (manner, frequency, time, etc.) ?"
505,https://github.com/nltk/nltk/issues/2134,2134,[],closed,2018-10-01 13:47:36+00:00,,2,WordNetError when loading lemmas with numeric values >= 10,"Since at least NLTK 3.3 running the below code  results in the error shown below.

```python
>>> from nltk.corpus import wordnet as wn
>>> wn.lemma('jump.v.11.jump')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/fabian/venv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py"", line 1272, in lemma
    raise WordNetError('no lemma %r in %r' % (lemma_name, synset_name))
nltk.corpus.reader.wordnet.WordNetError: no lemma '.jump' in 'jump.v.1'
```

I have reproduced it inside a clean venv with the latest nltk commit (https://github.com/nltk/nltk/commit/b991f244558154d09041d5a62b0c0e55faaab802 at the time of writing) on ubuntu 16.04 with python 3.6.5 and verified this bug does not happen with NLTK version 3.2.2

Based on experimentation I believe this bug affects all lemmas where the number XX (as in  jump.v.XX.jump) is 10 or above."
506,https://github.com/nltk/nltk/issues/2139,2139,"[{'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}]",open,2018-10-05 08:43:12+00:00,,0,Tree pretty print branch position is wrong,"When I represent the equation `(16 * 2) / 2` into the tree format `(Div (Mult 16 4) 2)` and  `2 / (16 * 2) ` into the tree format `(Div 2 (Mult 16 4))`, the `pretty_print` is the same. 

The behavior with `pretty_print` is different from `draw`, `draw` will give the right picture.

```
➜  coqa_rnet git:(rc) ✗ ipython                                                    [18/10/5| 3:38PM]
Python 3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 18:10:19) 
Type 'copyright', 'credits' or 'license' for more information
IPython 6.2.1 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import nltk

In [17]: nltk.__version__
Out[17]: '3.2.5'

In [2]: a = ""(Div (Mult 16 4) 2)""

In [3]: t = nltk.Tree(a)

In [4]: t = nltk.Tree.fromstring(a)

In [5]: t
Out[5]: Tree('Div', [Tree('Mult', ['16', '4']), '2'])

In [6]: t.pretty_print()
    Div         
  ___|___        
 |      Mult    
 |    ___|____   
 2   16       4 

In [8]: a = ""(Div 2 (Mult 16 4))""

In [9]: t = nltk.Tree.fromstring(a)

In [10]: t.pretty_print()
    Div         
  ___|___        
 |      Mult    
 |    ___|____   
 2   16       4 
```"
507,https://github.com/nltk/nltk/issues/2140,2140,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-10-12 15:47:56+00:00,,4,nltk.word_tokenize() (Treebank Tokenizer) tokenize double quotes into 2 backticks,"```
>>> nltk.word_tokenize('""')
['``']
```

So, is this the expected behavior?

And is it OK to use the Treebank tokenizer on languages other than English? (Maybe all Indo-European languages?)"
508,https://github.com/nltk/nltk/issues/2143,2143,"[{'id': 718738467, 'node_id': 'MDU6TGFiZWw3MTg3Mzg0Njc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/windows%20related', 'name': 'windows related', 'color': 'fce4ba', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",closed,2018-10-13 17:37:11+00:00,,2,nltk error," import nltk
nltk.download('Stopwords')
[nltk_data] Error loading Stopwords: <urlopen error [WinError 10054]
[nltk_data]     An existing connection was forcibly closed by the
[nltk_data]     remote host>"
509,https://github.com/nltk/nltk/issues/2144,2144,[],closed,2018-10-14 14:39:35+00:00,,2,build failing on latest develop,The latest [build](https://travis-ci.org/nltk/nltk) is failing for `develop` branch due to a unit test `test_in_topic_domains`. Any new pull requests that do no include a fix for this will also have failing builds.
510,https://github.com/nltk/nltk/issues/2145,2145,"[{'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}, {'id': 910625822, 'node_id': 'MDU6TGFiZWw5MTA2MjU4MjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/CI', 'name': 'CI', 'color': '76dbed', 'default': False, 'description': ''}, {'id': 1012587336, 'node_id': 'MDU6TGFiZWwxMDEyNTg3MzM2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3.7', 'name': 'python3.7', 'color': 'a4e85c', 'default': False, 'description': ''}]",closed,2018-10-14 15:10:32+00:00,,1,python 3.7 compatibility: re.sub() behavior,"[re.sub()](https://docs.python.org/3/library/re.html#re.sub) in python 3.7 returns different result to previous versions for some regular expressions. This is due to a fix for a bug where previously re.sub() didn't replace empty matches adjacent to a previous non-empty match.

The rationale for the fix is here: https://bugs.python.org/issue32308
The code change is here: https://github.com/python/cpython/commit/fbb490fd2f38bd817d99c20c05121ad0168a38ee
The change is reflected in the documentation through the following paragraph [here](https://docs.python.org/3.7/library/re.html#re.sub):
`Empty matches for the pattern are replaced when adjacent to a previous non-empty match.`

For NLTK this means that the following line produces different result in 3.7 compared to <= 3.6:
https://github.com/nltk/nltk/blob/ad415c94b177b43561fc8e48bf1cda24bb66afc7/nltk/corpus/util.py#L69
As a result, many unit tests fail when running python 3.7+ due to corpus loader not finding the data files.

Example of behaviour in python <= 3.6
```
$ python
Python 3.6.6 (default, Oct 13 2018, 12:50:59) 
[GCC 7.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import re
>>> re.sub(r'(([^/]*)(/.*)?)', r'\2.zip/\1/', 'test')
'test.zip/test/'
```

and in python 3.7+
```
$ python
Python 3.7.0 (default, Oct 13 2018, 17:53:23) 
[GCC 7.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import re
>>> re.sub(r'(([^/]*)(/.*)?)', r'\2.zip/\1/', 'test')
'test.zip/test/.zip//'
>>>
```"
511,https://github.com/nltk/nltk/issues/2147,2147,[],closed,2018-10-15 16:14:20+00:00,,2,LookupError: Resource \x1b[93mstopwords\x1b[0m not found.,"```python
download('stopwords', download_dir=tempfile.gettempdir())  # Download stopwords list.
stop_words = stopwords.words('english')
```

Looking at the logs this seems to search the dictionary in default folders but not the specified one `[Mon Oct 15 16:06:09.191137 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]   >>> nltk.download('stopwords')`, but I have specified the `download_dir` folder.

whole logging follows:

```
[Mon Oct 15 16:06:09.191052 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]     except LookupError: raise e
[Mon Oct 15 16:06:09.191077 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240] LookupError: 
[Mon Oct 15 16:06:09.191086 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240] **********************************************************************
[Mon Oct 15 16:06:09.191095 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]   Resource \x1b[93mstopwords\x1b[0m not found.
[Mon Oct 15 16:06:09.191103 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]   Please use the NLTK Downloader to obtain the resource:
[Mon Oct 15 16:06:09.191111 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240] 
[Mon Oct 15 16:06:09.191119 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]   \x1b[31m>>> import nltk
[Mon Oct 15 16:06:09.191137 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]   >>> nltk.download('stopwords')
[Mon Oct 15 16:06:09.191145 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]   \x1b[0m
[Mon Oct 15 16:06:09.191152 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]   Searched in:
[Mon Oct 15 16:06:09.191159 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]     - '/var/www/nltk_data'
[Mon Oct 15 16:06:09.191167 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]     - '/usr/share/nltk_data'
[Mon Oct 15 16:06:09.191174 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]     - '/usr/local/share/nltk_data'
[Mon Oct 15 16:06:09.191181 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]     - '/usr/lib/nltk_data'
[Mon Oct 15 16:06:09.191189 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]     - '/usr/local/lib/nltk_data'
[Mon Oct 15 16:06:09.191196 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]     - '/usr/nltk_data'
[Mon Oct 15 16:06:09.191203 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]     - '/usr/share/nltk_data'
[Mon Oct 15 16:06:09.191211 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]     - '/usr/lib/nltk_data'
[Mon Oct 15 16:06:09.191218 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240] **********************************************************************
[Mon Oct 15 16:06:09.191225 2018] [wsgi:error] [pid 6934:tid 140310493574912] [client 172.17.0.1:40240]
```

I have previously downloaded 

```python
def load_resources():
    # nltk dataset
    start = time()
    logger.info(""Downloading stopwords and tokenizer..."")
    download('punkt', download_dir=tempfile.gettempdir())  # Download data for tokenizer.
    download('stopwords', download_dir=tempfile.gettempdir())  # Download stopwords list.
    logger.info('Downloading took %.2f seconds to run.' % (time() - start))
```
so the files should be there since I can see the logging:




"
512,https://github.com/nltk/nltk/issues/2148,2148,"[{'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}, {'id': 910625822, 'node_id': 'MDU6TGFiZWw5MTA2MjU4MjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/CI', 'name': 'CI', 'color': '76dbed', 'default': False, 'description': ''}, {'id': 1012587336, 'node_id': 'MDU6TGFiZWwxMDEyNTg3MzM2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3.7', 'name': 'python3.7', 'color': 'a4e85c', 'default': False, 'description': ''}]",closed,2018-10-17 11:16:03+00:00,,3,python 3.7 compatibility PEP 479,"There are several places in the code base that result in `RuntimeError: generator raised StopIteration` when executed with python 3.7+.

This happens because [PEP 479](https://www.python.org/dev/peps/pep-0479/) made a change to generators: when [StopIteration](https://docs.python.org/3/library/exceptions.html#StopIteration) is raised inside a generator, it is replaced with [RuntimeError](https://docs.python.org/3/library/exceptions.html#RuntimeError) when the exception is about to bubble out of the generator.
This behaviour is not backwards compatible and was slowly introduced with this [transition plan](https://www.python.org/dev/peps/pep-0479/#transition-plan).

In python 3.6 the code emits a non-silent deprecation warning, but starting with python 3.7 it becomes a runtime error.

Code changes are necessary to enable python 3.7 compatibility for PEP 479.
"
513,https://github.com/nltk/nltk/issues/2151,2151,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-10-17 13:10:48+00:00,,6,Converting russian RNC tagset to universal tagset,"It seems that the tagset that nltk.pos_tag() used for russian texts is different from that used in the mapping table when nltk.map_tag() is called. So most of the tags are converted to just X.


```
>>> import nltk
>>> tokens = nltk.word_tokenize('Ру́сский язы́к ([ˈruskʲɪi̯ jɪˈzɨk] Информация о файле слушать)[~ 3][⇨] — один из восточнославянских языков, национальный язык русского народа. Является одним из наиболее распространённых языков мира — шестым среди всех языков мира по общей численности говорящих и восьмым по численности владеющих им как родным[9]. Русский является также самым распространённым славянским языком[10] и самым распространённым языком в Европе — географически и по числу носителей языка как родного[7].')
>>> nltk.pos_tag(tokens, lang = 'rus')
[('Ру́сский', 'A=m'), ('язы́к', 'S'), ('(', 'NONLEX'), ('[', 'NONLEX'), ('ˈruskʲɪi̯', 'NONLEX'), ('jɪˈzɨk', 'NONLEX'), (']', 'NONLEX'), ('Информация', 'S'), ('о', 'PR'), ('файле', 'S'), ('слушать', 'V'), (')', 'NONLEX'), ('[', 'NONLEX'), ('~', 'NONLEX'), ('3', 'NUM=ciph'), (']', 'NONLEX'), ('[', 'NONLEX'), ('⇨', 'NONLEX'), (']', 'NONLEX'), ('—', 'NONLEX'), ('один', 'A-PRO=m'), ('из', 'PR'), ('восточнославянских', 'A=pl'), ('языков', 'S'), (',', 'NONLEX'), ('национальный', 'A=m'), ('язык', 'S'), ('русского', 'A=m'), ('народа', 'S'), ('.', 'NONLEX'), ('Является', 'V'), ('одним', 'A-PRO=m'), ('из', 'PR'), ('наиболее', 'ADV'), ('распространённых', 'V'), ('языков', 'S'), ('мира', 'S'), ('—', 'NONLEX'), ('шестым', 'V'), ('среди', 'PR'), ('всех', 'A-PRO=pl'), ('языков', 'S'), ('мира', 'S'), ('по', 'PR'), ('общей', 'A=f'), ('численности', 'S'), ('говорящих', 'A=pl'), ('и', 'CONJ'), ('восьмым', 'V'), ('по', 'PR'), ('численности', 'S'), ('владеющих', 'V'), ('им', 'S-PRO'), ('как', 'CONJ'), ('родным', 'A=m'), ('[', 'NONLEX'), ('9', 'NUM=ciph'), (']', 'NONLEX'), ('.', 'NONLEX'), ('Русский', 'A=m'), ('является', 'V'), ('также', 'ADV'), ('самым', 'A-PRO=m'), ('распространённым', 'V'), ('славянским', 'A=m'), ('языком', 'S'), ('[', 'NONLEX'), ('10', 'NUM=ciph'), (']', 'NONLEX'), ('и', 'CONJ'), ('самым', 'A-PRO=m'), ('распространённым', 'A=m'), ('языком', 'S'), ('в', 'PR'), ('Европе', 'S'), ('—', 'NONLEX'), ('географически', 'ADV'), ('и', 'CONJ'), ('по', 'PR'), ('числу', 'S'), ('носителей', 'S'), ('языка', 'S'), ('как', 'CONJ'), ('родного', 'A=m'), ('[', 'NONLEX'), ('7', 'NUM=ciph'), (']', 'NONLEX'), ('.', 'NONLEX')]
>>> nltk.pos_tag(tokens, tagset = 'universal', lang = 'rus')
[('Ру́сский', 'X'), ('язы́к', 'X'), ('(', 'X'), ('[', 'X'), ('ˈruskʲɪi̯', 'X'), ('jɪˈzɨk', 'X'), (']', 'X'), ('Информация', 'X'), ('о', 'X'), ('файле', 'X'), ('слушать', 'X'), (')', 'X'), ('[', 'X'), ('~', 'X'), ('3', 'X'), (']', 'X'), ('[', 'X'), ('⇨', 'X'), (']', 'X'), ('—', 'X'), ('один', 'X'), ('из', 'X'), ('восточнославянских', 'X'), ('языков', 'X'), (',', 'X'), ('национальный', 'X'), ('язык', 'X'), ('русского', 'X'), ('народа', 'X'), ('.', 'X'), ('Является', 'X'), ('одним', 'X'), ('из', 'X'), ('наиболее', 'X'), ('распространённых', 'X'), ('языков', 'X'), ('мира', 'X'), ('—', 'X'), ('шестым', 'X'), ('среди', 'X'), ('всех', 'X'), ('языков', 'X'), ('мира', 'X'), ('по', 'X'), ('общей', 'X'), ('численности', 'X'), ('говорящих', 'X'), ('и', 'X'), ('восьмым', 'X'), ('по', 'X'), ('численности', 'X'), ('владеющих', 'X'), ('им', 'X'), ('как', 'X'), ('родным', 'X'), ('[', 'X'), ('9', 'X'), (']', 'X'), ('.', 'X'), ('Русский', 'X'), ('является', 'X'), ('также', 'X'), ('самым', 'X'), ('распространённым', 'X'), ('славянским', 'X'), ('языком', 'X'), ('[', 'X'), ('10', 'X'), (']', 'X'), ('и', 'X'), ('самым', 'X'), ('распространённым', 'X'), ('языком', 'X'), ('в', 'X'), ('Европе', 'X'), ('—', 'X'), ('географически', 'X'), ('и', 'X'), ('по', 'X'), ('числу', 'X'), ('носителей', 'X'), ('языка', 'X'), ('как', 'X'), ('родного', 'X'), ('[', 'X'), ('7', 'X'), (']', 'X'), ('.', 'X')]
```

The mapping table used in NLTK 3.3:

```
!	.
A	ADJ
AD	ADV	
C	CONJ
COMP	CONJ
IJ	X
NC	NUM
NN	NOUN
P	ADP
PTCL	PRT
V	VERB
VG	VERB
VI	VERB
VP	VERB
YES_NO_SENT	X
Z	X
```
"
514,https://github.com/nltk/nltk/issues/2154,2154,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",open,2018-10-18 13:45:40+00:00,,5,Sentence tokenizer fails on simple case,"Every time there is ""no."" meaning ""number"", the sentence tokenizer fails.

I have to replace ""no."" with ""shorthand_number"" and replace back after sentence splitting. (My rule is a bit more complex, but in general)"
515,https://github.com/nltk/nltk/issues/2155,2155,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-10-19 02:18:31+00:00,,9,Import terrible slow,"Hi, my code is taking sooo long after I used ""nltk.downlod()"".
Now, when I use import nltk it tooks more than a minute

Anyone?"
516,https://github.com/nltk/nltk/issues/2158,2158,"[{'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-10-24 06:25:57+00:00,,3,CERTIFICATE_VERIFY_FAILED] certificate verify failed,"I am trying to run this code:
```
import nltk
nltk.download()
```
but I am getting this issue:
[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed
[nltk_data]     (_ssl.c:726)>
"
517,https://github.com/nltk/nltk/issues/2159,2159,"[{'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-10-24 09:29:27+00:00,,16,"Unable to find ""/usr/share/nltk_data"" when adding binary and data files.","I get this error when I try to create an executable of a python3 file with pyinstaller. 
Precisely I get:

> 38 INFO: PyInstaller: 3.4
> 38 INFO: Python: 3.6.6
> 39 INFO: Platform: Linux-4.15.0-36-generic-x86_64-with-Ubuntu-18.04-bionic
> 39 INFO: wrote /home/user/Documents/py_stuff/chatBotV3/broize.spec
> 40 INFO: UPX is not available.
> 41 INFO: Extending PYTHONPATH with paths
> ['/home/user/Documents/py_stuff/chatBotV3',
>  '/home/user/Documents/py_stuff/chatBotV3']
> 42 INFO: checking Analysis
> 42 INFO: Building Analysis because Analysis-00.toc is non existent
> 42 INFO: Initializing module dependency graph...
> 43 INFO: Initializing module graph hooks...
> 44 INFO: Analyzing base_library.zip ...
> 2756 INFO: running Analysis Analysis-00.toc
> 2787 INFO: Caching module hooks...
> 2793 INFO: Analyzing /home/user/Documents/py_stuff/chatBotV3/broize.py
> 3285 INFO: Processing pre-safe import module hook   six.moves
> 4363 INFO: Processing pre-find module path hook   distutils
> 5352 INFO: Processing pre-safe import module hook   setuptools.extern.six.moves
> 5725 INFO: Processing pre-find module path hook   site
> 5725 INFO: site: retargeting to fake-dir '/home/user/.local/lib/python3.6/site-packages/PyInstaller/fake-modules'
> 9893 INFO: Processing pre-safe import module hook   urllib3.packages.six.moves
> 13100 INFO: Loading module hooks...
> 13100 INFO: Loading module hook ""hook-numpy.core.py""...
> 13194 INFO: Loading module hook ""hook-xml.etree.cElementTree.py""...
> 13194 INFO: Loading module hook ""hook-encodings.py""...
> 13253 INFO: Loading module hook ""hook-xml.py""...
> 13254 INFO: Loading module hook ""hook-certifi.py""...
> 13255 INFO: Loading module hook ""hook-pkg_resources.py""...
> 13466 INFO: Processing pre-safe import module hook   win32com
> 13673 INFO: Loading module hook ""hook-pydoc.py""...
> 13673 INFO: Loading module hook ""hook-lib2to3.py""...
> 13674 INFO: Loading module hook ""hook-sqlite3.py""...
> 13724 INFO: Loading module hook ""hook-sysconfig.py""...
> 13733 INFO: Loading module hook ""hook-cryptography.py""...
> 13924 INFO: Loading module hook ""hook-pytest.py""...
> 14535 INFO: Loading module hook ""hook-distutils.py""...
> 14535 INFO: Loading module hook ""hook-nltk.py""...
> Unable to find ""/usr/share/nltk_data"" when adding binary and data files.
"
518,https://github.com/nltk/nltk/issues/2160,2160,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-10-25 16:49:52+00:00,,4,Maybe the corpus_bleu method is wrong,"The wrong file is nltk.translate.bleu_score.py.
I think the implementation of corpus_bleu is wrong .
When we calculate the corpus bleu,we need to take every sentence into consideration.
Assume that a corpus has 3 sentences and each sentence's word appear time is 2/5,3/6,4/8 (Here we only consider the unigram).
And all of the corpus' appear time is (2+3+4)/(5+6+8)=9/19.
If the above description is right, the implementation of the `corpus_bleu` is wrong.
Here is part of the code in `corpus_bleu`
```python
    # Iterate through each hypothesis and their corresponding references.
    for references, hypothesis in zip(list_of_references, hypotheses):
        # For each order of ngram, calculate the numerator and
        # denominator for the corpus-level modified precision.
        for i, _ in enumerate(weights, start=1):
            p_i = _modified_precision(references, hypothesis, i)
            p_numerators[i] += p_i.numerator
            p_denominators[i] += p_i.denominator

        # Calculate the hypothesis length and the closest reference length.
        # Adds them to the corpus-level hypothesis and reference counts.
        hyp_len = len(hypothesis)
        hyp_lengths += hyp_len
        ref_lengths += _closest_ref_length(references, hyp_len)

        # Calculate corpus-level brevity penalty.
    bp = _brevity_penalty(ref_lengths, hyp_lengths)
    # Collects the various precision values for the different ngram orders.
    p_n = [Fraction(p_numerators[i], p_denominators[i])
           for i, _ in enumerate(weights, start=1)]

```

The problem is that : The `Fraction` class will automatically simplify the fraction.
In the above example, Fraction(4/8) will become 1/2,and Fraction(3,6) will become 1/2.
So the final result will become :(2+1+1)/(5+2+2)=4/9. It is defenitely different from the above result.
I think the above method is right.
The main code is 
```python
            p_i = _modified_precision(references, hypothesis, i)
            p_numerators[i] += p_i.numerator
            p_denominators[i] += p_i.denominator
```
The `_modified_precision` method returns a `Fraction` and the numerator and denominator is wrong .
As as new bee in nlp,maybe I am wrong.Hoping someone can answer my question."
519,https://github.com/nltk/nltk/issues/2162,2162,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}]",open,2018-10-28 15:11:01+00:00,,13,Runtime warning when using nltk.downloader from CLI,"I'm not sure it's because my environment wasn't set up wrongly or because the `nltk.downloader` code is somehow violating Python causing the `RuntimeWarning`

```python
$ python3 -m nltk.downloader reuters
    /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour
      warn(RuntimeWarning(msg))
    [nltk_data] Downloading package reuters to
    [nltk_data]     /Users/liling.tan/nltk_data...
    [nltk_data]   Package reuters is already up-to-date!
```"
520,https://github.com/nltk/nltk/issues/2166,2166,[],closed,2018-10-31 11:55:31+00:00,,0,Bug in collocations.TrigramCollocationsFinder,"Hey everyone! 

Isn't it a bug [here](https://github.com/nltk/nltk/blob/develop/nltk/collocations.py#L221)? I think `wfd[w1] += 1` should be out of the loop which iterates over all combinations of `w2, w3`, no? Why overcounting single words? 🤔 


[upd]: seems like I get it, that's because we count non-contiguous ngrams"
521,https://github.com/nltk/nltk/issues/2167,2167,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-10-31 13:34:23+00:00,,3,Unable to find french synonym ,"```
import nltk
from nltk.corpus import wordnet
print(wordnet.synsets('bien', lang='fra'))
# OUTPUT
# [Synset('wholly.r.01'), Synset('well.r.01'), Synset('well.r.13'), Synset('well.r.02'), Synset('well.r.06'), Synset('well.r.05'), Synset('well.r.12'), Synset('well.r.11'), Synset('well.r.07'), Synset('well.r.10'), Synset('well.r.09'), Synset('well.r.08'), Synset('well.r.04'), Synset('okay.r.01'), Synset('possession.n.02'), Synset('very_well.r.02'), Synset('mindfully.r.01'), Synset('comfortably.r.02'), Synset('rightly.r.01'), Synset('smoothly.r.01'), Synset('rightfully.r.01'), Synset('palatably.r.01'), Synset('possession.n.07'), Synset('good.s.13'), Synset('sleep_together.v.01'), Synset('love.v.01'), Synset('love.v.03'), Synset('love.v.02'), Synset('all_right.s.01'), Synset('commodity.n.01'), Synset('well.n.04'), Synset('good.n.03'), Synset('possession.n.05'), Synset('oklahoma.n.01'), Synset('property.n.01')]
```

All the words in output is in english. Should it not be in French?

NLTK version: 3.2.5"
522,https://github.com/nltk/nltk/issues/2168,2168,[],closed,2018-11-02 14:22:29+00:00,,7,Code question,"Hi!

Could you explain the necessity of this code line?
https://github.com/nltk/nltk/blob/develop/nltk/internals.py#L30"
523,https://github.com/nltk/nltk/issues/2171,2171,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-11-08 20:14:28+00:00,,3,Wordnet: synset_from_sense_key returns the wrong Synset,"https://github.com/nltk/nltk/blob/15e7f74874cf562a44747957f006cf30869a15cd/nltk/corpus/reader/wordnet.py#L1532

**Issue**: synset_from_sense_key does not always return the correct Synset.

**Example**:

```python
sense_key = 'deal%1:10:00::'

synset_from_lemma = wn.lemma_from_key(sense_key).synset()
synset_from_key = wn.synset_from_sense_key(sense_key)

print(synset_from_lemma)
print(synset_from_key)
```
outputs

```
Synset('bargain.n.01')
Synset('deal.n.09')
```
While the same Synset should be returned by both

**Source of the issue**: It seems synset_from_sense_key directly concatenates the lemma name contained in the sense key to the newly created Synset ID, while the correct synset could be using an ID with a different main lemma."
524,https://github.com/nltk/nltk/issues/2177,2177,[],closed,2018-11-14 10:47:39+00:00,,3,documation issue with pos_tag,"in the `nltk.help.upenn_tagset('PRP$')` there is a spell issue:
PRP$: pronoun, possessive
    her his mine my our ours their **thy** your

suppose to be they"
525,https://github.com/nltk/nltk/issues/2178,2178,[],closed,2018-11-14 12:34:26+00:00,,6,HTTPError when trying to use CoreNLPParser with multiprocessing,"I'm using `from nltk.parse.corenlp import CoreNLPParser` for POS. To try and speed it up I thought I'd try and use multiprocessing. So I create the parser `parser = CoreNLPParser(url='http://localhost:9000', tagtype='pos')` and use `multiprocessing.Pool.map` where I pass a list of files and a `partial` of a function which processes each file (with the `parser` in the `partial`).

Unfortunately after running for a bit (maybe 30min using ~20 threads), it always crashes with

```
  File ""/usr/local/lib/python3.5/dist-packages/nltk/parse/corenlp.py"", line 382, in tag
    return self.tag_sents([sentence])[0]
  File ""/usr/local/lib/python3.5/dist-packages/nltk/parse/corenlp.py"", line 361, in tag_sents
    return [sentences[0] for sentences in self.raw_tag_sents(sentences)]
  File ""/usr/local/lib/python3.5/dist-packages/nltk/parse/corenlp.py"", line 361, in <listcomp>
    return [sentences[0] for sentences in self.raw_tag_sents(sentences)]
  File ""/usr/local/lib/python3.5/dist-packages/nltk/parse/corenlp.py"", line 401, in raw_tag_sents
    tagged_data = self.api_call(sentence, properties=default_properties)
  File ""/usr/local/lib/python3.5/dist-packages/nltk/parse/corenlp.py"", line 257, in api_call
    response.raise_for_status()
  File ""/home/seni/.local/lib/python3.5/site-packages/requests/models.py"", line 935, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: http://localhost:9000/?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22ssplit.isOneSentence%22%3A+%22true%22%2C+%22annotators%22%3A+%22tokenize%2Cssplit%2Cpos%22%7D

```

What could be going wrong?"
526,https://github.com/nltk/nltk/issues/2182,2182,[],closed,2018-11-19 00:12:20+00:00,,2,Code typo in CoreNLPParser.raw_parse_sents(),"I noticed an issue due to a typo in the code at: [nltk/parse/corenlp.py#L269](https://github.com/nltk/nltk/blob/94d2f365f7df4c7ef984f5e6b6bc43895cc66b71/nltk/parse/corenlp.py#L269).
**'ssplit.ssplit.eolonly'** is incorrect. Changing the property to **'ssplit.eolonly': 'true'** provides the correct behaviour.

Currently if the input sentences don't end with an EOS symbol then they are concatenated by the Core NLP server, meaning you get less parse results back than expected.

Reproducible example:

```
from nltk.parse.corenlp import CoreNLPParser, CoreNLPServer

_JAR_PATH = r'<path>\stanford-corenlp-full-2018-10-05\stanford-corenlp-3.9.2.jar'
_MODEL_PATH = r'<path>\stanford-corenlp-full-2018-10-05\stanford-corenlp-3.9.2-models.jar'

def parse_sents_example():
    sents = [['Latest', 'corporate', 'unbundler', 'reveals', 'etc', 'talks', 'to', 'Frank', 'Kane'],
             ['By', 'FRANK', 'KANE'],
             ['IT', 'SEEMS', 'that', 'Roland', 'Franklin', 'etc', 'packaging', 'group', 'DRG', '.'],
             ['He', 'has', 'not', 'properly', 'investigated', 'the', 'target', ""'s"", 'dining', 'facilities', '.']]
    parse_trees = []
    server = CoreNLPServer(path_to_jar=_JAR_PATH, path_to_models_jar=_MODEL_PATH)
    server.start()
    parser = CoreNLPParser(url='http://localhost:9000')

    # Default
    parse_results = parser.parse_sents(sents)
    for parse in parse_results:
        parse_trees.append(next(parse))
    print(len(parse_trees))   # Outputs 2 - first 3 sentences are concatenated

    parse_trees.clear()

    # Using correct property
    parse_results = parser.parse_sents(sents, properties={'ssplit.eolonly': 'true'})
    for parse in parse_results:
        parse_trees.append(next(parse))
    print(len(parse_trees))   # Outputs 4

    server.stop()

parse_sents_example()
```"
527,https://github.com/nltk/nltk/issues/2184,2184,[],closed,2018-11-19 23:07:37+00:00,,5,Language parameter not being passed in nltk.tag.__init__.pos_tag_sents(),"The `lang` parameter of pos_tag_sents() in nltk/tag/__init__.py is not being passed.

Coupled with the change to exception ordering in commit 69583ceaaaff7e51dd9f07f4f226d3a2b75bea69 (lines 110-116 of nltk/tag/__init__.py), this now results in an error of ""NotImplementedError('Currently, NLTK pos_tag only supports English and Russian (i.e. lang='eng' or lang='rus')'"" when tagging a sentence."
528,https://github.com/nltk/nltk/issues/2185,2185,[],closed,2018-11-20 13:39:16+00:00,,1,Tokenization - Custom Tokens,"I am a new to NLP and was reading through tokenization and wondering if we have a tokenizer existing in this framework that would actually consider a ""Keyword"" or set of keywords and splits the sentence or the entire paragraph based on the keyword?.

Any documentation help on the existing API would help. Appreciate your help!"
529,https://github.com/nltk/nltk/issues/2188,2188,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-11-22 19:01:29+00:00,,0,parallelism in nltk,"why you don't use any kind of parallelism in `nltk`?
Things like `word_tokenize` and other stuffs take so long time and I saw people wrote their helper for these tasks, I was wonder why you guys don't use any parallelism approach inside `nltk`?"
530,https://github.com/nltk/nltk/issues/2189,2189,"[{'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-11-23 09:06:15+00:00,,7,wordnet (value error),"Hi All,

i am using nltk 3.3 and python 3.6 versions
i am getting valueerror when i tried below code. can some one help me to rectify this error.

![value_error](https://user-images.githubusercontent.com/42936846/48935202-d2db7f80-ef2c-11e8-8c95-2bf76363cecb.PNG)


Thanks,
raja"
531,https://github.com/nltk/nltk/issues/2191,2191,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",closed,2018-11-27 13:00:38+00:00,,4,NLTK download method fails with error,"Please advise.

My code is 

```
import nltk
import ssl
print('The nltk version is {}.'.format(nltk.__version__))
ssl._create_default_https_context = ssl._create_unverified_context
nltk.download('all')

```
The nltk version is 3.3.
Traceback (most recent call last):
  File ""r.py"", line 7, in <module>
    nltk.download('all')
  File ""/usr/lib/python2.7/site-packages/nltk-3.3-py2.7.egg/nltk/downloader.py"", line 672, in download
    for msg in self.incr_download(info_or_id, download_dir, force):
  File ""/usr/lib/python2.7/site-packages/nltk-3.3-py2.7.egg/nltk/downloader.py"", line 540, in incr_download
    try: info = self._info_or_id(info_or_id)
  File ""/usr/lib/python2.7/site-packages/nltk-3.3-py2.7.egg/nltk/downloader.py"", line 514, in _info_or_id
    return self.info(info_or_id)
  File ""/usr/lib/python2.7/site-packages/nltk-3.3-py2.7.egg/nltk/downloader.py"", line 885, in info
    self._update_index()
  File ""/usr/lib/python2.7/site-packages/nltk-3.3-py2.7.egg/nltk/downloader.py"", line 833, in _update_index
    ElementTree.parse(urlopen(self._url)).getroot())
  File ""/usr/lib64/python2.7/xml/etree/ElementTree.py"", line 1182, in parse
    tree.parse(source, parser)
  File ""/usr/lib64/python2.7/xml/etree/ElementTree.py"", line 656, in parse
    parser.feed(data)
  File ""/usr/lib64/python2.7/xml/etree/ElementTree.py"", line 1642, in feed
    self._raiseerror(v)
  File ""/usr/lib64/python2.7/xml/etree/ElementTree.py"", line 1506, in _raiseerror
    raise err
xml.etree.ElementTree.ParseError: not well-formed (invalid token): line 9, column 50
"
532,https://github.com/nltk/nltk/issues/2192,2192,[],closed,2018-11-28 03:53:24+00:00,,2,Using REPP Tokenizer on Windows,"The [doc](https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software#repp-tokenizer) gives instructions for Linux and Mac users, but what about Windows?"
533,https://github.com/nltk/nltk/issues/2193,2193,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2018-11-29 09:16:06+00:00,,0,Cider score for Image Captioning is not present in NLTK,"Most of the papers I have seen in the area of Image Captioning use Bleu, Meteor and Cider. Bleu exists in NLTK. I have seen someone taking up Meteor. I think it is worthy to have Cider incorporated in NLTK.

Cider is implemented in this paper. 
https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vedantam_CIDEr_Consensus-Based_Image_2015_CVPR_paper.pdf"
534,https://github.com/nltk/nltk/issues/2194,2194,"[{'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-11-29 10:47:07+00:00,,5,Import Prolog parse tree into ntlk.Tree,"Hi, is it possible to import into nltk.Tree a parse tree generate by using Prolog parsing with DCG predicate?
I've the following parse tree obtained by a prolog script I've written:

```
?- sentence(ParseTree, [the, block, b, is, blue],[]).
ParseTree = sentence(assertion(subject_phrase(determiner(the), noun(block), name(b)), verb_phrase(verb(is)), complement_phrase(adjective(blue)))) .

```
I've created another project  that does the same thing but with nltk library and if I insert the sentence (""the block b is blue"") I get the following parse tree:

```
(A
  (SP (Det the) (Noun block) (Name b))
  (VP (V is))
  (CP (MP (Adj blue))))
```

Is there a way that I can use to import tree like `nltk.Tree.fromstring(parse_tree_in_prolog)`?
Thanks."
535,https://github.com/nltk/nltk/issues/2195,2195,[],closed,2018-11-30 18:22:22+00:00,,3,Dozens of test failures from unchanged develop branch,"I followed the instructions in `CONTRIBUTING.md` to setup up a development environment, and run the tests with `tox -e py36 > test_output.txt`. (Running python 3.6.6.)

Here are the full contents of test_output.txt:

[test_output.txt](https://github.com/nltk/nltk/files/2634537/test_output.txt)


Observations:
* Several deprecation warnings.
* Some resources, i.e. omw, must not have been installed when I called `python3 -m nltk.downloader tests`

Is this possibly related to the documentation being outdated as mentioned in #1853? Did I miss a step in the setup?
"
536,https://github.com/nltk/nltk/issues/2196,2196,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 1004152779, 'node_id': 'MDU6TGFiZWwxMDA0MTUyNzc5', 'url': 'https://api.github.com/repos/nltk/nltk/labels/text', 'name': 'text', 'color': 'bfdbfc', 'default': False, 'description': ''}]",closed,2018-11-30 18:34:26+00:00,,2,Programmatic access to collocation/concordance lists,"The functions `nltk.Text.collocations()` and `nltk.Text.concordances()` print their output directly to the console, but I would like to be able to access and manipulate those lists through code. I'd propose adding helper functions `get_collocations()` and `get_concordances()` that return lists of objects (`ConcordanceInfo` and `CollocationInfo`, or maybe just tuples) that contain raw information, and refactoring the existing methods to print output based on those helper results.

I'll open a PR soon when I start working on this."
537,https://github.com/nltk/nltk/issues/2200,2200,[],open,2018-12-03 15:29:26+00:00,,4,BigramCollocationFinder.score_ngrams with BigramAssocMeasure.likelihood_ratio raises ValueError: math domain error,"I want to compute bigram collocations per sentence and tried to output my demo results in a jupyter notebook. Only for those four/five sentence combinations and a window size of larger than 4, the ordering with loglikelihood-ratio fails. Probably because of a negative log computation.  
I'm not sure what to do at this point. Below my demo code to recreate the error.

```python
import collections
import nltk
from nltk.collocations import BigramCollocationFinder
from nltk.metrics import BigramAssocMeasures
from somajo import Tokenizer

sentences = [
    ""Ich gehe gerne nach Hause."",
    ""Arbeit an der Uni macht Spaß."",
    ""Ich esse gerne Eis."",
    #""Warum ist die Erde rund?"",
    ""Jemand anderes isst gerne Eis.""
]

sentences_tok = [tokenizer.tokenize(s) for s in sentences]
all_tok = [word for sentence in sentences_tok for word in sentence]

def compute_neighbor_collocations(sentences_tok, window_size=2):
    sentences_tok = iter(sentences_tok)
    
    sentence_tok = next(sentences_tok)
    first_finder = BigramCollocationFinder.from_words(sentence_tok, window_size=window_size)
    for sentence_tok in sentences_tok:
        finder = BigramCollocationFinder.from_words(sentence_tok, window_size=window_size)
        first_finder.ngram_fd.update(finder.ngram_fd)
        first_finder.word_fd.update(finder.word_fd)
    
    return first_finder

# window_size=2 raises no error
finder_sentence = compute_neighbor_collocations(sentences_tok, window_size=255)
scored_sentence = finder_sentence.score_ngrams(BigramAssocMeasures.likelihood_ratio)
```
```python
--------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-39-3af87a5a33c6> in <module>
      1 #finder_sentence.apply_freq_filter(3)
      2 #finder_sentence.nbest(BigramAssocMeasures.raw_freq, 100)
----> 3 scored_sentence = finder_sentence.score_ngrams(BigramAssocMeasures.likelihood_ratio)
      4 scored_sentence

/opt/miniconda3/envs/ekoerner/lib/python3.6/site-packages/nltk/collocations.py in score_ngrams(self, score_fn)
    119         lowest score, as determined by the scoring function provided.
    120         """"""
--> 121         return sorted(self._score_ngrams(score_fn), key=lambda t: (-t[1], t[0]))
    122 
    123     def nbest(self, score_fn, n):

/opt/miniconda3/envs/ekoerner/lib/python3.6/site-packages/nltk/collocations.py in _score_ngrams(self, score_fn)
    111         """"""
    112         for tup in self.ngram_fd:
--> 113             score = self.score_ngram(score_fn, *tup)
    114             if score is not None:
    115                 yield tup, score

/opt/miniconda3/envs/ekoerner/lib/python3.6/site-packages/nltk/collocations.py in score_ngram(self, score_fn, w1, w2)
    183         n_ix = self.word_fd[w1]
    184         n_xi = self.word_fd[w2]
--> 185         return score_fn(n_ii, (n_ix, n_xi), n_all)
    186 
    187 

/opt/miniconda3/envs/ekoerner/lib/python3.6/site-packages/nltk/metrics/association.py in likelihood_ratio(cls, *marginals)
    141         return (cls._n *
    142                 sum(obs * _ln(obs / (exp + _SMALL) + _SMALL)
--> 143                     for obs, exp in zip(cont, cls._expected_values(cont))))
    144 
    145     @classmethod

/opt/miniconda3/envs/ekoerner/lib/python3.6/site-packages/nltk/metrics/association.py in <genexpr>(.0)
    141         return (cls._n *
    142                 sum(obs * _ln(obs / (exp + _SMALL) + _SMALL)
--> 143                     for obs, exp in zip(cont, cls._expected_values(cont))))
    144 
    145     @classmethod

ValueError: math domain error
```"
538,https://github.com/nltk/nltk/issues/2201,2201,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-12-09 11:44:32+00:00,,2,TreebankWordDetokenizer or MosesDetokenizer?,"According to Issue #2000, the Moses Tokenizer has been removed out of NLTK since the incompatibility of its licence, but the docstring of nltk.tokenize.treebank.TreebankWordDetokenizer says it's a ""Python port of the Moses detokenizer"". (or Moses Tokenizer's adapted version of TreebankWordDetokenizer?)

```
def tokenize(self, tokens, convert_parentheses=False):
        """"""
        Python port of the Moses detokenizer.

        :param tokens: A list of strings, i.e. tokenized text.
        :type tokens: list(str)
        :return: str
        """"""
```
Should this detokenizer be also removed?"
539,https://github.com/nltk/nltk/issues/2202,2202,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",open,2018-12-11 08:41:10+00:00,,9,"Improve tokenization of Multi Word Expressions by including ""python partitioner""","I suspect that @jakerylandwilliams & @andyreagan's https://github.com/jakerylandwilliams/partitioner could significantly improve the tokenization quality of NLTK, specifically when it comes to MWEs (Multi Word Expressions).

@NeelShah18 recently ported it to Python 3:

https://github.com/jakerylandwilliams/partitioner/pull/7

So, including it in NLTK should seem easy enough.

For more information on the approach used there, see here:

https://noisy-text.github.io/2017/pdf/WNUT01.pdf

And here:
https://arxiv.org/abs/1710.07729

It's Apache 2.0 licensed, so the licenses seem compatible as well.
"
540,https://github.com/nltk/nltk/issues/2203,2203,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2018-12-11 09:16:47+00:00,,10,Include/port SyllabiPy (which includes LegaliPy) for Syllable Tokenization,"See here for the project by @henchc:

https://github.com/henchc/syllabipy

Here for the project homepage:

http://syllabipy.com/

And here for the research paper both originate from:
https://escholarship.org/uc/item/13c6h2z2

See PDF page 44 (document page 31) for a schematic of how it works.

Incorporating this would definitely require some refactoring (throwing out util.py to make the tokenizer freely chosable, for example) and properly abstracting the current hardcoding. @Yomguithereal seems to have already done this for their port of it into their Talisman library:

https://github.com/Yomguithereal/talisman/tree/master/src/tokenizers/syllables

However, Talisman uses Javascript - otherwise I'd just suggest including most of Talisman (albeit not all, since Talisman itself uses NLTK, so that'd become a tad bit too recursive.😉) in NLTK. 😉

Both Talisman & SyllabiPy/LegaliPy use the MIT license, which should have compatibility.
"
541,https://github.com/nltk/nltk/issues/2204,2204,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}]",closed,2018-12-16 14:23:55+00:00,,4,ZeroDivisionError when computing bleu_score,"Python version: 3.5
NLTK version: 3.2.5
```txt
  File ""/usr/local/lib/python3.5/dist-packages/nltk/translate/bleu_score.py"", line 89, in sentence_bleu
    emulate_multibleu)
  File ""/usr/local/lib/python3.5/dist-packages/nltk/translate/bleu_score.py"", line 199, in corpus_bleu
    hyp_len=hyp_len, emulate_multibleu=emulate_multibleu)
  File ""/usr/local/lib/python3.5/dist-packages/nltk/translate/bleu_score.py"", line 544, in method4
    incvnt = i+1 * self.k / math.log(hyp_len) # Note that this K is different from the K from NIST.
ZeroDivisionError: float division by zero
```"
542,https://github.com/nltk/nltk/issues/2209,2209,"[{'id': 718746024, 'node_id': 'MDU6TGFiZWw3MTg3NDYwMjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stem/lemma', 'name': 'stem/lemma', 'color': '53bab8', 'default': False, 'description': None}]",open,2018-12-27 09:25:35+00:00,,3,Lemmatizer on contractions and some pronouns,"Hello,

The WordNetLemmatizer does not seem to account for contractions like ""'ll"" or pronouns like ""her"".

Usage:
`lemmatizer.lemmatize(""'ll"")` returns `""'ll""` when expected `""will""`
`lemmatizer.lemmatize(""her"")` returns `""her""` when expected `""she""`
"
543,https://github.com/nltk/nltk/issues/2210,2210,[],open,2019-01-03 21:01:14+00:00,,0,Adding Constraint to Parser in nltk.parse.corenlp,"When using CoreNLP's standard Java interface, constraints can be used to increase the accuracy of parse results based on known characteristics of a given data set. For instance, if a user knows that all sentences have a finite matrix clause, the parser can be constrained to produce an `S` label at root level, and then rank the most probable parses accordingly.

See [this test file](https://github.com/stanfordnlp/CoreNLP/blob/master/itest/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserITest.java) hosted on CoreNLP's Github, for the shift reduce parser in particular.

I'd like to impose similar constraints on my parser output, but retain the functionality of NLTK and Python codebase kindly made possible by the nltk.parse.corenlp module. Currently, I establish a connection to the parser as follows

```
$ java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -annotators ""parse"" -port 9000 -timeout 30000`
```

and then set up the NLTK interface with

```
from nltk.parse.corenlp import CoreNLPParser
parser = CoreNLPParser(url='http://localhost:9000')
```

Since my only interaction with CoreNLP's Java underpinnings is the command line call to the server, I do not see any way to modify the input to the nltk.parse.corenlp interface such that a constraint could be added to the parser. Is there a way to add a contraint from within the NLTK interface? If not, is there a way to pass a constraint to the Java side, for instance, when establishing the CoreNLP server connection?

Thank you very much!"
544,https://github.com/nltk/nltk/issues/2212,2212,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",open,2019-01-09 02:01:25+00:00,,4,Error loading home: Package 'home' not found in index and nltk.data.path becomes empty,"I want to use `nltk.pos_tag`, and I have downloaded `punkt` and `averaged_perceptron_tagger`. My codes work fine on MacOS. Then I copy my `nltk_data` folder to another linux and configure the nltk.data.path. When I run the same codes again, the `nltk.word_tokenize` can work fine but `nltk.post_tag` triggers an error 
```
LookupError:
**********************************************************************
  Resource \u001b[93mhome\u001b[0m not found.
  Please use the NLTK Downloader to obtain the resource:
  \u001b[31m>>> import nltk
  >>> nltk.download('home')
  \u001b[0m
  Attempted to load \u001b[93m/home/admin/work/nltk_data.zip/nltk_data/taggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m
  Searched in:
    - u''
**********************************************************************
```
However, when I try to run `nltk.download('home')`, I have another error:
```
Error loading home: Package 'home' not found in index
```
And I feel very strange the search path becomes empty.
Anyone can give me some suggestions?"
545,https://github.com/nltk/nltk/issues/2213,2213,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}]",closed,2019-01-10 08:20:35+00:00,,7,nltk  affects lower() function,"When I import nltk, lower() function doesn't work fine. For example:

s=""What's the best way to archive GIF's?  I zip them""
print(s.lower())

output: what's the best way to archive gIf's? I zip them

It doesn't lowercase 'I' character. Why?"
546,https://github.com/nltk/nltk/issues/2214,2214,[],open,2019-01-13 21:44:13+00:00,,0,String to tree: ValueError: range() arg 3 must not be zero,"```
nltk.tree.Tree.fromstring('''(S
  (NP (Det the) (N dog))
  (VP
      (V saw)
      (NP (Det a) (N man))
      (PP (P in) (NP (Det the) (N park)))
  ))''')
```

Raises ValueError: range() arg 3 must not be zero.

This is associated with the example taken from http://www.nltk.org/book/ch08.html. 

```
grammar1 = nltk.CFG.fromstring(""""""
  S -> NP VP
  VP -> V NP | V NP PP
  PP -> P NP
  V -> ""saw"" | ""ate"" | ""walked""
  NP -> ""John"" | ""Mary"" | ""Bob"" | Det N | Det N PP
  Det -> ""a"" | ""an"" | ""the"" | ""my""
  N -> ""man"" | ""dog"" | ""cat"" | ""telescope"" | ""park""
  P -> ""in"" | ""on"" | ""by"" | ""with""
  """""")

sent = ""the dog saw a man in the park"".split()
rd_parser = nltk.RecursiveDescentParser(grammar1)
for tree in rd_parser.parse(sent):
    print(tree)
```

Printing the tree can be displayed correctly. However, when calling 

```
display.display(tree) # IPython.display
```

Same error was raised.

NLTK version 3.3. Python version 3.6.5 Anaconda distribution. MacOSX 10.14.2"
547,https://github.com/nltk/nltk/issues/2215,2215,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",open,2019-01-15 02:09:59+00:00,,0,Deprecating REPP wrapper and using pydelphin,"The original C++ REPP is nice but it doesn't have continuous support for different OS, c.f. #1985 #2192 

It would be nice to deprecate the wrapper and incorporate REPP from `pydelphin` into NLTK to replace the current wrapper we have now: https://github.com/delph-in/pydelphin/issues/202 "
548,https://github.com/nltk/nltk/issues/2217,2217,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2019-01-23 03:40:44+00:00,,3,"Why the same content, but the low score?","I tried to run the code as follow:

```
from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu
sentence_bleu([['left', 'right']], ['left', 'right'], smoothing_function=cc.method0)
```
```
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  the n-gram count for each individual precision score would be:
/home/yzhou/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  the n-gram count for each individual precision score would be:
1.491668146240062e-154
```

I wonder why the BLEU score is so low, and how can I change the score to 1?"
549,https://github.com/nltk/nltk/issues/2219,2219,[],open,2019-01-23 19:20:53+00:00,,2,Wordnet : synset not found for many existing sense keys,"The function nltk.corpus.wordnet.lemma_from_key(key) does not return synsets for many valid sensekeys.
`print wn.lemma_from_key(hot%3:00:00:tasty:00).synset()`
prints out 
`WordNetError: No synset found for key 'hot%3:00:00:tasty:00'`
There are many similar keys resulting the same outcome.
Also I am using WordNet 3.1 princeton web search to verify the validity of keys. But nltk has a lower wordnet version (3.0). I wonder if this is causing the problem. If I am right can we get the 3.1 upgrade on nltk soon? Desperately need this for research project.
Some other keys are:
need%2:37:00::
fair%3:00:00:beautiful:00
clarify%2:30:00::
previous%5:00:00:preceding(a):00
old%3:00:00:familiar:00
And so on..."
550,https://github.com/nltk/nltk/issues/2220,2220,[],closed,2019-01-24 08:23:11+00:00,,1,Treebank detokenizer parenthesis issue,"Python version: Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)] on win32
NLTK version: 3.4

Parentheses in the input are not correctly detokenized.

```python
from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer
t = TreebankWordTokenizer()
d = TreebankWordDetokenizer()
d.detokenize(t.tokenize(""Hello (world)""))
```
Output: `'Hello (world )'` (notice the space)"
551,https://github.com/nltk/nltk/issues/2221,2221,[],closed,2019-01-29 06:48:41+00:00,,0,Minimum Edit Distance Word Alignment,"I've been using NLTK for work and one of the features that would be quite useful is aligning words / characters between two strings by using a minimum edit distance. The alignment tools I came across in NLTK were looking at translations between two different languages. In my case, I was looking at the quality of OCR'd documents and needed to compare the output with a reference and highlight the changes.

If accepted, I would like to work on this issue. It would involve using wagner-fischer algorithm to create the optimal alignment followed by a backtrace to retrieve it."
552,https://github.com/nltk/nltk/issues/2222,2222,[],open,2019-01-31 00:54:48+00:00,,0,TweetTokenizer and punctuation inside URLs,"The twitter tokenizer exhibits (what I think is) undesirable behavior when tokenizing URLs. For example:
```
tok.tokenize('http://t.co/LYsklSmIVS “http://t.co/LYsklSmIVS” “http://t.co/LYsklSmIVS”xxx')
```
yields
```
['http://t.co/LYsklSmIVS',  '“',  'http://t.co/LYsklSmIVS',  '”',  '“',  'http://t.co/LYsklSmIVS”xxx']
```
where
```
['http://t.co/LYsklSmIVS',  '“',  'http://t.co/LYsklSmIVS',  '”',  '“',  'http://t.co/LYsklSmIVS',  '”',  'xxx']```
is what I would prefer.

The issue is that the regular expression used to tokenize URLs looks for the longest substring that could be an URL, and technically most punctuation marks can occur inside an URL. The regex does make an exception for a single punctuation mark at the end of an URL before a word break, but that doesn't help if there's a space missing before the next token.

The current URL matcher is, in my opinion, too greedy for working with casual online texts.  While it's legally possible for an URL to have a character like `”` in the middle of it, it's much more likely that `”` ought to be split off as a separate token.

In a way, parsing URLs in tweets should be trivial (because they all take the form `http://t.co/...`) and there's no real need for any fancy URL-matching regex.  But, on the other hand, twitter might change their URL format at any time and people might be using this tagger for parsing texts from other sites, so we don't want to make too many assumptions.

Any thoughts about the best way to handle this?  "
553,https://github.com/nltk/nltk/issues/2223,2223,[],closed,2019-02-01 19:47:19+00:00,,5,python 3.7 compatibility PEP 479: ngrams,"Hi,

Similar to #2148 I just found that using the command _ngrams_ is also raising the RuntimeError: generator raised StopIteration. This happens when I try to loop over the ngrams generated, for example:

`data = ngrams(sentence, 2) \\
for gram in data: \\
    print(gram)`

And the error happens at the second line.

Could you look into that issue as well? Thanks! 

PS: I tried re-installing the package on Feb 1, 2019 and the problem seems to be there still

"
554,https://github.com/nltk/nltk/issues/2224,2224,[],closed,2019-02-03 09:47:03+00:00,,0,Minor typo in the docstring of common_contexts(),"The parameter name is `words`, but the docstring uses `word`.

![image](https://user-images.githubusercontent.com/22998607/52175231-63de9a80-27a9-11e9-98da-6ebc77dcbc9d.png)
"
555,https://github.com/nltk/nltk/issues/2228,2228,"[{'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",closed,2019-02-08 18:03:51+00:00,,4,make it installable with pip or at least link download page,"To reproduce:

- start with an environment without `nltk`
- run `sudo pip install nltk`
- create file with simple `nltk` script (see below)
- run this script

I tested with 

```import nltk

print(nltk.word_tokenize(""test string     aaaaa\n\n\nfinal.""))
``` 

based on https://stackoverflow.com/a/37559340/4130619


I got error where relevant part is 
```
LookupError: 
**********************************************************************
  Resource punkt not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('punkt')
  
  Attempted to load tokenizers/punkt/PY3/english.pickle
```

It is surprising as I would expect pip to install package and all necessary resources, not only parts of it.

If it is really necessary to make installation more complicated - please link the real download page or help page in the error message.

full error message: https://pastebin.com/raw/VsjrxvZ5"
556,https://github.com/nltk/nltk/issues/2231,2231,[],closed,2019-02-10 16:38:52+00:00,,0,IndexError: string index out of range,"I have been trying to execute the example present on the main website of [https://www.nltk.org/](url).
This is my code. 

```
import nltk
 import Tkinter
 Sentence = ''' I live on Mars. '''
 tokens = nltk.word_tokenize('Sentence')
 tagged = nltk.pos_tag('tokens')
 entities = nltk.chunk.ne_chunk('tagged')[0:6]
 from nltk.corpus import treebank
 t = treebank.parsed_sents('wsj_0001.mrg')[0]
 t.draw()



This is the Terminal Output of the Error. 

> Traceback (most recent call last):
  File ""NLTK.py"", line 11, in <module>
    entities = nltk.chunk.ne_chunk('tagged')[0:6]
  File ""/Library/Python/2.7/site-packages/nltk/chunk/__init__.py"", line 186, in ne_chunk
    return chunker.parse(tagged_tokens)
  File ""/Library/Python/2.7/site-packages/nltk/chunk/named_entity.py"", line 128, in parse
    tagged = self._tagger.tag(tokens)
  File ""/Library/Python/2.7/site-packages/nltk/tag/sequential.py"", line 64, in tag
    tags.append(self.tag_one(tokens, i, tags))
  File ""/Library/Python/2.7/site-packages/nltk/tag/sequential.py"", line 84, in tag_one
    tag = tagger.choose_tag(tokens, index, history)
  File ""/Library/Python/2.7/site-packages/nltk/tag/sequential.py"", line 652, in choose_tag
    featureset = self.feature_detector(tokens, index, history)
  File ""/Library/Python/2.7/site-packages/nltk/tag/sequential.py"", line 699, in feature_detector
    return self._feature_detector(tokens, index, history)
  File ""/Library/Python/2.7/site-packages/nltk/chunk/named_entity.py"", line 59, in _feature_detector
    pos = simplify_pos(tokens[index][1])
IndexError: string index out of range
"
557,https://github.com/nltk/nltk/issues/2232,2232,[],closed,2019-02-11 14:41:01+00:00,,1,Uninstall,"How can you uninstall nltk, after you've previously run
`sudo python setup.y install`
?"
558,https://github.com/nltk/nltk/issues/2234,2234,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2019-02-14 11:09:03+00:00,,2,Get raw sentences,How can I get a list of raw sentences from a piece of text after using sentence splitting 'ssplit' ?
559,https://github.com/nltk/nltk/issues/2235,2235,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",open,2019-02-15 14:21:56+00:00,,0,Uniform interface for similarity measures in Wordnet,"Hello,

played around a little with NLTK and noticed that similarity measures in Wordnet behave differently when comparing similarity of different parts-of-speech - say, a noun and a verb. Some quietly return None, some throw an exception.

For example - having

noun = nltk.corpus.wordnet.synset('table.n.01')
verb = nltk.corpus.wordnet.synset('go.v.01')

and

ic = nltk.corpus.wordnet_ic.ic(""ic_brown.dat"")

the different methods to calculate similarity will behave as follows:

noun.jcn_similarity(verb, ic) # throws a WordNetError exception
noun.lch_similarity(verb) # throws a WordNetError exception
noun.lin_similarity(verb, ic) # throws a WordNetError exception
noun.path_similarity(verb) # returns None
noun.res_similarity(verb, ic) # throws a WordNetError exception
noun.wup_similarity(verb) # returns None

So, 2 out of 6 similarity measures return None, and 4 of 6 throw an exception in this case. What is the preferred behaviour for this case and why should not it be uniform for all the similarity measures?"
560,https://github.com/nltk/nltk/issues/2237,2237,[],closed,2019-02-19 03:42:50+00:00,,0,childes URL is out of date and invalid,nltk/corpus/reader/childes.py needs to be fixed to look for CHILDES data at https://childes.talkbank.org rather than http://childes.psy.cmu.edu because we renamed the server some years ago.
561,https://github.com/nltk/nltk/issues/2239,2239,[],closed,2019-02-23 22:07:23+00:00,,2,Plot methods don't return Axes object,"The `.plot()` methods on `FreqDist` and `ConditionalFreqDist` currently return `None`. This makes it difficult to customize the resulting plot.

In addition, returning `None` here is counterintuitive to users coming from other scientific computing packages. As an example, Matplotlib, Pandas, and Seaborn, all typically return an `Axes` object from their plotting methods.

I propose changing the `.plot()` methods to return an `Axes` object."
562,https://github.com/nltk/nltk/issues/2241,2241,[],open,2019-02-24 20:22:56+00:00,,2,Add words to the Portuguese stopword list,"A few very common words in Portuguese are not included in the stopword list. At least `é`, `ser` and `ter` should be included for consistency, since these are verbs whose other inflected forms are already included."
563,https://github.com/nltk/nltk/issues/2242,2242,[],closed,2019-02-26 10:41:04+00:00,,2,singledispatch dependency not needed for Python >= 3.4,"The `singledispatch` dependency (introduced in #2077) is only used in `lm/vocabulary.py` (though I might be missing something), and it is only needed for Python 2.6 - 3.3 (it has become a part of the built-in library `functools` since 3.4), so I suppose that this dependency should be inlcuded for Python < 3.4 only in `setup.py` with `'singledispatch; python_version < ""3.4""'`."
564,https://github.com/nltk/nltk/issues/2244,2244,"[{'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}]",open,2019-02-27 06:36:48+00:00,,0,CoreNLPServer on a non-default port,"If port is specified for `CoreNLPServer`, it is tested, put into the URL, but never put into `corenlp_options` ([here](https://github.com/nltk/nltk/blob/ac6a2394800138e4638279abc47a2394093cca72/nltk/parse/corenlp.py#L84)). Thus, `CoreNLPServer(port=9001).url` will be `http://localhost:9001`, but it will actually listen at `9000`. Should there not be another line there, to the effect of

    corenlp_options.extend(['-port', str(port)])

Similarly, two lines above, in case `None` was passed and the default `9000` is not available, `try_port`  tries to discover an available port; it is appended to `corenlp_options`, but without the `-port` option, and thus AFAIK does not have an effect. That line, too, should likely be changed to the line quoted above."
565,https://github.com/nltk/nltk/issues/2245,2245,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2019-03-03 03:55:36+00:00,,2,the BLEU socre calculated by the version 3.4 is different from the version 3.2,"run this code:
from nltk.translate.bleu_score import sentence_bleu
reference = [['the', 'cat',""is"",""sitting"",""on"",""the"",""mat""]]
test = [""on"",'the',""mat"",""is"",""a"",""cat""]
score = sentence_bleu(  reference, test)
print(score)

version 3.2 print:
0.4548019047027907
/home/jren/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: 
Corpus/Sentence contains 0 counts of 4-gram overlaps.
BLEU scores might be undesirable; use SmoothingFunction().
  warnings.warn(_msg)

version 3.4 print:
D:\ProgramData\Anaconda3\lib\site-packages\nltk\translate\bleu_score.py:523: UserWarning:
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
5.5546715329196825e-78"
566,https://github.com/nltk/nltk/issues/2247,2247,[],closed,2019-03-06 11:37:50+00:00,,3,Making dispersion_plot faster / leaner / more readable,"I noticed that the data preparation part of `nltk.draw.dispersion_plot` does repeated passes over the data and allocates some of the intermediate results, which seems unnecessary:

https://github.com/nltk/nltk/blob/39d8eeb2f249ad67d11300d97971f972039bdc63/nltk/draw/dispersion.py#L33-L52

I also find the nested listcomp which creates the `points` and then the `list(zip(*...))` idiom hard to read, though I admit this may be subjective. But more to the point, these seem like an unnecessarily convoluted way of going about generating `x` and `y`, which are then used for plotting.

Is there any particular reason for doing it this way that I may be missing/unaware of? If not, would you be generally open to a pull request along the following lines (replacing the snippet above)?

```python
words_dict = {word.casefold() if ignore_case else word: y_i
              for y_i, word in enumerate(reversed(words))}
x, y = [], []
for x_i, token in enumerate(text):
    token = token.casefold() if ignore_case else token
    y_i = words_dict.get(token)
    if y_i is not None:
        x.append(x_i)
        y.append(y_i)
```

Using the following sample data, `%%timeit` tells me it's about 400 ms (former snippet) vs 300 ms (latter snippet), but YMMV of course :)

```python
from nltk.corpus import inaugural
text = inaugural.words()
words = [""America"", ""duties"", ""citizen"", ""values""]
ignore_case = False
```"
567,https://github.com/nltk/nltk/issues/2248,2248,[],closed,2019-03-08 15:10:08+00:00,,1,Enhancement: K means clusterer random seed initialization,"I would like to have the option to initialize the random seed of the K-means clusterer in an easier manner, to allow reproducibility of the results. "
568,https://github.com/nltk/nltk/issues/2249,2249,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2019-03-09 13:06:18+00:00,,0,CHILDESCorpusReader tagged_sents/tagged_words: broken replacement feature,"Hi,

While using the `tagged_sents` and `tagged_words` functions of `CHILDESCorpusReader`, I ran into a problem with replacement comments within the utterance lines. 
When I call `tagged_sents(replace=False)`, the replacement comments are included as additional words in the output instead of being skipped. With `tagged_sents(replace=True)`, *every* word in the sentence is replaced, not just the one that the replacement comment belongs to. The same behaviour occurs with `tagged_words`.

Python version: 3.7.2 (via Anaconda)
NLTK version: 3.4

An example with data from [Brown/Adam/030916](https://childes.talkbank.org/browser/index.php?url=Eng-NA/Brown/Adam/030916.cha):

Lines in the CHAT file:
```
235	*CHI:	what is dat [: that] inside his eye ?
...
826	*CHI:	how do they put their foots [: feet] [*] in there ?
```

The relevant parts of the corresponding XML file, saved as `data/test.xml`:
```
<?xml version=""1.0"" encoding=""UTF-8""?>
<CHAT xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
      xmlns=""http://www.talkbank.org/ns/talkbank""
      xsi:schemaLocation=""http://www.talkbank.org/ns/talkbank https://talkbank.org/software/talkbank.xsd""
      PID=""11312/c-00015669-1""
      Version=""2.7.1""
      Lang=""eng""
      Corpus=""Brown""
      Date=""1964-04-20"">

  <u who=""CHI"" uID=""u63"">
    <w>what<mor type=""mor""><mw><pos><c>pro</c><s>int</s></pos><stem>what</stem></mw><gra type=""gra"" index=""1"" head=""2"" relation=""SUBJ""/></mor></w>
    <w>is<mor type=""mor""><mw><pos><c>cop</c></pos><stem>be</stem><mk type=""sfxf"">3S</mk></mw><gra type=""gra"" index=""2"" head=""0"" relation=""ROOT""/></mor></w>
    <w>dat<replacement><w>that<mor type=""mor""><mw><pos><c>pro</c><s>dem</s></pos><stem>that</stem></mw><gra type=""gra"" index=""3"" head=""2"" relation=""PRED""/></mor></w></replacement></w>
    <w>inside<mor type=""mor""><mw><pos><c>prep</c></pos><stem>inside</stem></mw><gra type=""gra"" index=""4"" head=""3"" relation=""JCT""/></mor></w>
    <w>his<mor type=""mor""><mw><pos><c>det</c><s>poss</s></pos><stem>his</stem></mw><gra type=""gra"" index=""5"" head=""6"" relation=""MOD""/></mor></w>
    <w>eye<mor type=""mor""><mw><pos><c>n</c></pos><stem>eye</stem></mw><gra type=""gra"" index=""6"" head=""4"" relation=""POBJ""/></mor></w>
    <t type=""q""><mor type=""mor""><mt type=""q""/><gra type=""gra"" index=""7"" head=""2"" relation=""PUNCT""/></mor></t>
  </u>
  <u who=""CHI"" uID=""u232"">
    <w>how<mor type=""mor""><mw><pos><c>pro</c><s>rel</s></pos><stem>how</stem></mw><gra type=""gra"" index=""1"" head=""4"" relation=""LINK""/></mor></w>
    <w>do<mor type=""mor""><mw><pos><c>mod</c></pos><stem>do</stem></mw><gra type=""gra"" index=""2"" head=""4"" relation=""AUX""/></mor></w>
    <w>they<mor type=""mor""><mw><pos><c>pro</c><s>sub</s></pos><stem>they</stem></mw><gra type=""gra"" index=""3"" head=""4"" relation=""SUBJ""/></mor></w>
    <w>put<mor type=""mor""><mw><pos><c>v</c></pos><stem>put</stem><mk type=""sfxf"">ZERO</mk></mw><gra type=""gra"" index=""4"" head=""0"" relation=""ROOT""/></mor></w>
    <w>their<mor type=""mor""><mw><pos><c>det</c><s>poss</s></pos><stem>their</stem></mw><gra type=""gra"" index=""5"" head=""6"" relation=""MOD""/></mor></w>
    <g><w>foots<replacement><w>feet<mor type=""mor""><mw><pos><c>n</c></pos><stem>foot</stem><mk type=""sfxf"">PL</mk></mw><gra type=""gra"" index=""6"" head=""4"" relation=""OBJ""/></mor></w></replacement></w><error/></g>
    <w>in<mor type=""mor""><mw><pos><c>prep</c></pos><stem>in</stem></mw><gra type=""gra"" index=""7"" head=""4"" relation=""JCT""/></mor></w>
    <w>there<mor type=""mor""><mw><pos><c>adv</c></pos><stem>there</stem></mw><gra type=""gra"" index=""8"" head=""7"" relation=""POBJ""/></mor></w>
    <t type=""q""><mor type=""mor""><mt type=""q""/><gra type=""gra"" index=""9"" head=""4"" relation=""PUNCT""/></mor></t>
  </u>
</CHAT>
```

Python input/output:
```
>>> from nltk.corpus.reader import CHILDESCorpusReader
>>> reader = CHILDESCorpusReader('data', 'test.xml')
>>> for s in reader.tagged_sents():
...     print(s)
...
[('what', 'pro:int'), ('is', 'cop'), ('dat', 'pro:dem'), ('that', 'pro:dem'), ('inside', 'prep'), ('his', 'det:poss'), ('eye', 'n')]
[('how', 'pro:rel'), ('do', 'mod'), ('they', 'pro:sub'), ('put', 'v'), ('their', 'det:poss'), ('foots', 'n'), ('feet', 'n'), ('in', 'prep'), ('there', 'adv')]
>>> for s in reader.tagged_sents(replace=True):
...     print(s)
...
[('that', 'pro:dem'), ('that', 'pro:dem'), ('that', 'pro:dem'), ('that', 'pro:dem'), ('that', 'pro:dem'), ('that', 'pro:dem'), ('that', 'pro:dem')]
[('feet', 'n'), ('feet', 'n'), ('feet', 'n'), ('feet', 'n'), ('feet', 'n'), ('feet', 'n'), ('feet', 'n'), ('feet', 'n'), ('feet', 'n')]
```

Versus the output I would expect:
```
# replacement=False (default):
[('what', 'pro:int'), ('is', 'cop'), ('dat', 'pro:dem'), ('inside', 'prep'), ('his', 'det:poss'), ('eye', 'n')]
[('how', 'pro:rel'), ('do', 'mod'), ('they', 'pro:sub'), ('put', 'v'), ('their', 'det:poss'), ('foots', 'n'), ('in', 'prep'), ('there', 'adv')]
# replacement=True:
[('what', 'pro:int'), ('is', 'cop'), ('that', 'pro:dem'), ('inside', 'prep'), ('his', 'det:poss'), ('eye', 'n')]
[('how', 'pro:rel'), ('do', 'mod'), ('they', 'pro:sub'), ('put', 'v'), ('their', 'det:poss'), ('feet', 'n'), ('in', 'prep'), ('there', 'adv')]
```"
569,https://github.com/nltk/nltk/issues/2250,2250,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",open,2019-03-10 09:47:05+00:00,,0,PunktTokenizer does not use the correct version of the pickled model on Python 3.x,"Hi, I'm trying to package my program with NLTK and nltk_data using PyInstaller. So to minimize the size of the data file, I removed the zip file and all models for Python 2.x in `nltk_data/tokenizers/punkt` (only the `PY3` folder is left).

But it seems that the `PunktTokenizer` always uses the Python 2.x version of the pickled model regardless of Python version I'm using. And the error message says that it can't find `tokenizers/punkt/english.pickle` instead of `tokenizers/punkt/PY3/english.pickle`.

Removing the `PY3` folder is okay, so it seems that the Python 3 version of the pickled model is never used.

OS: Windows 10 64-bit
Python version: 3.7.2 64-bit
NLTK version: 3.4"
570,https://github.com/nltk/nltk/issues/2251,2251,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 81645781, 'node_id': 'MDU6TGFiZWw4MTY0NTc4MQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/language-model', 'name': 'language-model', 'color': 'd4c5f9', 'default': False, 'description': ''}, {'id': 1004152779, 'node_id': 'MDU6TGFiZWwxMDA0MTUyNzc5', 'url': 'https://api.github.com/repos/nltk/nltk/labels/text', 'name': 'text', 'color': 'bfdbfc', 'default': False, 'description': ''}]",closed,2019-03-17 09:46:57+00:00,,5,text.generate,"Look into reinstating ```text.generate()``` now that the language model package ```lm``` is available.

cf https://github.com/nltk/nltk/commit/73f7e7b9ec301c39dc11035f138a27feb3c436cf#diff-f5cb9102f911e93ca5fefa00b56a1e3f
"
571,https://github.com/nltk/nltk/issues/2252,2252,[],closed,2019-03-17 10:04:48+00:00,,1,It still isn't working for me I have tried all of the above ones and yet I still get,"It still isn't working for me I have tried all of the above ones and yet I still get 
""module 'nltk' has no attribute 'internals' ""

_Originally posted by @ajaykumar-br in https://github.com/nltk/nltk/issues/1961#issuecomment-473650946_"
572,https://github.com/nltk/nltk/issues/2255,2255,[],open,2019-03-21 12:36:46+00:00,,0,update prior in hmm unsupervised,"In relation to the comment in line 1074 in hmm.py:

> ""Rabiner says the priors don't need to be updated. I don't believe him. FIXME

I suppose Rabiner is referring to left-to-right HMMs only (see equation 46 in his paper). For general HMMs we do have to update priors, don't we?"
573,https://github.com/nltk/nltk/issues/2256,2256,[],closed,2019-03-23 02:59:17+00:00,,0,Unable to find synonym for multiple words,"This is my code, but it doesn't work as expected.
```
str = ""Action, Adventure, Drama""

def process_genre(str):
    for genre in str.split("",""):
        result = []
        for syn in wordnet.synsets(genre):
            for l in syn.lemmas():
                result.append(l.name())
        print(result)
process_genre(str)
```


This is the output
```
['action', 'action', 'activity', 'activeness', 'military_action', 'action', 'natural_process', 'natural_action', 'action', 'activity', 'action', 'action', 'action', 'action_mechanism', 'legal_action', 'action', 'action_at_law', 'action', 'action', 'action', 'sue', 'litigate', 'process', 'carry_through', 'accomplish', 'execute', 'carry_out', 'action', 'fulfill', 'fulfil']
[]
[]
```

The list for ""Adventure"" and ""Drama"" prints empty, which is supposed to have its synonym."
574,https://github.com/nltk/nltk/issues/2258,2258,[],closed,2019-03-26 02:32:37+00:00,,5,problems configuring NLTK 3.4 to use Stanford segmenter,"win10 64bit+python3.6 64bit
I find this instruction website and followed: http://www.nltk.org/api/nltk.tokenize.html?highlight=segmenter#module-nltk.tokenize.stanford_segmenter//
With enviroment varable set,still the program is a failure.

# -*- coding:utf-8 -*-
import nltk, re, pprint,os,codecs
java_path = r""C:\Program Files\Java\jre1.8.0_202\bin\java.exe""
os.environ['JAVAHOME'] = java_path

from nltk.tokenize.stanford_segmenter import StanfordSegmenter
seg = StanfordSegmenter()
seg.default_config('zh')
sent = u'这是斯坦福中文分词器测试'
print(seg.segment(sent))

Result:
Traceback (most recent call last):
Invoked on Tue Mar 26 10:24:04 CST 2019 with arguments: -loadClassifier D:\BaiduNetdiskDownload\stanford-jar\data\pku.gz -keepAllWhitespaces false -textFile C:\Users\ADMINI~1\AppData\Local\Temp\tmpxzs78r0w -serDictionary D:\BaiduNetdiskDownload\stanford-jar\data\dict-chris6.ser.gz -sighanCorporaDict D:\BaiduNetdiskDownload\stanford-segmenter-2018-10-16\stanford-segmenter-2018-10-16\data\./data/ -sighanPostProcessing true -inputEncoding UTF-8
  File ""D:/System/My Documents/HikStorageData/Download/test2 lenovo.py"", line 10, in <module>
serDictionary=D:\BaiduNetdiskDownload\stanford-jar\data\dict-chris6.ser.gz
    print(seg.segment(sent))
loadClassifier=D:\BaiduNetdiskDownload\stanford-jar\data\pku.gz
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\nltk\tokenize\stanford_segmenter.py"", line 236, in segment
sighanCorporaDict=D:\BaiduNetdiskDownload\stanford-segmenter-2018-10-16\stanford-segmenter-2018-10-16\data\./data/
    return self.segment_sents([tokens])
inputEncoding=UTF-8
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\nltk\tokenize\stanford_segmenter.py"", line 274, in segment_sents
keepAllWhitespaces=false
    stdout = self._execute(cmd)
textFile=C:\Users\ADMINI~1\AppData\Local\Temp\tmpxzs78r0w
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\nltk\tokenize\stanford_segmenter.py"", line 294, in _execute
sighanPostProcessing=true
    cmd, classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE
Loading classifier from D:\BaiduNetdiskDownload\stanford-jar\data\pku.gz ... done [10.2 sec].
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\nltk\internals.py"", line 145, in java
    raise OSError('Java command failed : ' + str(cmd))
OSError: Java command failed : ['C:\\Program Files\\Java\\jre1.8.0_202\\bin\\java.exe', '-mx2g', '-cp', 'D:\\BaiduNetdiskDownload\\stanford-segmenter-2018-10-16\\stanford-segmenter-2018-10-16\\stanford-segmenter.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', 'D:\\BaiduNetdiskDownload\\stanford-jar\\data\\pku.gz', '-keepAllWhitespaces', 'false', '-textFile', 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpxzs78r0w', '-serDictionary', 'D:\\BaiduNetdiskDownload\\stanford-jar\\data\\dict-chris6.ser.gz', '-sighanCorporaDict', 'D:\\BaiduNetdiskDownload\\stanford-segmenter-2018-10-16\\stanford-segmenter-2018-10-16\\data\\./data/', '-sighanPostProcessing', 'true', '-inputEncoding', 'UTF-8']
Loading Chinese dictionaries from 1 file:
  D:\BaiduNetdiskDownload\stanford-jar\data\dict-chris6.ser.gz
Done. Unique words in ChineseDictionary is: 423200.
Loading unnormalized dictionary from D:\BaiduNetdiskDownload\stanford-segmenter-2018-10-16\stanford-segmenter-2018-10-16\data\./data//dict/pku.non
Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: java.io.IOException: Unable to open ""D:\BaiduNetdiskDownload\stanford-segmenter-2018-10-16\stanford-segmenter-2018-10-16\data\./data//dict/pku.non"" as class path, filename or URL
	at edu.stanford.nlp.wordseg.CorpusDictionary.readDict(CorpusDictionary.java:80)
	at edu.stanford.nlp.wordseg.CorpusDictionary.<init>(CorpusDictionary.java:40)
	at edu.stanford.nlp.wordseg.CorpusDictionary.<init>(CorpusDictionary.java:35)
	at edu.stanford.nlp.wordseg.NonDict2.<init>(NonDict2.java:32)
	at edu.stanford.nlp.wordseg.Gale2007ChineseSegmenterFeatureFactory.featuresCpC(Gale2007ChineseSegmenterFeatureFactory.java:482)
	at edu.stanford.nlp.wordseg.Gale2007ChineseSegmenterFeatureFactory.getCliqueFeatures(Gale2007ChineseSegmenterFeatureFactory.java:88)
	at edu.stanford.nlp.ie.crf.CRFClassifier.makeDatum(CRFClassifier.java:992)
	at edu.stanford.nlp.ie.crf.CRFClassifier.documentToDataAndLabels(CRFClassifier.java:452)
	at edu.stanford.nlp.ie.crf.CRFClassifier.getSequenceModel(CRFClassifier.java:1185)
	at edu.stanford.nlp.ie.crf.CRFClassifier.classifyMaxEnt(CRFClassifier.java:1218)
	at edu.stanford.nlp.ie.crf.CRFClassifier.classify(CRFClassifier.java:1128)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier$2.process(AbstractSequenceClassifier.java:1172)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier$2.process(AbstractSequenceClassifier.java:1169)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyAndWriteAnswers(AbstractSequenceClassifier.java:1200)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyAndWriteAnswers(AbstractSequenceClassifier.java:1133)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyAndWriteAnswers(AbstractSequenceClassifier.java:1087)
	at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3067)
Caused by: java.io.IOException: Unable to open ""D:\BaiduNetdiskDownload\stanford-segmenter-2018-10-16\stanford-segmenter-2018-10-16\data\./data//dict/pku.non"" as class path, filename or URL
	at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:480)
	at edu.stanford.nlp.wordseg.CorpusDictionary.readDict(CorpusDictionary.java:54)
	... 16 more


Process finished with exit code 1

Any help would be appreciated facing this obstacle, thank you!
"
575,https://github.com/nltk/nltk/issues/2259,2259,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}, {'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}]",open,2019-03-30 05:35:37+00:00,,0,Stanford POS tags are missing'_' in nltk.tag.StanfordPOSTagger,"I am using the stanford-postagger-2018-10-16/stanford-postagger-3.9.2.jar. 
'_' is tagged as 'CD' by Stanford POS Tagger when running using Java jar program, and the same is getting missed when receiving in Python Nltk StanfordPOSTagger.

**Input Sentence:**  
""_ computer is made to implement simulation"" 

**Stanford Jar result:** 
__SYM computer_NN is_VBZ made_VBN to_TO implement_VB simulation_NN 

**NLTK TAG StanfordPOSTagger result:** 

[('', 'CD'), ('computer', 'NN'), ('is', 'VBZ'), ('made', 'VBN'), ('to', 'TO'), ('implement', 'VB'), ('simulation', 'NN')]


**Code Snippet:** 
`from nltk.tag import StanfordPOSTagger`
`TAGGER_MODEL = 'stanford-postagger-2018-10-16/models/english-bidirectional-distsim.tagger'`
`TAGGER_JAR = 'stanford-postagger-2018-10-16/stanford-postagger-3.9.2.jar' `
`stanford_tagger = StanfordPOSTagger(TAGGER_MODEL,TAGGER_JAR)`
`t ='_ computer is made to implement simulation'`
`ttk = nltk.tokenize.word_tokenize(t)`
`sfttk = stanford_tagger.tag(ttk)`
`print(sfttk)`

This is similar to #1632



"
576,https://github.com/nltk/nltk/issues/2261,2261,"[{'id': 718733436, 'node_id': 'MDU6TGFiZWw3MTg3MzM0MzY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/Off-topic', 'name': 'Off-topic', 'color': 'f7b4e1', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}, {'id': 888120541, 'node_id': 'MDU6TGFiZWw4ODgxMjA1NDE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/cluster', 'name': 'cluster', 'color': '2b5196', 'default': False, 'description': ''}]",closed,2019-04-05 00:35:37+00:00,,2,How to generate alignments using MMseqs2 tool ,We have seen your MMseqs2 tool in GitHub. It's really interesting. We are trying to run this tool and try to generate alignments to predict contacts using tools such as CCMpred/FreeContact. We didn't find the correct command to generate an .aln file from the dataset and protein fasta file as the input. It would be a great help if you guide us in the right way.
577,https://github.com/nltk/nltk/issues/2262,2262,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",closed,2019-04-08 11:03:27+00:00,,1,https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml,"Hi All,
Hope You are doing well. 
while i am trying to download(in my office laptop) nltk download () from python prompt. its showing below error.
 a connection party didn't  properly responded after a period of time.

Can you please suggest me ... what could be possibilities issue.

Regards
Dillip





![image](https://user-images.githubusercontent.com/49390723/55719288-7da39480-5a1b-11e9-8371-8709da378b10.png)
"
578,https://github.com/nltk/nltk/issues/2263,2263,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}, {'id': 1346305519, 'node_id': 'MDU6TGFiZWwxMzQ2MzA1NTE5', 'url': 'https://api.github.com/repos/nltk/nltk/labels/third-party', 'name': 'third-party', 'color': '42e5b7', 'default': False, 'description': ''}]",open,2019-04-08 14:39:17+00:00,,2,Why the same tokens have different tags using ‘chi_tagger.tag’？,"Hi guys, I want to tag my tokens using nltk for Chinese, but it became the same in the end, why?
![image](https://user-images.githubusercontent.com/18048381/55732724-130c4c00-5a4f-11e9-8445-62cccea52165.png)

Thanks a lot!"
579,https://github.com/nltk/nltk/issues/2264,2264,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 1012587336, 'node_id': 'MDU6TGFiZWwxMDEyNTg3MzM2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3.7', 'name': 'python3.7', 'color': 'a4e85c', 'default': False, 'description': ''}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",open,2019-04-11 07:10:13+00:00,,2,Different behavior of download for Jupyter and usual REPL,"In the usual REPL of Python, the following works.

```python
>>> import nltk
>>> nltk.download(""reuters"")
>>> paras = nltk.corpus.reuters.paras()

```
However, in Jupyter Notebook it fails with the following message
```
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
<ipython-input-27-49a61c024eed> in <module>()
      2 nltk.download(""reuters"")
      3 reuters = nltk.corpus.reuters
----> 4 paras=reuters.paras()

~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/nltk/corpus/reader/plaintext.py in paras(self, fileids, categories)
    172     def paras(self, fileids=None, categories=None):
    173         return PlaintextCorpusReader.paras(
--> 174             self, self._resolve(fileids, categories))
    175 
    176 # is there a better way?

~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/nltk/corpus/reader/plaintext.py in paras(self, fileids)
    115         return concat([self.CorpusView(path, self._read_para_block, encoding=enc)
    116                        for (path, enc, fileid)
--> 117                        in self.abspaths(fileids, True, True)])
    118 
    119     def _read_word_block(self, stream):

~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/nltk/corpus/reader/api.py in abspaths(self, fileids, include_encoding, include_fileid)
    191             fileids = [fileids]
    192 
--> 193         paths = [self._root.join(f) for f in fileids]
    194 
    195         if include_encoding and include_fileid:

~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/nltk/corpus/reader/api.py in <listcomp>(.0)
    191             fileids = [fileids]
    192 
--> 193         paths = [self._root.join(f) for f in fileids]
    194 
    195         if include_encoding and include_fileid:

~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/nltk/data.py in join(self, fileid)
    338     def join(self, fileid):
    339         _path = os.path.join(self._path, fileid)
--> 340         return FileSystemPathPointer(_path)
    341 
    342     def __repr__(self):

~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/nltk/compat.py in _decorator(*args, **kwargs)
    219     def _decorator(*args, **kwargs):
    220         args = (args[0], add_py3_data(args[1])) + args[2:]
--> 221         return init_func(*args, **kwargs)
    222     return wraps(init_func)(_decorator)
    223 

~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/nltk/data.py in __init__(self, _path)
    316         _path = os.path.abspath(_path)
    317         if not os.path.exists(_path):
--> 318             raise IOError('No such file or directory: %r' % _path)
    319         self._path = _path
    320 

OSError: No such file or directory: '/home/kato/nltk_data/corpora/reuters/test/14826'
```

These results are for the same environment of the same machine. I am using Anaconca 5.3.1 (Python 3.7.3, nltk 3.4). 

After that, I tried to unzip the data file as follows:
```
$ cd ~/nltk_data/corpora
$ unzip reuters.zip
```
Then it started to work even in Jupyter Notebook.

To conclude, it look that the usual Python REPL reads zipped file directly but Jupyter Notebook only works if there is unzipped data."
580,https://github.com/nltk/nltk/issues/2265,2265,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2019-04-16 05:06:15+00:00,,4,Wordnet gloss parsing issues,"From https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1396, it looks like the line parsing wasn't done correctly for the example vs the definition of the Synset gloss:

```python
	     # parse out the definitions and examples from the gloss
            columns_str, gloss = data_file_line.split('|')
            gloss = gloss.strip()
            definitions = []
            for gloss_part in gloss.split(';'):
                gloss_part = gloss_part.strip()
                if gloss_part.startswith('""'):
                    synset._examples.append(gloss_part.strip('""'))
                else:
                    definitions.append(gloss_part)
            synset._definition = '; '.join(definitions)
```

As a sanity check, from

```python
from nltk.corpus import wordnet

for ss in wordnet.all_synsets():
    if '""' in ss.definition():
        print(ss, ss.definition())
```

there are these problematic ones:

```
Synset('long.s.09') having or being more than normal or necessary:""long on brains""
Synset('unobjectionable.s.03') not objectionable; it's the means that one can't accept""
Synset('faulty.s.02') characterized by errors; not agreeing with a model or not following established rules; the wrong side of the road""
Synset('dead.s.17') devoid of activity; nothing ever happens here""
Synset('active.a.10') expressing that the subject of the sentence has the semantic function of actor: ""Hemingway favors active constructions""
Synset('intercalary.s.01') having a day or month inserted to make the calendar year correspond to the solar year: ""Feb. 29 is an intercalary day""
Synset('aggressive.a.01') having or showing determination and energetic pursuit of your ends; positive in his convictions""
Synset('slain.s.01') killed; `slain' is formal or literary as in ""slain warriors""
Synset('out_of_play.s.01') (of a ball) ""a ball that is out of play is dead""
Synset('fast_asleep.s.01') sleeping deeply; he's sound asleep""
Synset('bashful.s.01') self-consciously timid; lowering my head, I looked at the wall""- Ezra Pound
Synset('gutsy.a.01') marked by courage and determination in the face of difficulties or danger; robust and uninhibited; it was a gutsy thing to do""
Synset('cautious.a.01') showing careful forethought; never making swift decisions""
Synset('certificated.s.01') furnished with or authorized by a certificate: ""certificated teachers""
Synset('ineluctable.s.01') impossible to avoid or evade:""inescapable conclusion""
Synset('fixed.s.04') incapable of being changed or moved or undone; e.g. ""frozen prices""
Synset('irritating.s.03') causing physical discomfort; they can be very painful""
Synset('crisp.s.06') brief and to the point; effectively cut short; `yes'""
Synset('coordinating.a.01') serving to connect two grammatical constituents of identical construction; and so is `or' in `will you go or stay?'""
Synset('adjacent.s.02') having a common boundary or edge; abutting; touching; Massachusetts and Conncecticut""
Synset('received.s.02') widely accepted as true or worthy; surveys show otherwise""- Economist
Synset('thoughtless.s.02') without care or thought for others; `Let them eat cake'""
Synset('dimensional.s.02') having dimension--the quality or character or stature proper to a person; he is pasty, bland, faceless""- Norman Cousins
Synset('made.s.01') produced by a manufacturing process; rope and nails""
Synset('original.s.04') not derived or copied or translated from something else; not an adaptation""
Synset('hard-to-please.s.01') (of persons) ""his father was a hard-to-please taskmaster""
Synset('erratic.s.03') likely to perform unpredictably; sometimes it would start and sometimes it wouldn't""
Synset('commutative.s.01') (of a binary operation) independent of order; as in e.g. ""a x b = b x a""
Synset('unselective.s.01') not selective or discriminating; her choices seemed completely random""
Synset('dry.s.16') practicing complete abstinence from alcoholic beverages; I happen to be teetotal""
Synset('humdrum.s.02') tediously repetitious or lacking in variety; all work and no play""
Synset('outside.s.03') originating or belonging beyond some bounds:""the outside world""
Synset('weird.s.02') strikingly odd or unusual; some weird effect of shadow""- Bram Stoker
Synset('adamant.s.01') impervious to pleas, persuasion, requests, reason; she would have none of him""- W.Churchill
Synset('domesticated.s.02') accustomed to home life; others find gratification in it""
Synset('former.a.01') referring to the first of two things or persons mentioned (or the earlier one or ones of several); I prefer the former version to the latter one""
Synset('undrained.a.01') not drained; keep them undrained""
Synset('structural.s.03') affecting or involved in structure or construction; not ornamental elements""
Synset('dry.a.07') without a mucous or watery discharge; a small child with a dry nose""
Synset('divine.s.03') being or having the nature of a god; 'tis God-like to create""-J.G.Saxe
Synset('mundane.s.03') belonging to this earth or world; not ideal or heavenly; yet a mundane wonder of unimagined kind""
Synset('doughy.s.01') having the consistency of dough because of insufficient leavening or improper cooking; it's a doughy mess""
Synset('burdensome.s.01') not easily borne; wearing; I only had to greet the guests""
Synset('fooling.s.01') characterized by a feeling of irresponsibility; it is no fooling matter""
Synset('heedless.a.01') marked by or paying little heed or attention; we know now that it is bad economics""--Franklin D. Roosevelt
Synset('inhumane.a.01') lacking and reflecting lack of pity or compassion; this explains much of the misery and suffering in the world""
Synset('humorless.a.01') lacking humor; a wink of warning""- Truman Capote
Synset('infectious.a.02') easily spread; children catch it from their elders""- Bertrand Russell
Synset('apathetic.s.02') marked by a lack of interest; it is simply indifferent""
Synset('enormous.s.01') extraordinarily large in size or extent or amount or power or degree; that a whole civilization should be dependent on technology""- Walter Lippman
Synset('lowercase.a.01') relating to small (not capitalized) letters that were kept in the lower half of a compositor's type case; a and b and c etc""
Synset('uppercase.a.01') relating to capital letters which were kept in the top half of a compositor's type case; X and Y and Z etc""
Synset('meaty.a.01') like or containing meat; let's have a meaty meal""
Synset('fake.s.02') not genuine or real; being an imitation of the genuine article; it's real synthetic fur""
Synset('obedient.a.01') dutifully complying with the commands or instructions of those in authority; the refractory remain unburdened""- Edmund Burke
Synset('three-piece.a.01') (of clothing) made in or consisting of three parts or pieces; jacket and trousers and vest""
Synset('laid.s.01') set down according to a plan:""a carefully laid table with places set for four people""
Synset('impotent.a.01') lacking power or ability; morality without technology is impotent""- Freeman J.Dyson
Synset('attendant.s.01') being present (at meeting or event etc.) ""attendant members of the congreation""
Synset('insular.s.02') suggestive of the isolated life of an island; so deeply private as to seem inaccessible to the scrutiny of a novelist""- Leonard Michaels
Synset('skin-deep.s.01') penetrating no deeper than the skin: ""her beauty is only skin-deep""
Synset('for_sure.s.01') not open to doubt; that was for sure""
Synset('real.a.01') being or occurring in fact or actuality; having verified existence; not illusory; not ghosts""
Synset('proper.s.02') having all the qualities typical of the thing specified; not just a snack""
Synset('treated.a.02') given medical care or treatment; if left untreated it lasts two weeks""
Synset('infirm.s.02') lacking firmness of will or character or purpose; give me the daggers"" - Shakespeare
Synset('flimsy.s.03') lacking substance or significance; a fragile claim to fame""
Synset('double.s.01') having more than one decidedly dissimilar aspects or qualities; public preaching and private influence""- R.W.Emerson
Synset('unsized.a.02') not fashioned to sizes; one size fits all""
Synset('antisocial.s.01') shunning contact with others; just shy""
Synset('solved.a.01') explained or answered; problems resolved and unresolved""
Synset('unacceptable.s.04') not conforming to standard usage; `You can access your cash at any of 300 automatic tellers'""
Synset('ocular.s.03') visible; give me the ocular proof""- Shakespeare
Synset('well.a.01') in good health especially after having suffered illness or injury; at least I feel well""
Synset('auricular.a.02') relating to or perceived by or shaped like the organ of hearing; I must hear what I read""- George Santayana
Synset('igneous.a.01') produced under conditions involving intense heat; especially from molten magma""
Synset('functional.a.03') relating to or based on function especially as opposed to structure; it is a functional one""
Synset('canonist.a.01') pertaining to or characteristic of a body of rules and principles accepted as axiomatic; e.g. ""canonist communism""
Synset('classificatory.a.01') relating to or involving classification:""classificatory criteria""
Synset('vietnamese.a.01') of or relating to or characteristic of Vietnam or its people or its language; the Vietnamese tones""
Synset('sometimes.r.01') on certain occasions or in certain cases but not always; at other times for six months""
Synset('anyhow.r.01') used to indicate that a statement explains or supports a previous statement; anyhow, they're quiet""; anyway, it's gone""; in any case, there was a brief scuffle""
Synset('as_it_is.r.01') in the actual state of affairs and often contrary to expectations; as it is he was severely injured""
Synset('only.r.05') except that; only this time she came out better""
Synset('however.r.02') by contrast; on the other hand; the second, however, took hours""
Synset('besides.r.01') making an additional point; anyway; besides, we can't afford it""
Synset('furthermore.r.01') in addition; furthermore, their quality is improving""; moreover, mice nested there""
Synset('further.r.02') in addition or furthermore; further, they should be here already""
Synset('rather.r.01') on the contrary; rather (or instead), he wrote her a letter""
Synset('enough.r.01') as much as necessary; (`plenty' is nonstandard) ""I've had plenty, thanks""
Synset('contrarily.r.02') contrary to expectations; on the contrary, he went out with his friends""
Synset('fearfully.r.01') in fear, ""she hurried down the stairs fearfully""
Synset('testily.r.01') in a petulant manner; `Go away!'""
Synset('arrogantly.r.01') in an arrogant manner; afraid of her husband and yet arrogantly proud that she had a husband strong and fierce enough to make her afraid""
Synset('boorishly.r.01') like a boor, ""he behaved boorishly at the party""
Synset('indecisively.r.02') without finality; inconclusively; neither side had clearly won but neither side admitted defeat""
Synset('ecologically.r.01') with respect to ecology; economically, it is a disaster""
Synset('inspirationally.r.01') with inspiration; in an inspiring manner, ""he talked inspirationally""
Synset('irreverently.r.02') in an irreverent manner; clergymen had been turned from their cures, and churches irreverently used""
Synset('perchance.r.01') through chance, ""To sleep, perchance to dream..""
Synset('singularly.r.01') in a singular manner or to a singular degree; he acquired three wives and fourteen children during his Portuguese embassy alone""
Synset('therewithal.r.01') together with all that; besides; and therewithal remit thy other forfeits""- Shakespeare
Synset('untying.n.01') loosening the ties that fasten something; the untying is easy""
Synset('substitution.n.02') the act of putting one thing or person in the place of another: ""he sent Smith in for Jones but the substitution came too late to help""
Synset('stride.n.03') significant progress (especially in the phrase ""make strides"")
Synset('putt.n.01') hitting a golf ball that is on the green using a putter; he didn't sink a single putt over three feet""
Synset('behalf.n.01') as the agent of or on someone's part (usually expressed as ""on behalf of"" rather than ""in behalf of""); 
Synset('duty.n.01') the social force that binds you to the courses of action demanded by that force; every opportunity, an obligation; every possession, a duty""- John D.Rockefeller Jr
Synset('mobilization.n.01') act of assembling and putting into readiness for war or other emergency: ""mobilization of the troops""
Synset('carrot.n.04') promise of reward as in ""carrot and stick""; 
Synset('vengeance.n.01') the act of taking revenge (harming someone in retaliation for something harmful that they have done) especially in the next life; I will repay, saith the Lord""--Romans 12:19
Synset('cow.n.01') female of domestic cattle: ""`moo-cow' is a child's term""
Synset('job.n.03') a workplace; as in the expression ""on the job""; 
Synset('worsted.n.01') a woolen fabric with a hard textured surface and no nap; woven of worsted yarns ""he wore a worsted suit""
Synset('adequacy.n.01') the quality of being able to meet a need satisfactorily: ""he questioned the adequacy of the usual sentimental interpretation of the Golden Rule""
Synset('quaintness.n.01') the quality of being quaint and old-fashioned; its quaintness was appealing""
Synset('scope.n.01') an area in which something acts or operates or has power or control: ""the range of a supersonic jet""
Synset('gamut.n.01') a complete extent or range: ""a face that expressed a gamut of emotions""
Synset('evil.n.02') that which causes harm or destruction or misfortune; the good is oft interred with their bones""- Shakespeare
Synset('right.n.01') an abstract idea of that which is due to a person or governmental body by law or tradition or nature; it is something that nobody can take away""
Synset('writer's_block.n.01') an inability to write; the words wouldn't come""
Synset('conversion.n.02') a change in the units or form of an expression: ""conversion from Fahrenheit to Centigrade""
Synset('exception.n.02') an instance that does not conform to a rule or generalization; the only exception was her last child""
Synset('attractive_nuisance.n.01') anything on your premises that might attract children into danger or harm; they should fence it in""
Synset('roundup.n.02') a summary list; as in e.g. ""a news roundup""
Synset('capital.n.04') one of the large alphabetic characters used as the first letter in writing or printing proper names and sometimes for emphasis; capitals were kept in the upper half of the type case and so became known as upper-case letters""
Synset('indirect_discourse.n.01') a report of a discourse in which deictic terms are modified appropriately (e.g., ""he said `I am a fool' would be modified to `he said he is a fool'"")
Synset('direct_discourse.n.01') a report of the exact words used in a discourse (e.g., ""he said `I am a fool'"")
Synset('call.n.04') a demand especially in the phrase ""the call of duty""
Synset('unknown_quantity.n.01') a factor in a given situation whose bearing and importance is not apparent; he's still an unknown quantity""
Synset('post_office.n.01') a local branch where postal services are available""
Synset('conferee.n.01') a person on whom something is bestowed; the conferees were...""
Synset('dribbler.n.02') a person who dribbles; he needs a bib""
Synset('fast_buck.n.01') quick or easy earnings, ""they are traders out to make a fast buck""
Synset('ultimacy.n.01') the state or degree of being ultimate; the final or most extreme in degree or size or time or distance, ""the ultimacy of these social values""
Synset('oppression.n.02') the state of being kept down by unjust use of force or authority: ""after years of oppression they finally revolted""
Synset('lap_of_the_gods.n.01') beyond human control or responsibility; it's in the lap of the gods now""
Synset('diversification.n.02') the condition of being varied; every day it is the same""
Synset('chip_away.v.01') remove or withdraw gradually: ""These new customs are chipping away at the quality of life""
Synset('limit.v.02') restrict or confine, ""I limit you to two visits to the pub a day""
Synset('puff.v.07') to swell or cause to enlarge, ""Her faced puffed up from the drugs""
Synset('focus.v.05') put (an image) into focus; we cannot enjoy the movie""
Synset('come.v.06') be found or available; The furniture comes unassembled""
Synset('switch.v.04') make a shift in or exchange of; then we switched""
Synset('complete.v.01') come or bring to a finish or an end; others finished in over 4 hours""
Synset('study.v.05') learn by reading books; I must hit the books now""
Synset('draw_out.v.01') cause to speak, ""Can you draw her out--she is always so quiet""
Synset('threaten.v.02') to utter intentions of injury or punishment against:""He threatened me when I tried to call the police""
Synset('threaten.v.03') to be a menacing indication of something:""The clouds threaten rain""
Synset('drop.v.06') utter with seeming casualness; drop names""
Synset('decline.v.07') inflect for number, gender, case, etc., ""in many languages, speakers decline nouns, pronouns, and adjectives""
Synset('notate.v.01') put into notation, as of music or choreography; in the old days, the steps had to be memorized""
Synset('compromise.v.01') make a compromise; arrive at a compromise; we all must compromise""
Synset('sign_up.v.02') join a club, an activity, etc. with the intention to join or participate, ""Sign up for yoga classes""
Synset('beset.v.02') assail or attack on all sides: ""The zebra was beset by leopards""
Synset('live_out.v.02') work in a house where one does not live; he can easily commute from his home""
Synset('pleat.v.02') fold into pleats, ""Pleat the cloth""
Synset('knock.v.01') deliver a sharp blow or push :""He knocked the glass clear across the room""
Synset('prepose.v.01') place before another constituent in the sentence; Japanese postposes them""
Synset('strew.v.01') spread by scattering (""straw"" is archaic)
Synset('dribble.v.03') propel, ""Carry the ball""
Synset('croquet.v.01') drive away by hitting with one's ball, ""croquet the opponent's ball""
Synset('hitch.v.05') connect to a vehicle: ""hitch the trailer to the car""
Synset('bear.v.05') bring forth, ""The apple tree bore delicious apples this year""
Synset('manufacture.v.01') put together out of artificial or natural components or parts; He manufactured a popular cereal""
Synset('recite.v.03') render verbally, ""recite a poem""
Synset('conduct.v.02') lead, as in the performance of a composition; Barenboim conducted the Chicago symphony for years""
Synset('stay.v.02') stay put (in a certain place); we are not moving to Cincinnati""
Synset('wallow.v.02') roll around, ""pigs were wallowing in the mud""
Synset('push.v.01') move with force, ""He pushed the table into a corner""
Synset('arouse.v.06') to begin moving, ""As the thunder started the sleeping children began to stir""
Synset('crawl.v.05') swim by doing the crawl; they often don't know how to crawl""
Synset('ascend.v.01') travel up, ""We ascended the mountain""
Synset('sink.v.04') go under, ""The raft sank and its occupants drowned""
Synset('roll_up.v.03') arrive in a vehicle: ""He rolled up in a black Mercedes""
Synset('reach.v.07') reach a goal, e.g., ""make the first team""
Synset('feel.v.06') undergo passive experience of:""We felt the effects of inflation""
Synset('refocus.v.01') focus once again; The physicist refocused the light beam""
Synset('trust.v.06') extend credit to; I won't pay her debts anymore""
Synset('baby-sit.v.02') work or act as a baby-sitter; I have too much homework to do""
Synset('imprison.v.02') confine as if in a prison; he does not let them go out without a chaperone""
Synset('coerce.v.01') to cause to do through pressure or necessity, by physical, moral or intellectual means :""She forced him to take a job in the city""
Synset('form.v.02') to compose or represent:""This wall forms the background of the stage setting""
Synset('keep.v.01') keep in a certain state, position, or activity; e.g., ""keep clean""
Synset('enclose.v.02') close in; darkness enclosed him""
Synset('be.v.05') happen, occur, take place; this was during the visit to my parents' house""
```
"
581,https://github.com/nltk/nltk/issues/2266,2266,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",open,2019-04-16 08:09:15+00:00,,1,"Cyclic initialization between Lemma, Synset and WordNetCorpusReader","The cyclic initialization between Lemma, Synset and WordNetCorpusReader is sort of ""un-wanted"" which actually makes #1548 quite impossible

Looking at the definition, 

 - `WordNetCorpusReader` creates `Synset` and `Lemma` objects on the fly, keeping them in caches https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1121 

 - To create a `Synset`, the class is so tied to the `WordNetCorpusReader` such that creation of empty synsets becomes meaningless https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1121 and only created at [`_synset_from_pos_and_line()`](https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1396) function

 - To create a `Lemma`, it needs to be tied to the synsets, https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L236, by definition of the lemma. 

A suggestion would be

 -  first to decouple lemma vs synset such that the initialization is not dependent on each other, then link them with some optional kwargs. 

 - unload the loaded functions in the WordNetCorpusReader, such that it reads only the lines from the wordnet files. 

 - from the output of the wordnet files, 
    - create the synset independent of the lemmas and 
    - create the lemma independent of the synset
    - create an attribute that links the synset to lemma without having them as attributes of each other.

----

For lemmas, the only uses of the synset are:

 - `__repr__` accessing the synset name, this should be easy to fetch or initialize
 - `_related` accesing synset's `_lemma_pointers` which could be been added to initialize the lemma. "
582,https://github.com/nltk/nltk/issues/2267,2267,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",closed,2019-04-17 10:46:58+00:00,,1,Unnecessary inner search when fetching lemma_pos_offset_map in wn.synset,"From the [code that tries to load synset](https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/wordnet.py#L1327), there are some sanity checks:

```python
    def synset(self, name):
        # split name into lemma, part of speech and synset number
        lemma, pos, synset_index_str = name.lower().rsplit('.', 2)
        synset_index = int(synset_index_str) - 1

        # get the offset for this synset
        try:
            offset = self._lemma_pos_offset_map[lemma][pos][synset_index]
        except KeyError:
            message = 'no lemma %r with part of speech %r'
            raise WordNetError(message % (lemma, pos))
        except IndexError:
            n_senses = len(self._lemma_pos_offset_map[lemma][pos])
            message = ""lemma %r with part of speech %r has only %i %s""
            if n_senses == 1:
                tup = lemma, pos, n_senses, ""sense""
            else:
                tup = lemma, pos, n_senses, ""senses""
            raise WordNetError(message % tup)
```

For the first key error, actually the final inner `self._lemma_pos_offset_map[lemma][pos][synset_index]` is not possible because `_lemma_pos_offset_map` is a `defaultdict(dict)`, the last nested level dictionary doesn't exist. 

Although the code still works because Python throws an error once the second key doesn't exist, the first try-except should remove the final key search and be:

```python
        try:
            offset = self._lemma_pos_offset_map[lemma][pos]
        except KeyError:
            message = 'no lemma %r with part of speech %r'
            raise WordNetError(message % (lemma, pos))
```
"
583,https://github.com/nltk/nltk/issues/2269,2269,[],closed,2019-04-18 02:59:44+00:00,,2,Incorrect quotation labelling,"When running the following code:

```python
import nltk
from nltk.corpus import wordnet as wn
from nltk.stem import WordNetLemmatizer

nltk.download('averaged_perceptron_tagger')

lemmatizer = WordNetLemmatizer()

tokens = ['is', 'the', 'other', 'equal', 'protection', 'and', 'criminal', 'do', 'you', 'set', 'forth', 'the', 'whole', 'history', 'of', 'this', 'problem', 'or', 'is', 'there', 'any', 'place', 'that', 'this', 'must', 'have', 'a', 'considerable', 'history', 'to', 'strike', 'does', 'it', 'not']

# returns list of tuples (word, tag) - penn bank style
tagTuples = nltk.pos_tag(tokens)

print(tagTuples)
```

I get the results:
```
[('is', 'VBZ'), ('the', 'DT'), ('other', 'JJ'), ('equal', 'JJ'), ('protection', 'NN'), ('and', 'CC'), ('criminal', 'JJ'), ('do', 'VBP'), ('you', 'PRP'), ('set', 'VB'), ('forth', ""''""), ('the', 'DT'), ('whole', 'JJ'), ('history', 'NN'), ('of', 'IN'), ('this', 'DT'), ('problem', 'NN'), ('or', 'CC'), ('is', 'VBZ'), ('there', 'EX'), ('any', 'DT'), ('place', 'NN'), ('that', 'IN'), ('this', 'DT'), ('must', 'MD'), ('have', 'VB'), ('a', 'DT'), ('considerable', 'JJ'), ('history', 'NN'), ('to', 'TO'), ('strike', 'NN'), ('does', 'VBZ'), ('it', 'PRP'), ('not', 'RB')]
```
Note the word and tag tuple `('forth', ""''"")`.

'forth' in this context is being returned as a quotation mark ""''"". I believe this is a bug. I ran into this issue on my local computer, but just tested it out on Google Colab and recieved the same results. "
584,https://github.com/nltk/nltk/issues/2270,2270,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",open,2019-04-18 04:06:46+00:00,,2,Hypernyms related functions in Wordnets can be unified,"These functions in wordnet's `Synset()` object could be unified:
 
 - hypernym_paths
 - min_depth
 - max_depth
 - root_hypernyms

The while loops / recursion achieve the same purpose of reaching the root hypernyms, while doing so, the min and max depth should be logged and so should the hypernym paths. 

We can derive the other three attributes given the hypernym_paths, e.g. 

```python
from nltk.corpus import wordnet as nltk_wn
ss = nltk_wn.synset('sweet.n.1')
hypernym_paths = ss.hypernym_paths()
assert ss.max_depth() == max(len(path) for path in hypernym_paths) - 1
assert ss.min_depth() == min(len(path) for path in hypernym_paths) - 1
assert list(set([path[0] for path in hypernym_paths])) == ss.root_hypernyms()
```

----

Something like this:

```python
    def init_hypernym_paths(self):
        """"""
        Get the path(s) from this synset to the root, where each path is a
        list of the synset nodes traversed on the way to the root.
        :return: A list of lists, where each list gives the node sequence
        connecting the initial ``Synset`` node and a root node.
        """"""
        self._hyperpaths = []
        hypernyms = self.hypernyms() + self.instance_hypernyms()
        if len(hypernyms) == 0:
            paths = [[self]]
        for hypernym in hypernyms:
            for ancestor_list in hypernym.hypernym_paths():
                ancestor_list.append(self)
                self._hyperpaths.append(ancestor_list)
        # Compute the path related statistics.
        self._min_depth = min(len(path) for path in self._hyperpaths)
        self._max_depth = max(len(path) for path in self._hyperpaths)
        # Compute the store the root hypernyms.
        self._root_hypernyms = list(set([path[0] for path in self._hyperpaths]))

    def hypernym_paths():
        return self._hyperpaths

    def min_depth():
        return self._min_depth

    def max_depth():
        return self._max_depth

    def root_hypernyms():
        return self._root_hypernyms
```"
585,https://github.com/nltk/nltk/issues/2271,2271,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}]",closed,2019-04-19 18:35:35+00:00,,0,docstring for load() does not match the implementation,"The load() method docstring mentions that it uses weak reference, but the implementation uses a dictionary.  Docstring needs fixes. 

```
 :param cache: If true, add this resource to a cache.  If load()
        finds a resource in its cache, then it will return it from the
        cache rather than loading it.  The cache uses weak references,
        so a resource wil automatically be expunged from the cache
        when no more objects are using it.
```

https://github.com/nltk/nltk/blob/952a9636db4b1178dceb1789c98d56161deda3ab/nltk/data.py#L584

"
586,https://github.com/nltk/nltk/issues/2273,2273,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",open,2019-04-22 02:14:25+00:00,,3,Max depth of the all wordnet POS should be returned and kept as static,"The `_compute_max_depth()` used for `lch_similarity()` returns `None` regardless of POS :

```python
from nltk.corpus import wordnet as nltk_wn
from nltk.corpus.reader.wordnet import POS_LIST

for pos in POS_LIST:
    print(pos, 
          nltk_wn._compute_max_depth(pos, simulate_root=True), 
          nltk_wn._compute_max_depth(pos, simulate_root=False))
          print(pos, nltk_wn._max_depth)
```

[out]:

```
n None None
n defaultdict(<class 'dict'>, {'v': 12, 'a': 0, 'r': 0, 'n': 19})
v None None
v defaultdict(<class 'dict'>, {'v': 12, 'a': 0, 'r': 0, 'n': 19})
a None None
a defaultdict(<class 'dict'>, {'v': 12, 'a': 0, 'r': 0, 'n': 19})
r None None
r defaultdict(<class 'dict'>, {'v': 12, 'a': 0, 'r': 0, 'n': 19})
```

The `self._compute_max_depth` doesn't return anything, my suggestion is to expose it as a public function and let it return the depth while setting the max depth.

----

Another point is that the `max_depth` will call the `all_synsets()` once and this is costly, it could be saved as static value from the start and access to the integer would be much simpler."
587,https://github.com/nltk/nltk/issues/2274,2274,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2019-04-22 10:18:13+00:00,,1,Wordnet's all_lemma_names returns inconsistent types,"Depending on the language, Wordnet's `all_lemma_names()` returns inconsistent types

```python
>>> from nltk.corpus import wordnet as wn
>>> type(wn.all_lemma_names(lang='eng'))
<class 'dict_keyiterator'>
>>> type(wn.all_lemma_names(lang='cat'))
<class 'list'>
```"
588,https://github.com/nltk/nltk/issues/2275,2275,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2019-04-22 16:13:32+00:00,,1,OMW lemmas sometimes has additional underscore,"OMW lemmas sometimes has additional underscore, e.g. `preobrat_v_mišljenju_` in:

```python
>>> from nltk.corpus import wordnet as wn
>>> wn.synset('about-face.n.02')
Synset('about-face.n.02')
>>> wn.synset('about-face.n.02').lemma_names(lang='slv')
['popolna_sprememba_v_mišljenju', 'popoln_obrat', 'preobrat', 'preobrat_v_mišljenju_']
```

From the raw file there isn't any tailing underscore:

```
~/nltk_data/corpora/omw/slv$ grep -n ""preobrat v mišljenju"" wn-data-slv.tab  
2383:00163406-n	slv:lemma	preobrat v mišljenju 
```
"
589,https://github.com/nltk/nltk/issues/2276,2276,"[{'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}, {'id': 1012587336, 'node_id': 'MDU6TGFiZWwxMDEyNTg3MzM2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3.7', 'name': 'python3.7', 'color': 'a4e85c', 'default': False, 'description': ''}]",closed,2019-04-23 10:03:41+00:00,,3,AttributeError: type object 'scipy.interpolate.interpnd.array' has no attribute '__reduce_cython__',"Getting this error when importing NLTK:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-1-1d2184025e54> in <module>
----> 1 import nltk

~/miniconda3/lib/python3.7/site-packages/nltk/__init__.py in <module>
    127 # Import top-level functionality into top-level namespace
    128 
--> 129 from nltk.collocations import *
    130 from nltk.decorators import decorator, memoize
    131 from nltk.featstruct import *

~/miniconda3/lib/python3.7/site-packages/nltk/collocations.py in <module>
     38 from nltk.util import ngrams
     39 # these two unused imports are referenced in collocations.doctest
---> 40 from nltk.metrics import ContingencyMeasures, BigramAssocMeasures, TrigramAssocMeasures
     41 from nltk.metrics.spearman import ranks_from_scores, spearman_correlation
     42 

~/miniconda3/lib/python3.7/site-packages/nltk/metrics/__init__.py in <module>
     37 from nltk.metrics.segmentation import windowdiff, ghd, pk
     38 from nltk.metrics.agreement import AnnotationTask
---> 39 from nltk.metrics.association import (
     40     NgramAssocMeasures,
     41     BigramAssocMeasures,

~/miniconda3/lib/python3.7/site-packages/nltk/metrics/association.py in <module>
     28 
     29 try:
---> 30     from scipy.stats import fisher_exact
     31 except ImportError:
     32 

~/miniconda3/lib/python3.7/site-packages/scipy/stats/__init__.py in <module>
    365 from __future__ import division, print_function, absolute_import
    366 
--> 367 from .stats import *
    368 from .distributions import *
    369 from .morestats import *

~/miniconda3/lib/python3.7/site-packages/scipy/stats/stats.py in <module>
    171 from scipy._lib._util import _lazywhere
    172 import scipy.special as special
--> 173 from . import distributions
    174 from . import mstats_basic
    175 from ._stats_mstats_common import _find_repeats, linregress, theilslopes, siegelslopes

~/miniconda3/lib/python3.7/site-packages/scipy/stats/distributions.py in <module>
      8 from __future__ import division, print_function, absolute_import
      9 
---> 10 from ._distn_infrastructure import (entropy, rv_discrete, rv_continuous,
     11                                     rv_frozen)
     12 

~/miniconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py in <module>
     14 import warnings
     15 
---> 16 from scipy.misc import doccer
     17 from ._distr_params import distcont, distdiscrete
     18 from scipy._lib._util import check_random_state

~/miniconda3/lib/python3.7/site-packages/scipy/misc/__init__.py in <module>
     66 from numpy import who as _who, source as _source, info as _info
     67 import numpy as np
---> 68 from scipy.interpolate._pade import pade as _pade
     69 from scipy.special import (comb as _comb, logsumexp as _lsm,
     70         factorial as _fact, factorial2 as _fact2, factorialk as _factk)

~/miniconda3/lib/python3.7/site-packages/scipy/interpolate/__init__.py in <module>
    173 from __future__ import division, print_function, absolute_import
    174 
--> 175 from .interpolate import *
    176 from .fitpack import *
    177 

~/miniconda3/lib/python3.7/site-packages/scipy/interpolate/interpolate.py in <module>
     30 from . import _ppoly
     31 from .fitpack2 import RectBivariateSpline
---> 32 from .interpnd import _ndim_coords_from_arrays
     33 from ._bsplines import make_interp_spline, BSpline
     34 

~/miniconda3/lib/python3.7/site-packages/scipy/interpolate/interpnd.cpython-37m-x86_64-linux-gnu.so in init scipy.interpolate.interpnd()


**My setup is:**
Python version 3.7.3
NLTK version 3.4.1
Cython 0.29.7
Scipy 1.2.1"
590,https://github.com/nltk/nltk/issues/2277,2277,[],open,2019-04-24 00:26:38+00:00,,0,"If only one  relationship , nltk.sem.relextract.extract_rels() can't work ","If only one  relationship appears in the sentence, nltk.sem.relextract.extract_rels() outputs an empty list. Suggest change relextract.py:

def semi_rel2reldict(pairs, window=5, trace=False):
   ... 
    **while len(pairs) > 2:  # line 184:**
        reldict = defaultdict(str)
        reldict['lcon'] = _join(pairs[0][0][-window:])
        reldict['subjclass'] = pairs[0][1].label()
        reldict['subjtext'] = _join(pairs[0][1].leaves())
        reldict['subjsym'] = list2sym(pairs[0][1].leaves())
        reldict['filler'] = _join(pairs[1][0])
        reldict['untagged_filler'] = _join(pairs[1][0], untag=True)
        reldict['objclass'] = pairs[1][1].label()
        reldict['objtext'] = _join(pairs[1][1].leaves())
        reldict['objsym'] = list2sym(pairs[1][1].leaves())
        **reldict['rcon'] = _join(pairs[2][0][:window])   # line 195**
       ...
Suggest : 
def semi_rel2reldict(pairs, window=5, trace=False):
   ... 
 **while len(pairs) >= 2:**
        reldict = defaultdict(str)
        reldict['lcon'] = _join(pairs[0][0][-window:])
        reldict['subjclass'] = pairs[0][1].label()
        reldict['subjtext'] = _join(pairs[0][1].leaves())
        reldict['subjsym'] = list2sym(pairs[0][1].leaves())
        reldict['filler'] = _join(pairs[1][0])
        reldict['untagged_filler'] = _join(pairs[1][0], untag=True)
        reldict['objclass'] = pairs[1][1].label()
        reldict['objtext'] = _join(pairs[1][1].leaves())
        reldict['objsym'] = list2sym(pairs[1][1].leaves())
        **reldict['rcon'] = _join(pairs[2][0][:window]) if len(pairs) > 2 else ''**
       ..."
591,https://github.com/nltk/nltk/issues/2278,2278,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",closed,2019-04-24 01:29:19+00:00,,1,Wordnet similarity quirky when only one synsets needs root,"Sometimes when only one synset needs root, the similarity between the synsets are not commutative:

```python
from nltk.corpus import wordnet as nltk_wn

ncat = nltk_wn.synset('cat.n.01')
nbuy = nltk_wn.synset('buy.v.01')

print(nltk_wn.path_similarity(nbuy, ncat), nltk_wn.wup_similarity(nbuy, ncat))
print(nltk_wn.path_similarity(ncat, nbuy), nltk_wn.wup_similarity(ncat, nbuy))
```

[out]:

```
0.058823529411764705 0.1111111111111111
None None
```

Details on https://stackoverflow.com/q/20075335/610569 "
592,https://github.com/nltk/nltk/issues/2279,2279,[],closed,2019-04-25 19:21:32+00:00,,2,ImportError: Module use of python35.dll conflicts with this version of Python.,"Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda\envs\NLP\lib\site-packages\nltk\__init__.py"", line 137, in <module>
    from nltk.stem import *
  File ""C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda\envs\NLP\lib\site-packages\nltk\stem\__init__.py"", line 29, in <module>
    from nltk.stem.snowball import SnowballStemmer
  File ""C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda\envs\NLP\lib\site-packages\nltk\stem\snowball.py"", line 32, in <module>
    from nltk.corpus import stopwords
  File ""C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda\envs\NLP\lib\site-packages\nltk\corpus\__init__.py"", line 66, in <module>
    from nltk.corpus.reader import *
  File ""C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda\envs\NLP\lib\site-packages\nltk\corpus\reader\__init__.py"", line 105, in <module>
    from nltk.corpus.reader.panlex_lite import *
  File ""C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda\envs\NLP\lib\site-packages\nltk\corpus\reader\panlex_lite.py"", line 15, in <module>
    import sqlite3
  File ""C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda\envs\NLP\lib\sqlite3\__init__.py"", line 23, in <module>
    from sqlite3.dbapi2 import *
  File ""C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""C:\Anaconda\envs\NLP\lib\sqlite3\dbapi2.py"", line 27, in <module>
    from _sqlite3 import *
  File ""C:\Program Files\JetBrains\PyCharm 2018.2.5\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
ImportError: Module use of python35.dll conflicts with this version of Python."
593,https://github.com/nltk/nltk/issues/2280,2280,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",closed,2019-04-29 04:54:49+00:00,,18,nltk.download() error : not well-formed (invalid token),"can anyone help me?
whenever i run nltk.download() i always got this error : 
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\ASUS\AppData\Local\Programs\Python\Python36\lib\site-packages\nltk\downloader.py"", line 773, in download
    self._interactive_download()
  File ""C:\Users\ASUS\AppData\Local\Programs\Python\Python36\lib\site-packages\nltk\downloader.py"", line 1123, in _interactive_download
    DownloaderGUI(self).mainloop()
  File ""C:\Users\ASUS\AppData\Local\Programs\Python\Python36\lib\site-packages\nltk\downloader.py"", line 1423, in __init__
    self._fill_table()
  File ""C:\Users\ASUS\AppData\Local\Programs\Python\Python36\lib\site-packages\nltk\downloader.py"", line 1755, in _fill_table
    items = self._ds.collections()
  File ""C:\Users\ASUS\AppData\Local\Programs\Python\Python36\lib\site-packages\nltk\downloader.py"", line 600, in collections
    self._update_index()
  File ""C:\Users\ASUS\AppData\Local\Programs\Python\Python36\lib\site-packages\nltk\downloader.py"", line 962, in _update_index
    ElementTree.parse(urlopen(self._url)).getroot()
  File ""C:\Users\ASUS\AppData\Local\Programs\Python\Python36\lib\xml\etree\ElementTree.py"", line 1196, in parse
    tree.parse(source, parser)
  File ""C:\Users\ASUS\AppData\Local\Programs\Python\Python36\lib\xml\etree\ElementTree.py"", line 597, in parse
    self._root = parser._parse_whole(source)
xml.etree.ElementTree.ParseError: not well-formed (invalid token): line 1, column 0

please help me. i cant do my project any further because of this."
594,https://github.com/nltk/nltk/issues/2281,2281,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",closed,2019-05-01 04:44:13+00:00,,3,nltk.pos_tag EOFError on nltk 3.4/python3.7,"Hi, Currently getting the following error when trying to use the pos_tag function in nltk version 3.4/Python3.7 running on Arch Linux. Any idea what the issue might be/how to fix it? Thanks! 

`In [7]: nltk.pos_tag([""who"",""are"",""you""])
---------------------------------------------------------------------------
EOFError                                  Traceback (most recent call last)
<ipython-input-7-11accc8eac78> in <module>
----> 1 nltk.pos_tag([""who"",""are"",""you""])

~/.local/lib/python3.7/site-packages/nltk/tag/__init__.py in pos_tag(tokens, tagset, lang)
    159     :rtype: list(tuple(str, str))
    160     """"""
--> 161     tagger = _get_tagger(lang)
    162     return _pos_tag(tokens, tagset, tagger, lang)
    163

~/.local/lib/python3.7/site-packages/nltk/tag/__init__.py in _get_tagger(lang)
    105         tagger.load(ap_russian_model_loc)
    106     else:
--> 107         tagger = PerceptronTagger()
    108     return tagger
    109

~/.local/lib/python3.7/site-packages/nltk/tag/perceptron.py in __init__(self, load)
    144                 find('taggers/averaged_perceptron_tagger/' + PICKLE)
    145             )
--> 146             self.load(AP_MODEL_LOC)
    147
    148     def tag(self, tokens):

~/.local/lib/python3.7/site-packages/nltk/tag/perceptron.py in load(self, loc)
    224         '''
    225
--> 226         self.model.weights, self.tagdict, self.classes = load(loc)
    227         self.model.classes = self.classes
    228

~/.local/lib/python3.7/site-packages/nltk/data.py in load(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)
    872     elif format == 'pickle':
    873         print(opened_resource)
--> 874         resource_val = pickle.load(opened_resource)
    875     elif format == 'json':
    876         import json

EOFError: Ran out of input
`"
595,https://github.com/nltk/nltk/issues/2282,2282,[],closed,2019-05-01 16:09:15+00:00,,2,Cannot import nltk.chunk,"I have the latest version of nltk and numpy installed, see:

`
Installing collected packages: nltk
  Found existing installation: nltk 3.2.5
    Uninstalling nltk-3.2.5:
      Successfully uninstalled nltk-3.2.5
  Running setup.py install for nltk ... done
Successfully installed nltk-3.4.1
You are using pip version 18.1, however version 19.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
Admins-MacBook-Pro-4:proofs kylefoley$ pip install -U numpy
Collecting numpy
  Downloading https://files.pythonhosted.org/packages/ae/76/4a4c012bca5688881c18f6e04694d221b88daa5d38526a4df87d75711199/numpy-1.16.3-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (13.9MB)
    100% |████████████████████████████████| 13.9MB 2.3MB/s 
Installing collected packages: numpy
  Found existing installation: numpy 1.15.2
    Uninstalling numpy-1.15.2:
      Successfully uninstalled numpy-1.15.2
Successfully installed numpy-1.16.3

`
However, when I try to import nltk I run into sys.exit().  More specifically, the following module is to blame:

`
import nltk.chunk
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3267, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-0412f32b0e38>"", line 1, in <module>
    import nltk.chunk
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/__init__.py"", line 143, in <module>
    from nltk.chunk import *
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/chunk/__init__.py"", line 157, in <module>
    from nltk.chunk.api import ChunkParserI
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/chunk/api.py"", line 13, in <module>
    from nltk.parse import ParserI
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/parse/__init__.py"", line 102, in <module>
    from nltk.parse.corenlp import CoreNLPParser, CoreNLPDependencyParser
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/parse/corenlp.py"", line 19, in <module>
    from nltk.tag.api import TaggerI
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tag/__init__.py"", line 70, in <module>
    from nltk.tag.sequential import (
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tag/sequential.py"", line 26, in <module>
    from nltk.classify import NaiveBayesClassifier
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/classify/__init__.py"", line 101, in <module>
    from nltk.classify.textcat import TextCat
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/classify/textcat.py"", line 47, in <module>
    import regex as re
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Users/kylefoley/PycharmProjects/inference_engine2/inference2/proofs/other/study/regex.py"", line 370, in <module>
    sys.exit()
SystemExit
/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.
  warn(""To exit: use 'exit', 'quit', or Ctrl-D."", stacklevel=1)
`
"
596,https://github.com/nltk/nltk/issues/2283,2283,"[{'id': 718733436, 'node_id': 'MDU6TGFiZWw3MTg3MzM0MzY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/Off-topic', 'name': 'Off-topic', 'color': 'f7b4e1', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}, {'id': 1346305519, 'node_id': 'MDU6TGFiZWwxMzQ2MzA1NTE5', 'url': 'https://api.github.com/repos/nltk/nltk/labels/third-party', 'name': 'third-party', 'color': '42e5b7', 'default': False, 'description': ''}]",closed,2019-05-03 13:30:27+00:00,,2,Bug in portuguese text,"Hi!

I'm using the exemple code from:
https://github.com/parulnith/Building-a-Simple-Chatbot-in-Python-using-NLTK

I changed the code in the line 23:
`word_tokens = nltk.word_tokenize(raw)# converts to list of words`
to
`word_tokens = nltk.word_tokenize(raw, language='portuguese')# converts to list of words
`
And the line 61 from:
` TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')`
to
` TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=stopwords.words('portuguese'))`

When I use a portuguese(brazilian) text with letter ""í"" I get this error:
`Traceback (most recent call last):
  File ""chatbot.py"", line 22, in <module>
    sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences 
  File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/__init__.py"", line 105, in sent_tokenize
    return tokenizer.tokenize(text)
  File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 1277, in tokenize
    return list(self.sentences_from_text(text, realign_boundaries))
  File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 1331, in sentences_from_text
    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]
  File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 1321, in span_tokenize
    for sl in slices:
  File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 1362, in _realign_boundaries
    for sl1, sl2 in _pair_iter(slices):
  File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 321, in _pair_iter
    for el in it:
  File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 1337, in _slices_from_text
    if self.text_contains_sentbreak(context):
  File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 1383, in text_contains_sentbreak
    for t in self._annotate_tokens(self._tokenize_words(text)):
  File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 1519, in _annotate_second_pass
    for t1, t2 in _pair_iter(tokens):
  File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 318, in _pair_iter
    prev = next(it)
  File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 599, in _annotate_first_pass
    for aug_tok in tokens:
  File ""/usr/local/lib/python2.7/dist-packages/nltk/tokenize/punkt.py"", line 561, in _tokenize_words
    for line in plaintext.split('\n'):
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 5: ordinal not in range(128)`

for the 'ã', 'ç' and others there isn't error.
"
597,https://github.com/nltk/nltk/issues/2284,2284,"[{'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}]",open,2019-05-04 20:35:04+00:00,,0,ChartParser cannot parse grammar with inverted order,"The ChartParser can parse this grammar:
```python
gr = nltk.CFG.fromstring(""""""
        S -> L
        S -> S '+' S | S '-' S
        L -> '1' | '2' 
        """""")
        parser = nltk.ChartParser(gr)
        parse_obj = parser.parse('1 + 2'.split())
        tree = next(parse_obj)
```
But not this one:
```python
gr = nltk.CFG.fromstring(""""""
        L -> '1' | '2' 
        S -> L
        S -> S '+' S | S '-' S
        """""")
        parser = nltk.ChartParser(gr)
        parse_obj = parser.parse('1 + 2'.split())
        tree = next(parse_obj) # breaks (StopIteration)
```
Is this the expected behavior?"
598,https://github.com/nltk/nltk/issues/2290,2290,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}]",open,2019-05-07 02:12:33+00:00,,0,Is it alright if we automatically install the resource if it's not found?,"It has been quite some time where new users are still getting stymied with `resource_not_found` errors, e.g. https://stackoverflow.com/questions/55955060/resource-punkt-not-found 

**Would it be a good thing if we did what more modern libraries are doing by automatically installing them the first them if users don't already have them?** E.g. when using [tensorflow hub ](https://www.tensorflow.org/hub) or [pytorch pretrained bert](https://github.com/huggingface/pytorch-pretrained-BERT), or [sklearn datasets](https://scikit-learn.org/stable/datasets/index.html#id5)"
599,https://github.com/nltk/nltk/issues/2295,2295,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",closed,2019-05-09 04:10:52+00:00,,1,Treebank detokenizer fails to undo quote conversion,"Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 23:09:28) [MSC v.1916 64 bit (AMD64)] on win32
NLTK version: 3.4.1

Quotes `""` are converted to ` `` ` or `''` by the treebank tokenizer. The detokenizer is supposed to revert its regexes but fails to do so for these quotes. The space is lost also.

```python
>>> from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer
>>> tmp = TreebankWordTokenizer().tokenize('How ""are"" you?')
>>> tmp
['How', '``', 'are', ""''"", 'you', '?']
>>> TreebankWordDetokenizer().tokenize(tmp)
'How``are""you?'
```"
600,https://github.com/nltk/nltk/issues/2296,2296,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}, {'id': 738078305, 'node_id': 'MDU6TGFiZWw3MzgwNzgzMDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python2.7', 'name': 'python2.7', 'color': '7be833', 'default': False, 'description': None}]",closed,2019-05-10 08:47:12+00:00,,4,Dropping Python 2.7 Support,"From https://travis-ci.org/nltk/nltk/jobs/530566954, some of the dependencies NLTK is dependent on no longer supports Python 2.7. I think it's good timing to also drop support for Python 2.7 so that our CI continues to work and move the library forward."
601,https://github.com/nltk/nltk/issues/2299,2299,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}, {'id': 1004152779, 'node_id': 'MDU6TGFiZWwxMDA0MTUyNzc5', 'url': 'https://api.github.com/repos/nltk/nltk/labels/text', 'name': 'text', 'color': 'bfdbfc', 'default': False, 'description': ''}]",closed,2019-05-15 08:19:00+00:00,,12,collocations function returns error,"I was going through chapter 1 of the book and the collocations function returns an error. It seems like line 440 in text.py is redundant, since the collocation_list function has been introduced. I fixed the issue by rewriting the current line 440 and line 441 in text.py.

old code:
 collocation_strings = [w1 + ' ' + w2 for w1, w2 in self.collocation_list(num, window_size)]*
 print(tokenwrap(collocation_strings, separator=""; ""))

new code:
print(tokenwrap(self.collocation_list(), separator=""; ""))
 "
602,https://github.com/nltk/nltk/issues/2301,2301,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}, {'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2019-05-16 09:15:09+00:00,,2,Difference between stanford-corenlp-full-2018-02-27  and stanford-corenlp-full-2018-10-05,"when I follow the tutorial on `pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')` with `stanford-corenlp-full-2018-10-05 package`, I got an error:

`TypeError: __init__() got an unexpected keyword argument 'tagtype'`

The error was solved when I choosing the` stanford-corenlp-full-2018-02-27`. so I think there maybe something wrong for the latest released package."
603,https://github.com/nltk/nltk/issues/2302,2302,[],open,2019-05-20 01:33:55+00:00,,2,metrics.agreement.AnnotationTask kappa result seems wrong,"I got strange kappa result from `AnnotationTask`

This is how I calculated kappa.

```python
from nltk.metrics.agreement import AnnotationTask

evaluator1 = [3, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 5, 5, 5, 5, 3, 5, 5, 5, 5]
evaluator2 = [0, 4, 5, 2, 2, 1, 0, 0, 5, 0, 5, 0, 5, 2, 4, 0, 0, 4, 0, 2]

data = []
for i in range(len(evaluator1)):
    for j, k in zip(evaluator1, evaluator2):
        data.append(('1', str(i), str(j)))
        data.append(('2', str(i), str(k)))

task = AnnotationTask(data)

print(task.kappa()) # 1.0158730158730158
```

Since kappa is bigger than 1, it seems wrong enough.

In addition to this computation, I checked kappa from `sklearn`.

```python
from sklearn.metrics import cohen_kappa_score

print(cohen_kappa_score(evaluator1, evaluator2)) # -0.011904761904761862
```"
604,https://github.com/nltk/nltk/issues/2303,2303,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",closed,2019-05-21 13:51:29+00:00,,1,TreebankWordTokenizer [BUG],"Looks like there's a broken regexp on Treebank's `PUNCTUATION` patterns:

```
([^\.])(\.)([\]\)}>""\']*)\s*$
```
should `""` be escaped?
"
605,https://github.com/nltk/nltk/issues/2304,2304,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}]",closed,2019-05-21 16:37:52+00:00,,6,Wrong documentation,"A correct documentation could be for example: Combining method 4 with the match interpolation of method 5 (cf. http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf)

https://github.com/nltk/nltk/blob/39d8eeb2f249ad67d11300d97971f972039bdc63/nltk/translate/bleu_score.py#L635-L640"
606,https://github.com/nltk/nltk/issues/2305,2305,[],closed,2019-05-21 19:49:45+00:00,,1,nltk.pos_tag_sents throwing exception ,"I'm new to Phyton and I have a script im trying to run and its failing on this line: 

` sent_tag = nltk.pos_tag_sents(sent)`

even when I ass the lang param still getting same error :
          
`sent_tag = nltk.pos_tag_sents(sent, lang='eng')`


```
(base) C:\Windows\system32>conda -V
conda 4.6.14

(base) C:\Windows\system32>py --version
Python 3.7.3
```

any thoughts?

```
Traceback (most recent call last):
  File ""C:\Users\na\AppData\Local\Continuum\anaconda3\lib\multiprocessing\pool.py"", line 121, in worker
    result = (True, func(*args, **kwds))
  File ""C:\na\code\philter_ucsf-master\philter.py"", line 431, in filter_task
    sent_tag = nltk.pos_tag_sents(sent)
  File ""C:\Users\na\AppData\Local\Continuum\anaconda3\lib\site-packages\nltk\tag\__init__.py"", line 180, in pos_tag_sents
    return [_pos_tag(sent, tagset, tagger) for sent in sentences]
  File ""C:\Users\na\AppData\Local\Continuum\anaconda3\lib\site-packages\nltk\tag\__init__.py"", line 180, in <listcomp>
    return [_pos_tag(sent, tagset, tagger) for sent in sentences]
  File ""C:\Users\na\AppData\Local\Continuum\anaconda3\lib\site-packages\nltk\tag\__init__.py"", line 115, in _pos_tag
    ""Currently, NLTK pos_tag only supports English and Russian ""
NotImplementedError: Currently, NLTK pos_tag only supports English and Russian (i.e. lang='eng' or lang='rus')


```


Thanks

"
607,https://github.com/nltk/nltk/issues/2306,2306,"[{'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",closed,2019-05-23 01:30:57+00:00,,2,can't load with symlink,"For whatever reason this command:

```bash
export NLTK_DATA=/nltk_data
python3 -m nltk.downloader all
```

but for some reason it went into `/root/nltk_data` (it did *not* honor the env variable)

So I symlinked it:

```bash
ln -sf /root/nltk_data /nltk_data
```

but it couldn't handle the symlinks which took me a while to figure out, so I just had to do this:

```bash
cp -r /root/nltk_data /nltk_data
```

the cp is slow, but more importantly it took awhile to figure out that symlinked dir wouldn't work :(  Maybe this is a python problem.


"
608,https://github.com/nltk/nltk/issues/2308,2308,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",closed,2019-05-24 00:07:14+00:00,,3,"Error Downloading Punkt ""W/python.stderr: [nltk_data] Error loading punkt: <urlopen error [Errno 7] No address     [nltk_data]     associated with hostname>""","I am using the module chaquopy to use python on android and am trying to install nltk dependencies. Upon calling the nltk.download(""punkt"") I get the following error:

```
W/python.stderr: [nltk_data] Error loading punkt: <urlopen error [Errno 7] No address
    [nltk_data]     associated with hostname>
```

I was unable to find an error matching here or on stackoverflow."
609,https://github.com/nltk/nltk/issues/2309,2309,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",open,2019-05-24 11:45:08+00:00,,1,wordnet morphy features with french words,"Hi , I saw this from nltk wordnet documentation:

```
from nltk.corpus import wordnet as wn 
print(wn.morphy('dogs'))
```
And we get `dog`.

If I want to use this feature for french words, how should I do it?"
610,https://github.com/nltk/nltk/issues/2312,2312,[],closed,2019-06-06 12:43:56+00:00,,1,Importing nltk (v3.4.2) fails because of unmanaged dependency on Numpy,"Hi

So, this cropped up today thanks to my lazy self forgetting to pin the nltk version in a couple of Dockerfiles (don't try this at home, kids). I can confirm that, as of version 3.4.2 (release 3.4.1 works), importing nltk is no longer possible without numpy being installed (or without installing the optional `[machine_learning]` extras, where the numpy dependency is specified). The culprit is an implicit dependency chain somewhere in the top-level \_\_init\_\_.py. As the stacktrace below suggests, the statement `from nltk.chunk import *` would probably be the place to start looking.

Here are the steps to reproduce the issue: 

1. 
```
$ pip install nltk
Collecting nltk                       
packages (from nltk) (1.12.0)                                                       
Installing collected packages: nltk                                      
Successfully installed nltk-3.4.2  
```
2. 

```
# tested with python3.6 and python3.7
$ python3 -c ""import nltk""
```
This gives the following stacktrace:

```
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/lemontheme/.virtualenvs/nltk/lib/python3.7/site-packages/nltk/__init__.py"", line 143, in <module>                                                                 
    from nltk.chunk import *
  File ""/home/lemontheme/.virtualenvs/nltk/lib/python3.7/site-packages/nltk/chunk/__init__.py"", line 157, in <module>                                                           
    from nltk.chunk.api import ChunkParserI
  File ""/home/lemontheme/.virtualenvs/nltk/lib/python3.7/site-packages/nltk/chunk/api.py"", line 13, in <module>                                                                 
    from nltk.parse import ParserI
  File ""/home/lemontheme/.virtualenvs/nltk/lib/python3.7/site-packages/nltk/parse/__init__.py"", line 102, in <module>                                                           
    from nltk.parse.corenlp import CoreNLPParser, CoreNLPDependencyParser
  File ""/home/lemontheme/.virtualenvs/nltk/lib/python3.7/site-packages/nltk/parse/corenlp.py"", line 19, in <module>                                                             
    from nltk.tag.api import TaggerI
  File ""/home/lemontheme/.virtualenvs/nltk/lib/python3.7/site-packages/nltk/tag/__init__.py"", line 92, in <module>                                                              
    from nltk.tag.perceptron import PerceptronTagger
  File ""/home/lemontheme/.virtualenvs/nltk/lib/python3.7/site-packages/nltk/tag/perceptron.py"", line 24, in <module>                                                            
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
```

Current solution -- in case anyone else is having the same problem -- is to pin nltk to 3.4.1, e.g. `pip install nltk==3.4.1`. "
611,https://github.com/nltk/nltk/issues/2314,2314,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",open,2019-06-11 13:54:39+00:00,,10,Infinite loop in Wordnet closure and tree,"When I try to compute the hyponymic closure of `restrain.v.01` I get stuck in an infinite loop. Example code:
```python
>>> import nltk
>>> nltk.__version__
'3.4.3'
>>> from nltk.corpus import wordnet as wn
>>> ss = wn.synset('restrain.v.01')
>>> list(ss.closure(lambda s:s.hyponyms()))
```
And from here it just hangs. I guess that the reason is that the method is based on [`breadth_first`](http://www.nltk.org/api/nltk.html#nltk.util.breadth_first), which explicitly states in the documentation that is not designed to handle loops.

The same happens when running
```python
>>> list(ss.tree(lambda s:s.hyponyms()))
```

Which fails with a `RecursionError`. 

This seems to be closely related to #52, which is now closed, but the bug seems to be still present nonetheless. 

"
612,https://github.com/nltk/nltk/issues/2315,2315,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}]",open,2019-06-11 18:33:56+00:00,,0,Inaccuracies with Perceptron Tagger,"I'm using NLTK to tokenize and tag text pulled from webpages (similar to how the perceptron tagger was trained via the WSJ text) and I'm finding that it is suffering from some inaccuracies. 
Basically, if I filter text to exclude everything but nouns and foreign words, it still outputs symbols that it tags as nouns and/or foreign words, which is obviously an error. 
It seems to be accurate with everything else, but for whatever reason, it keeps tagging symbols as nouns and/or foreign words, and thus improperly filtering the text.
If you're looking for a code example, you could run the script I wrote that includes this function, which you can find in my repositories (it's the Narrative Web Scraper). Foxnews.com and Politico.com work well with the script, so I would recommend inserting those URLs into the script where it says 'url goes here'. 
To get around this error, I'm considering just writing some python code for my project that will filter out any symbols that slip past NLTK, but before I do so, if anyone has any ideas as to why this is happening and how it can be fixed, please let me know.
I have read that I could possibly re-train the tagger for my own project, but that seems excessive right now. I know I could also use another tagger, such as the Stanford tagger, but with the deprecating coming up and the fact that I would have to wrap it into my program, I feel like this could risk further bugs, so I'm holding off on that right now until the transition with the Stanford code is complete. 
Thanks"
613,https://github.com/nltk/nltk/issues/2317,2317,"[{'id': 718738467, 'node_id': 'MDU6TGFiZWw3MTg3Mzg0Njc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/windows%20related', 'name': 'windows related', 'color': 'fce4ba', 'default': False, 'description': None}, {'id': 719346674, 'node_id': 'MDU6TGFiZWw3MTkzNDY2NzQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/plot', 'name': 'plot', 'color': 'eddf76', 'default': False, 'description': None}]",open,2019-06-12 20:30:04+00:00,,1,Import Error/Variable Error in probability.ConditionalFreqDist.plot(),"It seems like changes going back to 42ffffd36cec0fa10418e7d19294d5b91c2b05b6 (2/23/2019) are causing me issues.

1. On attempting to run example from Chapter 2 of the nltk book (below) I get an ImportError.  Seems to be related to the change made on Feb 23 specifically related to the imports:
`cfd = nltk.ConditionalFreqDist( ( genre, word ) for genre in brown.categories( ) for word in brown.words( categories=genre ) )
    cfd.plot()`

> Traceback (most recent call last):
  File ""C:\XXX\PycharmProjects\ciExp\venv\lib\site-packages\nltk\probability.py"", line 1907, in plot
    from matplotlib import plt
ImportError: cannot import name 'plt' from 'matplotlib' (C:\XXX\PycharmProjects\ciExp\venv\lib\site-packages\matplotlib\__init__.py)

2. Correcting the import statement to `import matplotlib.pyplot as plt` as it was before Feb 23 results in an UnboundedLocalError in the same plot() function.  There was a change on May 7 to that line where it removed a comment from `if v in self`: 

> File ""C:\XXX\PycharmProjects\ciExp\venv\lib\site-packages\nltk\probability.py"", line 1919, in plot
    kwargs, 'samples', sorted(set(v for c in conditions
  File ""C:\XXX\PycharmProjects\ciExp\venv\lib\site-packages\nltk\probability.py"", line 1920, in <genexpr>
    if v in self
UnboundLocalError: local variable 'v' referenced before assignment

Code snippet from probability.py:
`samples = _get_kwarg(
            kwargs, 'samples', sorted(set(v for c in conditions
                                          if v in self
                                          for v in self[c]))
        )  # this computation could be wasted`

3. Commenting out the `if v in self` line (probably ill advised but I wanted to walk this back) results in a NameError:

>   File ""C:\XXX\PycharmProjects\ciExp\venv\lib\site-packages\nltk\probability.py"", line 1940, in plot
    ax.plot(freqs, *args, **kwargs)
NameError: name 'ax' is not defined

I can basically continue to walk this back until the code just hangs infinitely (as far as I can tell).  By reverting the probability.py file back before Feb 23, the matplotlib window will show up but (once again, as far as I can tell) it hangs infinitely without showing my data.  Other matplotlib function calls seem to work just fine (like those in chapter 1).  In my py 3.7 virtenv the version of nltk: 3.4.3 and matplotlib: 3.1.0 so I'm pretty sure it's not an installation issue.  Any help would be appreciated!"
614,https://github.com/nltk/nltk/issues/2320,2320,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}]",open,2019-06-15 14:39:19+00:00,,7,"One-shot BLEU-[2, 3, 4] computation","Hello everyone,

I need to compute the BLEU score with more than one ngram length (ideally, BLEU2, BLEU3, BLEU4, and BLEU5). In my case, this is a very long task, as every hypothesis has some thousand references.

Reading the implementation of the corpus_bleu function, which takes weights:Tuple between its parameters - and thus calculating BLEU-[len(weights)] - , I found out that it gets all the information to compute BLEU-m s.t. 2 <= m < len(weights).
Wouldn't it be nice to have a more general function that can compute BLEU with different ngram lengths at the same time? A possible implementation would be the weights parameter accepting a list of weight tuples as value, computing precisions that are useful for the longest tuple and using those precisions to get all the desired BLEU scores using the weights list.

This would result in a more general implementation and it would avoid the computation waste of calculating the same values more than one time."
615,https://github.com/nltk/nltk/issues/2321,2321,[],closed,2019-06-16 13:07:21+00:00,,1,CoreNLPDependencyParser stops parsing when it encounters dot,"`parse_tree1, = dep_parser.raw_parse(""The imams were removed from a US Airways flight awaiting departure from the Minneapolis-St . Paul airport ."")
print(parse_tree1.to_conll(10))`
the parse result as follows:
`1	The	the	DT	DT	_	2	det	_	_
2	imams	imam	NNS	NNS	_	4	nsubjpass	_	_
3	were	be	VBD	VBD	_	4	auxpass	_	_
4	removed	remove	VBN	VBN	_	0	ROOT	_	_
5	from	from	IN	IN	_	9	case	_	_
6	a	a	DT	DT	_	9	det	_	_
7	US	US	NNP	NNP	_	9	compound	_	_
8	Airways	Airways	NNPS	NNPS	_	9	compound	_	_
9	flight	flight	NN	NN	_	4	nmod	_	_
10	awaiting	await	VBG	VBG	_	9	acl	_	_
11	departure	departure	NN	NN	_	10	dobj	_	_
12	from	from	IN	IN	_	14	case	_	_
13	the	the	DT	DT	_	14	det	_	_
14	Minneapolis-St	minneapolis-st	NN	NN	_	11	nmod	_	_
15	.	.	.	.	_	4	punct	_	_
`
The result only have the context before the dot."
616,https://github.com/nltk/nltk/issues/2326,2326,[],closed,2019-06-30 19:36:21+00:00,,1,distinguish types of adjectives,"It would be great if we can use nltk to distinguish which semantic type of adjective / adverb it is. Here is an example list I use to do this manually :
```
       ""quantitative"": [""half"", ""some"", ""several"", ""much"", ""all"", ""many"", ""whole"", ""no"", ""enough"", ""little"", ""any"", ""sufficient"", ""none"", ""most"", ""few""],
        ""demonstrative"": [""this"", ""that"", ""these"", ""those""],
        ""possessive"": [""my"", ""his"", ""their"", ""your"", ""our"", ""mine"", ""his"", ""hers"", ""theirs"", ""yours"", ""ours""],
        ""interrogative"": [""which"", ""what"", ""whose""],
        ""distributive"": [""each"", ""every"", ""either"", ""neither"", ""any""],
        ""article"": [""a"", ""an"", ""the""],
```"
617,https://github.com/nltk/nltk/issues/2327,2327,[],closed,2019-07-01 17:00:17+00:00,,2,Different bleu scores in Google colab and Jupyter notebook,"I am trying get Bleu score for a list of 1000 English sentences and their references.
sample sentence is as below,
```
prediction_en = 'A man in an orange hat presenting something'
reference_en= 'A man in an orange hat starring at something'
```
Used code,
```
from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.bleu_score import SmoothingFunction
smoother = SmoothingFunction()

sentence_bleu(reference_en, prediction_en , smoothing_function=smoother.method4)
```
For this I am getting a Bleu score of ``` 0.768521``` in google colab but I am getting ``` 1.3767075064676063e-231``` score in jupyter notebook without smoothing and with smoothing ``` 0.3157039```  

Can you please help me what and where I am going wrong?"
618,https://github.com/nltk/nltk/issues/2328,2328,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",open,2019-07-02 09:21:42+00:00,,0,TreebankWordDetokenizer fails to reattach comma,"For tokenisation, the commit https://github.com/nltk/nltk/pull/223/commits/2414f9f2d99402e64e6cb5b2b3a26df94f6b8837 introduced an exception for commas and colons, stating that a comma can be a token in its own right (insert a space before and after it) if it is not followed by a digit, presumably in order to ensure 30,000,000 and 12:59 remain a single token.

However, when _detokenisation_ was added, I don't think the reverse of that rule makes sense:

```
(
    re.compile(r'\s([:,])\s([^\d])'),
    r'\1 \2',
)  # Keep right pad after comma/colon before non-digits.
```

(""Remove a space before the comma, as long as it is not followed by a space and a digit."")

If it were followed by a digit in the tokenisation, it would not have gained an intervening space: 30,000,000 remained 30,000,000, so `\s([:,])\s` already doesn't match (no need to restrict `[^\d]`).

On the other hand, if the pre-tokenised original had a comma, space and a digit, then a space would have been inserted before the comma, because the space between the comma and a digit qualified as a non-digit. However, in reverse, that space does not get deleted:

```
from nltk.tokenize.treebank import *
""From those 100, 10 were selected,""      # => 'From those 100, 10 were selected.'
TreebankWordTokenizer().tokenize(_)      # => ['From', 'those', '100', ',', '10', 'were', 'selected', '.']
TreebankWordDetokenizer().detokenize(_)  # => 'From those 100 , 10 were selected.'
```

Notice the comma not being restored to the normal position. I believe the reverse rule should simply say that if a comma is all by its lonesome, smush it with the previous token:

```
(re.compile(r'\s([:,])\s'), r'\1 ')
```

or maybe even this, in case the comma is the last token:

```
(re.compile(r'\s([:,])'), r'\1')
```
"
619,https://github.com/nltk/nltk/issues/2329,2329,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}]",open,2019-07-03 13:30:22+00:00,,0,sklearn DeprecationWarning:  the imp module is deprecated in favour of importlib,"```
/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
```

I have on macOS

```
$ python3 --version
Python 3.7.3
```

and latest nltk from pip3.
This issue should be related to https://github.com/scikit-learn/scikit-learn/issues/12434"
620,https://github.com/nltk/nltk/issues/2330,2330,[],closed,2019-07-04 07:50:08+00:00,,1,ConditionalFreqDist plot has three bugs,"`
def plot(self, *args, **kwargs):
        """"""
        Plot the given samples from the conditional frequency distribution.
        For a cumulative plot, specify cumulative=True.
        (Requires Matplotlib to be installed.)

        :param samples: The samples to plot
        :type samples: list
        :param title: The title for the graph
        :type title: str
        :param conditions: The conditions to plot (default is all)
        :type conditions: list
        """"""
        try:
            from matplotlib import plt
        except ImportError:
            raise ValueError(
                'The plot function requires matplotlib to be installed.'
                'See http://matplotlib.org/'
            )

        cumulative = _get_kwarg(kwargs, 'cumulative', False)
        percents = _get_kwarg(kwargs, 'percents', False)
        conditions = _get_kwarg(kwargs, 'conditions', sorted(self.conditions()))
        title = _get_kwarg(kwargs, 'title', '')
        samples = _get_kwarg(
            kwargs, 'samples', sorted(set(v for c in conditions
                                          if v in self
                                          for v in self[c]))
        )  # this computation could be wasted
        if ""linewidth"" not in kwargs:
            kwargs[""linewidth""] = 2

        for condition in conditions:
            if cumulative:
                freqs = list(self[condition]._cumulative_frequencies(samples))
                ylabel = ""Cumulative Counts""
                legend_loc = 'lower right'
                if percents:
                    freqs = [f / freqs[len(freqs) - 1] * 100 for f in freqs]
                    ylabel = ""Cumulative Percents""
            else:
                freqs = [self[condition][sample] for sample in samples]
                ylabel = ""Counts""
                legend_loc = 'upper right'
            # percents = [f * 100 for f in freqs] only in ConditionalProbDist?
            kwargs['label'] = ""%s"" % condition
            ax.plot(freqs, *args, **kwargs)


        ax.legend(loc=legend_loc)
        ax.grid(True, color=""silver"")
        ax.set_xticks(range(len(samples)), [text_type(s) for s in samples], rotation=90)

        if title:
            ax.set_title(title)
        ax.set_xlabel(""Samples"")
        ax.set_ylabel(ylabel)
        plt.show()

        return ax`

**must modify this:**

`
def plot(self, *args, **kwargs):
        """"""
        Plot the given samples from the conditional frequency distribution.
        For a cumulative plot, specify cumulative=True.
        (Requires Matplotlib to be installed.)

        :param samples: The samples to plot
        :type samples: list
        :param title: The title for the graph
        :type title: str
        :param conditions: The conditions to plot (default is all)
        :type conditions: list
        """"""
        try:
            import matplotlib.pyplot as plt
        except ImportError:
            raise ValueError(
                'The plot function requires matplotlib to be installed.'
                'See http://matplotlib.org/'
            )

        cumulative = _get_kwarg(kwargs, 'cumulative', False)
        percents = _get_kwarg(kwargs, 'percents', False)
        conditions = _get_kwarg(kwargs, 'conditions', sorted(self.conditions()))
        title = _get_kwarg(kwargs, 'title', '')
        samples = _get_kwarg(
            kwargs, 'samples', sorted(set(v for c in conditions
                                          if c in self
                                          for v in self[c]))
        )  # this computation could be wasted
        if ""linewidth"" not in kwargs:
            kwargs[""linewidth""] = 2

        ax = plt.gca()
        for condition in conditions:
            if cumulative:
                freqs = list(self[condition]._cumulative_frequencies(samples))
                ylabel = ""Cumulative Counts""
                legend_loc = 'lower right'
                if percents:
                    freqs = [f / freqs[len(freqs) - 1] * 100 for f in freqs]
                    ylabel = ""Cumulative Percents""
            else:
                freqs = [self[condition][sample] for sample in samples]
                ylabel = ""Counts""
                legend_loc = 'upper right'
            # percents = [f * 100 for f in freqs] only in ConditionalProbDist?
            kwargs['label'] = ""%s"" % condition
            ax.plot(freqs, *args, **kwargs)


        ax.legend(loc=legend_loc)
        ax.grid(True, color=""silver"")
        ax.set_xticks(range(len(samples)))
        ax.set_xticklabels([text_type(s) for s in samples], rotation=90)

        if title:
            ax.set_title(title)
        ax.set_xlabel(""Samples"")
        ax.set_ylabel(ylabel)
        plt.show()

        return ax`"
621,https://github.com/nltk/nltk/issues/2331,2331,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}]",closed,2019-07-04 22:55:39+00:00,,3,PickleCorpusView raising UnicodeDecodeError when reading file,"I have been trying to save a modified corpus produced with `LazyMap` using`PickleCorpusView.write` as shown in the [PickleCorpusView documentation](https://www.nltk.org/_modules/nltk/corpus/reader/util.html#PickleCorpusView).

The writing seems to be working properly. However, when trying to read the pickled file, it raises a `UnicodeDecodeError`.

The following code does not even use a `LazyMap` but raises the same error

```python
Python 3.7.3 (default, Jun 24 2019, 04:54:02) 
[GCC 9.1.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import nltk
>>> nltk.corpus.reader.PickleCorpusView.write('In the real use case, this would be the result of LazyMap producing a list of list of tuples (word and PoS tag).', 'test.pickle')
>>> nltk.corpus.reader.PickleCorpusView('test.pickle')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/user/.virtualenv/venv/lib/python3.7/site-packages/nltk/collections.py"", line 231, in __repr__
    for elt in self:
  File ""/home/user/.virtualenv/venv/lib/python3.7/site-packages/nltk/corpus/reader/util.py"", line 306, in iterate_from
    tokens = self.read_block(self._stream)
  File ""/home/user/.virtualenv/venv/lib/python3.7/site-packages/nltk/corpus/reader/util.py"", line 525, in read_block
    result.append(pickle.load(stream))
  File ""/home/user/.virtualenv/venv/lib/python3.7/site-packages/nltk/data.py"", line 1177, in read
    chars = self._read(size)
  File ""/home/user/.virtualenv/venv/lib/python3.7/site-packages/nltk/data.py"", line 1469, in _read
    chars, bytes_decoded = self._incr_decode(bytes)
  File ""/home/user/.virtualenv/venv/lib/python3.7/site-packages/nltk/data.py"", line 1491, in _incr_decode
    return self.decode(bytes, 'strict')
  File ""/home/user/.virtualenv/venv/lib/python3.7/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
```

Am I missing something from the documentation?

I am using Python 3.7.3 and NLTK 3.4.1 running on Arch Linux (kernel 5.1.15). The same error also occurred in Ubuntu."
622,https://github.com/nltk/nltk/issues/2332,2332,"[{'id': 81645781, 'node_id': 'MDU6TGFiZWw4MTY0NTc4MQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/language-model', 'name': 'language-model', 'color': 'd4c5f9', 'default': False, 'description': ''}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}]",closed,2019-07-05 12:26:25+00:00,,1,ZeroDivisionError: in nltk.lm,"It seems KneserNeyInterpolated can't handle unseen prefixes

to reproduce the error:

```
from functools import partial

from nltk.lm import KneserNeyInterpolated
from nltk.lm.preprocessing import pad_both_ends
from nltk.util import everygrams
from itertools import chain
flatten = chain.from_iterable

data = [[""i"", ""don'"", 't', ""go""], [""i"", ""dont"", ""know""], [""i"", ""don'"", 't', ""go"", ""what""]]
ngram = 4
padding_fn = partial(pad_both_ends, n=ngram)
train_data = (everygrams(list(padding_fn(sent)), max_len=ngram) for sent in data)

vocab = flatten(map(padding_fn, data))
model = KneserNeyInterpolated(ngram)
model.fit(train_data, vocab)

model.score('what', ['i', 'dont', 'go'])
```

I get
```
~\Anaconda3\lib\site-packages\nltk\lm\smoothing.py in alpha(self, word, prefix_counts)
     55 
     56     def alpha(self, word, prefix_counts):
---> 57         return max(prefix_counts[word] - self.discount, 0.0) / prefix_counts.N()
     58 
     59     def gamma(self, prefix_counts):

ZeroDivisionError: float division by zero
```
"
623,https://github.com/nltk/nltk/issues/2333,2333,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",open,2019-07-05 21:37:07+00:00,,0,"Sentence and word tokenizers fail for unicode double quotes u""\u201C"" and u""\u201D""","This is related to #1675 and #1682.

```
import nltk

text = '“First sentence.” Next one.'

sentences = nltk.tokenize.sent_tokenize(text)
print(sentences)
print(nltk.tokenize.word_tokenize(sentences[0]))
```
doesn't split the two sentences and doesn't tokenize `sentence.` into `sentence` and `.`:
```
['“First sentence.” Next one.']
['“', 'First', 'sentence.', '”', 'Next', 'one', '.']
```
I'm wondering why it **works correctly if you leave out the second sentence**:
`text = '“First sentence.”'` is correctly tokenized into: `['“', 'First', 'sentence', '.', '”']`
 
(nltk 3.4.4 / Python 3.4.3 (default, Nov 12 2018, 22:25:49) [GCC 4.8.4] on linux)"
624,https://github.com/nltk/nltk/issues/2334,2334,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2019-07-09 08:51:19+00:00,,3,TypeError in FreqDist.tabulate for bigrams,"I use the following code:

```python
with open(corpus_filename) as corpus_f:
    tokens = preprocess(corpus_f.read())
    bgs = nltk.bigrams(tokens)
    fdist =  nltk.FreqDist(bgs)
    print(fdist.tabulate())
```

What happens is the `tabulate` method can't format the tuples generated by `bigrams`:
`width = max(len(""%s"" % s) for s in samples)`

When it gets to any key like `('hello', 'world)`, it breaks:
```python
Traceback (most recent call last):
  File ""learner.py"", line 32, in <module>
    print(fdist.tabulate())
  File ""<edited>/env/lib/python3.7/site-packages/nltk/probability.py"", line 333, in tabulate
    width = max(len(""%s"" % s) for s in samples)
  File ""<edited>/env/lib/python3.7/site-packages/nltk/probability.py"", line 333, in <genexpr>
    width = max(len(""%s"" % s) for s in samples)
TypeError: not all arguments converted during string formatting
```

This is because the string formatting doesn't work:
```bash
$ ipython
Python 3.7.3 (default, Apr  3 2019, 05:39:12) 
Type 'copyright', 'credits' or 'license' for more information
IPython 7.6.1 -- An enhanced Interactive Python. Type '?' for help.

In [1]: ""%s"" % ('hello', 'world')                                                                                                                                                             
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-1-60c0688ae365> in <module>
----> 1 ""%s"" % ('hello', 'world')

TypeError: not all arguments converted during string formatting

In [2]: ""{}"".format(('hello', 'world'))                                                                                                                                                       
Out[2]: ""('hello', 'world')""
```

Would it be OK to replace the `%s` formatting with the [""new style"" string formatting operator](https://pyformat.info/)?"
625,https://github.com/nltk/nltk/issues/2335,2335,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",closed,2019-07-12 18:58:36+00:00,,4,nltk wordnet concurrency error WordNetError,"When running wordnet in multiple threads I get strange errors. 
I am trying to get the synsets of a word. 
Is Wordnet thread safe? Is there a way to solve it? If there is not immediate solution, where is the database of all words and their functions (verb/adjective/noun) stored?

The exceptions look like this:

```
raise WordNetError('line %r: %s' % (data_file_line, e))
  File ""/usr/local/lib/python2.7/dist-packages/nltk/corpus/reader/wordnet.py"", line 1342, in synset_from_pos_and_offs
et
    assert synset._offset == offset
*******************************************WordNetError: line u'02203362 40 v 03 have 0 have_got 0 hold 0 013 + 13244109 n 0303 + 10529202630189 42 v 02 have 0 feature 0 023 $ 02203362 v 0000 $ 02204692 v 0000 + 05601758 n 0201 + 05849789 n 0201 ! 02632353 v 0102 ~ 00047610 v 0000 ~ 00047745 v 0000 ~ 01123609 v 0000 ~ 02630734 v 0000 ~ 02630871 v 0000 ~ 02631005 v 0000 ~ 02631163 v 0000 ~ 02631349 v 0000 ~ 02631659 v 0000 ~ 02632167 v 0000 ~ 02636132 v 0000 ~ 02715456 v 0000 ~ 02715595 v 0000 ~ 02715812 v 0000 ~ 02717102 v 0000 ~ 02730813 v 0000 ~ 02740034 v 000at can only carry a small sail""  \n': need more than 1 value to unpack
AssertionError
```"
626,https://github.com/nltk/nltk/issues/2338,2338,"[{'id': 22487250, 'node_id': 'MDU6TGFiZWwyMjQ4NzI1MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3', 'name': 'python3', 'color': 'b3fca6', 'default': False, 'description': ''}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2019-07-16 23:45:45+00:00,,5,DeprecationWarnings for formatargspec and collections in python3.7,"Hi there, I was upgrading python2.7 to python3.7. When running pytests, bunch of warnings pop up from nltk library. 

Line 68 in decorators.py
`DeprecationWarning: 'formatargspec' is deprecated since Python 3.5. Use 'signature' and the 'Signature' object directly` 

Line 15 in counter.py and line 13 in vocabulary.py
`DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working`
"
627,https://github.com/nltk/nltk/issues/2339,2339,"[{'id': 888120541, 'node_id': 'MDU6TGFiZWw4ODgxMjA1NDE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/cluster', 'name': 'cluster', 'color': '2b5196', 'default': False, 'description': ''}]",open,2019-07-17 00:57:11+00:00,,0,svd_dimensions in VectorSpaceClusterer is broken again. ,"So I would like to try using svd_dimensions in KMeansCluster. So you set it to what you want when you instantiate the class. Then, when you run kclusterer.cluster, it's supposed to pick up that setting and apply it to your input. 

But then it complains about mismatched dimensions, so it must not be doing that part correctly. 

Exception has occurred: ValueError
shapes (500,1124) and (500,) not aligned: 1124 (dim 1) != 500 (dim 0)

Any idea what's going on? I have 1124 dimensions, so SVD would be helpful. 

Thanks"
628,https://github.com/nltk/nltk/issues/2340,2340,"[{'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2019-07-19 09:43:05+00:00,,2,OSError: Java command failed when using stanford parser example (Standford Tagger2018),"Hi,

I am trying to run the stanford parser example. E.g.
`create_train_data_restaurant(fn_train, args.out_dir+args.word_idx, args.out_dir, args.StanfordPOSTag_dir, args.domain,'Train', sent_len, sent_num)`

executing the last command results with an error:

raise OSError('Java command failed : ' + str(cmd))

`OSError: Java command failed : ['/usr/bin/java', '-mx1000m', '-cp', 'stanford-postagger-full/stanford-postagger.jar', 'edu.stanford.nlp.tagger.maxent.MaxentTagger', '-model', 'stanford-postagger-full/models/english-left3words-distsim.tagger', '-textFile', '/var/folders/dm/1gjd_wh55fn_qy6bmsh17c340000gn/T/tmpess89s0u', '-tokenize', 'false', '-outputFormatOptions', 'keepEmptySentences', '-encoding', 'utf8']`

Thank you!"
629,https://github.com/nltk/nltk/issues/2341,2341,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}]",open,2019-07-22 02:11:23+00:00,,5,A wrong implementation in bleu_score.SmoothingFunction,"I found the bug in the condition like this:

```
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction

hyp = ['a', 'a', 'a', 'a', 'a', 'a', 'a']
ref = [['a', 'a', 'a', 'd', 'a', 'a', 'a']]
print(corpus_bleu([ref], [hyp], (0, 0, 0, 1), smoothing_function=SmoothingFunction().method4))

hyp = ['a', 'b', 'b', 'b', 'b', 'b', 'b']
ref = [['a', 'a', 'a', 'd', 'a', 'a', 'a']]
print(corpus_bleu([ref], [hyp], (0, 0, 0, 1), smoothing_function=SmoothingFunction().method4))
```

The output is:

```
0.17954959837224665
0.17954959837224665
```

It's quite strange why they can have same smoothing 4-gram precision, so I went to the code at https://github.com/nltk/nltk/blob/develop/nltk/translate/bleu_score.py#L576-L590

```
def method4(self, p_n, references, hypothesis, hyp_len, *args, **kwargs):
        """"""
        Smoothing method 4:
        Shorter translations may have inflated precision values due to having
        smaller denominators; therefore, we give them proportionally
        smaller smoothed counts. Instead of scaling to 1/(2^k), Chen and Cherry
        suggests dividing by 1/ln(len(T)), where T is the length of the translation.
        """"""
        for i, p_i in enumerate(p_n):
            if p_i.numerator == 0 and hyp_len != 0:
                incvnt = i + 1 * self.k / math.log(
                    hyp_len
                )  # Note that this K is different from the K from NIST.
                p_n[i] = 1 / incvnt
        return p_n
```

`p_n[i]` is a fraction object, once `p_n[i].numberator == 0`, it will turn to a constant of float that `p_n[i] = 1 / (i + self.k / math.log(hyp_len))`.

I find the original paper and it says:

**the numerator (to be precise, the matched n-gram count) should be `1 / (i + self.k / math.log(hyp_len))`, but the denominator should remain the same !!**

I go through the other smoothing function roughly, it seems Smoothing 4-7 have the same problem... I think it should be fixed immediately, because I have already find some papers' results are wrong because of ``nltk``....

"
630,https://github.com/nltk/nltk/issues/2343,2343,[],closed,2019-07-22 16:05:22+00:00,,1,CONTRIBUTING.md broken,I want to make a simple suggestion. But information about how to contribute (i.e. CONTRIBUTING.md file) is not loading.
631,https://github.com/nltk/nltk/issues/2344,2344,"[{'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2019-07-23 06:20:20+00:00,,2,LookupError:    NLTK was unable to find stanford-parser\.jar! Set the CLASSPATH   environment variable.,"I did follow this article for Stanford Parser:
https://gist.github.com/alvations/e1df0ba227e542955a8a 
My error code:
```
from nltk.internals import find_jars_within_path
from nltk.parse.stanford import StanfordDependencyParser
dep_parser=StanfordDependencyParser(model_path=""edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz"")
stanford_dir = dep_parser._classpath[0].rpartition('/')[0]
# or in windows comment the line above and uncomment the one below:
#stanford_dir = st._stanford_jar.rpartition(""\\"")[0]
stanford_jars = find_jars_within_path(stanford_dir)
st.stanford_jar = ':'.join(stanford_jars)
```
And I did google many topics for solving but it didn't help me at all. Thank you for your help!

"
632,https://github.com/nltk/nltk/issues/2345,2345,"[{'id': 81645781, 'node_id': 'MDU6TGFiZWw4MTY0NTc4MQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/language-model', 'name': 'language-model', 'color': 'd4c5f9', 'default': False, 'description': ''}]",closed,2019-07-23 10:50:26+00:00,,11,Saving a language model,"Hi there,
I am wondering how I can save a trained language model (KneserNeyInterpolated, or anything that is derived from the base LanguageModel class), The api does not seem to exhibit a save()-function and pickling didn't work. Any hints?
Cheers,
a"
633,https://github.com/nltk/nltk/issues/2346,2346,"[{'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}]",open,2019-07-29 23:55:39+00:00,,0,Possible Install Solve,"Possible Solve. no errors so far. working with 64 bit
installed pip, numpy, wheel, pipenv, setuptools, virtualenv, virtualwrapper, NLTK, etc.

pip needs to be 19+ version and python 3+ version

system-> Advanced system settings -> Environment Variables -> EDIT path -> NEW paths

C:\Users\USERNAME\AppData\Local\Programs\Python\Python37\Scripts
C:\Users\USERNAME\AppData\Local\Programs\Python\Python37
C:\Users\USERNAME\AppData\Roaming\Python\Python37\Scripts
C:\Users\USERNAME\AppData\Roaming\Python\Python37

of course these will differ for where ever you installed pip & python and your username
IMPORTANT you do Local and Roaming paths

Hope this helps! seems to work for me

_Originally posted by @KyleRaney in https://github.com/nltk/nltk/issues/1079#issuecomment-516202186_"
634,https://github.com/nltk/nltk/issues/2347,2347,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}]",closed,2019-07-30 16:18:47+00:00,,0,METEOR metric does not use user set stemmer ,The Meteor metric offers the ability to use a custom stemmer but does not pass it through to the relevant function. So in every case even if the function meteor_score is called with a custom stemmer it still uses the default PorterStemmer 
635,https://github.com/nltk/nltk/issues/2350,2350,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",closed,2019-07-31 01:03:41+00:00,,1,"""kern"" (v) in nltk.corpus.wordnet is antonymous to itself!","^ that is pretty much it. I am trying to build a graph based on wordnet's verb structure, and when I'm building antonymous relationship between antonymous words, it basically says ""kern"" is antonymous to ""kern"", which I assume is a bug based on the dictionary definitions of ""kern"".

Any help would be much appreciated! :)"
636,https://github.com/nltk/nltk/issues/2351,2351,"[{'id': 14019255, 'node_id': 'MDU6TGFiZWwxNDAxOTI1NQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pleaseverify', 'name': 'pleaseverify', 'color': 'd7e102', 'default': False, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",open,2019-07-31 01:27:01+00:00,,2,WordNet lemmas do not contain non-zero causes() and entailments(),"Hey! I have been trying to look for the causes() and entailments() relationship between verbs like in the WordNet documentation by Christiane Fellbaum, but those functions return no non-empty lists given any lemma. Based on Christiane's documentation, Lemma('kill').causes() should return a list that contains at least one element, 'die'. Is this an unimplemented feature of NLTK WordNet, or is it a bug?

Thanks! :)"
637,https://github.com/nltk/nltk/issues/2352,2352,"[{'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}]",open,2019-08-01 16:01:53+00:00,,1,Escape parentheses in NLTK parse tree,"In NLTK we can convert a parentheses tree into an actual Tree object. However, when a token contains parentheses, the parsing is not what you would expect since NLTK parses those parentheses as a new node.

As an example, take the sentence

> They like(d) it a lot

This could be parsed as 

```
(S (NP (PRP They)) (VP like(d) (NP (PRP it)) (NP (DT a) (NN lot))) (. .))
```

But if you parse this with NLTK into a tree, and output it - it is clear that the `(d)` is parsed as a new node, which is no surprise.


```python
from nltk import Tree

s = '(S (NP (PRP They)) (VP like(d) (NP (PRP it)) (NP (DT a) (NN lot))) (. .))'

tree = Tree.fromstring(s)
print(tree)
```

The result is

```
(S
  (NP (PRP They))
  (VP like (d ) (NP (PRP it)) (NP (DT a) (NN lot)))
  (. .))
```

So `(d )` is a node inside the VP rather than part of the token `like`. Is there a way in the tree parser to escape parentheses?

[[cross post from SO](https://stackoverflow.com/questions/57293069/escape-parentheses-in-nltk-parse-tree)]"
638,https://github.com/nltk/nltk/issues/2354,2354,[],closed,2019-08-04 07:07:16+00:00,,1,Cannot install ntlk library,"![image](https://user-images.githubusercontent.com/33275883/62420705-7fb2f280-b69f-11e9-802e-e4a2ffd457dc.png)
"
639,https://github.com/nltk/nltk/issues/2356,2356,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}]",closed,2019-08-05 05:02:53+00:00,,2,`samples` keyword arg ignored in `FreqDist.tabulate`,"It looks like the `samples` keyword argument mentioned in the docstring of `FreqDist.tabulate()` is ignored. Is this the intended behavior?

Here are the arguments from [the docstring](https://github.com/nltk/nltk/blob/1805fe870635afb7ef16d4ff5373e1c3d97c9107/nltk/probability.py#L308):

>:param samples: The samples to plot (default is all samples)
>:type samples: list
>:param cumulative: A flag to specify whether the freqs are cumulative (default = False)
>:type title: bool

Here's an example of `samples` being ignored:

```python
from nltk import FreqDist

fd = FreqDist([""foo"", ""bar""])
# I expect to get just ""foo"", but get ""foo"" *and* ""bar"".
fd.tabulate(samples=[""foo""])
```

This is different from what `ConditionalFreqDist.tabulate()` does, so I'm not sure what's intended here. Either way, it would be nice for the docstring to be clearer."
640,https://github.com/nltk/nltk/issues/2357,2357,"[{'id': 739423158, 'node_id': 'MDU6TGFiZWw3Mzk0MjMxNTg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tkinter', 'name': 'tkinter', 'color': '03c4a7', 'default': False, 'description': None}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",open,2019-08-07 08:05:29+00:00,,5,nltk.download() logs out on OS X,"Hello,

I have a macbook running OS X 10.14.6, Python 3.7.3, conda 4.7.5 .

When I try run interactive installer from NLTK python package, following the steps from: http://www.nltk.org/data.html in my terminal, after

`>> nltk.download()`

Instead of opening NLTK Downloader in a new window, I get logged out (ok, i assume something related to GUI crashes at this point). `nltk.download_shell()`works instead, as it will not try to open GUI installer, but I still want to figure out why GUI won't work for me. Any suggestions would be appreciated!
"
641,https://github.com/nltk/nltk/issues/2360,2360,"[{'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}, {'id': 719374994, 'node_id': 'MDU6TGFiZWw3MTkzNzQ5OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/GUI', 'name': 'GUI', 'color': 'f9b3d9', 'default': False, 'description': None}, {'id': 739423158, 'node_id': 'MDU6TGFiZWw3Mzk0MjMxNTg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tkinter', 'name': 'tkinter', 'color': '03c4a7', 'default': False, 'description': None}]",closed,2019-08-10 06:39:33+00:00,,5,nltk.app.wordnet() error,"Hi,
I get an error message when trying to run nltk.app.wordnet() (see below).
System specifications: Win 7 64 bit, Enthought Canopy environment version 2.1.9.3717 (64 bit) , Python version 3.5.2, NLTK version: 3.2.4-5 (I know it's not the latest released version, but for some reason Canopy insists that there aren't newer versions).
Any suggestions?
Thanks,
Chen

```
---------------------------------------------------------------------------
GetoptError                               Traceback (most recent call last)
<ipython-input-43-a9319fd796ca> in <module>()
----> 1 nltk.app.wordnet()

C:\Users\Chen G\AppData\Local\Enthought\Canopy\edm\envs\User\lib\site-packages\nltk\app\wordnet_app.py in app()
    945     # Parse and interpret options.
    946     (opts, _) = getopt.getopt(argv[1:], ""l:p:sh"",
--> 947                               [""logfile="", ""port="", ""server-mode"", ""help""])
    948     port = 8000
    949     server_mode = False
C:\Users\Chen G\AppData\Local\Enthought\Canopy\edm\envs\User\lib\getopt.py in getopt(args, shortopts, longopts)
     93             opts, args = do_longs(opts, args[0][2:], longopts, args[1:])
     94         else:
---> 95             opts, args = do_shorts(opts, args[0][1:], shortopts, args[1:])
     96 
     97     return opts, args
C:\Users\Chen G\AppData\Local\Enthought\Canopy\edm\envs\User\lib\getopt.py in do_shorts(opts, optstring, shortopts, args)
    193     while optstring != '':
    194         opt, optstring = optstring[0], optstring[1:]
--> 195         if short_has_arg(opt, shortopts):
    196             if optstring == '':
    197                 if not args:
C:\Users\Chen G\AppData\Local\Enthought\Canopy\edm\envs\User\lib\getopt.py in short_has_arg(opt, shortopts)
    209         if opt == shortopts[i] != ':':
    210             return shortopts.startswith(':', i+1)
--> 211     raise GetoptError(_('option -%s not recognized') % opt, opt)
    212 
    213 if __name__ == '__main__':
GetoptError: option -f not recognized
```"
642,https://github.com/nltk/nltk/issues/2361,2361,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}, {'id': 910625822, 'node_id': 'MDU6TGFiZWw5MTA2MjU4MjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/CI', 'name': 'CI', 'color': '76dbed', 'default': False, 'description': ''}]",closed,2019-08-10 22:18:42+00:00,,3,tqdm dependency,"It's imported in the `util` module, but not listed in the requirements. Somehow the tox tests pass, but when I tried running the unit tests for the LM module I had to install the package to get them to run.

@alvations what am I missing?"
643,https://github.com/nltk/nltk/issues/2365,2365,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}]",open,2019-08-14 00:01:56+00:00,,4,What is the proper way to compute a corpus level METEOR score?,"For BLEU, there is a `corpus_bleu` function, but there is no such function for METEOR. I am reading the METEOR paper and code, and it looks like it's more complicated than a simple average of sentence METEOR scores. How should I do it then? Should I concatenate all the hypothesis & reference sentences, and then compute the METEOR score?"
644,https://github.com/nltk/nltk/issues/2369,2369,"[{'id': 918210764, 'node_id': 'MDU6TGFiZWw5MTgyMTA3NjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/inactive', 'name': 'inactive', 'color': 'cecece', 'default': False, 'description': ''}]",closed,2019-08-19 01:29:30+00:00,,1," for features, labels in training_data:       2     x.append(features)       3     y.append(label)       4 x = np.array(x).reshape(-1, IMG_SIZE, IMG_SIZE, 1)  ValueError: not enough values to unpack (expected 2, got 1)",Does anyone know why I got this error while running that piece of code?
645,https://github.com/nltk/nltk/issues/2371,2371,"[{'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}]",open,2019-08-19 01:56:13+00:00,,0,Replace nltk.decorators.py with decorator library,"From @Copper-Head, 

Can we get rid of `decorators.py` by using the [decorator](https://pypi.org/project/decorator/) library?"
646,https://github.com/nltk/nltk/issues/2372,2372,"[{'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}]",closed,2019-08-19 18:45:59+00:00,,3,CoreNLPServer occasionally raises 'Could not connect to the server.' error even when rerunning with context manager.,"I am running a batch script to generate constituency parse trees for text files in a directory using CoreNLPServer:

```
from nltk.tokenize import sent_tokenize
from nltk.parse import CoreNLPParser
from nltk.parse.corenlp import CoreNLPServer

parent_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
_STANFORD_CORENLP_PATH = os.path.join(parent_path, ""stanford-corenlp-full-2018-10-05"")
_jars = (
    os.path.join(_STANFORD_CORENLP_PATH, ""stanford-corenlp-3.9.2.jar""),
    os.path.join(_STANFORD_CORENLP_PATH, ""stanford-corenlp-3.9.2-models.jar""),
)

def get_text_metrics_bulk(input_dir):
    text_metric_dict_list = []
    file_list = sorted(os.listdir(input_dir))
    with CoreNLPServer(*_jars):
        constituency_parser = CoreNLPParser()
        for i, file in enumerate(file_list):
            with open(os.path.join(input_dir, file), 'r') as ifile:
                text = ' '.join([line.strip() for line in ifile.readlines()])

            parse_trees = []
            for sentence in sent_tokenize(text):
                parse_trees.append(next(constituency_parser.raw_parse(sentence)))
return parse_trees
```

The script works fine most of the time except occasionally there would be hiccups where CoreNLP [raises 'Could not connect to the server.' error](https://github.com/nltk/nltk/blob/289f560fe4b58c25a842e4c5b1a1c820af4a840e/nltk/parse/corenlp.py#L153) This is the response I get from the coreNLP server when when I send a request: 
`requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=49585): Max retries exceeded with url: /live (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f7c3c972dd8>: Failed to establish a new connection: [Errno 111] Connection refused'))
`

When this error occurs I will have to manually kill the java process in the system monitor before I can rerun the script again. I am a bit confused because I thought using a context manager `with CoreNLPServer(*_jars):` would kill the process automatically in this situation. For now I just placed a` self.stop()` before the error raise to stop needing to automatically kill the process. 



"
647,https://github.com/nltk/nltk/issues/2373,2373,"[{'id': 159235791, 'node_id': 'MDU6TGFiZWwxNTkyMzU3OTE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/dependency%20parsing', 'name': 'dependency parsing', 'color': 'eb6420', 'default': False, 'description': None}, {'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}]",open,2019-08-20 09:35:29+00:00,,0,MaltParser bug for parse_tagged_sents,"maltparser version 1.9.2.
I've input several sentences but the parser only output the result of the first sentence.
Then I read the examples in maltparser and I found that the input and output .conll files only use one blank line to split two sentences.
It seems that nltk.parse.util.taggedsents_to_conll(sentences) should deal with this problem."
648,https://github.com/nltk/nltk/issues/2374,2374,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}]",open,2019-08-21 16:03:04+00:00,,1,"problem with pos_tag and the cardinal ""zero""","in the following code, first sentence tagging unexpectedly tags ""zero"" as 'NN', second sentence tagging correctly tags ""zero"" as 'CD'

def preprocess(sent):
	sent = nltk.word_tokenize(sent)
	sent = nltk.pos_tag(sent)
	return sent

sentence = """"""my social security number is nine one three seven four four six five zero definitely.""""""
sent = preprocess(sentence)
print(sent)
sentence = """"""my social security number is nine one three seven four four six zero five definitely.""""""
sent = preprocess(sentence)
print(sent)
"
649,https://github.com/nltk/nltk/issues/2375,2375,[],closed,2019-08-26 17:26:32+00:00,,2,Breakdown edit_distance so as to see how many operations of each type are required?,"Is it possible that in addition to getting the output of edit_distance metric to also get statistics regarding how many insertions, substitutions, deletions are required to transform string s1 into s2?"
650,https://github.com/nltk/nltk/issues/2376,2376,"[{'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",open,2019-08-26 17:44:51+00:00,,3,Splitting sentences fails on some corner cases,"I understand how difficult it is to split sentences that contain abbreviations and that adding abbreviations can have pitfalls, as it is nicely explained in #2154. However, I have stumbled upon some corner cases that I would like to ask about. It looks like using any of the following
- e.g.
- i.e.
- et al.

in the sentence will split the sentence in a wrong way.

Example for i.e. and e.g.

```python
>>> sentence = (""Even though exempli gratia and id est are both Latin ""
                ""(and therefore italicized), no need to put e.g. or i.e. in ""
                ""italics when they’re in abbreviated form."")
>>> sent_tokenize_list = sent_tokenize(sentence)                                                                                                                           

>>> sent_tokenize_list                                                                                                                                            
['Even though exempli gratia and id est are both Latin (and therefore italicized), no need to put e.g.',
 'or i.e.',
 'in italics when they’re in abbreviated form.']
```

Example for et al.

```python                                                 
>>> from nltk.tokenize import sent_tokenize
>>> sentence = (""If David et al. get the financing, we can move forward ""
                ""with the prototype. However, this is very unlikely be cause ""
                ""they did not publish sufficiently last year."")
>>> sent_tokenize_list = sent_tokenize(sentence)
>>> sent_tokenize_list
['If David et al.',
 'get the financing, we can move forward with the prototype.',
 'However, this is very unlikely because they did not publish sufficiently last year.']
```

On my laptop I am using  `nltk.__version__`  3.4.5.

As I see it this issue is different than #2154 because these are well known and commonly used abbreviations (especially in academic circles). 
"
651,https://github.com/nltk/nltk/issues/2378,2378,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}]",closed,2019-08-28 10:48:54+00:00,,14,Update various regex escape sequences,"The latest versions of Python are more strict wrt. escape in regex.
For instance with 3.6.8, there are 10+ warnings like this one:
```
...
lib/python3.6/site-packages/nltk/featstruct.py:2092: DeprecationWarning: invalid escape sequence \d
    RANGE_RE = re.compile('(-?\d+):(-?\d+)')
```

The regex(es) should be updated to silence these warnings."
652,https://github.com/nltk/nltk/issues/2405,2405,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2019-09-03 08:13:11+00:00,,2,Why the word 'puppy' is 'JJ'? When get its tag in the sentence.,"[('puppy', 'JJ'),
 ('is', 'VBZ'),
 ('walking', 'VBG'),
 ('on', 'IN'),
 ('the', 'DT'),
 ('street', 'NN'),
 ('to', 'TO'),
 ('meet', 'VB'),
 ('its', 'PRP$'),
 ('father', 'NN')]

 'puppy is walking on the street to meet its father'"
653,https://github.com/nltk/nltk/issues/2409,2409,[],closed,2019-09-08 16:25:37+00:00,,3,"NLTK's Twitter Tokenizer splits all characters in Hindi, Kannada, Marathi, Sinhala, Tamil, Telugu and maybe other languages","When tokenizing texts in languages like Hindi, Kannada, Marathi, Sinhala, Tamil and Telugu, NLTK's Tweeter Tokenizer would split all characters in the texts (NLTK's other tokenizers like Penn Treebank Tokenizer, Tok-tok Tokenizer and NIST Tokenizer do not have this issue), and I guess that the issue would also occur in the case of some other languages.

```
>>> import nltk
>>> tokenizer =  nltk.TweetTokenizer()

>>> SENTENCE_HIN = 'हिन्दी विश्व की एक प्रमुख भाषा है एवं भारत की राजभाषा है।'
>>> SENTENCE_KAN = 'ದ್ರಾವಿಡ ಭಾಷೆಗಳಲ್ಲಿ ಪ್ರಾಮುಖ್ಯವುಳ್ಳ ಭಾಷೆಯೂ ಭಾರತದ ಪುರಾತನವಾದ ಭಾಷೆಗಳಲ್ಲಿ ಒಂದೂ ಆಗಿರುವ ಕನ್ನಡ ಭಾಷೆಯನ್ನು ಅದರ ವಿವಿಧ ರೂಪಗಳಲ್ಲಿ ಸುಮಾರು ೪೫ ದಶಲಕ್ಷ ಜನರು ಆಡು ನುಡಿಯಾಗಿ ಬಳಸುತ್ತಲಿದ್ದಾರೆ.'
>>> SENTENCE_MAR = 'मराठीभाषा ही इंडो-युरोपीय भाषाकुलातील एक भाषा आहे.'
>>> SENTENCE_SIN = 'ශ්‍රී ලංකාවේ ප්‍රධාන ජාතිය වන සිංහල ජනයාගේ මව් බස සිංහල වෙයි.'
>>> SENTENCE_TAM = 'தமிழ் மொழி (Tamil language) தமிழர்களினதும், தமிழ் பேசும் பலரதும் தாய்மொழி ஆகும்.'
>>> SENTENCE_TEL = 'ఆంధ్ర ప్రదేశ్, తెలంగాణ రాష్ట్రాల అధికార భాష తెలుగు.'

>>> tokenizer.tokenize(SENTENCE_HIN)
['ह', 'ि', 'न', '्', 'द', 'ी', 'व', 'ि', 'श', '्', 'व', 'क', 'ी', 'एक', 'प', '्', 'रम', 'ु', 'ख', 'भ', 'ा', 'ष', 'ा', 'ह', 'ै', 'एव', 'ं', 'भ', 'ा', 'रत', 'क', 'ी', 'र', 'ा', 'जभ', 'ा', 'ष', 'ा', 'ह', 'ै', '।']
>>> tokenizer.tokenize(SENTENCE_KAN)
['ದ', '್', 'ರ', 'ಾ', 'ವ', 'ಿ', 'ಡ', 'ಭ', 'ಾ', 'ಷ', 'ೆ', 'ಗಳಲ', '್', 'ಲ', 'ಿ', 'ಪ', '್', 'ರ', 'ಾ', 'ಮ', 'ು', 'ಖ', '್', 'ಯವ', 'ು', 'ಳ', '್', 'ಳ', 'ಭ', 'ಾ', 'ಷ', 'ೆ', 'ಯ', 'ೂ', 'ಭ', 'ಾ', 'ರತದ', 'ಪ', 'ು', 'ರ', 'ಾ', 'ತನವ', 'ಾ', 'ದ', 'ಭ', 'ಾ', 'ಷ', 'ೆ', 'ಗಳಲ', '್', 'ಲ', 'ಿ', 'ಒ', 'ಂ', 'ದ', 'ೂ', 'ಆಗ', 'ಿ', 'ರ', 'ು', 'ವ', 'ಕನ', '್', 'ನಡ', 'ಭ', 'ಾ', 'ಷ', 'ೆ', 'ಯನ', '್', 'ನ', 'ು', 'ಅದರ', 'ವ', 'ಿ', 'ವ', 'ಿ', 'ಧ', 'ರ', 'ೂ', 'ಪಗಳಲ', '್', 'ಲ', 'ಿ', 'ಸ', 'ು', 'ಮ', 'ಾ', 'ರ', 'ು', '೪೫', 'ದಶಲಕ', '್', 'ಷ', 'ಜನರ', 'ು', 'ಆಡ', 'ು', 'ನ', 'ು', 'ಡ', 'ಿ', 'ಯ', 'ಾ', 'ಗ', 'ಿ', 'ಬಳಸ', 'ು', 'ತ', '್', 'ತಲ', 'ಿ', 'ದ', '್', 'ದ', 'ಾ', 'ರ', 'ೆ', '.']
>>> tokenizer.tokenize(SENTENCE_MAR)
['मर', 'ा', 'ठ', 'ी', 'भ', 'ा', 'ष', 'ा', 'ह', 'ी', 'इ', 'ं', 'ड', 'ो', '-', 'य', 'ु', 'र', 'ो', 'प', 'ी', 'य', 'भ', 'ा', 'ष', 'ा', 'क', 'ु', 'ल', 'ा', 'त', 'ी', 'ल', 'एक', 'भ', 'ा', 'ष', 'ा', 'आह', 'े', '.']
>>> tokenizer.tokenize(SENTENCE_SIN)
['ශ', '්', '\u200d', 'ර', 'ී', 'ල', 'ං', 'ක', 'ා', 'ව', 'ේ', 'ප', '්', '\u200d', 'රධ', 'ා', 'න', 'ජ', 'ා', 'ත', 'ි', 'ය', 'වන', 'ස', 'ි', 'ං', 'හල', 'ජනය', 'ා', 'ග', 'ේ', 'මව', '්', 'බස', 'ස', 'ි', 'ං', 'හල', 'ව', 'ෙ', 'ය', 'ි', '.']
>>> tokenizer.tokenize(SENTENCE_TAM)
['தம', 'ி', 'ழ', '்', 'ம', 'ொ', 'ழ', 'ி', '(', 'Tamil', 'language', ')', 'தம', 'ி', 'ழர', '்', 'கள', 'ி', 'னத', 'ு', 'ம', '்', ',', 'தம', 'ி', 'ழ', '்', 'ப', 'ே', 'ச', 'ு', 'ம', '்', 'பலரத', 'ு', 'ம', '்', 'த', 'ா', 'ய', '்', 'ம', 'ொ', 'ழ', 'ி', 'ஆக', 'ு', 'ம', '்', '.']
>>> tokenizer.tokenize(SENTENCE_TEL)
['ఆ', 'ం', 'ధ', '్', 'ర', 'ప', '్', 'రద', 'ే', 'శ', '్', ',', 'త', 'ె', 'ల', 'ం', 'గ', 'ా', 'ణ', 'ర', 'ా', 'ష', '్', 'ట', '్', 'ర', 'ా', 'ల', 'అధ', 'ి', 'క', 'ా', 'ర', 'భ', 'ా', 'ష', 'త', 'ె', 'ల', 'ు', 'గ', 'ు', '.']
```

OS: Windows 10 64-bit
Python Version: 3.7.4 64-bit
NLTK Version: 3.4.5"
654,https://github.com/nltk/nltk/issues/2411,2411,[],closed,2019-09-10 13:35:49+00:00,,1,The conll output format,"I used the stanford corenlp API in NLTk to parse a Chinese sentence and the result yielded sth as follows:

[[(('成为', 'VV'), 'nsubj', ('四川', 'NR')),
  (('成为', 'VV'), 'advmod', ('已', 'AD')),
  (('成为', 'VV'), 'dobj', ('明星', 'NN')),
  (('明星', 'NN'), 'acl', ('升起', 'VV')),
  (('升起', 'VV'), 'advmod:loc', ('开放', 'NN')),
  (('开放', 'NN'), 'compound:nn', ('西部', 'NN')),
  (('西部', 'NN'), 'nmod:assmod', ('中国', 'NR')),
  (('开放', 'NN'), 'amod', ('对外', 'JJ')),
  (('开放', 'NN'), 'case', ('中', 'LC')),
  (('升起', 'VV'), 'mark', ('的', 'DEC')),
  (('明星', 'NN'), 'nummod', ('一', 'CD')),
  (('一', 'CD'), 'mark:clf', ('颗', 'M'))]]

The output format is similar to the English example in the wiki (https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK). Can anybody help me change the output into a conll format?

Thanks!"
655,https://github.com/nltk/nltk/issues/2412,2412,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}, {'id': 910625822, 'node_id': 'MDU6TGFiZWw5MTA2MjU4MjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/CI', 'name': 'CI', 'color': '76dbed', 'default': False, 'description': ''}]",closed,2019-09-18 03:44:28+00:00,,2,CI instructions are out of date,"http://www.nltk.org/dev/jenkins.html should now refer to https://www.travis-ci.org/nltk/nltk
"
656,https://github.com/nltk/nltk/issues/2414,2414,"[{'id': 1012587336, 'node_id': 'MDU6TGFiZWwxMDEyNTg3MzM2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3.7', 'name': 'python3.7', 'color': 'a4e85c', 'default': False, 'description': ''}, {'id': 1569756102, 'node_id': 'MDU6TGFiZWwxNTY5NzU2MTAy', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python3.8', 'name': 'python3.8', 'color': '006b75', 'default': False, 'description': ''}]",closed,2019-09-21 08:39:41+00:00,,2,python 3 deprecations since 3.5 and some stop working in 3.8,"```
$ python
>>> import sys
>>> sys.version.split()[0]
'3.7.3'

$ python -W module -c 'from nltk.parse.generate import generate'
/python/vyv37/lib/python3.7/site-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly
  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: """"
/python/vyv37/lib/python3.7/site-packages/nltk/lm/counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  from collections import Sequence, defaultdict
/python/vyv37/lib/python3.7/site-packages/nltk/lm/vocabulary.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  from collections import Counter, Iterable
```



"
657,https://github.com/nltk/nltk/issues/2415,2415,"[{'id': 718741986, 'node_id': 'MDU6TGFiZWw3MTg3NDE5ODY=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stanford%20api', 'name': 'stanford api', 'color': 'f4473a', 'default': False, 'description': None}]",open,2019-09-23 14:27:19+00:00,,2,How to parse a (Chinese) paragraph/passage using Stanford CoreNLP API in NLTK,"Dear all,
I'm new to natural language processing. Now I'm trying to do dependency parsing for a text. The examples given are all single sentences. Can anybody help me out about the depdendency parsing of a paragraph or even a passage? And it would be very helpful if the example is given for Chinese multiple sentences parsing.
Thank you very much!"
658,https://github.com/nltk/nltk/issues/2416,2416,[],open,2019-09-24 11:05:35+00:00,,0,nltk.corpus.reader.NKJPCorpusReader fails on Python 3.x,"NKJPCorpusReader object inside is using tempfile library which is different in v. 2.7 and 3.5. In Python v.2.7:
```
self.write_file = tempfile.NamedTemporaryFile(delete=False)
```
creates file with mode='w'

In python v3.x

creates file with mode='w+b' which makes line 274 of '~/.pyenv/versions/3.7.4/lib/python3.7/site-packages/nltk/corpus/reader/nkjp.py' failing with error:

TypeError: a bytes-like object is required, not 'str'

Addition of explicit mode is required to fix it."
659,https://github.com/nltk/nltk/issues/2417,2417,[],closed,2019-09-25 02:00:38+00:00,,2,nltk.text --> findall improviment to return hits,"To improve findall in nltk.text I suggest to change ""print(tokenwrap(hits, ""; ""))"" in def findall(self, regexp) to ""return hits"". In this way it would be possible to get a list of values returned by findall. 

The way it is implemented if you do: 

>>> x = text5.findall(""<.*><.*><bro>"") 
you rule bro; telling you bro; u twizted bro
>>> x 

This return only a print; x will be empty. 
This, will be no control of the output.

If ""print(tokenwrap(hits, ""; ""))"" would be change to ""return hits"", x would be:
>>> x
['you rule bro', 'telling you bro', 'u twizted bro']

Seems much better!

Thank you.




"
660,https://github.com/nltk/nltk/issues/2420,2420,[],open,2019-09-25 14:30:08+00:00,,3,WordNet sense keys mismatch,"Hi,

I'm having problems with retrieving the correct definitions from the nltk wordnet corpus for a given sense key.  Please see [this](https://github.com/getalp/UFSAC/issues/2) related issue from the UFSAC repo.

The current ntlk WordNet version is 3.0:
```
import nltk
nltk.download('wordnet')
from nltk.corpus import wordnet as wn

print(wn.get_version())
```
gives:
```
3.0
```

Looking at the official WordNet 3.0 database (http://wordnetcode.princeton.edu/3.0/WordNet-3.0.tar.gz), the key ""drive%1:04:03::"" has a synset ID of 00572489 (according to the `dict/index.sense` file), and the definition related to this synset ID is ""hitting a golf ball off of a tee with a driver"" (according to the `dict/data.noun` file).

The key associated with the above definition from the official WordNet online search is also ""drive%1:04:03::"" (along with ""driving%1:04:03::""), even though the online search version is 3.1.

Firstly, using the key ""drive%1:04:03::"" to look up the definition in ntlk as follows
```
print(wn.synset_from_sense_key(""drive%1:04:03::""))
print(wn.synset_from_sense_key(""drive%1:04:03::"").definition())
print(wn.synset_from_sense_key(""drive%1:04:03::"").offset())
```
gives:
```
Synset('campaign.n.02')
a series of actions advancing a principle or tending toward a particular end
798245
```
This definition and ID doesn't correspond to either the WordNet online search or the database.  However, querying nltk with the key ""drive%1:04:06::"" (replacing 03 with 06)
```
print(wn.synset_from_sense_key(""drive%1:04:06::""))
print(wn.synset_from_sense_key(""drive%1:04:06::"").definition())
print(wn.synset_from_sense_key(""drive%1:04:06::"").offset())
```
gives the expected definition and ID:
```
Synset('drive.n.06')
hitting a golf ball off of a tee with a driver
572489
```
For this key, why doesn't ntlk give the definition as recorded in the database and the online search?

Also, when using the key ""driving%1:04:03::"" (referring to the second synonym for that sense), to obtain the synset with
```
print(wn.synset_from_sense_key(""driving%1:04:03::""))
```
I get the following error:
```
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
~/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py in synset(self, name)
   1333         try:
-> 1334             offset = self._lemma_pos_offset_map[lemma][pos][synset_index]
   1335         except KeyError:

IndexError: list index out of range

During handling of the above exception, another exception occurred:

WordNetError                              Traceback (most recent call last)
<ipython-input-83-1778759d53e3> in <module>
----> 1 print(wn.synset_from_sense_key(""driving%1:04:03::""))

~/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py in synset_from_sense_key(self, sense_key)
   1563 
   1564         synset_id = '.'.join([lemma, synset_types[int(ss_type)], lex_id])
-> 1565         return self.synset(synset_id)
   1566 
   1567     #############################################################

~/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py in synset(self, name)
   1343             else:
   1344                 tup = lemma, pos, n_senses, ""senses""
-> 1345             raise WordNetError(message % tup)
   1346 
   1347         # load synset information from the appropriate file

WordNetError: lemma 'driving' with part of speech 'n' has only 2 senses
```

Any advice on the above issues will be much appreciated... thanks!"
661,https://github.com/nltk/nltk/issues/2421,2421,[],open,2019-09-26 05:44:49+00:00,,1,Cannot get WordNet synsets for English without lemmatization,"Querying wordnet with `wordnet.synsets()` will lemmatize the query word, but only for English. While this is useful for many applications, sometimes I do not want such lemmatization. For instance, I have dictionary forms for multiple languages (from, e.g., Swadesh lists) and I want to detect differences in polysemy between languages, but the lemmatization inflates the apparent polysemy for English. There appears to be no way (in the public API) to do an English query without lemmatization.

For example:

```python
>>> wn.synsets('eyeglasses')
[Synset('spectacles.n.01'), Synset('monocle.n.01')]
>>> wn.synsets('eyeglasses')[0].lemma_names()
['spectacles', 'specs', 'eyeglasses', 'glasses']
>>> wn.synsets('eyeglasses')[1].lemma_names()
['monocle', 'eyeglass']
```

The second synset (`monocle.n.01`) was found because 'eyeglass' appears in its lemmas, but not 'eyeglasses', which is only in the first synset. Sometimes specifying the POS can help, as with 'scissors' and 'scissor.v.01', but not always (as with 'eyeglasses' above, both are 'n'). I end up needing to write a wrapper like this:

```python
def synsets(lemma, pos=None, lang='eng', check_exceptions=True):
    results = wn.synsets(lemma,
                         pos=pos,
                         lang=lang,
                         check_exceptions=check_exceptions)
    if lang == 'eng':
        results = [ss for ss in results if lemma in ss.lemma_names()]
    return results
```

Am I missing something or is this currently the best way around the issue?"
662,https://github.com/nltk/nltk/issues/2423,2423,"[{'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",open,2019-10-03 00:22:09+00:00,,2,Extract wordnet into a separate package,"@alvations has created https://github.com/nltk/wordnet
We need to deprecate the existing NLTK wordnet corpus reader
"
663,https://github.com/nltk/nltk/issues/2425,2425,[],open,2019-10-10 12:35:39+00:00,,0,"Docstring for word_tokenize ""preserve_line"" parameter is unclear","From the `word_tokenize()` docstring:

```
:param preserve_line: An option to keep the preserve the sentence and not sentence tokenize it.
```

Huh?  It seems there's at least an extra ""the"" in there.

On top of that, it does not actually tell a new reader _what this does_.  I'm assuming this is a reference to the fact that `TreebankWordTokenizer` expects sentence-tokenized inputs.  From

```
sentences = [text] if preserve_line else sent_tokenize(text, language)
```

It _looks_ like what this parameter does is to say, ""if you're telling `word_tokenize()` that the input is a single sentence, then skip the sent tokenization step.""  But that's entirely unclear from the docstring."
664,https://github.com/nltk/nltk/issues/2427,2427,[],open,2019-10-14 13:26:50+00:00,,0,How to use the user/custom dictionary when tokenizing Chinese texts using Stanford CoreNLP API in NLTK?,"When I try to do dependency parsing using the stanford parser, it tokenizes the texts first. However, the results are not always satisfactory concerning Chinese texts. Can I add my own words to the dictionary so that it will not split one Chinese word into two tokens?
Thanks very much!"
665,https://github.com/nltk/nltk/issues/2429,2429,[],open,2019-10-16 07:59:23+00:00,,3, Chinese characters not  show in tree.draw. Maybe there is a deeper problem.,"nltk tree draw cannot plot the syntax tree of Chinese Characters
I can use the pretty_print to draw.
but in tree.draw() Chinese Characters has gone

![image](https://user-images.githubusercontent.com/29203912/66900019-988c3c80-f02e-11e9-9261-814b33b2b9d6.png)



![image](https://user-images.githubusercontent.com/29203912/66899499-a5f4f700-f02d-11e9-99fb-f0cdffccab51.png)

"
666,https://github.com/nltk/nltk/issues/2431,2431,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2019-10-18 10:22:29+00:00,,0,punctuation of plain text,I have plain text in lower case German language without any stop or comma or any punctuation. How to get back the punctuation and stop of sentences? to get the text into sentence structure. I tried but unsuccessful. Please guide. 
667,https://github.com/nltk/nltk/issues/2433,2433,[],closed,2019-10-20 14:41:13+00:00,,1,corpus_ribes divide 0 error,In ribes_score.py line 290. I encounter the divide 0 error. We should prevent num_possible_pairs from being 0.
668,https://github.com/nltk/nltk/issues/2434,2434,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}]",open,2019-10-20 20:40:14+00:00,,2,Bad solving of issue #2151,"https://github.com/nltk/nltk/blob/2a5aece8624504ebdaf363b2005e01f81dedade7/nltk/tag/mapping.py#L90-L112

This patch from #2151 just don't work, because `source == 'ru-rnc-new'` failed on line
https://github.com/nltk/nltk/blob/2a5aece8624504ebdaf363b2005e01f81dedade7/nltk/tag/mapping.py#L90
with LookupError for file 'ru-rnc-new.map'

So, why don't change 'ru-rnc-new' to `ru-rnc.map`, or just create `ru-rnc-new.map`?

P.S. this is a @alvations patch, so requesting the author"
669,https://github.com/nltk/nltk/issues/2436,2436,[],open,2019-10-25 16:30:10+00:00,,1,Syntax error in parse string of higher-order expressions,"https://github.com/nltk/nltk/blob/2a5aece8624504ebdaf363b2005e01f81dedade7/nltk/inference/tableau.py#L681-L694

```
from nltk.inference.tableau import *
testHigherOrderTableauProver()
```
Gives the following error:
```
---------------------------------------------------------------------------
UnexpectedTokenException                  Traceback (most recent call last)
~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/sem/logic.py in parse(self, data, signature)
    155         try:
--> 156             result = self.process_next_expression(None)
    157             if self.inRange(0):

~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/sem/logic.py in process_next_expression(self, context)
    291 
--> 292         accum = self.handle(tok, context)
    293 

~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/sem/logic.py in handle(self, tok, context)
    304         if self.isvariable(tok):
--> 305             return self.handle_variable(tok, context)
    306 

~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/sem/logic.py in handle_variable(self, tok, context)
    360                 )
--> 361             self.assertNextToken(Tokens.CLOSE)
    362         return accum

~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/sem/logic.py in assertNextToken(self, expected)
    572             if tok != expected:
--> 573                 raise UnexpectedTokenException(self._currentIndex, tok, expected)
    574 

UnexpectedTokenException: Unexpected token: '&'.  Expected token ')'.

During handling of the above exception, another exception occurred:

LogicalExpressionException                Traceback (most recent call last)
<ipython-input-13-36a31f27b2ca> in <module>
----> 1 testHigherOrderTableauProver()

~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/inference/tableau.py in testHigherOrderTableauProver()
    681 
    682 def testHigherOrderTableauProver():
--> 683     tableau_test('believe(j, -lie(b))', ['believe(j, -lie(b) & -cheat(b))'])
    684     tableau_test('believe(j, lie(b) & cheat(b))', ['believe(j, lie(b))'])
    685     tableau_test(

~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/inference/tableau.py in tableau_test(c, ps, verbose)
    698 def tableau_test(c, ps=None, verbose=False):
    699     pc = Expression.fromstring(c)
--> 700     pps = [Expression.fromstring(p) for p in ps] if ps else []
    701     if not ps:
    702         ps = []

~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/inference/tableau.py in <listcomp>(.0)
    698 def tableau_test(c, ps=None, verbose=False):
    699     pc = Expression.fromstring(c)
--> 700     pps = [Expression.fromstring(p) for p in ps] if ps else []
    701     if not ps:
    702         ps = []

~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/sem/logic.py in fromstring(cls, s, type_check, signature)
    961             return cls._type_checking_logic_parser.parse(s, signature)
    962         else:
--> 963             return cls._logic_parser.parse(s, signature)
    964 
    965     def __call__(self, other, *additional):

~/.conda/envs/data_science/lib/python3.6/site-packages/nltk/sem/logic.py in parse(self, data, signature)
    159         except LogicalExpressionException as e:
    160             msg = '%s\n%s\n%s^' % (e, data, ' ' * mapping[e.index - 1])
--> 161             raise LogicalExpressionException(None, msg)
    162 
    163         if self.type_check:

LogicalExpressionException: Unexpected token: '&'.  Expected token ')'.
believe(j, -lie(b) & -cheat(b))
```
It appears the `Expression.fromstring` method is getting hung up on the lack of parenthesis around the propositional argument for the higher-order predicate. The following seems to work:
```
from nltk.inference.tableau import *
tableau_test(""believe(j, -lie(b))"", [""believe(j, (-lie(b) & -cheat(b)))""])
tableau_test(""believe(j, (lie(b) & cheat(b)))"", [""believe(j, lie(b))""])
tableau_test(
    ""believe(j, lie(b))"", [""lie(b)""]
)  # how do we capture that John believes all things that are true
tableau_test(
    ""believe(j, know(b, cheat(b)))"",
    [""believe(j, (know(b, lie(b)) & know(b, (steals(b) & cheat(b)))))""],
)
tableau_test(""P(Q(y), (R(y) & R(z)))"", [""P((Q(x) & Q(y)), (R(y) & R(z)))""])

tableau_test(""believe(j, (cheat(b) & lie(b)))"", [""believe(j, (lie(b) & cheat(b)))""])
tableau_test(""believe(j, (-cheat(b) & -lie(b)))"", [""believe(j, (-lie(b) & -cheat(b)))""])
```
Willing to do further testing or PR, if needed. Cheers!

[Edit: removed unintended ""not""; fixed typo ""to""]"
670,https://github.com/nltk/nltk/issues/2438,2438,[],closed,2019-10-27 12:17:20+00:00,,1,Whatever happened to NLTK-Lite?,"I'm trying to find out what happened to NLTK-Lite? The last reference to this is in Oct 2007: https://www.nltk.org/news.html

Is this project dead? Or was it merged into the main NLTK and made the default way to use NLTK? Thanks."
671,https://github.com/nltk/nltk/issues/2439,2439,[],closed,2019-10-27 15:39:11+00:00,,1,Automatic import of open italian book database from Liber Liber,"It would be really amazing if you could insert a command to automatically configure NLTK for use with a language, for instance Italian, already configured to import all the repository of italian books from Liber Liber, whose books are freely accessible to the public with no fee.
This way it would be easier for an italian to analize and learn since the very beginning, instead of having to learn a lot about the english configuration and so on.
For instance, on the python shell you type: set lang=it (this works for every language)
and NLTK already autoimport italian tokenizer, italian parser, italian books and so on.
Thank you"
672,https://github.com/nltk/nltk/issues/2440,2440,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}, {'id': 1641876486, 'node_id': 'MDU6TGFiZWwxNjQxODc2NDg2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/critical', 'name': 'critical', 'color': 'ff2050', 'default': False, 'description': ''}]",closed,2019-10-27 15:48:21+00:00,,2,Unsafe call to yaml.load,"https://github.com/nltk/nltk/blob/2a5aece8624504ebdaf363b2005e01f81dedade7/nltk/data.py#L886

This should be replace with e.g. `yaml.safe_load`

See https://nvd.nist.gov/vuln/detail/CVE-2017-18342"
673,https://github.com/nltk/nltk/issues/2441,2441,"[{'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",open,2019-10-27 15:54:29+00:00,,2,Wordnet synsets query problem ,"Doing:
`print wn.synsets(""tablesssssssssssssssss"")`
returns results (it shouldn't, as there is no such word in English),
But if we use the Wordnet online API here:
`http://wordnetweb.princeton.edu/perl/webwn`
for the same word, we get no results (correct).
Why is this problem?"
674,https://github.com/nltk/nltk/issues/2442,2442,"[{'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",open,2019-10-27 16:25:37+00:00,,3,Issue with synset_from_sense_key function to access adjective satellites,"I tried to use the synset_from_sense_key function and here is the error:

File ""/home/izorar/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py"", line 1356, in synset raise WordNetError(message % lemma)

WordNetError: adjective satellite requested but only plain adjective found for lemma 'first'

Any idea on how to fix the error?"
675,https://github.com/nltk/nltk/issues/2444,2444,[],closed,2019-10-28 14:36:01+00:00,,0,Py2.7 compatibility: subprocess has no attribute 'DEVNULL',"Hi,

A call to tag() when using nltk.tag.stanford.StanfordPOSTagger fails on python 2.7.

```
File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/nltk/tag/stanford.py"", line 116, in tag_sents
    cmd, classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE

File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/nltk/internals.py"", line 112, in java
    subprocess_output_dict = {'pipe': subprocess.PIPE, 'stdout': subprocess.STDOUT, 'devnull': subprocess.DEVNULL}

AttributeError: 'module' object has no attribute 'DEVNULL'
```

This worked well on nltk 3.4 and it seems to have been introduced with [commit 0d3d086](https://github.com/nltk/nltk/commit/0d3d086a41f7bf4fb8418820b221e44a6a7e20ee#diff-6d3d9c7d1794a239ab676fc6fce3131f).

There has been an [SO question](https://stackoverflow.com/questions/57720341/stanford-part-of-speech-tagger-gives-attribute-error) about this. The accepted answer suggests to use corenlp instead.

This could however be a simple fix (see [this other SO answer](https://stackoverflow.com/questions/11269575/how-to-hide-output-of-subprocess-in-python-2-7) about a way to fix it). I understand py2.7 is fairly close to endlife, but as python 2.7 is still listed as a compatible version, I think it would be nice to fix this error.

Let me know if you want me to make a pull request.

Edit: for a bit more context, I was incentivized to bump the nltk version due to the vulnerability fixed in 3.4.5. However, going from 3.4 to 3.4.5 lead to this error. It might discourage some users to update their dependency to nltk."
676,https://github.com/nltk/nltk/issues/2445,2445,[],closed,2019-10-29 20:08:21+00:00,,3,regex isn't consistent in python2/python3 for non-standard unicode word characters,"I noticed this issue when running the following command in python2 and python3:
```
nltk.pos_tag([u'açai'])
```
In python2, it is categorized as `NN` whereas in python3 it is categorized as `NNP`.

After doing some digging, i noticed that the featureset is different for these two cases. In python2, the feature ""shape"" is `other`. In python3, the feature ""shape"" is `mixedcase` (which explains the proper noun labeling).

The word ""shape"" is assigned in`nltk/tag/sequential.py:ClassifierPasedPOSTagger`, where we have
```
        elif re.match('\w+$', word):
            shape = 'mixedcase'
```
The `\w` matches all Unicode word characters in python3, but only `[a-zA-Z0-9_]` in python2.

I believe changing these regex calls to use the `ASCII` flag (e.g. `re.match('\w+$', word, re.ASCII)` will make the code consistent between python2 and python3."
677,https://github.com/nltk/nltk/issues/2447,2447,[],closed,2019-10-30 16:47:25+00:00,,1,"""bad escape \u"" due to `import nltk` in py3.7.4","Upgrading codebase from py2.7, getting an error in nltk 3.4.5. 

```
(venv3) gmoss$ python -m nltk.downloader punkt maxent_treebank_pos_tagger wordnet
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py"", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File ""/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py"", line 109, in _get_module_details
    __import__(pkg_name)
  File ""/Users/gmoss/Documents/constructor/autocomplete/venv3/lib/python3.7/site-packages/nltk/__init__.py"", line 150, in <module>
    from nltk.translate import *
  File ""/Users/gmoss/Documents/constructor/autocomplete/venv3/lib/python3.7/site-packages/nltk/translate/__init__.py"", line 23, in <module>
    from nltk.translate.meteor_score import meteor_score as meteor
  File ""/Users/gmoss/Documents/constructor/autocomplete/venv3/lib/python3.7/site-packages/nltk/translate/meteor_score.py"", line 10, in <module>
    from nltk.stem.porter import PorterStemmer
  File ""/Users/gmoss/Documents/constructor/autocomplete/venv3/lib/python3.7/site-packages/nltk/stem/__init__.py"", line 29, in <module>
    from nltk.stem.snowball import SnowballStemmer
  File ""/Users/gmoss/Documents/constructor/autocomplete/venv3/lib/python3.7/site-packages/nltk/stem/snowball.py"", line 314, in <module>
    class ArabicStemmer(_StandardStemmer):
  File ""/Users/gmoss/Documents/constructor/autocomplete/venv3/lib/python3.7/site-packages/nltk/stem/snowball.py"", line 326, in ArabicStemmer
    r'[\u064b-\u064c-\u064d-\u064e-\u064f-\u0650-\u0651-\u0652]'
  File ""/Users/gmoss/Documents/constructor/autocomplete/venv3/lib/python3.7/re.py"", line 234, in compile
    return _compile(pattern, flags)
  File ""/Users/gmoss/Documents/constructor/autocomplete/venv3/lib/python3.7/re.py"", line 286, in _compile
    p = sre_compile.compile(pattern, flags)
  File ""/Users/gmoss/Documents/constructor/autocomplete/venv3/lib/python3.7/sre_compile.py"", line 764, in compile
    p = sre_parse.parse(p, flags)
  File ""/Users/gmoss/Documents/constructor/autocomplete/venv3/lib/python3.7/sre_parse.py"", line 930, in parse
    p = _parse_sub(source, pattern, flags & SRE_FLAG_VERBOSE, 0)
  File ""/Users/gmoss/Documents/constructor/autocomplete/venv3/lib/python3.7/sre_parse.py"", line 426, in _parse_sub
    not nested and not items))
  File ""/Users/gmoss/Documents/constructor/autocomplete/venv3/lib/python3.7/sre_parse.py"", line 536, in _parse
    code1 = _class_escape(source, this)
  File ""/Users/gmoss/Documents/constructor/autocomplete/venv3/lib/python3.7/sre_parse.py"", line 337, in _class_escape
    raise source.error('bad escape %s' % escape, len(escape))
re.error: bad escape \u at position 1
```

Thank you for any assistance."
678,https://github.com/nltk/nltk/issues/2449,2449,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2019-10-31 16:04:57+00:00,,0,Arabic Text Normalization,"I have plain text in the Arabic language with stops, commas, punctuations, and diacritics. How to get the clean text by removing the stops, commas, punctuations, and diacritics from the input texts.
Example:  input = "".إن القراء يقرؤون القرآن قراءة جميلة"" 
                 output = ""ان القراء يقرءون القران قراءة جميلة"""
679,https://github.com/nltk/nltk/issues/2450,2450,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2019-10-31 16:09:01+00:00,,1,Strip Diacritics ,"I have a plain text in the Arabic language with diacritics. How to get the clean text by removing the diacritics from the input texts.
Example:
  Input  =""الْعَرَبِيةُ""
  output =""العربية"""
680,https://github.com/nltk/nltk/issues/2451,2451,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2019-10-31 16:15:42+00:00,,0,Mesure Arabic Text similarity ,"I need to find the similarity between many documents that contain Arabic language plain texts.  
Example: input: text1=""اهلا وسهلا"", text2=""اهلا وسهلا"" 
                Output: Similarity = 1.0"
681,https://github.com/nltk/nltk/issues/2452,2452,[],closed,2019-11-03 06:15:21+00:00,,1,grammar.CFG.eliminate_start throws TypeError when a new start symbol is needed,"The `grammar.CFG.eliminate_start` method throws `TypeError: 'Nonterminal' object is not iterable` when the original start symbol of the grammar appears on the right side of a production. This is exactly the case where this method does anything at all—for instance, this is the first step in converting a CFG to Chomsky normal form (see `grammar.CFG.chomsky_normal_form`).

The following code produces the error:
```
>>> import nltk.grammar
>>> G = nltk.grammar.CFG.fromstring(""S -> S 'a'"")
>>> nltk.grammar.CFG.eliminate_start(G)
```

This error is thrown because the new start symbol is used as the right side of a new production in the grammar, without being wrapped in an iterable as would typically be done. A simple solution is to replace
```
result.append(Production(start, grammar.start()))
```
with
```
result.append(Production(start, [grammar.start()]))
```"
682,https://github.com/nltk/nltk/issues/2454,2454,[],closed,2019-11-03 06:55:47+00:00,,1,grammar.CFG.chomsky_normal_form returns None if grammar is already in CNF,"Given a context free grammar `G`, if `G` is not in Chomsky normal form, then `G.chomsky_normal_form()` returns a new CFG with an equivalent language to `G`, but in normal form. However, if `G` is already in Chomsky normal form, then it returns `None`.

Here's an example:
```
>>> import nltk.grammar
>>> G = nltk.grammar.CFG.fromstring(""S -> 'a'"")
>>> print(G.chomsky_normal_form())
None
```

Cause:
```
if self.is_chomsky_normal_form():
            return
```

Should be:
```
if self.is_chomsky_normal_form():
            return self
```"
683,https://github.com/nltk/nltk/issues/2456,2456,[],open,2019-11-03 07:27:13+00:00,,1,grammar.CFG.is_chomsky_normal_form returns True even if start symbol is produced by some production,"In a Chomsky normal form grammar, the start symbol must not occur on the right side of a production by definition. However, the current implementation does not check for this. As a result, NLTK indicates that the grammar `S -> S S` is in normal form, even though it does not comply with the definition.
```
>>> import nltk.grammar
>>> G = nltk.grammar.CFG.fromstring(""S -> S S"")
>>> G.is_chomsky_normal_form()
True
```"
684,https://github.com/nltk/nltk/issues/2457,2457,[],open,2019-11-07 15:52:32+00:00,,0,Issue with averaged_perceptron_tagger,"Faced with issue when using averaged_perceptron_tagger.

LookupError: 
**********************************************************************
  Resource C: not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('C:')
  
  For more information see: https://www.nltk.org/data.html

  Attempted to load /C:/Users/YW291553/Desktop/Caleres/Sentiment/VDIDFS/ProfileData/FolderRedirection/YW291553/Application Data/nltk_data/taggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle

  Searched in:
    - ''
"
685,https://github.com/nltk/nltk/issues/2459,2459,"[{'id': 975531894, 'node_id': 'MDU6TGFiZWw5NzU1MzE4OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/metrics', 'name': 'metrics', 'color': 'f442c8', 'default': False, 'description': ''}]",closed,2019-11-10 16:17:21+00:00,,3,nltk.metrics.distance.jaro_similarity returns lower values than it should,"Jaro similarity is supposed to give the same results if the strings are reversed:

```
from nltk.metrics import distance as dist
a='rureiter'
b='enreiter'
print(""regular={}, reversed={}"".format(dist.jaro_similarity(a, b), dist.jaro_similarity(a[::-1], b[::-1])))
```
The code above prints `regular=0.722222222222, reversed=0.833333333333`.

In fact, manual pen-and-paper examination shows that the correct similarity metric, in both cases 
is 0.833333333333:

Separate the strings into prefix and suffix:
```
a = 'ru' + 'reiter'
b = 'en' + 'reiter'
```
The suffixes of `a` and `b` are equal. Because the suffixes are equal, and the prefixes have nothing in common between them, then the expected values are `matches == 6`  and `transpositions == 0`. With these values:

```
a = 'ru' + 'reiter'
b = 'en' + 'reiter'
matches = 6
transpositions = 0
print(
            1
            / 3
            * (
                matches / len(a)
                + matches / len(b)
                + (matches - transpositions // 2) / matches
            )
        )
```
The above code, unlike nltk, gives the correct answer of 0.8333333333333333 .

The issue lies in the fact that the current implementation does not try to minimize the number of transpositions in its algorithm, contrary to its documentation:

> The Jaro distance between is the min no. of single-character transpositions
>     required to change one word into another

The implementation simply finds the first occurrence of each character of the first string (`a`) in the second string (`b`). This order of matching does not guarantee that the match will be optimal.  In this example, the match is:
The first character of `a`  is ""r"", and is matched against the third character of `b`. From that point, the suffix ""reiter"" can't be matched in full. Worse, the next match of `a` is character ""e"" which is matched against the first character of `b`. This matching makes a transposition:

```
r u r e i t e r
 \  _/    
  \/    
  /\   
 /  | 
e n r e i t e r
```
Later, things get even worse. The last ""e"" of `a` gets matched against the middle ""e"" of `b`:
```
r u r e i t e r
 \  _/     /
  \/    __/
  /\   /
 /  | /
e n r e i t e r
```
This matching causes more transpositions, for no reason.

A correct Jaro algorithm should find the minimum value of transposition possible.

With `matches=6`, and `transpositions=4` the result is 0.722222222222 .

Note that `transpositions=4` because the list of matched indices of the second string is sorted, while the first is not. Before sorting:
```
flagged_1 = [0, 3, 4, 5, 6] 
flagged_2 = [2, 0, 4, 5, 3] 
```
After sorting:
```
flagged_1 = [0, 3, 4, 5, 6] 
flagged_2 = [0, 2, 3, 4, 5]  # 4 different entries
```
"
686,https://github.com/nltk/nltk/issues/2460,2460,[],closed,2019-11-14 14:23:29+00:00,,6,Danish sentence tokenizer fails to split on newline,"When sentences are terminated with a newline rather than .!? the tokenizer fails to split. If it's wrong in Danish, it is probably wrong in a bunch of other languages. 

nltk version 3.4.5
Python version 3.6.9
Ubuntu Linux 18.04

```python
import nltk 
sent_tokenizer = nltk.data.load('tokenizers/punkt/danish.pickle')
text = """"""Den normale kropstemperatur er 37,0ºC, når den måles i endetarmen; temperaturen er lavere, når den måles i f.eks. øret eller munden
Feber defineres som kernetemperatur over 38°C målt i endetarmen
De allerfleste sygdomme med feber er harmløse og helbreder sig selv. Det gælder f.eks. forkølelser og andre virussygdomme
I nogle tilfælde kan feberen skyldes alvorlige infektioner, som f.eks. hjernehindebetændelse (meningitis), blodforgiftning (sepsis) og nyrebækkenbetændelse (pyelonefritis)
""""""
print(sent_tokenizer.tokenize(text))
```

```
['Den normale kropstemperatur er 37,0ºC, når den måles i endetarmen; temperaturen er lavere, når den måles i f.eks. øret eller munden\nFeber defineres som kernetemperatur over 38°C målt i endetarmen\nDe allerfleste sygdomme med feber er harmløse og helbreder sig selv.', 'Det gælder f.eks. forkølelser og andre virussygdomme\nI nogle tilfælde kan feberen skyldes alvorlige infektioner, som f.eks. hjernehindebetændelse (meningitis), blodforgiftning (sepsis) og nyrebækkenbetændelse (pyelonefritis)']
```"
687,https://github.com/nltk/nltk/issues/2463,2463,[],closed,2019-11-18 14:23:28+00:00,,1,Is there a wrapper of coreNLP for open information extraction?,"Among the Stanford CoreNLP API documents I couldn't find the api for stanford's Open Information Extraction (OpenIE) annotator.
Is there any?
thanks a lot"
688,https://github.com/nltk/nltk/issues/2467,2467,[],open,2019-12-01 19:35:49+00:00,,0,corpus formatting error in sinica_treebank,"### Issue

I bounced to an error when I try to use the parsed sentences provided in  `sinica_treebank`.

Code to reproduce the error:
```python
import nltk
from nltk.corpus import sinica_treebank
nltk.download('sinica_treebank')
raw_treebank = sinica_treebank.parsed_sents()
for i in range(10000):
    try:
        tree = raw_treebank[i]
    except Exception as e:
        print(i, e)
```
As you can see from the output message, the parser failed to parse the corpus starting from the line index `5347`.

###  Root Cause
There is missing a space in the line `5347` in `{central_installation_folder}/corpora/sinica_treebank/parsed` file. 

Currently, the line `5437` starts with the text 
```#963:00963..[0]VP(evaluation:Dbb:仍然|...``` 

Change it to 
```#963:00963..[0] VP(evaluation:Dbb:仍然|```

will fix the parser error, i.e., makes `sinica_treebank.parsed_sents()` works again.

Please help to corect the corpus file. Thank you!


"
689,https://github.com/nltk/nltk/issues/2471,2471,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2019-12-03 12:48:14+00:00,,0,same word but differnnet tokenizer,"hi，when I processed my data， I find the same word may get different tokenizer.
for example，the word： <can't>, may get  ，<can>  <'t> or <ca> <n't>.
Could you tell me the reason, thank you very much."
690,https://github.com/nltk/nltk/issues/2472,2472,[],closed,2019-12-03 21:13:55+00:00,,0,cannot be skolemized,"I am new to this
I encounter the error Exception: '{-P(x,y), Q(x,y,f(x,y))}' cannot be skolemized

It's from this 
x1 = Clause([-P(x,y),Q(x,y,f(x,y))])

Am I encoding this clause correctly?"
691,https://github.com/nltk/nltk/issues/2475,2475,"[{'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}, {'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2019-12-13 01:46:46+00:00,,3,jaccard_distance does not match the data formula,"from nltk.metrics.distance import jaccard_distance
I think the jaccard_distance  does not match the data formula, the jaccard_distance might be:

    return len(label1.intersection(label2)) / len(
        label1.union(label2)
    )

but the source code is :

    return (len(label1.union(label2)) - len(label1.intersection(label2))) / len(
        label1.union(label2)
    )
"
692,https://github.com/nltk/nltk/issues/2477,2477,[],open,2019-12-13 12:03:15+00:00,,0,Installation of nltk 3.4.5 problem with pip on Ubuntu and python 3.6.8,"I am using latest pip (19.3.1) and when I try to install nltk (following the installation documentation) I get:

```
Collecting nltk
  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)
     |████████████████████████████████| 1.5MB 2.0MB/s 
Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.13.0)
Building wheels for collected packages: nltk
  Building wheel for nltk (setup.py) ... error
  ERROR: Command errored out with exit status 1:
   command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-nnqqbh5b/nltk/setup.py'""'""'; __file__='""'""'/tmp/pip-install-nnqqbh5b/nltk/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-wd9ue65u --python-tag cp36
       cwd: /tmp/pip-install-nnqqbh5b/nltk/
  Complete output (405 lines):
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib
  creating build/lib/nltk
  copying nltk/grammar.py -> build/lib/nltk
  copying nltk/book.py -> build/lib/nltk
  copying nltk/jsontags.py -> build/lib/nltk
  copying nltk/toolbox.py -> build/lib/nltk
  copying nltk/treeprettyprinter.py -> build/lib/nltk
  copying nltk/tree.py -> build/lib/nltk
  copying nltk/lazyimport.py -> build/lib/nltk
  copying nltk/featstruct.py -> build/lib/nltk
  copying nltk/collections.py -> build/lib/nltk
  copying nltk/treetransforms.py -> build/lib/nltk
  copying nltk/__init__.py -> build/lib/nltk
  copying nltk/data.py -> build/lib/nltk
  copying nltk/compat.py -> build/lib/nltk
  copying nltk/tgrep.py -> build/lib/nltk
  copying nltk/wsd.py -> build/lib/nltk
  copying nltk/util.py -> build/lib/nltk
  copying nltk/text.py -> build/lib/nltk
  copying nltk/decorators.py -> build/lib/nltk
  copying nltk/collocations.py -> build/lib/nltk
  copying nltk/downloader.py -> build/lib/nltk
  copying nltk/help.py -> build/lib/nltk
  copying nltk/internals.py -> build/lib/nltk
  copying nltk/probability.py -> build/lib/nltk
  creating build/lib/nltk/sentiment
  copying nltk/sentiment/__init__.py -> build/lib/nltk/sentiment
  copying nltk/sentiment/util.py -> build/lib/nltk/sentiment
  copying nltk/sentiment/vader.py -> build/lib/nltk/sentiment
  copying nltk/sentiment/sentiment_analyzer.py -> build/lib/nltk/sentiment
  creating build/lib/nltk/cluster
  copying nltk/cluster/kmeans.py -> build/lib/nltk/cluster
  copying nltk/cluster/gaac.py -> build/lib/nltk/cluster
  copying nltk/cluster/__init__.py -> build/lib/nltk/cluster
  copying nltk/cluster/api.py -> build/lib/nltk/cluster
  copying nltk/cluster/util.py -> build/lib/nltk/cluster
  copying nltk/cluster/em.py -> build/lib/nltk/cluster
  creating build/lib/nltk/chat
  copying nltk/chat/eliza.py -> build/lib/nltk/chat
  copying nltk/chat/rude.py -> build/lib/nltk/chat
  copying nltk/chat/__init__.py -> build/lib/nltk/chat
  copying nltk/chat/zen.py -> build/lib/nltk/chat
  copying nltk/chat/util.py -> build/lib/nltk/chat
  copying nltk/chat/iesha.py -> build/lib/nltk/chat
  copying nltk/chat/suntsu.py -> build/lib/nltk/chat
  creating build/lib/nltk/tag
  copying nltk/tag/brill.py -> build/lib/nltk/tag
  copying nltk/tag/stanford.py -> build/lib/nltk/tag
  copying nltk/tag/mapping.py -> build/lib/nltk/tag
  copying nltk/tag/__init__.py -> build/lib/nltk/tag
  copying nltk/tag/senna.py -> build/lib/nltk/tag
  copying nltk/tag/hunpos.py -> build/lib/nltk/tag
  copying nltk/tag/api.py -> build/lib/nltk/tag
  copying nltk/tag/hmm.py -> build/lib/nltk/tag
  copying nltk/tag/util.py -> build/lib/nltk/tag
  copying nltk/tag/perceptron.py -> build/lib/nltk/tag
  copying nltk/tag/sequential.py -> build/lib/nltk/tag
  copying nltk/tag/brill_trainer.py -> build/lib/nltk/tag
  copying nltk/tag/crf.py -> build/lib/nltk/tag
  copying nltk/tag/tnt.py -> build/lib/nltk/tag
  creating build/lib/nltk/ccg
  copying nltk/ccg/combinator.py -> build/lib/nltk/ccg
  copying nltk/ccg/__init__.py -> build/lib/nltk/ccg
  copying nltk/ccg/api.py -> build/lib/nltk/ccg
  copying nltk/ccg/logic.py -> build/lib/nltk/ccg
  copying nltk/ccg/lexicon.py -> build/lib/nltk/ccg
  copying nltk/ccg/chart.py -> build/lib/nltk/ccg
  creating build/lib/nltk/twitter
  copying nltk/twitter/common.py -> build/lib/nltk/twitter
  copying nltk/twitter/twitter_demo.py -> build/lib/nltk/twitter
  copying nltk/twitter/__init__.py -> build/lib/nltk/twitter
  copying nltk/twitter/api.py -> build/lib/nltk/twitter
  copying nltk/twitter/util.py -> build/lib/nltk/twitter
  copying nltk/twitter/twitterclient.py -> build/lib/nltk/twitter
  creating build/lib/nltk/metrics
  copying nltk/metrics/association.py -> build/lib/nltk/metrics
  copying nltk/metrics/paice.py -> build/lib/nltk/metrics
  copying nltk/metrics/__init__.py -> build/lib/nltk/metrics
  copying nltk/metrics/agreement.py -> build/lib/nltk/metrics
  copying nltk/metrics/distance.py -> build/lib/nltk/metrics
  copying nltk/metrics/scores.py -> build/lib/nltk/metrics
  copying nltk/metrics/spearman.py -> build/lib/nltk/metrics
  copying nltk/metrics/confusionmatrix.py -> build/lib/nltk/metrics
  copying nltk/metrics/aline.py -> build/lib/nltk/metrics
  copying nltk/metrics/segmentation.py -> build/lib/nltk/metrics
  creating build/lib/nltk/lm
  copying nltk/lm/smoothing.py -> build/lib/nltk/lm
  copying nltk/lm/preprocessing.py -> build/lib/nltk/lm
  copying nltk/lm/models.py -> build/lib/nltk/lm
  copying nltk/lm/counter.py -> build/lib/nltk/lm
  copying nltk/lm/__init__.py -> build/lib/nltk/lm
  copying nltk/lm/api.py -> build/lib/nltk/lm
  copying nltk/lm/util.py -> build/lib/nltk/lm
  copying nltk/lm/vocabulary.py -> build/lib/nltk/lm
  creating build/lib/nltk/inference
  copying nltk/inference/resolution.py -> build/lib/nltk/inference
  copying nltk/inference/prover9.py -> build/lib/nltk/inference
  copying nltk/inference/nonmonotonic.py -> build/lib/nltk/inference
  copying nltk/inference/__init__.py -> build/lib/nltk/inference
  copying nltk/inference/tableau.py -> build/lib/nltk/inference
  copying nltk/inference/api.py -> build/lib/nltk/inference
  copying nltk/inference/discourse.py -> build/lib/nltk/inference
  copying nltk/inference/mace.py -> build/lib/nltk/inference
  creating build/lib/nltk/translate
  copying nltk/translate/ibm3.py -> build/lib/nltk/translate
  copying nltk/translate/ribes_score.py -> build/lib/nltk/translate
  copying nltk/translate/chrf_score.py -> build/lib/nltk/translate
  copying nltk/translate/ibm4.py -> build/lib/nltk/translate
  copying nltk/translate/metrics.py -> build/lib/nltk/translate
  copying nltk/translate/phrase_based.py -> build/lib/nltk/translate
  copying nltk/translate/gale_church.py -> build/lib/nltk/translate
  copying nltk/translate/nist_score.py -> build/lib/nltk/translate
  copying nltk/translate/__init__.py -> build/lib/nltk/translate
  copying nltk/translate/ibm5.py -> build/lib/nltk/translate
  copying nltk/translate/api.py -> build/lib/nltk/translate
  copying nltk/translate/gleu_score.py -> build/lib/nltk/translate
  copying nltk/translate/meteor_score.py -> build/lib/nltk/translate
  copying nltk/translate/stack_decoder.py -> build/lib/nltk/translate
  copying nltk/translate/ibm2.py -> build/lib/nltk/translate
  copying nltk/translate/bleu_score.py -> build/lib/nltk/translate
  copying nltk/translate/ibm1.py -> build/lib/nltk/translate
  copying nltk/translate/ibm_model.py -> build/lib/nltk/translate
  copying nltk/translate/gdfa.py -> build/lib/nltk/translate
  creating build/lib/nltk/corpus
  copying nltk/corpus/europarl_raw.py -> build/lib/nltk/corpus
  copying nltk/corpus/__init__.py -> build/lib/nltk/corpus
  copying nltk/corpus/util.py -> build/lib/nltk/corpus
  creating build/lib/nltk/tokenize
  copying nltk/tokenize/repp.py -> build/lib/nltk/tokenize
  copying nltk/tokenize/sexpr.py -> build/lib/nltk/tokenize
  copying nltk/tokenize/stanford.py -> build/lib/nltk/tokenize
  copying nltk/tokenize/toktok.py -> build/lib/nltk/tokenize
  copying nltk/tokenize/regexp.py -> build/lib/nltk/tokenize
  copying nltk/tokenize/casual.py -> build/lib/nltk/tokenize
  copying nltk/tokenize/__init__.py -> build/lib/nltk/tokenize
  copying nltk/tokenize/treebank.py -> build/lib/nltk/tokenize
  copying nltk/tokenize/api.py -> build/lib/nltk/tokenize
  copying nltk/tokenize/simple.py -> build/lib/nltk/tokenize
  copying nltk/tokenize/stanford_segmenter.py -> build/lib/nltk/tokenize
  copying nltk/tokenize/util.py -> build/lib/nltk/tokenize
  copying nltk/tokenize/punkt.py -> build/lib/nltk/tokenize
  copying nltk/tokenize/mwe.py -> build/lib/nltk/tokenize
  copying nltk/tokenize/texttiling.py -> build/lib/nltk/tokenize
  copying nltk/tokenize/sonority_sequencing.py -> build/lib/nltk/tokenize
  copying nltk/tokenize/nist.py -> build/lib/nltk/tokenize
  creating build/lib/nltk/app
  copying nltk/app/chartparser_app.py -> build/lib/nltk/app
  copying nltk/app/chunkparser_app.py -> build/lib/nltk/app
  copying nltk/app/nemo_app.py -> build/lib/nltk/app
  copying nltk/app/__init__.py -> build/lib/nltk/app
  copying nltk/app/rdparser_app.py -> build/lib/nltk/app
  copying nltk/app/wordnet_app.py -> build/lib/nltk/app
  copying nltk/app/concordance_app.py -> build/lib/nltk/app
  copying nltk/app/collocations_app.py -> build/lib/nltk/app
  copying nltk/app/srparser_app.py -> build/lib/nltk/app
  copying nltk/app/wordfreq_app.py -> build/lib/nltk/app
  creating build/lib/nltk/sem
  copying nltk/sem/relextract.py -> build/lib/nltk/sem
  copying nltk/sem/lfg.py -> build/lib/nltk/sem
  copying nltk/sem/chat80.py -> build/lib/nltk/sem
  copying nltk/sem/__init__.py -> build/lib/nltk/sem
  copying nltk/sem/linearlogic.py -> build/lib/nltk/sem
  copying nltk/sem/hole.py -> build/lib/nltk/sem
  copying nltk/sem/drt_glue_demo.py -> build/lib/nltk/sem
  copying nltk/sem/boxer.py -> build/lib/nltk/sem
  copying nltk/sem/cooper_storage.py -> build/lib/nltk/sem
  copying nltk/sem/util.py -> build/lib/nltk/sem
  copying nltk/sem/glue.py -> build/lib/nltk/sem
  copying nltk/sem/logic.py -> build/lib/nltk/sem
  copying nltk/sem/drt.py -> build/lib/nltk/sem
  copying nltk/sem/skolemize.py -> build/lib/nltk/sem
  copying nltk/sem/evaluate.py -> build/lib/nltk/sem
  creating build/lib/nltk/stem
  copying nltk/stem/isri.py -> build/lib/nltk/stem
  copying nltk/stem/wordnet.py -> build/lib/nltk/stem
  copying nltk/stem/regexp.py -> build/lib/nltk/stem
  copying nltk/stem/__init__.py -> build/lib/nltk/stem
  copying nltk/stem/api.py -> build/lib/nltk/stem
  copying nltk/stem/porter.py -> build/lib/nltk/stem
  copying nltk/stem/lancaster.py -> build/lib/nltk/stem
  copying nltk/stem/util.py -> build/lib/nltk/stem
  copying nltk/stem/snowball.py -> build/lib/nltk/stem
  copying nltk/stem/cistem.py -> build/lib/nltk/stem
  copying nltk/stem/rslp.py -> build/lib/nltk/stem
  copying nltk/stem/arlstem.py -> build/lib/nltk/stem
  creating build/lib/nltk/parse
  copying nltk/parse/bllip.py -> build/lib/nltk/parse
  copying nltk/parse/corenlp.py -> build/lib/nltk/parse
  copying nltk/parse/stanford.py -> build/lib/nltk/parse
  copying nltk/parse/recursivedescent.py -> build/lib/nltk/parse
  copying nltk/parse/generate.py -> build/lib/nltk/parse
  copying nltk/parse/__init__.py -> build/lib/nltk/parse
  copying nltk/parse/pchart.py -> build/lib/nltk/parse
  copying nltk/parse/api.py -> build/lib/nltk/parse
  copying nltk/parse/shiftreduce.py -> build/lib/nltk/parse
  copying nltk/parse/util.py -> build/lib/nltk/parse
  copying nltk/parse/viterbi.py -> build/lib/nltk/parse
  copying nltk/parse/malt.py -> build/lib/nltk/parse
  copying nltk/parse/transitionparser.py -> build/lib/nltk/parse
  copying nltk/parse/featurechart.py -> build/lib/nltk/parse
  copying nltk/parse/dependencygraph.py -> build/lib/nltk/parse
  copying nltk/parse/projectivedependencyparser.py -> build/lib/nltk/parse
  copying nltk/parse/earleychart.py -> build/lib/nltk/parse
  copying nltk/parse/evaluate.py -> build/lib/nltk/parse
  copying nltk/parse/nonprojectivedependencyparser.py -> build/lib/nltk/parse
  copying nltk/parse/chart.py -> build/lib/nltk/parse
  creating build/lib/nltk/chunk
  copying nltk/chunk/named_entity.py -> build/lib/nltk/chunk
  copying nltk/chunk/regexp.py -> build/lib/nltk/chunk
  copying nltk/chunk/__init__.py -> build/lib/nltk/chunk
  copying nltk/chunk/api.py -> build/lib/nltk/chunk
  copying nltk/chunk/util.py -> build/lib/nltk/chunk
  creating build/lib/nltk/classify
  copying nltk/classify/decisiontree.py -> build/lib/nltk/classify
  copying nltk/classify/positivenaivebayes.py -> build/lib/nltk/classify
  copying nltk/classify/__init__.py -> build/lib/nltk/classify
  copying nltk/classify/senna.py -> build/lib/nltk/classify
  copying nltk/classify/api.py -> build/lib/nltk/classify
  copying nltk/classify/svm.py -> build/lib/nltk/classify
  copying nltk/classify/naivebayes.py -> build/lib/nltk/classify
  copying nltk/classify/textcat.py -> build/lib/nltk/classify
  copying nltk/classify/util.py -> build/lib/nltk/classify
  copying nltk/classify/tadm.py -> build/lib/nltk/classify
  copying nltk/classify/weka.py -> build/lib/nltk/classify
  copying nltk/classify/rte_classify.py -> build/lib/nltk/classify
  copying nltk/classify/maxent.py -> build/lib/nltk/classify
  copying nltk/classify/scikitlearn.py -> build/lib/nltk/classify
  copying nltk/classify/megam.py -> build/lib/nltk/classify
  creating build/lib/nltk/test
  copying nltk/test/segmentation_fixt.py -> build/lib/nltk/test
  copying nltk/test/semantics_fixt.py -> build/lib/nltk/test
  copying nltk/test/translate_fixt.py -> build/lib/nltk/test
  copying nltk/test/inference_fixt.py -> build/lib/nltk/test
  copying nltk/test/discourse_fixt.py -> build/lib/nltk/test
  copying nltk/test/doctest_nose_plugin.py -> build/lib/nltk/test
  copying nltk/test/__init__.py -> build/lib/nltk/test
  copying nltk/test/all.py -> build/lib/nltk/test
  copying nltk/test/classify_fixt.py -> build/lib/nltk/test
  copying nltk/test/gluesemantics_malt_fixt.py -> build/lib/nltk/test
  copying nltk/test/wordnet_fixt.py -> build/lib/nltk/test
  copying nltk/test/compat_fixt.py -> build/lib/nltk/test
  copying nltk/test/probability_fixt.py -> build/lib/nltk/test
  copying nltk/test/gensim_fixt.py -> build/lib/nltk/test
  copying nltk/test/nonmonotonic_fixt.py -> build/lib/nltk/test
  copying nltk/test/corpus_fixt.py -> build/lib/nltk/test
  copying nltk/test/childes_fixt.py -> build/lib/nltk/test
  copying nltk/test/runtests.py -> build/lib/nltk/test
  copying nltk/test/portuguese_en_fixt.py -> build/lib/nltk/test
  creating build/lib/nltk/tbl
  copying nltk/tbl/rule.py -> build/lib/nltk/tbl
  copying nltk/tbl/demo.py -> build/lib/nltk/tbl
  copying nltk/tbl/__init__.py -> build/lib/nltk/tbl
  copying nltk/tbl/api.py -> build/lib/nltk/tbl
  copying nltk/tbl/template.py -> build/lib/nltk/tbl
  copying nltk/tbl/erroranalysis.py -> build/lib/nltk/tbl
  copying nltk/tbl/feature.py -> build/lib/nltk/tbl
  creating build/lib/nltk/draw
  copying nltk/draw/tree.py -> build/lib/nltk/draw
  copying nltk/draw/__init__.py -> build/lib/nltk/draw
  copying nltk/draw/util.py -> build/lib/nltk/draw
  copying nltk/draw/table.py -> build/lib/nltk/draw
  copying nltk/draw/dispersion.py -> build/lib/nltk/draw
  copying nltk/draw/cfg.py -> build/lib/nltk/draw
  creating build/lib/nltk/misc
  copying nltk/misc/chomsky.py -> build/lib/nltk/misc
  copying nltk/misc/sort.py -> build/lib/nltk/misc
  copying nltk/misc/__init__.py -> build/lib/nltk/misc
  copying nltk/misc/minimalset.py -> build/lib/nltk/misc
  copying nltk/misc/wordfinder.py -> build/lib/nltk/misc
  copying nltk/misc/babelfish.py -> build/lib/nltk/misc
  creating build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/ycoe.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/sentiwordnet.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/panlex_lite.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/pros_cons.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/switchboard.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/chunked.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/toolbox.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/nkjp.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/categorized_sents.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/reviews.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/childes.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/knbc.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/aligned.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/wordnet.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/pl196x.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/comparative_sents.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/__init__.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/senseval.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/rte.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/plaintext.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/sinica_treebank.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/api.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/dependency.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/crubadan.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/verbnet.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/panlex_swadesh.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/nps_chat.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/util.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/opinion_lexicon.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/mte.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/lin.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/cmudict.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/udhr.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/indian.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/twitter.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/bnc.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/conll.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/string_category.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/tagged.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/framenet.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/chasen.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/ipipan.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/wordlist.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/timit.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/nombank.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/xmldocs.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/propbank.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/ieer.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/semcor.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/ppattach.py -> build/lib/nltk/corpus/reader
  copying nltk/corpus/reader/bracket_parse.py -> build/lib/nltk/corpus/reader
  creating build/lib/nltk/test/unit
  copying nltk/test/unit/test_collocations.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_naivebayes.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_brill.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_2x_compat.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_corpus_views.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_twitter_auth.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_tgrep.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_nombank.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_tokenize.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/__init__.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_json2csv_corpus.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_wordnet.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_corpora.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_rte_classify.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_tag.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/utils.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_classify.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_cfg2chomsky.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_concordance.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_seekable_unicode_stream_reader.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_senna.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_chunk.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_stem.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_pos_tag.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_hmm.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_aline.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_disagreement.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_corenlp.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_data.py -> build/lib/nltk/test/unit
  copying nltk/test/unit/test_cfd_mutation.py -> build/lib/nltk/test/unit
  creating build/lib/nltk/test/unit/lm
  copying nltk/test/unit/lm/__init__.py -> build/lib/nltk/test/unit/lm
  copying nltk/test/unit/lm/test_preprocessing.py -> build/lib/nltk/test/unit/lm
  copying nltk/test/unit/lm/test_counter.py -> build/lib/nltk/test/unit/lm
  copying nltk/test/unit/lm/test_models.py -> build/lib/nltk/test/unit/lm
  copying nltk/test/unit/lm/test_vocabulary.py -> build/lib/nltk/test/unit/lm
  creating build/lib/nltk/test/unit/translate
  copying nltk/test/unit/translate/test_gdfa.py -> build/lib/nltk/test/unit/translate
  copying nltk/test/unit/translate/test_stack_decoder.py -> build/lib/nltk/test/unit/translate
  copying nltk/test/unit/translate/test_nist.py -> build/lib/nltk/test/unit/translate
  copying nltk/test/unit/translate/__init__.py -> build/lib/nltk/test/unit/translate
  copying nltk/test/unit/translate/test_ibm3.py -> build/lib/nltk/test/unit/translate
  copying nltk/test/unit/translate/test_ibm1.py -> build/lib/nltk/test/unit/translate
  copying nltk/test/unit/translate/test_ibm2.py -> build/lib/nltk/test/unit/translate
  copying nltk/test/unit/translate/test_ibm4.py -> build/lib/nltk/test/unit/translate
  copying nltk/test/unit/translate/test_bleu.py -> build/lib/nltk/test/unit/translate
  copying nltk/test/unit/translate/test_ibm_model.py -> build/lib/nltk/test/unit/translate
  copying nltk/test/unit/translate/test_ibm5.py -> build/lib/nltk/test/unit/translate
  copying nltk/test/stem.doctest -> build/lib/nltk/test
  Traceback (most recent call last):
    File ""<string>"", line 1, in <module>
    File ""/tmp/pip-install-nnqqbh5b/nltk/setup.py"", line 103, in <module>
      zip_safe=False,  # since normal files will be present too?
    File ""/usr/local/lib/python3.6/dist-packages/setuptools/__init__.py"", line 145, in setup
      return distutils.core.setup(**attrs)
    File ""/usr/lib/python3.6/distutils/core.py"", line 148, in setup
      dist.run_commands()
    File ""/usr/lib/python3.6/distutils/dist.py"", line 955, in run_commands
      self.run_command(cmd)
    File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command
      cmd_obj.run()
    File ""/home/01677387637/.local/lib/python3.6/site-packages/wheel/bdist_wheel.py"", line 192, in run
      self.run_command('build')
    File ""/usr/lib/python3.6/distutils/cmd.py"", line 313, in run_command
      self.distribution.run_command(command)
    File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command
      cmd_obj.run()
    File ""/usr/lib/python3.6/distutils/command/build.py"", line 135, in run
      self.run_command(cmd_name)
    File ""/usr/lib/python3.6/distutils/cmd.py"", line 313, in run_command
      self.distribution.run_command(command)
    File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command
      cmd_obj.run()
    File ""/usr/local/lib/python3.6/dist-packages/setuptools/command/build_py.py"", line 53, in run
      self.build_package_data()
    File ""/usr/local/lib/python3.6/dist-packages/setuptools/command/build_py.py"", line 126, in build_package_data
      srcfile in self.distribution.convert_2to3_doctests):
  TypeError: argument of type 'NoneType' is not iterable
  ----------------------------------------
  ERROR: Failed building wheel for nltk
  Running setup.py clean for nltk
Failed to build nltk
Installing collected packages: nltk
    Running setup.py install for nltk ... error
    ERROR: Command errored out with exit status 1:
     command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-nnqqbh5b/nltk/setup.py'""'""'; __file__='""'""'/tmp/pip-install-nnqqbh5b/nltk/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-hlea11gz/install-record.txt --single-version-externally-managed --compile
         cwd: /tmp/pip-install-nnqqbh5b/nltk/
    Complete output (407 lines):
    running install
    running build
    running build_py
    creating build
    creating build/lib
    creating build/lib/nltk
    copying nltk/grammar.py -> build/lib/nltk
    copying nltk/book.py -> build/lib/nltk
    copying nltk/jsontags.py -> build/lib/nltk
    copying nltk/toolbox.py -> build/lib/nltk
    copying nltk/treeprettyprinter.py -> build/lib/nltk
    copying nltk/tree.py -> build/lib/nltk
    copying nltk/lazyimport.py -> build/lib/nltk
    copying nltk/featstruct.py -> build/lib/nltk
    copying nltk/collections.py -> build/lib/nltk
    copying nltk/treetransforms.py -> build/lib/nltk
    copying nltk/__init__.py -> build/lib/nltk
    copying nltk/data.py -> build/lib/nltk
    copying nltk/compat.py -> build/lib/nltk
    copying nltk/tgrep.py -> build/lib/nltk
    copying nltk/wsd.py -> build/lib/nltk
    copying nltk/util.py -> build/lib/nltk
    copying nltk/text.py -> build/lib/nltk
    copying nltk/decorators.py -> build/lib/nltk
    copying nltk/collocations.py -> build/lib/nltk
    copying nltk/downloader.py -> build/lib/nltk
    copying nltk/help.py -> build/lib/nltk
    copying nltk/internals.py -> build/lib/nltk
    copying nltk/probability.py -> build/lib/nltk
    creating build/lib/nltk/sentiment
    copying nltk/sentiment/__init__.py -> build/lib/nltk/sentiment
    copying nltk/sentiment/util.py -> build/lib/nltk/sentiment
    copying nltk/sentiment/vader.py -> build/lib/nltk/sentiment
    copying nltk/sentiment/sentiment_analyzer.py -> build/lib/nltk/sentiment
    creating build/lib/nltk/cluster
    copying nltk/cluster/kmeans.py -> build/lib/nltk/cluster
    copying nltk/cluster/gaac.py -> build/lib/nltk/cluster
    copying nltk/cluster/__init__.py -> build/lib/nltk/cluster
    copying nltk/cluster/api.py -> build/lib/nltk/cluster
    copying nltk/cluster/util.py -> build/lib/nltk/cluster
    copying nltk/cluster/em.py -> build/lib/nltk/cluster
    creating build/lib/nltk/chat
    copying nltk/chat/eliza.py -> build/lib/nltk/chat
    copying nltk/chat/rude.py -> build/lib/nltk/chat
    copying nltk/chat/__init__.py -> build/lib/nltk/chat
    copying nltk/chat/zen.py -> build/lib/nltk/chat
    copying nltk/chat/util.py -> build/lib/nltk/chat
    copying nltk/chat/iesha.py -> build/lib/nltk/chat
    copying nltk/chat/suntsu.py -> build/lib/nltk/chat
    creating build/lib/nltk/tag
    copying nltk/tag/brill.py -> build/lib/nltk/tag
    copying nltk/tag/stanford.py -> build/lib/nltk/tag
    copying nltk/tag/mapping.py -> build/lib/nltk/tag
    copying nltk/tag/__init__.py -> build/lib/nltk/tag
    copying nltk/tag/senna.py -> build/lib/nltk/tag
    copying nltk/tag/hunpos.py -> build/lib/nltk/tag
    copying nltk/tag/api.py -> build/lib/nltk/tag
    copying nltk/tag/hmm.py -> build/lib/nltk/tag
    copying nltk/tag/util.py -> build/lib/nltk/tag
    copying nltk/tag/perceptron.py -> build/lib/nltk/tag
    copying nltk/tag/sequential.py -> build/lib/nltk/tag
    copying nltk/tag/brill_trainer.py -> build/lib/nltk/tag
    copying nltk/tag/crf.py -> build/lib/nltk/tag
    copying nltk/tag/tnt.py -> build/lib/nltk/tag
    creating build/lib/nltk/ccg
    copying nltk/ccg/combinator.py -> build/lib/nltk/ccg
    copying nltk/ccg/__init__.py -> build/lib/nltk/ccg
    copying nltk/ccg/api.py -> build/lib/nltk/ccg
    copying nltk/ccg/logic.py -> build/lib/nltk/ccg
    copying nltk/ccg/lexicon.py -> build/lib/nltk/ccg
    copying nltk/ccg/chart.py -> build/lib/nltk/ccg
    creating build/lib/nltk/twitter
    copying nltk/twitter/common.py -> build/lib/nltk/twitter
    copying nltk/twitter/twitter_demo.py -> build/lib/nltk/twitter
    copying nltk/twitter/__init__.py -> build/lib/nltk/twitter
    copying nltk/twitter/api.py -> build/lib/nltk/twitter
    copying nltk/twitter/util.py -> build/lib/nltk/twitter
    copying nltk/twitter/twitterclient.py -> build/lib/nltk/twitter
    creating build/lib/nltk/metrics
    copying nltk/metrics/association.py -> build/lib/nltk/metrics
    copying nltk/metrics/paice.py -> build/lib/nltk/metrics
    copying nltk/metrics/__init__.py -> build/lib/nltk/metrics
    copying nltk/metrics/agreement.py -> build/lib/nltk/metrics
    copying nltk/metrics/distance.py -> build/lib/nltk/metrics
    copying nltk/metrics/scores.py -> build/lib/nltk/metrics
    copying nltk/metrics/spearman.py -> build/lib/nltk/metrics
    copying nltk/metrics/confusionmatrix.py -> build/lib/nltk/metrics
    copying nltk/metrics/aline.py -> build/lib/nltk/metrics
    copying nltk/metrics/segmentation.py -> build/lib/nltk/metrics
    creating build/lib/nltk/lm
    copying nltk/lm/smoothing.py -> build/lib/nltk/lm
    copying nltk/lm/preprocessing.py -> build/lib/nltk/lm
    copying nltk/lm/models.py -> build/lib/nltk/lm
    copying nltk/lm/counter.py -> build/lib/nltk/lm
    copying nltk/lm/__init__.py -> build/lib/nltk/lm
    copying nltk/lm/api.py -> build/lib/nltk/lm
    copying nltk/lm/util.py -> build/lib/nltk/lm
    copying nltk/lm/vocabulary.py -> build/lib/nltk/lm
    creating build/lib/nltk/inference
    copying nltk/inference/resolution.py -> build/lib/nltk/inference
    copying nltk/inference/prover9.py -> build/lib/nltk/inference
    copying nltk/inference/nonmonotonic.py -> build/lib/nltk/inference
    copying nltk/inference/__init__.py -> build/lib/nltk/inference
    copying nltk/inference/tableau.py -> build/lib/nltk/inference
    copying nltk/inference/api.py -> build/lib/nltk/inference
    copying nltk/inference/discourse.py -> build/lib/nltk/inference
    copying nltk/inference/mace.py -> build/lib/nltk/inference
    creating build/lib/nltk/translate
    copying nltk/translate/ibm3.py -> build/lib/nltk/translate
    copying nltk/translate/ribes_score.py -> build/lib/nltk/translate
    copying nltk/translate/chrf_score.py -> build/lib/nltk/translate
    copying nltk/translate/ibm4.py -> build/lib/nltk/translate
    copying nltk/translate/metrics.py -> build/lib/nltk/translate
    copying nltk/translate/phrase_based.py -> build/lib/nltk/translate
    copying nltk/translate/gale_church.py -> build/lib/nltk/translate
    copying nltk/translate/nist_score.py -> build/lib/nltk/translate
    copying nltk/translate/__init__.py -> build/lib/nltk/translate
    copying nltk/translate/ibm5.py -> build/lib/nltk/translate
    copying nltk/translate/api.py -> build/lib/nltk/translate
    copying nltk/translate/gleu_score.py -> build/lib/nltk/translate
    copying nltk/translate/meteor_score.py -> build/lib/nltk/translate
    copying nltk/translate/stack_decoder.py -> build/lib/nltk/translate
    copying nltk/translate/ibm2.py -> build/lib/nltk/translate
    copying nltk/translate/bleu_score.py -> build/lib/nltk/translate
    copying nltk/translate/ibm1.py -> build/lib/nltk/translate
    copying nltk/translate/ibm_model.py -> build/lib/nltk/translate
    copying nltk/translate/gdfa.py -> build/lib/nltk/translate
    creating build/lib/nltk/corpus
    copying nltk/corpus/europarl_raw.py -> build/lib/nltk/corpus
    copying nltk/corpus/__init__.py -> build/lib/nltk/corpus
    copying nltk/corpus/util.py -> build/lib/nltk/corpus
    creating build/lib/nltk/tokenize
    copying nltk/tokenize/repp.py -> build/lib/nltk/tokenize
    copying nltk/tokenize/sexpr.py -> build/lib/nltk/tokenize
    copying nltk/tokenize/stanford.py -> build/lib/nltk/tokenize
    copying nltk/tokenize/toktok.py -> build/lib/nltk/tokenize
    copying nltk/tokenize/regexp.py -> build/lib/nltk/tokenize
    copying nltk/tokenize/casual.py -> build/lib/nltk/tokenize
    copying nltk/tokenize/__init__.py -> build/lib/nltk/tokenize
    copying nltk/tokenize/treebank.py -> build/lib/nltk/tokenize
    copying nltk/tokenize/api.py -> build/lib/nltk/tokenize
    copying nltk/tokenize/simple.py -> build/lib/nltk/tokenize
    copying nltk/tokenize/stanford_segmenter.py -> build/lib/nltk/tokenize
    copying nltk/tokenize/util.py -> build/lib/nltk/tokenize
    copying nltk/tokenize/punkt.py -> build/lib/nltk/tokenize
    copying nltk/tokenize/mwe.py -> build/lib/nltk/tokenize
    copying nltk/tokenize/texttiling.py -> build/lib/nltk/tokenize
    copying nltk/tokenize/sonority_sequencing.py -> build/lib/nltk/tokenize
    copying nltk/tokenize/nist.py -> build/lib/nltk/tokenize
    creating build/lib/nltk/app
    copying nltk/app/chartparser_app.py -> build/lib/nltk/app
    copying nltk/app/chunkparser_app.py -> build/lib/nltk/app
    copying nltk/app/nemo_app.py -> build/lib/nltk/app
    copying nltk/app/__init__.py -> build/lib/nltk/app
    copying nltk/app/rdparser_app.py -> build/lib/nltk/app
    copying nltk/app/wordnet_app.py -> build/lib/nltk/app
    copying nltk/app/concordance_app.py -> build/lib/nltk/app
    copying nltk/app/collocations_app.py -> build/lib/nltk/app
    copying nltk/app/srparser_app.py -> build/lib/nltk/app
    copying nltk/app/wordfreq_app.py -> build/lib/nltk/app
    creating build/lib/nltk/sem
    copying nltk/sem/relextract.py -> build/lib/nltk/sem
    copying nltk/sem/lfg.py -> build/lib/nltk/sem
    copying nltk/sem/chat80.py -> build/lib/nltk/sem
    copying nltk/sem/__init__.py -> build/lib/nltk/sem
    copying nltk/sem/linearlogic.py -> build/lib/nltk/sem
    copying nltk/sem/hole.py -> build/lib/nltk/sem
    copying nltk/sem/drt_glue_demo.py -> build/lib/nltk/sem
    copying nltk/sem/boxer.py -> build/lib/nltk/sem
    copying nltk/sem/cooper_storage.py -> build/lib/nltk/sem
    copying nltk/sem/util.py -> build/lib/nltk/sem
    copying nltk/sem/glue.py -> build/lib/nltk/sem
    copying nltk/sem/logic.py -> build/lib/nltk/sem
    copying nltk/sem/drt.py -> build/lib/nltk/sem
    copying nltk/sem/skolemize.py -> build/lib/nltk/sem
    copying nltk/sem/evaluate.py -> build/lib/nltk/sem
    creating build/lib/nltk/stem
    copying nltk/stem/isri.py -> build/lib/nltk/stem
    copying nltk/stem/wordnet.py -> build/lib/nltk/stem
    copying nltk/stem/regexp.py -> build/lib/nltk/stem
    copying nltk/stem/__init__.py -> build/lib/nltk/stem
    copying nltk/stem/api.py -> build/lib/nltk/stem
    copying nltk/stem/porter.py -> build/lib/nltk/stem
    copying nltk/stem/lancaster.py -> build/lib/nltk/stem
    copying nltk/stem/util.py -> build/lib/nltk/stem
    copying nltk/stem/snowball.py -> build/lib/nltk/stem
    copying nltk/stem/cistem.py -> build/lib/nltk/stem
    copying nltk/stem/rslp.py -> build/lib/nltk/stem
    copying nltk/stem/arlstem.py -> build/lib/nltk/stem
    creating build/lib/nltk/parse
    copying nltk/parse/bllip.py -> build/lib/nltk/parse
    copying nltk/parse/corenlp.py -> build/lib/nltk/parse
    copying nltk/parse/stanford.py -> build/lib/nltk/parse
    copying nltk/parse/recursivedescent.py -> build/lib/nltk/parse
    copying nltk/parse/generate.py -> build/lib/nltk/parse
    copying nltk/parse/__init__.py -> build/lib/nltk/parse
    copying nltk/parse/pchart.py -> build/lib/nltk/parse
    copying nltk/parse/api.py -> build/lib/nltk/parse
    copying nltk/parse/shiftreduce.py -> build/lib/nltk/parse
    copying nltk/parse/util.py -> build/lib/nltk/parse
    copying nltk/parse/viterbi.py -> build/lib/nltk/parse
    copying nltk/parse/malt.py -> build/lib/nltk/parse
    copying nltk/parse/transitionparser.py -> build/lib/nltk/parse
    copying nltk/parse/featurechart.py -> build/lib/nltk/parse
    copying nltk/parse/dependencygraph.py -> build/lib/nltk/parse
    copying nltk/parse/projectivedependencyparser.py -> build/lib/nltk/parse
    copying nltk/parse/earleychart.py -> build/lib/nltk/parse
    copying nltk/parse/evaluate.py -> build/lib/nltk/parse
    copying nltk/parse/nonprojectivedependencyparser.py -> build/lib/nltk/parse
    copying nltk/parse/chart.py -> build/lib/nltk/parse
    creating build/lib/nltk/chunk
    copying nltk/chunk/named_entity.py -> build/lib/nltk/chunk
    copying nltk/chunk/regexp.py -> build/lib/nltk/chunk
    copying nltk/chunk/__init__.py -> build/lib/nltk/chunk
    copying nltk/chunk/api.py -> build/lib/nltk/chunk
    copying nltk/chunk/util.py -> build/lib/nltk/chunk
    creating build/lib/nltk/classify
    copying nltk/classify/decisiontree.py -> build/lib/nltk/classify
    copying nltk/classify/positivenaivebayes.py -> build/lib/nltk/classify
    copying nltk/classify/__init__.py -> build/lib/nltk/classify
    copying nltk/classify/senna.py -> build/lib/nltk/classify
    copying nltk/classify/api.py -> build/lib/nltk/classify
    copying nltk/classify/svm.py -> build/lib/nltk/classify
    copying nltk/classify/naivebayes.py -> build/lib/nltk/classify
    copying nltk/classify/textcat.py -> build/lib/nltk/classify
    copying nltk/classify/util.py -> build/lib/nltk/classify
    copying nltk/classify/tadm.py -> build/lib/nltk/classify
    copying nltk/classify/weka.py -> build/lib/nltk/classify
    copying nltk/classify/rte_classify.py -> build/lib/nltk/classify
    copying nltk/classify/maxent.py -> build/lib/nltk/classify
    copying nltk/classify/scikitlearn.py -> build/lib/nltk/classify
    copying nltk/classify/megam.py -> build/lib/nltk/classify
    creating build/lib/nltk/test
    copying nltk/test/segmentation_fixt.py -> build/lib/nltk/test
    copying nltk/test/semantics_fixt.py -> build/lib/nltk/test
    copying nltk/test/translate_fixt.py -> build/lib/nltk/test
    copying nltk/test/inference_fixt.py -> build/lib/nltk/test
    copying nltk/test/discourse_fixt.py -> build/lib/nltk/test
    copying nltk/test/doctest_nose_plugin.py -> build/lib/nltk/test
    copying nltk/test/__init__.py -> build/lib/nltk/test
    copying nltk/test/all.py -> build/lib/nltk/test
    copying nltk/test/classify_fixt.py -> build/lib/nltk/test
    copying nltk/test/gluesemantics_malt_fixt.py -> build/lib/nltk/test
    copying nltk/test/wordnet_fixt.py -> build/lib/nltk/test
    copying nltk/test/compat_fixt.py -> build/lib/nltk/test
    copying nltk/test/probability_fixt.py -> build/lib/nltk/test
    copying nltk/test/gensim_fixt.py -> build/lib/nltk/test
    copying nltk/test/nonmonotonic_fixt.py -> build/lib/nltk/test
    copying nltk/test/corpus_fixt.py -> build/lib/nltk/test
    copying nltk/test/childes_fixt.py -> build/lib/nltk/test
    copying nltk/test/runtests.py -> build/lib/nltk/test
    copying nltk/test/portuguese_en_fixt.py -> build/lib/nltk/test
    creating build/lib/nltk/tbl
    copying nltk/tbl/rule.py -> build/lib/nltk/tbl
    copying nltk/tbl/demo.py -> build/lib/nltk/tbl
    copying nltk/tbl/__init__.py -> build/lib/nltk/tbl
    copying nltk/tbl/api.py -> build/lib/nltk/tbl
    copying nltk/tbl/template.py -> build/lib/nltk/tbl
    copying nltk/tbl/erroranalysis.py -> build/lib/nltk/tbl
    copying nltk/tbl/feature.py -> build/lib/nltk/tbl
    creating build/lib/nltk/draw
    copying nltk/draw/tree.py -> build/lib/nltk/draw
    copying nltk/draw/__init__.py -> build/lib/nltk/draw
    copying nltk/draw/util.py -> build/lib/nltk/draw
    copying nltk/draw/table.py -> build/lib/nltk/draw
    copying nltk/draw/dispersion.py -> build/lib/nltk/draw
    copying nltk/draw/cfg.py -> build/lib/nltk/draw
    creating build/lib/nltk/misc
    copying nltk/misc/chomsky.py -> build/lib/nltk/misc
    copying nltk/misc/sort.py -> build/lib/nltk/misc
    copying nltk/misc/__init__.py -> build/lib/nltk/misc
    copying nltk/misc/minimalset.py -> build/lib/nltk/misc
    copying nltk/misc/wordfinder.py -> build/lib/nltk/misc
    copying nltk/misc/babelfish.py -> build/lib/nltk/misc
    creating build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/ycoe.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/sentiwordnet.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/panlex_lite.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/pros_cons.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/switchboard.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/chunked.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/toolbox.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/nkjp.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/categorized_sents.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/reviews.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/childes.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/knbc.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/aligned.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/wordnet.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/pl196x.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/comparative_sents.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/__init__.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/senseval.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/rte.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/plaintext.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/sinica_treebank.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/api.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/dependency.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/crubadan.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/verbnet.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/panlex_swadesh.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/nps_chat.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/util.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/opinion_lexicon.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/mte.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/lin.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/cmudict.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/udhr.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/indian.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/twitter.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/bnc.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/conll.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/string_category.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/tagged.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/framenet.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/chasen.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/ipipan.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/wordlist.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/timit.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/nombank.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/xmldocs.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/propbank.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/ieer.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/semcor.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/ppattach.py -> build/lib/nltk/corpus/reader
    copying nltk/corpus/reader/bracket_parse.py -> build/lib/nltk/corpus/reader
    creating build/lib/nltk/test/unit
    copying nltk/test/unit/test_collocations.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_naivebayes.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_brill.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_2x_compat.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_corpus_views.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_twitter_auth.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_tgrep.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_nombank.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_tokenize.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/__init__.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_json2csv_corpus.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_wordnet.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_corpora.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_rte_classify.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_tag.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/utils.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_classify.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_cfg2chomsky.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_concordance.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_seekable_unicode_stream_reader.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_senna.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_chunk.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_stem.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_pos_tag.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_hmm.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_aline.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_disagreement.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_corenlp.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_data.py -> build/lib/nltk/test/unit
    copying nltk/test/unit/test_cfd_mutation.py -> build/lib/nltk/test/unit
    creating build/lib/nltk/test/unit/lm
    copying nltk/test/unit/lm/__init__.py -> build/lib/nltk/test/unit/lm
    copying nltk/test/unit/lm/test_preprocessing.py -> build/lib/nltk/test/unit/lm
    copying nltk/test/unit/lm/test_counter.py -> build/lib/nltk/test/unit/lm
    copying nltk/test/unit/lm/test_models.py -> build/lib/nltk/test/unit/lm
    copying nltk/test/unit/lm/test_vocabulary.py -> build/lib/nltk/test/unit/lm
    creating build/lib/nltk/test/unit/translate
    copying nltk/test/unit/translate/test_gdfa.py -> build/lib/nltk/test/unit/translate
    copying nltk/test/unit/translate/test_stack_decoder.py -> build/lib/nltk/test/unit/translate
    copying nltk/test/unit/translate/test_nist.py -> build/lib/nltk/test/unit/translate
    copying nltk/test/unit/translate/__init__.py -> build/lib/nltk/test/unit/translate
    copying nltk/test/unit/translate/test_ibm3.py -> build/lib/nltk/test/unit/translate
    copying nltk/test/unit/translate/test_ibm1.py -> build/lib/nltk/test/unit/translate
    copying nltk/test/unit/translate/test_ibm2.py -> build/lib/nltk/test/unit/translate
    copying nltk/test/unit/translate/test_ibm4.py -> build/lib/nltk/test/unit/translate
    copying nltk/test/unit/translate/test_bleu.py -> build/lib/nltk/test/unit/translate
    copying nltk/test/unit/translate/test_ibm_model.py -> build/lib/nltk/test/unit/translate
    copying nltk/test/unit/translate/test_ibm5.py -> build/lib/nltk/test/unit/translate
    copying nltk/test/stem.doctest -> build/lib/nltk/test
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/tmp/pip-install-nnqqbh5b/nltk/setup.py"", line 103, in <module>
        zip_safe=False,  # since normal files will be present too?
      File ""/usr/local/lib/python3.6/dist-packages/setuptools/__init__.py"", line 145, in setup
        return distutils.core.setup(**attrs)
      File ""/usr/lib/python3.6/distutils/core.py"", line 148, in setup
        dist.run_commands()
      File ""/usr/lib/python3.6/distutils/dist.py"", line 955, in run_commands
        self.run_command(cmd)
      File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/usr/local/lib/python3.6/dist-packages/setuptools/command/install.py"", line 61, in run
        return orig.install.run(self)
      File ""/usr/lib/python3.6/distutils/command/install.py"", line 589, in run
        self.run_command('build')
      File ""/usr/lib/python3.6/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/usr/lib/python3.6/distutils/command/build.py"", line 135, in run
        self.run_command(cmd_name)
      File ""/usr/lib/python3.6/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/usr/local/lib/python3.6/dist-packages/setuptools/command/build_py.py"", line 53, in run
        self.build_package_data()
      File ""/usr/local/lib/python3.6/dist-packages/setuptools/command/build_py.py"", line 126, in build_package_data
        srcfile in self.distribution.convert_2to3_doctests):
    TypeError: argument of type 'NoneType' is not iterable
    ----------------------------------------
ERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-nnqqbh5b/nltk/setup.py'""'""'; __file__='""'""'/tmp/pip-install-nnqqbh5b/nltk/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-hlea11gz/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.
```

Any idea of what to do?"
693,https://github.com/nltk/nltk/issues/2478,2478,[],open,2019-12-14 18:09:56+00:00,,3,ValueError when unpickling ParentedTree with Python 3.7 or higher,"While working in Python 3.6, when unpickling a ParentedTree with Python 3.7 or Python 3.8 following ValueError occurs:
`ValueError: Can not insert a subtree that already has a parent.`

Following example script produces the Error:

```python
import pickle
from nltk.tree import ParentedTree


tree = ParentedTree.fromstring('(S (NN x) (NP x) (NN x))')
pickled = pickle.dumps(tree)
tree_2 = pickle.loads(pickled)

print(tree)
print(tree_2)
```

Output of Python 3.6 (working):
```
(S (NN x) (NP x) (NN x))
(S (NN x) (NP x) (NN x))
```

Output in Python 3.7 / 3.8:
```
Traceback (most recent call last):
  File ""nltk_pickle_test.py"", line 7, in <module>
    tree_2 = pickle.loads(pickled)
  File ""pickletest/venv3.8/lib/python3.8/site-packages/nltk/tree.py"", line 1192, in extend
    self._setparent(child, len(self))
  File ""pickletest/venv3.8/lib/python3.8/site-packages/nltk/tree.py"", line 1358, in _setparent
    raise ValueError('Can not insert a subtree that already ' 'has a parent.')
ValueError: Can not insert a subtree that already has a parent.
```

The error also occurs when saving and loading via a file. Tested with nltk 3.4.5."
694,https://github.com/nltk/nltk/issues/2479,2479,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2019-12-15 19:54:50+00:00,,1,Server not working,"andeshs-MacBook-Pro:stanford-corenlp-full-2018-02-27 sandesh$ [main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - setting default constituency parser
[main] INFO CoreNLP - warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP - using: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz instead
[main] INFO CoreNLP - to use shift reduce parser download English models jar from:
[main] INFO CoreNLP - http://stanfordnlp.github.io/CoreNLP/download.html
[main] INFO CoreNLP -     Threads: 8
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.8 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.2 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
[main] ERROR CoreNLP - Could not pre-load annotators in server; encountered exception:
edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl
	at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:38)
	at edu.stanford.nlp.time.TimeExpressionExtractorFactory.create(TimeExpressionExtractorFactory.java:60)
	at edu.stanford.nlp.time.TimeExpressionExtractorFactory.createExtractor(TimeExpressionExtractorFactory.java:43)
	at edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.<init>(NumberSequenceClassifier.java:86)
	at edu.stanford.nlp.ie.NERClassifierCombiner.<init>(NERClassifierCombiner.java:135)
	at edu.stanford.nlp.pipeline.NERCombinerAnnotator.<init>(NERCombinerAnnotator.java:131)
	at edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(AnnotatorImplementations.java:68)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$44(StanfordCoreNLP.java:546)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$69(StanfordCoreNLP.java:625)
	at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)
	at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
	at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:495)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:201)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:194)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:181)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.main(StanfordCoreNLPServer.java:1497)
Caused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: MetaClass couldn't create public edu.stanford.nlp.time.TimeExpressionExtractorImpl(java.lang.String,java.util.Properties) with args [sutime, {}]
	at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:237)
	at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:382)
	at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:36)
	... 16 more
Caused by: java.lang.reflect.InvocationTargetException
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
	at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:233)
	... 18 more
Caused by: java.lang.NoClassDefFoundError: javax/xml/bind/JAXBException
	at de.jollyday.util.CalendarUtil.<init>(CalendarUtil.java:42)
	at de.jollyday.HolidayManager.<init>(HolidayManager.java:66)
	at de.jollyday.impl.DefaultHolidayManager.<init>(DefaultHolidayManager.java:46)
	at edu.stanford.nlp.time.JollyDayHolidays$MyXMLManager.<init>(JollyDayHolidays.java:148)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.ReflectAccess.newInstance(ReflectAccess.java:166)
	at java.base/jdk.internal.reflect.ReflectionFactory.newInstance(ReflectionFactory.java:404)
	at java.base/java.lang.Class.newInstance(Class.java:591)
	at de.jollyday.caching.HolidayManagerValueHandler.instantiateManagerImpl(HolidayManagerValueHandler.java:60)
	at de.jollyday.caching.HolidayManagerValueHandler.createValue(HolidayManagerValueHandler.java:41)
	at de.jollyday.caching.HolidayManagerValueHandler.createValue(HolidayManagerValueHandler.java:13)
	at de.jollyday.util.Cache.get(Cache.java:51)
	at de.jollyday.HolidayManager.createManager(HolidayManager.java:168)
	at de.jollyday.HolidayManager.getInstance(HolidayManager.java:148)
	at edu.stanford.nlp.time.JollyDayHolidays.init(JollyDayHolidays.java:57)
	at edu.stanford.nlp.time.Options.<init>(Options.java:119)
	at edu.stanford.nlp.time.TimeExpressionExtractorImpl.init(TimeExpressionExtractorImpl.java:44)
	at edu.stanford.nlp.time.TimeExpressionExtractorImpl.<init>(TimeExpressionExtractorImpl.java:39)
	... 24 more
Caused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:602)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)
	... 45 more
[main] INFO CoreNLP - Starting server...
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000
"
695,https://github.com/nltk/nltk/issues/2481,2481,[],closed,2019-12-19 14:12:10+00:00,,0,treeprettyprinter imports removed function cgi.escape,"Python 3.8 removed the `cgi.escape` function. Since this function is still used in the module `treeprettyprinter`, it is now unusable.

```python
Python 3.8.0 (default, Oct 23 2019, 18:51:26) 
[GCC 9.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from nltk import treeprettyprinter
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python3.8/site-packages/nltk/treeprettyprinter.py"", line 24, in <module>
    from cgi import escape
ImportError: cannot import name 'escape' from 'cgi' (/usr/lib/python3.8/cgi.py)
```

The Python 3.7 documentation states:

> Deprecated since version 3.2: This function is unsafe because quote is false by default, and therefore deprecated. Use html.escape() instead."
696,https://github.com/nltk/nltk/issues/2483,2483,[],closed,2019-12-23 18:34:13+00:00,,1,POS tagger crashes when text starts with a white space.,"Dear NLTK team,

the POS parsing crashes when the text starts with one (or several) white space(s). The problem occurs in the normalize() call.

/usr/local/lib/python3.6/dist-packages/nltk/tag/perceptron.py in normalize(self, word)
    238         elif word.isdigit() and len(word) == 4:
    239             return '!YEAR'
--> 240         elif word[0].isdigit():
    241             return '!DIGITS'
    242         else:

IndexError: string index out of range

I guess that testing the length() of the word could solve the issue.
lstriping the input text is not a correct one, as for many reasons, the text has to be preserved untouched by ntlk.

Best regards

Jerome"
697,https://github.com/nltk/nltk/issues/2484,2484,[],closed,2020-01-04 06:38:42+00:00,,0,time.clock removal in Python 3.8,`time.clock` was removed in Python 3.8 in https://github.com/python/cpython/pull/13270 . This will cause error in Python 3.8 for https://github.com/nltk/nltk/search?q=time.clock&unscoped_q=time.clock
698,https://github.com/nltk/nltk/issues/2488,2488,[],closed,2020-01-12 07:10:11+00:00,,1,SyntaxWarning in Python 3.8 over literal comparison`,"Python 3.8 generates `SyntaxWarning` over literal comparison using `is`. The fix is simple and would be to use `!=`. 

```
nltk/parse/malt.py:204: SyntaxWarning: ""is not"" with a literal. Did you mean ""!=""?
  if ret is not 0:
```"
699,https://github.com/nltk/nltk/issues/2489,2489,[],closed,2020-01-15 20:14:43+00:00,,0,Wrong documentation in bleu_scroe for smoothing method 2,"The paper referenced in the docstring of the BLEU smoothing method 2 is wrong.

(https://github.com/nltk/nltk/blob/develop/nltk/translate/bleu_score.py#L544)
```
        """"""
        Smoothing method 2: Add 1 to both numerator and denominator from
        Chin-Yew Lin and Franz Josef Och (2004) Automatic evaluation of
        machine translation quality using longest common subsequence and
        skip-bigram statistics. In ACL04.
        """"""
```

It should be Chin-Yew Lin and Franz Josef Och (2004) ORANGE: a method for evaluating automatic evaluation metrics for machine translation. In COLING04.
"
700,https://github.com/nltk/nltk/issues/2490,2490,[],closed,2020-01-23 01:28:11+00:00,,2,Release new version,"I'm looking forward to upgrade my Python to 3.8, and I was expecting to see the bug fixes mentioned in #2359 .

However those are currently only available in master.

Any plan to upgrade the package available in PyPI?"
701,https://github.com/nltk/nltk/issues/2491,2491,[],open,2020-01-24 09:29:35+00:00,,3,Reading a CFG from string with escaped single and double quotes in terminals results in failure,"As mentioned in the topic title, reading a grammar string which contains productions with escaped single and/or double quotes in the terminals on the RHS results in different ValueErrors.

Example A:
The following code:
```
grammar = CFG.fromstring(""""""
            S -> A
            A -> 'manager\'s, discount'
            """""")
```
raises:
```
   raise ValueError('Expected a nonterminal, found: ' + string[pos:])
ValueError: Expected a nonterminal, found: , discount'
```

Example B:
The following code:
```
grammar = CFG.fromstring(""""""
            S -> A
            A -> ""\""manager\'s discount\""""
            """""")
```
raises:
```
    raise ValueError('Unterminated string')
ValueError: Unterminated string
```

The reason for this is as follows:
The regex pattern `_TERMINAL_RE = re.compile(r'( ""[^""]+"" | \'[^\']+\' ) \s*', re.VERBOSE)` used by the helper function `def _read_production` in https://www.nltk.org/_modules/nltk/grammar.html does not account for escaped single or double quote characters.

Proposed solution:
For the case when a terminal is enclosed with single quotes, modifying the regex to ` r'( ""[^""]+"" | \'([^\']|[\\\'])+\' ) \s*'` should enable it to account for escaped single quotes. Something similar can be done for the case with double quotes in the regex."
702,https://github.com/nltk/nltk/issues/2492,2492,"[{'id': 718738467, 'node_id': 'MDU6TGFiZWw3MTg3Mzg0Njc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/windows%20related', 'name': 'windows related', 'color': 'fce4ba', 'default': False, 'description': None}, {'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}, {'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2020-01-30 09:55:42+00:00,,4,Install NTLK did not work on clean Anaconda python 3.7,"To reproduce
On a Windows 10 machine uninstall old versions of Anaconda and Pycharm. Delete by hand everything python or anaconda I could find (including the registry).
Install Anaconda
pip install ntlk.
You get an error ""could not find version to satisfy the requirement"". It doesn't say version for what.
What fixes the problem is
pip install ntlk=3.3
Which means I can't use the newest version of NTLK.
This used to work before. I noticed there is a new version now."
703,https://github.com/nltk/nltk/issues/2495,2495,[],open,2020-02-04 17:32:17+00:00,,0,Redundant operations on ngram language models cause high performance impact,"### General Info
**Module:** `nltk.lm`
**Type:** Performance
**Impact:** High
### Issue
Most ngram language models perform redundant operations causing a high performance impact
### Details
#### Language models affected
`Lidstone`, `Laplace`, `KneserNeyInterpolated`, `WittenBellInterpolated`
#### Root cause
Re-calculating `len(self.vocab)` and `self.counts[len(context) + 1].N()` instead of calculating them only once
#### Performance impact
100-1000x speed difference
### Background
While I was developing a noisy channel model I used NLTK's language models for the noisy channel model's language model. I observed a big difference in performance between `MLE` and the other language models provided by NLTK. After examining the code more thoroughly I identified the root cause of that difference to be some re-calculations being done unecessary every time the language model would calculate the probability of a ngram. I made a minimal set of changes to resolve the performance issue and observed a 100-1000x times increase in performance after I run some [tests](https://github.com/palasso/nltk/blob/9a3fa65cdf3b7175e2cda1eb704b7d57de1c2003/nltk.ipynb).
### Update
I updated the [tests](https://github.com/palasso/nltk/blob/30617c100ff07a889491fdb4ab2725359f20d1a8/nltk.ipynb) to include a different implementation that modifies only `Vocabulary` (based on discussion in PR #2496 ). This new implementation has the benefit of changing smaller parts of the `lm` module with the drawback that it doesn't resolve performance issues for `WittenBellInterpolated`."
704,https://github.com/nltk/nltk/issues/2497,2497,[],open,2020-02-08 21:03:23+00:00,,0,TextTiling cutoff_policy (HC is not set properly) ,"The TextTiling code Line 297 - 300 shows:
297:        if self.cutoff_policy == LC:
298:            cutoff = avg - stdev / 2.0
299:        else:
300:            cutoff = avg - stdev / 2.0

Line 300 should be
300:            cutoff = avg + stdev / 2.0"
705,https://github.com/nltk/nltk/issues/2498,2498,"[{'id': 975531894, 'node_id': 'MDU6TGFiZWw5NzU1MzE4OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/metrics', 'name': 'metrics', 'color': 'f442c8', 'default': False, 'description': ''}, {'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2020-02-11 22:33:30+00:00,,2,nltk bleu_score return 0,"I am using nltk's bleu_score package to compute blue score. However, I always got zero blue score for any sentence. Even if I use the cases mentioned in this post, I still got zero bleu score.
https://stackoverflow.com/questions/40542523/nltk-corpus-level-bleu-vs-sentence-level-bleu-score 

I run scripts on a fresh installed anaconda. Python 3.6 reproduce the same error.

```
>>> import sys
>>> import nltk
>>> print(sys.version)
3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]
>>> print(nltk.__version__)
3.4.5
>>> hypothesis = ['This', 'is', 'cat']
>>> reference = ['This', 'is', 'a', 'cat']
>>> references = [reference]
>>> list_of_references = [references]
>>> list_of_hypotheses = [hypothesis]
>>> nltk.translate.bleu_score.corpus_bleu(list_of_references, list_of_hypotheses)

C:\Users\user\AppData\Local\Continuum\anaconda3\lib\site-packages\nltk\translate\bleu_score.py:523: UserWarning:
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
C:\Users\user\AppData\Local\Continuum\anaconda3\lib\site-packages\nltk\translate\bleu_score.py:523: UserWarning:
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
**8.987727354491445e-155**
```

"
706,https://github.com/nltk/nltk/issues/2499,2499,[],closed,2020-02-13 00:04:02+00:00,,0,Synset offset with 7 digits only,"Hi,
property _offset of Synset object returns a 7 digits integer, while WordNet doc says that offsets are 8 digits integers. 
<img width=""502"" alt=""Screenshot 2020-02-13 at 01 01 43"" src=""https://user-images.githubusercontent.com/18500333/74388738-777d0200-4dfc-11ea-846b-1bd2cb73b879.png"">
I need them to build offsets of the form wn:4502851n and retrieve BabelNet synsets IDs. 
I'm also getting an error from BabelNet API when trying to retrieve WordNet IDs with these 7 digits offsets.
<img width=""768"" alt=""Screenshot 2020-02-13 at 01 11 17"" src=""https://user-images.githubusercontent.com/18500333/74389206-c5ded080-4dfd-11ea-919c-d63a994c64fb.png"">
 Any suggestion? Is this a bug?"
707,https://github.com/nltk/nltk/issues/2500,2500,[],open,2020-02-14 11:58:01+00:00,,2,sent_tokenize error in Russian,"I've got an error using NLTK parser for some Rusian classic books. The text below isn't one sentence:

``` python
import nltk
text = ""Но ведь вот что при этом, добрейший Родион Романович, наблюдать следует: ведь общего-то случая-с, того самого, на который все юридические формы и правила примерены и с которого они рассчитаны и в книжки записаны, вовсе не существует-с по тому самому, что всякое дело, всякое, хоть, например, преступление, как только оно случится в действительности, тотчас же и обращается в совершенно частный случай-с; да иногда ведь в какой: так-таки ни на что прежнее не похожий-с. Прекомические иногда случаи случаются в этом роде-с. Да оставь я иного-то господина совсем одного: не бери я его и не беспокой, но чтоб знал он каждый час и каждую минуту, или по крайней мере подозревал, что я всё знаю, всю подноготную, и денно и нощно слежу за ним, неусыпно его сторожу, и будь он у меня сознательно под вечным подозрением и страхом, так ведь, ей-богу, закружится, право-с, сам придет да, пожалуй, еще и наделает чего-нибудь, что уже на дважды два походить будет, так сказать, математический вид будет иметь, — оно и приятно-с. Это и с мужиком сиволапым может произойти, а уж с нашим братом, современно умным человеком, да еще в известную сторону развитым, и подавно!""
print(nltk.sent_tokenize(text, language=""russian""))
```

This text should be separated into several sentences:

```
Но ведь вот что при этом, добрейший Родион Романович, наблюдать следует: ведь общего-то случая-с, того самого, на который все юридические формы и правила примерены и с которого они рассчитаны и в книжки записаны, вовсе не существует-с по тому самому, что всякое дело, всякое, хоть, например, преступление, как только оно случится в действительности, тотчас же и обращается в совершенно частный случай-с; да иногда ведь в какой: так-таки ни на что прежнее не похожий-с. 

Прекомические иногда случаи случаются в этом роде-с. 

Да оставь я иного-то господина совсем одного: не бери я его и не беспокой, но чтоб знал он каждый час и каждую минуту, или по крайней мере подозревал, что я всё знаю, всю подноготную, и денно и нощно слежу за ним, неусыпно его сторожу, и будь он у меня сознательно под вечным подозрением и страхом, так ведь, ей-богу, закружится, право-с, сам придет да, пожалуй, еще и наделает чего-нибудь, что уже на дважды два походить будет, так сказать, математический вид будет иметь, — оно и приятно-с. 

Это и с мужиком сиволапым может произойти, а уж с нашим братом, современно умным человеком, да еще в известную сторону развитым, и подавно!
```"
708,https://github.com/nltk/nltk/issues/2501,2501,[],closed,2020-02-20 15:28:40+00:00,,1,"Clarify author, copyright and license on stopword corpus and fix link","Looking at https://www.nltk.org/nltk_data/ I see that the Stopwords Corpus as no specified author, copyright or license and the download link is no longer working.

I am wondering whether author, copyright or license could be specified as it would be interesting to include such data in Wikidata. Wikidata is Creative Commons Zero, so if there is any ""non-zero"" CC license it will not work.

In case it is CC0, I am wondering whether the individual lists are frozen or version changes might happen."
709,https://github.com/nltk/nltk/issues/2505,2505,[],closed,2020-02-24 10:23:26+00:00,,0,Python 3.4 ,"There are some code for Python 3.4 compatibility but it should be no longer needed as Python 3.5 is now the minimal supported version:

* [x] `nltk/translate/bleu_score.py` imports `Fraction` from `compat.py` which has a note that this should not be needed in Python >= 3.5
* [x] `BufferedGzipFile` is no longer needed in >= 3.5 as a buffer was built-in to the standard library one.

"
710,https://github.com/nltk/nltk/issues/2507,2507,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 718746024, 'node_id': 'MDU6TGFiZWw3MTg3NDYwMjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stem/lemma', 'name': 'stem/lemma', 'color': '53bab8', 'default': False, 'description': None}]",closed,2020-02-25 06:01:07+00:00,,5,Porter stemmer returns a capital instead of lowercase ,"This output is unexpected. The `In` returns the capitalize `In` from PorterStemmer's output.

```python
>>> from nltk.stem import PorterStemmer
>>> porter = PorterStemmer()
>>> porter.stem('In')
'In'
```

More details on https://stackoverflow.com/q/60387288/610569"
711,https://github.com/nltk/nltk/issues/2509,2509,[],closed,2020-02-25 21:24:41+00:00,,0,Remove outdated names in init.py,"Cf #2508 
"
712,https://github.com/nltk/nltk/issues/2516,2516,[],open,2020-03-18 07:22:57+00:00,,0,https://nltk.org SSL_ERROR_BAD_CERT_DOMAIN,"https://nltk.org (https) is a redirect to https://www.nltk.org/ , however it shows a cert error.

The cert is only valid for ""shortener.secureserver.net, www.shortener.secureserver.net"""
713,https://github.com/nltk/nltk/issues/2517,2517,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2020-03-21 21:04:41+00:00,,0,"Currently, NLTK pos_tag only supports English and Russian (i.e. lang='eng' or lang='rus')","Can anyone know that how to address this issue? btw, I have update the nltk package, thanks in advance."
714,https://github.com/nltk/nltk/issues/2518,2518,[],closed,2020-03-23 21:15:19+00:00,,1,Difficulty installing NLTK,"Hello,

I keep getting this error when I attempt to intall NLTK:

![NLTK](https://user-images.githubusercontent.com/62572582/77363845-d3a34200-6d29-11ea-9afb-0625aa67b652.png)

What do I do to resolve this problem?

Thanks very much,
Sophia"
715,https://github.com/nltk/nltk/issues/2519,2519,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",closed,2020-03-24 10:45:58+00:00,,0,PunktSentenceTokenizer doesn't work with custom end sentence character,"Example:

```
import nltk
from nltk.tokenize.punkt import PunktLanguageVars

class ExtLangVars(PunktLanguageVars):
    sent_end_chars = ('.', '?', '!', '^')

punkt_tk = nltk.data.load('tokenizers/punkt/english.pickle')
punkt_tk._lang_vars = ExtLangVars()

txt1 = 'Subject: Some subject. Attachments: Some attachemnts'
txt2 = 'Subject: Some subject^ Attachments: Some attachemnts'

r = punkt_tk.tokenize(txt1)
print(r)

r = punkt_tk.tokenize(txt2)
print(r)

d = punkt_tk.debug_decisions(txt1)
for x in d:
    print(nltk.tokenize.punkt.format_debug_decision(x))

#d = punkt_tk.debug_decisions(txt2)
#for x in d:
#    print(nltk.tokenize.punkt.format_debug_decision(x))

```
Also if you uncomment last lines, it will fail with index out of bounds. I think it's because of hardcoded 

`        self.period_final = tok.endswith('.')`

in PunktToken. Tried many ways to work around this, but no luck. Any suggestions would be appreciated."
716,https://github.com/nltk/nltk/issues/2520,2520,[],closed,2020-03-25 10:54:36+00:00,,1,computer restart after nltk.download(),"I've installed nltk through anoconda. when I print:
'
>>> import nltk
>>> nltk.download()
'
in terminal, then my mac went blank and restart.
Anyone knows what's the matter?"
717,https://github.com/nltk/nltk/issues/2521,2521,"[{'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2020-03-26 13:21:16+00:00,,1,"import nltk eror, No module named '_sqlite3'","I'm in ubuntu 16.04

here what i do:
```
sudo apt-get install sqlite-devel
sudo apt-get install libsqlite3-dev
sudo pip3 install pysqlite
```

I try to import nltk library.

line (1): import nltk
```
Traceback (most recent call last):
  File ""chat.py"", line 1, in <module>
    import nltk
  File ""/usr/local/lib/python3.7/site-packages/nltk/__init__.py"", line 150, in <module>
    from nltk.translate import *
  File ""/usr/local/lib/python3.7/site-packages/nltk/translate/__init__.py"", line 23, in <module>
    from nltk.translate.meteor_score import meteor_score as meteor
  File ""/usr/local/lib/python3.7/site-packages/nltk/translate/meteor_score.py"", line 10, in <module>
    from nltk.stem.porter import PorterStemmer
  File ""/usr/local/lib/python3.7/site-packages/nltk/stem/__init__.py"", line 29, in <module>
    from nltk.stem.snowball import SnowballStemmer
  File ""/usr/local/lib/python3.7/site-packages/nltk/stem/snowball.py"", line 32, in <module>
    from nltk.corpus import stopwords
  File ""/usr/local/lib/python3.7/site-packages/nltk/corpus/__init__.py"", line 66, in <module>
    from nltk.corpus.reader import *
  File ""/usr/local/lib/python3.7/site-packages/nltk/corpus/reader/__init__.py"", line 105, in <module>
    from nltk.corpus.reader.panlex_lite import *
  File ""/usr/local/lib/python3.7/site-packages/nltk/corpus/reader/panlex_lite.py"", line 15, in <module>
    import sqlite3
  File ""/usr/local/lib/python3.7/sqlite3/__init__.py"", line 23, in <module>
    from sqlite3.dbapi2 import *
  File ""/usr/local/lib/python3.7/sqlite3/dbapi2.py"", line 27, in <module>
    from _sqlite3 import *
ModuleNotFoundError: No module named '_sqlite3'
```

canyou help me with this?

Thanks for your time !"
718,https://github.com/nltk/nltk/issues/2522,2522,[],open,2020-03-26 22:54:33+00:00,,0,Security issues with pickles,"Pickles security nightmare because they alow arbitrary code execution and because this code is not explicitly visible, to extract and analyse it tools are needed that currently don't exist. They are good places to plant hardly discovered backdoors. But this lib relies heavily on them. It downloads some pickled pretrained stuff and doesn't work without it.

We need to solve this issue. There are several issues here:
1. Pickles are used. They should be replaced. The replacements can be some custom code and either a feature-specific binary format, or general purpose binary format, such as CBOR.
2. I haven't found the recepies to build the pretrained models. I mean for each pretrained model should be
    * a python file that:
        * fetches the needed datasets
        * preprocesses the data and trains the model
        * evaluating its performance
        * intentionally written the way to be easily auditable
    * and a JSON config file storing
        * hyperparams
        * datasets locations
        * previously achieved performance
    So, if using project- or thirdpparty devs-provided pickles is inacceptible because I cannot trust them, I should be able to recreate own pickles from scratch. Anyway, even if we replace pickles by something else, we still need the way to improve the pretrained models, i.e. by retraining them on better datasets or using better hyperparams (I have a lib for hyperparams tuning, BTW). So, IMHO for every pretrained model there should be the code reproducing its creation."
719,https://github.com/nltk/nltk/issues/2523,2523,[],closed,2020-03-30 00:18:22+00:00,,1,Deprecation Warnings,"When importing sent_tokenize from nltk.tokenize:

path/to/python3.7/dist-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly
  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: """"
path/to/python3.7/dist-packages/nltk/lm/counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  from collections import Sequence, defaultdict
path/to/python3.7/dist-packages/nltk/lm/vocabulary.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  from collections import Counter, Iterable"
720,https://github.com/nltk/nltk/issues/2524,2524,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2020-04-02 08:48:54+00:00,,0, Increase NLTK grammar and parser with my own words and sentences.,"I am working with NLTK semantics that called  ""Discourse Semantics"" and I want  to increase NLTK grammar and parser with my own words and sentences.
There are grammar files I downloaded with NLTK but not enough for my  application. When opening the grammar files I notice that they contain  Grammar Rules and Lexical Rules like the github link below.
I want to write as these rules for my utterence.

https://github.com/nltk/nltk_teach/blob/master/examples/grammars/book_grammars/simple-sem.fcfg
Thanks"
721,https://github.com/nltk/nltk/issues/2525,2525,[],closed,2020-04-04 15:18:52+00:00,,4,install NLTK ,"Last login: Sat Apr  4 17:02:21 on ttys000
hanadys-mbp:~ hanadyahmed$ import nltk
-bash: import: command not found
hanadys-mbp:~ hanadyahmed$ python
Python 2.7.10 (default, Jul 14 2015, 19:46:27) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import nltk
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: No module named nltk
>>> 
"
722,https://github.com/nltk/nltk/issues/2526,2526,[],closed,2020-04-05 16:39:22+00:00,,1,AttributeError: 'StringTokenizer' object has no attribute '_string',"I was trying to tokenize a doc using nltk.tokenize.api.StringTokenizer. (I've managed to tokenize it with word_tokenizer, so no need for solutions)

Is this normal behaviour?
```python
text1
Out[5]: ""event information speakers alison shafer customer success manager, millward brown digital sarah friedman customer success manager, millward brown digital for retail brands, the holiday season is already kicking into gear. from various traffic patterns, to keyword searches and conversion rates, having a holistic understanding of your strengths, weaknesses, and how your competitors fare is crucial to optimizing and improving your digital strategy. with compete pro, you can do just that this holiday season. during this webinar we’ll use retail as an example to walk through the search market share feature and its benefits in monitoring keywords that spark consumer interest to you and your competitors. we'll also help you understand industry conversion rates, and how compete pro can help you monitor referral and incoming/outgoing traffic sources driving engagement on your site. in doing so, you can accurately benchmark and analyze your effectiveness this holiday season.""
tokenizer = nltk.tokenize.api.StringTokenizer()
tokenizer.tokenize(text1)

Traceback (most recent call last):
  File ""C:\Users\Cristina\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3296, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-7-838af32942bb>"", line 1, in <module>
    tokenizer.tokenize(text1)
  File ""C:\Users\Cristina\Anaconda3\lib\site-packages\nltk\tokenize\api.py"", line 74, in tokenize
    return s.split(self._string)
AttributeError: 'StringTokenizer' object has no attribute '_string'
```"
723,https://github.com/nltk/nltk/issues/2527,2527,[],open,2020-04-08 07:13:36+00:00,,6,Quote author names mixed up in wordnet definitions,"If I run the following code:

```python
from nltk.corpus import wordnet

for ss in wordnet.all_synsets():
    if ' - ' in ss.definition():
        print(ss, ss.definition())
```
I get a list of definitions like this:
```
Synset('abstemious.a.01') sparing in consumption of especially food and drink; - John Galsworthy
Synset('ascetic.s.02') practicing great self-denial; - William James
Synset('dead-on.s.01') accurate and to the point; ; - Peter S.Prescott
Synset('used_to.s.01') in the habit; ; ; - Henry David Thoreau
Synset('predaceous.s.02') living by or given to victimizing others for personal gain; ; - Peter S. Prescott; - W.E.Swinton
Synset('passive.a.01') lacking in energy or will; - George Meredith
Synset('resistless.s.02') offering no resistance; ; - Theodore Roosevelt
Synset('alcoholic.s.02') addicted to alcohol; - Carl Van Doren
Synset('reductive.s.01') characterized by or causing diminution or curtailment;  - R.H.Rovere
Synset('mounted.s.02') decorated with applied ornamentation; often used in combination; - F.V.W.Mason
Synset('coordinated.s.02') being dexterous in the use of more than one set of muscle movements; - Mary McCarthy
Synset('light-fingered.s.01') having nimble fingers literally or figuratively; especially for stealing or picking pockets; - Harry Hansen; - Time
Synset('bumbling.s.01') lacking physical movement skills, especially with the hands; ; ; ; - Mary H. Vorse
Synset('uninfluenced.s.01') not influenced or affected; - V.L.Parrington
```

I'm concerned that these authors (such as `- Theodore Roosevelt`) possibly shouldn't be in the definition? I think these are the authors of the last example in the `ss.examples()` list, that haven't been parsed as part of the example because they aren't within the double quotes. "
724,https://github.com/nltk/nltk/issues/2528,2528,[],open,2020-04-09 01:08:07+00:00,,1,TypeError: attrib() got an unexpected keyword argument 'convert',"I've been getting this error recently. I have used nltk before without any problem. Why is this happening? 
I am using jupyter (python3) notebook. However, if I import nltk on terminal, there is no error.
```
TypeError                                 Traceback (most recent call last)
<ipython-input-1-53ff8bda7639> in <module>()
      1 import numpy as np
      2 import pandas as pd
----> 3 from nltk.tokenize import word_tokenize

/home/parth/.local/lib/python2.7/site-packages/nltk/__init__.pyc in <module>()
    141 ###########################################################
    142 
--> 143 from nltk.chunk import *
    144 from nltk.classify import *
    145 from nltk.inference import *

/home/parth/.local/lib/python2.7/site-packages/nltk/chunk/__init__.py in <module>()
    155 from nltk.data import load
    156 
--> 157 from nltk.chunk.api import ChunkParserI
    158 from nltk.chunk.util import (
    159     ChunkScore,

/home/parth/.local/lib/python2.7/site-packages/nltk/chunk/api.py in <module>()
     11 ##//////////////////////////////////////////////////////
     12 
---> 13 from nltk.parse import ParserI
     14 
     15 from nltk.chunk.util import ChunkScore

/home/parth/.local/lib/python2.7/site-packages/nltk/parse/__init__.py in <module>()
     98 from nltk.parse.malt import MaltParser
     99 from nltk.parse.evaluate import DependencyEvaluator
--> 100 from nltk.parse.transitionparser import TransitionParser
    101 from nltk.parse.bllip import BllipParser
    102 from nltk.parse.corenlp import CoreNLPParser, CoreNLPDependencyParser

/home/parth/.local/lib/python2.7/site-packages/nltk/parse/transitionparser.py in <module>()
     20     from numpy import array
     21     from scipy import sparse
---> 22     from sklearn.datasets import load_svmlight_file
     23     from sklearn import svm
     24 except ImportError:

/home/parth/.local/lib/python2.7/site-packages/sklearn/datasets/__init__.py in <module>()
     21 from .lfw import fetch_lfw_pairs
     22 from .lfw import fetch_lfw_people
---> 23 from .twenty_newsgroups import fetch_20newsgroups
     24 from .twenty_newsgroups import fetch_20newsgroups_vectorized
     25 from .mldata import fetch_mldata, mldata_filename

/home/parth/.local/lib/python2.7/site-packages/sklearn/datasets/twenty_newsgroups.py in <module>()
     42 from .base import _fetch_remote
     43 from .base import RemoteFileMetadata
---> 44 from ..feature_extraction.text import CountVectorizer
     45 from ..preprocessing import normalize
     46 from ..utils import deprecated

/home/parth/.local/lib/python2.7/site-packages/sklearn/feature_extraction/__init__.py in <module>()
      8 from .hashing import FeatureHasher
      9 from .image import img_to_graph, grid_to_graph
---> 10 from . import text
     11 
     12 __all__ = ['DictVectorizer', 'image', 'img_to_graph', 'grid_to_graph', 'text',

/home/parth/.local/lib/python2.7/site-packages/sklearn/feature_extraction/text.py in <module>()
     28 from ..externals import six
     29 from ..externals.six.moves import xrange
---> 30 from ..preprocessing import normalize
     31 from .hashing import FeatureHasher
     32 from .stop_words import ENGLISH_STOP_WORDS

/home/parth/.local/lib/python2.7/site-packages/sklearn/preprocessing/__init__.py in <module>()
      4 """"""
      5 
----> 6 from ._function_transformer import FunctionTransformer
      7 
      8 from .data import Binarizer

/home/parth/.local/lib/python2.7/site-packages/sklearn/preprocessing/_function_transformer.py in <module>()
      3 from ..base import BaseEstimator, TransformerMixin
      4 from ..utils import check_array
----> 5 from ..utils.testing import assert_allclose_dense_sparse
      6 from ..externals.six import string_types
      7 

/home/parth/.local/lib/python2.7/site-packages/sklearn/utils/testing.py in <module>()
    749 
    750 try:
--> 751     import pytest
    752 
    753     skip_if_32bit = pytest.mark.skipif(_IS_32BIT,

/usr/lib/python2.7/dist-packages/pytest.py in <module>()
     11     hookspec, hookimpl
     12 )
---> 13 from _pytest.fixtures import fixture, yield_fixture
     14 from _pytest.assertion import register_assert_rewrite
     15 from _pytest.freeze_support import freeze_includes

/usr/lib/python2.7/dist-packages/_pytest/fixtures.py in <module>()
    840 
    841 @attr.s(frozen=True)
--> 842 class FixtureFunctionMarker(object):
    843     scope = attr.ib()
    844     params = attr.ib(convert=attr.converters.optional(tuple))

/usr/lib/python2.7/dist-packages/_pytest/fixtures.py in FixtureFunctionMarker()
    842 class FixtureFunctionMarker(object):
    843     scope = attr.ib()
--> 844     params = attr.ib(convert=attr.converters.optional(tuple))
    845     autouse = attr.ib(default=False)
    846     ids = attr.ib(default=None, convert=_ensure_immutable_ids)

TypeError: attrib() got an unexpected keyword argument 'convert'
```"
725,https://github.com/nltk/nltk/issues/2529,2529,[],closed,2020-04-09 01:23:03+00:00,,5,corpus ribes scorer throws division by zero error,"Division by zero error is constantly thrown for ````corpus_ribes()```` function. 

To replicate: 
Take the example from the docs (https://www.nltk.org/api/nltk.translate.html) and shorten some
of the input sentences. In this case I only shortened hyp2 and hyp2a. 
````
from nltk.translate.ribes_score import corpus_ribes

hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures', 'that', 'the', 'military', 'always', 'obeys', 'the', 'commands', 'of', 'the', 'party']
ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',  'ensures', 'that', 'the', 'military', 'will', 'forever',  'heed', 'Party', 'commands']
ref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which', 'guarantees', 'the', 'military', 'forces', 'always', 'being', 'under', 'the', 'command', 'of', 'the', 'Party']
ref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',  'army', 'always', 'to', 'heed', 'the', 'directions',  'of', 'the', 'party']

hyp2 = ['he', 'read', 'the']
ref2a = ['he', 'was', 'interested', 'in', 'world', 'history', 'because', 'he']

list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]
hypotheses = [hyp1, hyp2]
corpus_ribes(list_of_references, hypotheses)
````
Output: 
````
--------------------------------------------------------------------------
ZeroDivisionError                         Traceback (most recent call last)
<ipython-input-175-b02599af69e7> in <module>
      1 list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]
      2 hypotheses = [hyp1, hyp2]
----> 3 corpus_ribes(list_of_references, hypotheses)

/anaconda3/envs/torch/lib/python3.7/site-packages/nltk/translate/ribes_score.py in corpus_ribes(list_of_references, hypotheses, alpha, beta)
    117     # Iterate through each hypothesis and their corresponding references.
    118     for references, hypothesis in zip(list_of_references, hypotheses):
--> 119         corpus_best_ribes += sentence_ribes(references, hypothesis, alpha, beta)
    120     return corpus_best_ribes / len(hypotheses)
    121 

/anaconda3/envs/torch/lib/python3.7/site-packages/nltk/translate/ribes_score.py in sentence_ribes(references, hypothesis, alpha, beta)
     53         # Collects the *worder* from the ranked correlation alignments.
     54         worder = word_rank_alignment(reference, hypothesis)
---> 55         nkt = kendall_tau(worder)
     56 
     57         # Calculates the brevity penalty

/anaconda3/envs/torch/lib/python3.7/site-packages/nltk/translate/ribes_score.py in kendall_tau(worder, normalize)
    288     num_possible_pairs = choose(worder_len, 2)
    289     # Kendall's Tau computation.
--> 290     tau = 2 * num_increasing_pairs / num_possible_pairs - 1
    291     if normalize:  # If normalized, the tau output falls between 0.0 to 1.0
    292         return (tau + 1) / 2

ZeroDivisionError: division by zero
````"
726,https://github.com/nltk/nltk/issues/2532,2532,[],closed,2020-04-14 06:48:13+00:00,,1,Lookup Error: Resource [93mstopwords[0m not found.   Please use the NLTK Downloader to obtain the resource,"I am using ntlk for the project and I have used mod_wsgi to configure with apache server. It worked on the localhost but while I tried with apache server to run the code. It shows LookUpError. It is something related to the path. I have tried `import nltk` and `nltk.download('stopwords')` in the shell and it shows downloading at path ""/home/ec2-user/nltk-data"". I also tried adding the path using `nltk.data.path.append('/home/ec2-user/nltk-data')`. I have also given permission to access the directory in wsgi configuration file. I had also used `import sys` and `sys.path.append('/home/ec2-user/nltk_data')`. But somehow the app is not searching nltk at the downloaded folder. I think appending the path would work but it didn't and still with this issue. I have also attached the output while accessing to 8000 port. Please do find the attached file and also suggest me for the possible solutions. Thank you.
![nltk](https://user-images.githubusercontent.com/43409127/79194327-18635a00-7e4c-11ea-99f5-d16cf88099ce.png)
 "
727,https://github.com/nltk/nltk/issues/2533,2533,[],closed,2020-04-14 23:39:08+00:00,,6,Syntax error when importing NLTK,"I have just installed NLTK via pip onto a new device it was working perfectly on my raspberry pi however when i try to import it i get this error
`Traceback (most recent call last):
  File ""<pyshell#0>"", line 1, in <module>
    import nltk
  File ""C:\Python27\lib\site-packages\nltk\__init__.py"", line 128, in <module>
    from nltk.collocations import *
  File ""C:\Python27\lib\site-packages\nltk\collocations.py"", line 35, in <module>
    from nltk.probability import FreqDist
  File ""C:\Python27\lib\site-packages\nltk\probability.py"", line 333
    print(""%*s"" % (width, samples[i]), end="" "")
                                          ^
SyntaxError: invalid syntax`
Any help would be greatly appreciated "
728,https://github.com/nltk/nltk/issues/2534,2534,[],closed,2020-04-22 05:40:39+00:00,,4,Import NLTK error in Python2.7,"I have Installed NLTK and when i try to import it is throwing the below error

import NLTK

File ""/jupyter/local/lib/python2.7/site-packages/nltk/probability.py"", line 333
    print(""%*s"" % (width, samples[i]), end="" "")
                                          ^
SyntaxError: invalid syntax

Collecting nltk
Requirement already satisfied: joblib in /jupyter/lib/python2.7/site-packages (from nltk) (0.14.1)
Requirement already satisfied: click in /jupyter/lib/python2.7/site-packages (from nltk) (7.1.1)
Requirement already satisfied: tqdm in /jupyter/lib/python2.7/site-packages (from nltk) (4.45.0)
Requirement already satisfied: regex in /jupyter/lib/python2.7/site-packages (from nltk) (2020.4.4)
Installing collected packages: nltk
Successfully installed nltk-3.5
You are using pip version 10.0.0b2, however version 20.1b1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
  File ""/jupyter/local/lib/python2.7/site-packages/nltk/probability.py"", line 333
    print(""%*s"" % (width, samples[i]), end="" "")
                                          ^
SyntaxError: invalid syntax

Please help."
729,https://github.com/nltk/nltk/issues/2535,2535,[],closed,2020-04-29 06:44:38+00:00,,2,Why does TreebankWordDetokenizer() work in console but not when called from a script?,"Recreating the error:
`python3`
`import nltk`
`from nltk.tokenize.treebank import TreebankWordDetokenizer`

I call a script called hpff_hpcanon_analysis.py and point_mutual_info.py that have these three imports at the top of the .py file and I get the following error:
<img width=""905"" alt=""tbdetok1"" src=""https://user-images.githubusercontent.com/31639520/80567452-d34a3680-89ba-11ea-8d47-e72b41d1fedd.png"">
<img width=""718"" alt=""tbdetok2"" src=""https://user-images.githubusercontent.com/31639520/80567650-3fc53580-89bb-11ea-911d-59072a831022.png"">

If I work in the terminal I can use the TreebankWordDetokenizer tool.  But if I call a script with this import at the top of the file it gives an import error `ImportError: cannot import name 'TreebankWordDetokenizer'`

I have used `pip install -U nltk` to update nltk and all requirements satisfied.  Any insight is much appreciated."
730,https://github.com/nltk/nltk/issues/2538,2538,[],open,2020-05-10 13:45:50+00:00,,8,Add wheel distribution(s) to PyPI,"Has nltk considered the feasibility of adding wheels to PyPI?

As of now it is one of ~10% of packages listed on https://pythonwheels.com/ that [does not provide wheels](https://pypi.org/project/nltk/#files).

It looks like nltk is pure-Python with no dependencies on shared libraries or the like.  That seems like it would make building the wheel itself pretty painless."
731,https://github.com/nltk/nltk/issues/2543,2543,[],open,2020-05-17 16:19:48+00:00,,5,NLTK Sentence tokenizer does not tokenize properly if there exists 'e.g.' or 'i.e.' in sentence.,"Like i have sentence:
'The first approach, single-molecule simulation, taken by the StochSim simulator, tracks individual molecules and their state (e.g., what other molecules they are bound to) so that only the complexes formed at any given time are enumerated (and not all possible complexes) [11].'

The sentence tokenizer splits this sentence and gives me following two sentences;
The first approach , single-molecule simulation , taken by the StochSim simulator , tracks individual molecules and their state ( e.g.

and

, what other molecules they are bound to ) so that only the complexes formed at any given time are enumerated ( and not all possible complexes ) [ 11 ] .

How can this be resolved?

"
732,https://github.com/nltk/nltk/issues/2544,2544,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2020-05-18 14:38:34+00:00,,1,grammar checker,How to check if a sentence is grammatically correct or not using NLTK
733,https://github.com/nltk/nltk/issues/2545,2545,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}]",open,2020-05-23 14:32:04+00:00,,2,Add a button to hide doctest arrows,"# THIS IS A FEATURE REQUEST

In the examples shown in the documentation the >>> arrows are annoying.
They and the output of the code have to manually removed after copy pasting the examples in order to run them.

Example of annoying arrows in documentation: [https://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu](https://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu)

# Solution:

[https://z4r.github.io/python/2011/12/02/hides-the-prompts-and-output/](https://z4r.github.io/python/2011/12/02/hides-the-prompts-and-output/)

The above link shows how to configure Sphinx to add a button that can hide the arrows
See the examples in numpy docs to see how it works: [https://numpy.org/doc/stable/reference/generated/numpy.mean.html](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)

The button hides the arrows as well as the output of each line to allow easy copy pasting.

![p1](https://user-images.githubusercontent.com/20921177/82733203-e5a45f80-9d2f-11ea-97e7-2caf576e57d5.png)
![p2](https://user-images.githubusercontent.com/20921177/82733204-e937e680-9d2f-11ea-814e-ee278c8a8a37.png)
"
734,https://github.com/nltk/nltk/issues/2548,2548,[],closed,2020-05-28 02:26:52+00:00,,2,nltk.lm Language Model: Perplexity performs unexpectedly,"The perplexity performs weirdly, being higher when evaluating with a large vocabulary.

Reproduce at [ https://ProxyHT@bitbucket.org/ProxyHT/language-model.git](https://ProxyHT@bitbucket.org/ProxyHT/language-model.git)

In _ngrams.py_, from line 15-21, I have configuration for training/validating/testing corpus. With the larger training corpus (data/train-80.txt), the perplexity evaluated on the test corpus is higher than the smaller training corpus (data/train-16.txt). The same thing happens with even larger files.
"
735,https://github.com/nltk/nltk/issues/2549,2549,[],open,2020-05-29 03:42:58+00:00,,0,CoreNLP server connection issue,"For some reason, I can't connect to the server. Can anyone help me out? Here's the code I'm running.

from nltk.parse.corenlp import CoreNLPServer
server = CoreNLPServer(""stanford-corenlp-4.0.0.jar"",""stanford-corenlp-4.0.0-models.jar"",verbose=True)
server.start()

from nltk.parse.corenlpnltk.pa import CoreNLPParser
parser = CoreNLPParser()
parse = next(parser.raw_parse(""I put the book in the box on the table.""))

server.stop()

I am running it in a file inside the folder that I downloaded for CoreNLP so that I don't have to worry about abs paths or anything.
Here's what it outputs:

[Found java: /usr/bin/java]
[Found java: /usr/bin/java]
Traceback (most recent call last):
File ""/Users/benstevens/Desktop/Bernstein2/Ben/stanford-corenlp-4.0.0/BenTest.py"", line 4, in
server.start()
File ""/Users/benstevens/Library/Python/3.8/lib/python/site-packages/nltk/parse/corenlp.py"", line 153, in start
raise CoreNLPServerError(""Could not connect to the server."")
nltk.parse.corenlp.CoreNLPServerError: Could not connect to the server.

I would greatly appreciate any help."
736,https://github.com/nltk/nltk/issues/2550,2550,[],closed,2020-06-02 03:21:10+00:00,,2,"WordNetLemmatizer replaces ""does"" with ""doe""","Hello,
I'm using `WordNetLemmatizer` to make the word lemmatization. My corpus contains a big number of occurrences of word ""does"". However, `WordNetLemmatizer` replaces all of them with ""doe"". I'm using it as follows:
```
from nltk.stem.wordnet import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemmatized_comments = [[lemmatizer.lemmatize(w) for w in c] for c in tqdm(prepared_comments)]
```
where `prepared_comments` is a list of tokenized word sequences. Am I using the `WordNetLemmatizer` wrong, or is it a bug of `WordNetLemmatizer`?"
737,https://github.com/nltk/nltk/issues/2551,2551,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}]",closed,2020-06-03 11:54:22+00:00,,1,'RegexpTagger' object has no attribute '_regexps',"Is the issue https://github.com/nltk/nltk/issues/287 back in the 3.5 version?

I test the same code in 3.4.5 and it works fine.

"
738,https://github.com/nltk/nltk/issues/2552,2552,[],closed,2020-06-08 08:36:46+00:00,,4,Contraction does not work for the word 'ill' and wrongly converting it into 'I will',"Input: He is to step down at the end of the week due to **ill** health
Output: He is to step down at the end of the week due to **I will** health

Have a look at below sample code:

#!pip install contractions
import contractions

text = He is to step down at the end of the week due to ill health
contractions.fix(text)
print(text)

> He is to step down at the end of the week due to I will health

"
739,https://github.com/nltk/nltk/issues/2554,2554,[],open,2020-06-12 19:45:30+00:00,,0,AttributeError: 'Tree' object has no attribute 'tree'. The class is 'nltk.tree.Tree',"I'm getting into the NLP world by reading and coding along with this https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72. Everything was working fine until I wanted to do Dependency Parsing. I want to leverage nltk and the StanfordDependencyParser to visualize and build out the dependency tree, the code is

    from nltk.parse.stanford import StanfordDependencyParser
    sdp = StanfordDependencyParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar', path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-        parser-3.5.2-models.jar')    

    result = list(sdp.raw_parse(sentence))  

    #print the dependency tree
    dep_tree = [parse.tree() for parse in result][0]
    print(dep_tree)

The variable ""sentence"" is defined earlier, and the paths are correct because I used them earlier. When I run this code in Jupyter notebook the output should be **""(beats (unveils US (supercomputer (world 's) (powerful most)))
China)""** but instead I get this error 

    AttributeError                            Traceback (most recent call last)
    <ipython-input-66-c3b0e2efe04f> in <module>
          6 
          7 # print the dependency tree
    ----> 8 dep_tree = [parse.tree() for parse in result][0]
          9 print(dep_tree)
    
    <ipython-input-66-c3b0e2efe04f> in <listcomp>(.0)
          6 
          7 # print the dependency tree
    ----> 8 dep_tree = [parse.tree() for parse in result][0]
          9 print(dep_tree)
    
    AttributeError: 'Tree' object has no attribute 'tree'
Any piece of information on this would be extremely helpful, thank you."
740,https://github.com/nltk/nltk/issues/2557,2557,[],closed,2020-06-18 03:02:25+00:00,,1,TweetTokenizer - OverflowError: signed integer is greater than maximum,"```
File ""/usr/local/lib/python3.7/dist-packages/nltk/tokenize/casual.py"", line 286, in tokenize
    text = _replace_html_entities(text)
  File ""/usr/local/lib/python3.7/dist-packages/nltk/tokenize/casual.py"", line 249, in _replace_html_entities
    return ENT_RE.sub(_convert_entity, _str_to_unicode(text, encoding))
  File ""/usr/local/lib/python3.7/dist-packages/nltk/tokenize/casual.py"", line 243, in _convert_entity
    return chr(number)
OverflowError: signed integer is greater than maximum
```

In the source code, it try to catch errors but it doesn't include the Overflow cases.
https://github.com/nltk/nltk/blob/88c437b0f84cffc7bb63545cd809ebc92239c477/nltk/tokenize/casual.py#L241-L245.

In python 3.7, `chr()` can raise `OverflowError` when value >= 0x80000000 (or < -0x80000000). Suggest to fix the line as 
```python
except ValueError, OverflowError:
    pass
```"
741,https://github.com/nltk/nltk/issues/2562,2562,"[{'id': 81645781, 'node_id': 'MDU6TGFiZWw4MTY0NTc4MQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/language-model', 'name': 'language-model', 'color': 'd4c5f9', 'default': False, 'description': ''}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 718785550, 'node_id': 'MDU6TGFiZWw3MTg3ODU1NTA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/pythonic', 'name': 'pythonic', 'color': '66e88d', 'default': False, 'description': None}]",closed,2020-06-20 19:56:18+00:00,,1,nltk.everygrams() does not work with non-reusable iterables and iterables without lengths.,"`nltk.util.everygrams()` ([code](https://github.com/nltk/nltk/blob/develop/nltk/util.py#L577-L600), [docs](https://nltk.readthedocs.io/en/latest/api/nltk.html?highlight=everygrams#nltk.util.everygrams)) claims to support both sequences and iterables. However, for iterables that are not reusable or iterables without a length function, everygrams either raises an exception or behaves incorrectly. In particular:
* When calling `everygrams(obj)` without its optional `max_len` argument, it attempts to take the length of its first argument, which may not support `len(obj)` resulting in a `TypeError`.
* When calling `everygrams(obj, max_len=2)` for example, if obj is an iterable that is not reusable it will be exhausted after only the ngrams of length `1` have been found and `everygrams()` ends up returning only the ngrams of length `1`.

I am happy to make a PR for this.

## Non-Working Examples

For example, starting with the example from the documentation
```python
>>> from nltk import everygrams
>>> sent = 'a b c'.split()
>>> list(everygrams(sent))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
>>> list(everygrams(sent, max_len=2))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c')]
```

We should expect that `everygrams(iter(sent))` should have the same output as `everygrams(sent)`. Instead, it raises a `TypeError`.
```python
>>> list(everygrams(sent))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
>>> list(everygrams(iter(sent)))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../python3.8/site-packages/nltk/util.py"", line 603, in everygrams
    max_len = len(sequence)
TypeError: object of type 'list_iterator' has no len()
```
Likewise, we should expect that `everygrams(iter(sent), max_len=2)` should have the same output as `everygrams(sent, max_len=2)`. This is not the case.
```python
>>> list(everygrams(sent, max_len=2))
[('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c')]
>>> list(everygrams(iter(sent), max_len=2))
[('a',), ('b',), ('c',)]
```


## Possible Solutions

There are several possible solutions:
1. Do not accept iterables.
    * Since nltk is commonly used to process large amounts of text data, iterables are the most natural and practical approach to avoid loading data into memory all at once.
2. If the argument is an iterable, cache it, computing its length in the process, in case it cannot be reused.
    * This would require loading the data into memory all at once, which should be avoided.
3. Use a `history` sliding window of size `max_len` to produce the ngrams. This strategy is similar to `history` in the implementation of ngram.
    * This would be my suggested approach since it provides the most functionality. However, it would have the drawback of changing the order of the output ngrams. For example, 
      ```python
      >>> list(old_everygrams(iter(sent)))
      [('a',), ('b',), ('c',), ('a', 'b'), ('b', 'c'), ('a', 'b', 'c')]
      >>> list(new_everygrams(iter(sent)))
      [('a',), ('a', 'b'), ('a', 'b', 'c'), ('b',), ('b', 'c'), ('c',)]
      ```

I am happy to make a PR for this."
742,https://github.com/nltk/nltk/issues/2565,2565,[],closed,2020-06-24 15:32:47+00:00,,1,WordNet: Loop in hyponyms hierarchy,"Hello,

navigating the hierarchy of hyponyms I end up in a loop:

```
>>> from nltk.corpus import wordnet as wn
>>> synset=wn.synset('restrain.v.01'); print(synset)
Synset('restrain.v.01')
>>> synset=synset.hyponyms()[3]; print(synset)
Synset('inhibit.v.04')
>>> synset=synset.hyponyms()[2]; print(synset)
Synset('restrain.v.01')
>>> synset=synset.hyponyms()[3]; print(synset)
Synset('inhibit.v.04')
>>> synset=synset.hyponyms()[2]; print(synset)
Synset('restrain.v.01')
....
```

Is this meant to be?


nltk Version: 3.4.5 on Python 3.7

Giovanni"
743,https://github.com/nltk/nltk/issues/2567,2567,[],open,2020-06-29 19:20:21+00:00,,2,WordNetLemmatizer not properly lemmatizing some words,"Some words are lemmatized improperly, due to picking the smallest possible lemma: 

```py
lemmatizer = WordNetLemmatizer()
lemmatizer.lemmatize('dose', 'n') # returns ""dose""
lemmatizer.lemmatize('doses', 'n') # returns ""dos""
wordnet._morphy('doses', 'n') # returns [""dose"", ""dos""]
wordnet.morphy('doses', 'n') # returns ""dose""
```
"
744,https://github.com/nltk/nltk/issues/2569,2569,[],closed,2020-07-08 13:26:18+00:00,,2,"Calling LancasterStemmer with language causes ""ValueError: The rule e is invalid""","As LancasterStemmer inherits from StemmerI, it is possible to call it with a language as so:

`ls = LancasterStemmer('english')`

This causes the following ValueError:

```
ne 218, in stem
    self.parseRules()
  File ""/home/user/anaconda3/envs/uniqueness/lib/python3.8/site-packages/nltk/stem/lancaster.py"", line 199, in parseRules
    raise ValueError(""The rule {0} is invalid"".format(rule))
ValueError: The rule e is invalid

```
This is because the rule tuple is passed in from the parent class and the rules do not fit the valid_rule regex on line 192.

`        valid_rule = re.compile(""^[a-z]+\*?\d[a-z]*[>\.]?$"")
`

A workaround is to call the stemmer without setting the language.

`ls = LancasterStemmer()`"
745,https://github.com/nltk/nltk/issues/2571,2571,[],closed,2020-07-17 21:12:21+00:00,,0,Method2 of smoothing function in nltk.translate.bleu_score needs to ignore unigram precision score,"According to [Lin & Och, 2004](http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf), second smoothing function should add 1 to both numerator & denominator of precision score for all n-grams where n >= 2. However, we can see from the code that it adds 1 to precision score corresponding to every n-gram including unigrams.

```python
def method2(self, p_n, *args, **kwargs):
        """"""
        Smoothing method 2: Add 1 to both numerator and denominator from
        Chin-Yew Lin and Franz Josef Och (2004) Automatic evaluation of
        machine translation quality using longest common subsequence and
        skip-bigram statistics. In ACL04.
        """"""
        return [
            Fraction(p_i.numerator + 1, p_i.denominator + 1, _normalize=False)
            for p_i in p_n
        ]
```"
746,https://github.com/nltk/nltk/issues/2573,2573,[],open,2020-07-20 15:08:26+00:00,,1,"Hack to keep NLTK's ""tokenize"" module fails ","in decorators.py file line 24:
`sys.path = [p for p in sys.path if  ""nltk"" not in p]`
causes an error while importing nltk when there are other objects than strings in the path variable(such as pathlib.Posixpath)
this issue can be solved simply just by replacing the above code with:
`sys.path = [p for p in sys.path if (type(p) == str and ""nltk"" not in p)]`
"
747,https://github.com/nltk/nltk/issues/2576,2576,[],closed,2020-07-30 02:39:07+00:00,,1,Connection timeout,"Got error when run `nltk.download('all')` from windows:
```
Error downloading 'framenet_v17' from
[nltk_data]    |     <https://raw.githubusercontent.com/nltk/nltk_data
[nltk_data]    |     /gh-pages/packages/corpora/framenet_v17.zip>:
[nltk_data]    |     <urlopen error [WinError 10060] A connection
[nltk_data]    |     attempt failed because the connected party did
[nltk_data]    |     not properly respond after a period of time, or
[nltk_data]    |     established connection failed because connected
[nltk_data]    |     host has failed to respond>`
```
Also got the similar error from Azure databricks as
```
<urlopen error [Errno 110] Connection timed out>
```"
748,https://github.com/nltk/nltk/issues/2577,2577,[],open,2020-07-30 13:17:51+00:00,,0,Averaged perceptron tagger cannot load data from paths with %-encoded characters in their names,"If you use a non-default data directory that happens to have something that looks like a URL-encoded character in its name, you can't use `PerceptronTagger`, because both in `__init__.py` (for Russian) and in `perceptron.py`, it does

    url = ""file:"" + str(find(NAME_OF_PICKLE))
    tagger.load(url)

(You can see this pattern in the `_get_tagger()` function on line 100 of `__init__.py`, as well as in the `__init__()` method of `PerceptronTagger` on line 167.)

The problem is that `find()` returns a path, not a URL fragment. For this code to be valid, it needs to url-encode the result of the `find()` call before prepending ""file"". As it stands, what will happen is that the `load()` call will eventually call `find()` again, which will url-decode the path even though it wasn't actually url-encoded. And then it will fail to find the file, because it won't be using the correct path name any more."
749,https://github.com/nltk/nltk/issues/2578,2578,[],open,2020-08-04 15:01:57+00:00,,1,TweetTokenizer add new emoticons characters,"Hello! There is any way to add new characters to the emoticons tokenization ?
For example:
from nltk.tokenize import TweetTokenizer
tknzr = TweetTokenizer()
s1 = ""This is a cooool :-*""
tknzr.tokenize(s1)

It gives as result:
['This', 'is', 'a', 'cooool', ':', '-', '*']

But I need to preserve these specific emotion "":-*"".
Thanks for advance!"
750,https://github.com/nltk/nltk/issues/2579,2579,[],closed,2020-08-04 21:29:39+00:00,,4,AttributeError: 'list' object has no attribute 'get',"The issue happens in the nltk/tag/sequential.py file when trying to tag a list of words:
Stacktrace:
File ""/home/mario/Desktop/Projects/AlbanianNLP/simple_test.py"", line 10, in main
    tagged_words = unitager.tag(words)
  File ""/usr/local/lib/python3.8/dist-packages/nltk/tag/sequential.py"", line 63, in tag
    tags.append(self.tag_one(tokens, i, tags))
  File ""/usr/local/lib/python3.8/dist-packages/nltk/tag/sequential.py"", line 83, in tag_one
    tag = tagger.choose_tag(tokens, index, history)
  File ""/usr/local/lib/python3.8/dist-packages/nltk/tag/sequential.py"", line 142, in choose_tag
    return self._context_to_tag.get(context)
AttributeError: 'list' object has no attribute 'get'
"
751,https://github.com/nltk/nltk/issues/2580,2580,[],closed,2020-08-11 07:34:20+00:00,,1,warning,"'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour
  warn(RuntimeWarning(msg))."
752,https://github.com/nltk/nltk/issues/2581,2581,[],closed,2020-08-12 19:25:13+00:00,,0,Bug introduced in nltk 3.5 leads to nondeterministic behavior in nltk.sentiment.vader,"Test script test.py
```python
#!/usr/bin/env python3

import json
from nltk.sentiment.vader import SentimentIntensityAnalyzer

line=""BUT fuck but thats it""

si = SentimentIntensityAnalyzer()
print(json.dumps(si.polarity_scores(line), sort_keys=True))
```

Run it multiple times and compare outputs
```shell
$ for ((i=0;i<10;i++)); do ./test.py; done | sort | uniq -c
      4 {""compound"": -0.3071, ""neg"": 0.36, ""neu"": 0.64, ""pos"": 0.0}
      6 {""compound"": -0.6956, ""neg"": 0.543, ""neu"": 0.457, ""pos"": 0.0}
```

The problem is this method
https://github.com/nltk/nltk/blob/b0cd83ded0c3c2394f878d8577d71187fa3f9ae4/nltk/sentiment/vader.py#L451-L460

The core of the probem is relying on order in sets which is random:
```shell
$ for ((i=0;i<10;i++)); do python -c ""print(set(['BUT', 'but']))""; done | sort | uniq -c
      7 {'but', 'BUT'}
      3 {'BUT', 'but'}
```

So in one case `_but_check` triggers one `BUT`, in another case it triggers another `but`. That's why test.py dumps different scores for the same string.

The original text to trigger the error was this reddit comment:
```
We've got some studs prospects, but thats all they are, prospects. BUT FUCK WE WON THE LOTTERY
```
"
753,https://github.com/nltk/nltk/issues/2582,2582,[],closed,2020-08-16 18:29:40+00:00,,0,demo_sent_subjectivity throws 'save_file' not defined error,"When trying to run `demo_sent_subjectivity` on text, nltk begins training a classifier and then goes to save the classifier but throws the error: 
`NameError: name 'save_file' is not defined`

I think it is coming from the line: https://github.com/nltk/nltk/blob/develop/nltk/sentiment/util.py#L688

I am using nltk version 3.5

Here is full code and stacktrace

```
import nltk
nltk.download('subjectivity')

from nltk.sentiment.util import demo_sent_subjectivity

demo_sent_subjectivity(""This is a test sentence"")

Cannot find the sentiment analyzer you want to load.
Training a new one using NaiveBayesClassifier.
Training classifier
Most Informative Features
            contains(--) = True             subj : obj    =     61.0 : 1.0
        contains(film's) = True             subj : obj    =     33.7 : 1.0
       contains(decides) = True              obj : subj   =     28.3 : 1.0
      contains(discover) = True              obj : subj   =     26.3 : 1.0
    contains(girlfriend) = True              obj : subj   =     25.0 : 1.0
          contains(town) = True              obj : subj   =     23.4 : 1.0
  contains(entertaining) = True             subj : obj    =     22.2 : 1.0
     contains(detective) = True              obj : subj   =     21.0 : 1.0
        contains(mother) = True              obj : subj   =     20.1 : 1.0
         contains(finds) = True              obj : subj   =     19.9 : 1.0
Evaluating NaiveBayesClassifier results...
Traceback (most recent call last):
  File ""/Users/<my_username>/anaconda3/envs/outpost/lib/python3.7/site-packages/nltk/sentiment/util.py"", line 717, in demo_sent_subjectivity
    sentim_analyzer = load(""sa_subjectivity.pickle"")
  File ""/Users/<my_username>/anaconda3/envs/outpost/lib/python3.7/site-packages/nltk/data.py"", line 752, in load
    opened_resource = _open(resource_url)
  File ""/Users/<my_username>/anaconda3/envs/outpost/lib/python3.7/site-packages/nltk/data.py"", line 877, in _open
    return find(path_, path + [""""]).open()
  File ""/Users/<my_username>/anaconda3/envs/outpost/lib/python3.7/site-packages/nltk/data.py"", line 562, in find
    resource_zipname = resource_name.split(""/"")[1]
IndexError: list index out of range

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/<my_username>/anaconda3/envs/outpost/lib/python3.7/site-packages/nltk/sentiment/util.py"", line 721, in demo_sent_subjectivity
    sentim_analyzer = demo_subjectivity(NaiveBayesClassifier.train, True)
  File ""/Users/<my_username>/anaconda3/envs/outpost/lib/python3.7/site-packages/nltk/sentiment/util.py"", line 688, in demo_subjectivity
    save_file(sentim_analyzer, ""sa_subjectivity.pickle"")
NameError: name 'save_file' is not defined

```
"
754,https://github.com/nltk/nltk/issues/2584,2584,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2020-08-19 14:35:14+00:00,,3,Cannot import all translation modules,"Some submodules under translate are implemented and described in the documentation, but cannot be imported and used. 

import nltk
nltk.translate.gleu_score.corpus_gleu

fails with this error:
AttributeError: module 'nltk.translate' has no attribute 'gleu_score'"
755,https://github.com/nltk/nltk/issues/2586,2586,[],closed,2020-08-20 20:04:05+00:00,,5,"PunktTokenizer: Inconsistency in two snippets, different languages","I am trying to work with the following example snippet [I found on StackOverflow](https://stackoverflow.com/questions/29746635/nltk-sentence-tokenizer-custom-sentence-starters)

**Example 1: Works**
```py
from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktLanguageVars

class BulletPointLangVars(PunktLanguageVars):
    sent_end_chars = ('.', '?', '!', '•')

tokenizer = PunktSentenceTokenizer(lang_vars = BulletPointLangVars())
sentences = tokenizer.tokenize(u""• I am a sentence • I am another sentence"")
for sentence in sentences:
    print(sentence)
```

The above works, and provides the expected output. 
Edit: The above fails if I remove the space preceding •, after some more debugging.

Now, I'm trying to work with the same with minor modifications using a unicode full-stop corresponding to a different language. 


**Example 2: Fails**
```py

from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktLanguageVars

class BulletPointLangVars(PunktLanguageVars):
    sent_end_chars = ('.', '?', '!', '\u0964')

tokenizer = PunktSentenceTokenizer(lang_vars = BulletPointLangVars())
sentences = tokenizer.tokenize(u""উপরাষ্ট্রপতি শ্রী এম ভেঙ্কাইয়া নাইডু সোমবার আই আই টি দিল্লির হীরক জয়ন্তী উদযাপনের উদ্বোধন করেছেন। অনলাইনের মাধ্যমে এই অনুষ্ঠানে কেন্দ্রীয় মানব সম্পদ উন্নয়নমন্ত্রী শ্রী রমেশ পোখরিয়াল ‘নিশাঙ্ক’  উপস্থিত ছিলেন। এই উপলক্ষ্যে উপরাষ্ট্রপতি হীরকজয়ন্তীর লোগো এবং ২০৩০-এর জন্য প্রতিষ্ঠানের লক্ষ্য ও পরিকল্পনার নথি প্রকাশ করেছেন।   অনুষ্ঠানে বক্তব্য রাখতে গিয়ে শ্রী নাইডু বলেছেন, জলবায়ু পরিবর্তন থেকে স্বাস্থ্য সমস্যা ౼ মানবজাতি আজ যে সমস্ত সমস্যার মুখোমুখি হচ্ছে, সেগুলিকে সমাধানের জন্য আইআইটি সহ অন্যান্য উচ্চ শিক্ষা প্রতিষ্ঠানের গবেষণার উপর গুরুত্ব দেওয়া উচিৎ। দেশের সমস্যাগুলির স্থিতিশীল সমাধানের ভারতীয় প্রতিষ্ঠানগুলি যখন এমন কিছু কাজ করবে, যাতে সমাজের উপর তার ইতিবাচক প্রভাব পরে, তাহলেই সেগুলি বিশ্বে সর্বশ্রেষ্ঠ প্রতিষ্ঠান হয়ে উঠতে পারবে।    সামাজিক বিভিন্ন সমস্যার সমাধান খুজতে বেসরকারি ক্ষেত্রকে,  শিক্ষাক্ষেত্রে গবেষণা ও উন্নয়নের জন্য খোলা মনে বিনিয়োগের তিনি আহ্বান জানিয়েছেন। আইআইটির ছাত্রছাত্রীদের  গ্রামীণ ভারত ও কৃষকদের নানা সমসয়ার সমাধান ছাড়াও কি করে পুষ্টিকর ও প্রোটিন সমৃদ্ধ শস্য উৎপ দন বৃদ্ধি করা যায়, শ্রী নাইডু  সেই বিষয়গুলি নিয়েও কাজ করার পরামর্শ দেন। উচ্চশিক্ষা প্রতিষ্ঠানগুলিকে এককভাবে নয়, শিল্প সংস্থাগুলির সঙ্গে জোট বেঁধে অতাধুনিক প্রযুক্তি উদ্ভাবন করতে হবে। এর ফলে দ্রুত ও ফলাফল ভিত্তিক নানা প্রকল্প বাস্তবায়নে সুবিধে হবে।   নতুন শিক্ষানীতির প্রসঙ্গে উপরাষ্ট্রপতি বলেছেন, ভারতকে আন্তর্জাতিক শিক্ষা প্রতিষ্ঠানের গন্তব্যে পরিণত করার জন্য এই নীতি সহায়ক হবে। উচ্চ শিক্ষা প্রতিষ্ঠানের মানোন্নয়নে সরকার, বিশ্ববিদ্যালয়, শিক্ষাবিদ ও বেসরকারী প্রতিষ্ঠানগুলিকে একযোগে কাজ করতে হবে।   আইআইটি দিল্লি শিল্পোদ্যোগ গড়ার কেন্দ্র হয়ে উঠছে বলে উপরাষ্ট্রপতি সন্তোষ প্রকাশ করেছেন। এই প্রসঙ্গে তিনি মানব সম্পদ উন্নয়ন মন্ত্রকের ‘উন্নত ভারত অভিযান’ কর্মসূচীতে দিল্লি আইআইটি অনুঘটকের ভূমিকা পালন করায় এই প্রতিষ্ঠানের প্রশংসা করেছেন।    শ্রী পোখরিয়াল, এই অনুষ্ঠানের উদ্বোধন করার জন্য উপরাষ্ট্রপতির প্রতি কৃতজ্ঞতা জানিয়েছেন। তিনি বলেছেন, আমাদের দেশের ছাত্রছাত্রীদের জন্য একটি আধুনিক ও উন্নত শিক্ষা ব্যবস্থা গড়ে তুলতে২০২০-র নতুন জাতীয় শিক্ষানীতি সহায়ক হবে। আইআইটি দিল্লির  গৌরবময় ৬০ বছরের উল্লেখ করে তিনি বলেছেন সারা দেশ যখন ভিড-১৯ মহামারীর বিরুদ্ধে লড়াই করছে, তখন আইআইটি দিল্লি নানাভাবে কারিগরি সহায়তা দিয়েছে, যা সময়োপযোগী ও মূল্যবান। বিগত ৫ বছরে এই শিক্ষা প্রতিষ্ঠানের শিক্ষক শিক্ষিকা ও ছাত্রছাত্রীরা ৫০০-র বেশী পেটেন্টের আবেদন করেছেন এবং তাঁদের ১০হাজারের বেশী গবেষণা পত্র বিভিন্ন আন্তর্জাতিক পত্র পত্রিকায় ছাপানো হয়েছে। ২০১৬ সালে সরকার এই প্রতিষ্ঠানকে যেখানে গবেষণার জন্য ১০০ কোটি টাকা দিয়েছিল, ২০১৯ সালে তা বেড়ে হয়েছে ৪০০ কোটি টাকা।    দিল্লি আইআইটির ডিরেক্টর অধ্যাপক ভি রামগোপাল রাও জানিয়েছেন, ২০৩০ সালের যে লক্ষ মাত্রা এই শিক্ষা প্রতিষ্ঠা  নিয়েছে, তার ফলে ছাত্রছাত্রী, প্রাক্তনী, শিক্ষক শিক্ষিকা ও কর্মীবর্গদের জীবনে ইতিবাচক প্রভাব পড়বে এবং আগামী দিনে দেশের প্রগতিতে তা সহায়ক হবে।   অনুষ্ঠানের দ্বিতীয় পর্বে অধ্যাপক দেবাঙ্খখর, অধ্যাপক এম বালাকৃষ্ণাণ বক্তব্য রাখেন। ‘আই আইটি দিল্লিঃ ৬০ বছরের উৎকর্ষতার স্মৃতিচারণা ও ভবিষ্যৎ পরিকল্পনা’ শীর্ষক এক আলোচনায় এই প্রতিষ্ঠানের প্রাক্তন ডিরেক্টর অধ্যাপক ভি এস রাজু, অধ্যাপক আর এস শিরোহী, অধ্যাপক সুরেন্দ্র প্রসাদ ও অধ্যাপক আর কে শেভগাওকর অংশ নেন।"")
for sentence in sentences:
    print(sentence)

```

`\u0964` corresponds to the Devanagiri full-stop (I have tried putting the normal one as well). I am not getting similar results as example 1. What could be going wrong here? "
756,https://github.com/nltk/nltk/issues/2588,2588,[],open,2020-08-29 02:12:44+00:00,,0,Possible additions to english stopwords,"This is a proposal to improve current list of english stop words. 

**Code used to access stop words:**

```
from nltk.corpus import stopwords
print(stopwords.words(""english""))
```
**Output:**
_['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', ""you're"", ""you've"", ""you'll"", ""you'd"", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', ""she's"", 'her', 'hers', 'herself', 'it', ""it's"", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', ""that'll"", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', ""don't"", 'should', ""should've"", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', ""aren't"", 'couldn', ""couldn't"", 'didn', ""didn't"", 'doesn', ""doesn't"", 'hadn', ""hadn't"", 'hasn', ""hasn't"", 'haven', ""haven't"", 'isn', ""isn't"", 'ma', 'mightn', ""mightn't"", 'mustn', ""mustn't"", 'needn', ""needn't"", 'shan', ""shan't"", 'shouldn', ""shouldn't"", 'wasn', ""wasn't"", 'weren', ""weren't"", 'won', ""won't"", 'wouldn', ""wouldn't""]_

**Proposal:**
While inspecting stop words, I have noticed that these variations of stop words could potentially be added to the corpus: _""cannot"", ""could"", ""done"", ""let"", ""may"" ""mayn"",  ""might"",  ""must"", ""need"", ""ought"", ""oughtn"", ""shall"",  ""would""_. "
757,https://github.com/nltk/nltk/issues/2590,2590,[],closed,2020-08-30 19:49:07+00:00,,1,Move from nose to pytest or nose2,"https://nose.readthedocs.io/en/latest/ -- nose is on life support. I personally prefer pytest, but nose2 may also be considered. Has this been discussed very much yet?"
758,https://github.com/nltk/nltk/issues/2591,2591,[],closed,2020-08-31 08:14:52+00:00,,0,Error in downloading ,"![WhatsApp Image 2020-08-31 at 12 46 26 PM](https://user-images.githubusercontent.com/65659281/91697813-5c52c380-eb8f-11ea-8e2b-1986ab534cfa.jpeg)
Trying to download stopwards from nltk , I used
import nltk
nltk.download()
I'm getting this error how to resolve this?"
759,https://github.com/nltk/nltk/issues/2592,2592,[],open,2020-08-31 09:08:46+00:00,,0,NLTK vader seems to not be updated with Vader's Github repo,"It seems like some Vader issues have been fixed by the founder, but have not been implemented in NLTK. Comparison of the two vader.py files clearly shows different codes, this one (https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vaderSentiment.py) being more complete. Why?"
760,https://github.com/nltk/nltk/issues/2593,2593,[],open,2020-09-03 08:42:40+00:00,,0,Missing ccg.pdf,"In help(nltk.ccg) it is written:

DESCRIPTION
    For more information see nltk/doc/contrib/ccg/ccg.pdf

However I could not find this pdf document anywhere in my computer (nor anywhere online). "
761,https://github.com/nltk/nltk/issues/2594,2594,[],closed,2020-09-05 11:40:19+00:00,,2,Potential Regex Denial of Service (ReDoS) in util,"**Type of Issue**
Potential Regex Denial of Service (ReDoS)

**Description**
The vulnerable regular expression is located in

https://github.com/nltk/nltk/blob/ca357e5cdcdb137f40c45346bb8bfea618dd863f/nltk/chunk/util.py#L517

The ReDOS vulnerability of the regex is mainly due to the sub-pattern `\s+[^>]*?` and the regex can be exploited with the following string
`""<b_b""+"" "" * 5000 + ""!""`

I think you can limit the input length or modify the regex."
762,https://github.com/nltk/nltk/issues/2596,2596,[],closed,2020-09-06 03:34:04+00:00,,0,Potential Regex Denial of Service (ReDoS) in xmldocs,"Type of Issue
Potential Regex Denial of Service (ReDoS)

Description
The vulnerable regular expression is located in

https://github.com/nltk/nltk/blob/ca357e5cdcdb137f40c45346bb8bfea618dd863f/nltk/corpus/reader/xmldocs.py#L225

The ReDOS vulnerability of the regex is mainly due to the sub-pattern `\s*/?\s*` and can be exploited with the following string
`""<""+"" "" * 5000`

I think you can limit the input length or modify the regex."
763,https://github.com/nltk/nltk/issues/2598,2598,"[{'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}, {'id': 975531894, 'node_id': 'MDU6TGFiZWw5NzU1MzE4OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/metrics', 'name': 'metrics', 'color': 'f442c8', 'default': False, 'description': ''}]",closed,2020-09-10 19:36:12+00:00,,3,raw_freq score does not sum to 1,"Noticed that when calculating bigram frequency using raw_freq. The result score does not sum to 1.
Checked the source code, finding that it is using the total number of words as TOTAL for the denominator instead of total number of bigrams.

Wondering if this is a bug or it has some rationale behind it?"
764,https://github.com/nltk/nltk/issues/2599,2599,[],closed,2020-09-11 11:55:49+00:00,,3,Unigram scores incorrectly computed for Kneser-Ney smoothing causing high perplexities ,"The unigram scores in KneserNey smoothing introduced in this commit https://github.com/nltk/nltk/commit/2759b03d85983fedd29b646380c713c3f4ccff66 are computed as a uniform distribution over the vocabulary, which is clearly not how any variant of KneserNey computes it. 

https://github.com/nltk/nltk/blob/5023d6b933ef1a5b1f25fba1d5ed11a8a43a47e4/nltk/lm/smoothing.py#L47"
765,https://github.com/nltk/nltk/issues/2601,2601,[],closed,2020-09-11 15:45:07+00:00,,1,"Link to ""Whoosh"" library in AUTHORS.md throws 404","Hello,

The link to `Whoosh` library on [line 280 of AUTHORS.md](https://github.com/nltk/nltk/blame/develop/AUTHORS.md#L280), throws a 404 exception.
This is the link that is used: https://bitbucket.org/mchaput/whoosh/wiki/Home

Perhaps either https://github.com/whoosh-community/whoosh or https://github.com/mchaput/whoosh works as a good replacement, assuming the reference to Whoosh should be kept at all."
766,https://github.com/nltk/nltk/issues/2602,2602,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2020-09-13 09:35:50+00:00,,1,"Exception in thread ""main"" java.lang.NullPointerException when training a MaltParser","I follow the guide in this url:https://cl.lingfil.uu.se/~nivre/research/parseme_lab.html

I want to try to train a parser soI ran a command: java -jar -Xmx2g D:\env\maltParser\maltparser-1.8.1\maltparser-1.8.1.jar  -c en-parser -m learn -i E:\data\UD_English-GUM-master\en_gum-ud-train.conllu.

but then cmd output like this following:
-----------------------------------------------------------------------------
                          MaltParser 1.8.1
-----------------------------------------------------------------------------
         MALT (Models and Algorithms for Language Technology) Group
             Vaxjo University and Uppsala University
                             Sweden
-----------------------------------------------------------------------------

Started: Sun Sep 13 17:19:25 CST 2020
  Transition system    : Arc-Eager
  Parser configuration : Nivre with allow_root=true, allow_reduce=false and enforce_tree=false
  Oracle               : Arc-Eager
  Data Format          : file:/E:/data/en-parser/conllx.xml
Exception in thread ""main"" java.lang.NullPointerException
        at org.maltparser.parser.algorithm.nivre.ArcEagerOracle.predict(ArcEagerOracle.java:30)
        at org.maltparser.parser.BatchTrainer.parse(BatchTrainer.java:47)
        at org.maltparser.parser.SingleMalt.oracleParse(SingleMalt.java:222)
        at org.maltparser.parser.SingleMaltChartItem.process(SingleMaltChartItem.java:128)
        at org.maltparser.core.flow.FlowChartInstance.process(FlowChartInstance.java:200)
        at org.maltparser.Engine.process(Engine.java:63)
        at org.maltparser.MaltConsoleEngine.maltParser(MaltConsoleEngine.java:99)
        at org.maltparser.MaltConsoleEngine.startEngine(MaltConsoleEngine.java:76)
        at org.maltparser.Malt.main(Malt.java:18)

It remainded me there is a NullPointerExceptionin thread.

But I ran a command: java -jar D:\env\maltParser\maltparser-1.8.1\maltparser-1.8.1.jar. It can produce correct output:
-----------------------------------------------------------------------------
                          MaltParser 1.8.1
-----------------------------------------------------------------------------
         MALT (Models and Algorithms for Language Technology) Group
             Vaxjo University and Uppsala University
                             Sweden
-----------------------------------------------------------------------------

Usage:
   java -jar maltparser-1.8.1.jar -f <path to option file> <options>
   java -jar maltparser-1.8.1.jar -h for more help and options

help                  (  -h) : Show options
-----------------------------------------------------------------------------
option_file           (  -f) : Path to option file
-----------------------------------------------------------------------------
verbosity            *(  -v) : Verbosity level
  debug      - Logging of debugging messages
  error      - Logging of error events
  fatal      - Logging of very severe error events
  info       - Logging of informational messages
  off        - Logging turned off
  warn       - Logging of harmful situations
-----------------------------------------------------------------------------

Documentation: docs/index.html


Can someone give me a direction to solve this problem?


"
767,https://github.com/nltk/nltk/issues/2607,2607,[],closed,2020-09-27 13:18:30+00:00,,2,Wrong result for word more'n in Word Tokenizer and PTB Tokenizer,"Let sentence: `It's more'n enough`

If I'm not mistaken, the PTB Tokenizer should result something like this:

`[""It"", ""'s"", ""more"", ""'n"", ""enough""]`

But, it's not. It returns:

`[""It"", ""'s"", ""more"", ""'"", ""n"", ""enough""]`

Since Word Tokenizer trying to implement PTB contraction, the result should be like that, right?

**PS**: Word Tokenizer contraction is `mor'n`, while original PTB contraction is `more'n`. Need clarification."
768,https://github.com/nltk/nltk/issues/2609,2609,[],open,2020-10-01 18:04:39+00:00,,3,Possible issue while parsing a specific sentence with EarleyChartParser and InsideChartParser,"While trying to parse a specific sentence from the Treebank corpus these two parsers behave strangely.
This is the sentence from treebank:
`['A', 'form', 'of', 'asbestos', 'once', 'used', '*', '*', 'to', 'make', 'Kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of', 'workers', 'exposed', '*', 'to', 'it', 'more', 'than', '30', 'years', 'ago', ',', 'researchers', 'reported', '0', '*T*-1', '.']`

The EarleyChartParser, while using a simple CFG grammar, will produce a very big forest of 24284 trees finding the correct one.
Instead InsideChartParser seems to go in an infinite loop while sorting a queue, one of its internal data structures.

For the parser of the PCFG I am using this function:

```python
def pchart(parser, sentence, gold_tree): #here the parser is a InsideChartParser
    test_trees = list(parser.parse(sentence))
    print(""PCHART PARSER - TREES FOUND: "", len(test_trees))
    best_prob = 0.0
    for idx, test_tree in enumerate(test_trees):
        print(""TREE: %d"" %idx)
        print(test_tree)
        curr_prob = test_tree.prob()
        if curr_prob > best_prob:
            best_prob = curr_prob
            best_tree = test_tree
        if test_tree.productions() == gold_tree.productions(): #check if the tree iterated now is the correct one
            print(""CORRECT TREE"")
        else:
            print(""WRONG TREE"")
    return best_tree
```

While for the Early parser I am using this one:

```python
def earley(parser, sentence, gold_tree): #here the parser is a EarleyChartParser
    test_trees = list(parser.parse(sentence)) #creates a forest of trees, with every trees being able to parse that sentence
    print(""EARLEY PARSER - TREES FOUND: "", len(test_trees))
    for idx, test_tree in enumerate(test_trees):
        print(""TREE: %d"" %idx)
        print(test_tree)
        if test_tree.productions() == gold_tree.productions():
            print(""CORRECT TREE"")
        else:
            print(""WRONG TREE"")`
```

After enumerating those lots of trees Early stops and the InsideChartParser starts but goes in a very long loop, I think due to the enormous amount of trees or productions at this point, and does not terminate any time soon:

```Traceback (most recent call last)
<ipython-input-9-641b936c1306> in <module>
     24 
     25 if __name__ == '__main__':
---> 26     main()

<ipython-input-9-641b936c1306> in main()
     18     gold_tree = treebank.parsed_sents()[3]
     19     earley(cfg_earley_parser, sentence, gold_tree) #run earley parser and compare it with gold tree and shows the various trees
---> 20     tree = pchart(pcfg_pchart_parser, sentence, gold_tree) #shows the possible trees and returns the one with maximum probability
     21     print(""BEST TREE WITH PROBABILITY: %.12e"" %tree.prob())
     22     tree.draw()

<ipython-input-8-5e94e1748d29> in pchart(parser, sentence, gold_tree)
      1 def pchart(parser, sentence, gold_tree):
----> 2     test_trees = list(parser.parse(sentence))
      3     print(""PCHART PARSER - TREES FOUND: "", len(test_trees))
      4 
      5     best_prob = 0.0

F:\Development\anaconda3\lib\site-packages\nltk\parse\pchart.py in parse(self, tokens)
    244         while len(queue) > 0:
    245             # Re-sort the queue.
--> 246             self.sort_queue(queue, chart)
    247 
    248             # Prune the queue to the correct size if a beam was defined

F:\Development\anaconda3\lib\site-packages\nltk\parse\pchart.py in sort_queue(self, queue, chart)
    359         :rtype: None
    360         """"""
--> 361         queue.sort(key=lambda edge: edge.prob())
    362 
    363 

F:\Development\anaconda3\lib\site-packages\nltk\parse\pchart.py in <lambda>(edge)
    359         :rtype: None
    360         """"""
--> 361         queue.sort(key=lambda edge: edge.prob())
    362 
    363
```

For a grammar so simple, is it a right behaviour?
Because this does not happen for other sentences I used from the treebank corpus.

Thank you for your time!"
769,https://github.com/nltk/nltk/issues/2611,2611,[],closed,2020-10-08 21:50:58+00:00,,1,download issue of nltk,"pip3 install nltk
Processing /Users/chanlu/Library/Caches/pip/wheels/ff/d5/7b/f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155/nltk-3.5-py3-none-any.whl
Requirement already satisfied: joblib in /usr/local/lib/python3.8/site-packages (from nltk) (0.17.0)
Collecting regex
  Using cached regex-2020.9.27.tar.gz (690 kB)
Requirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from nltk) (4.50.2)
Requirement already satisfied: click in /usr/local/lib/python3.8/site-packages (from nltk) (7.1.2)
Building wheels for collected packages: regex
  Building wheel for regex (setup.py) ... error
  ERROR: Command errored out with exit status 1:
   command: /usr/local/opt/python@3.8/bin/python3.8 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-install-kns0dngq/regex/setup.py'""'""'; __file__='""'""'/private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-install-kns0dngq/regex/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-wheel-r8jsu98t
       cwd: /private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-install-kns0dngq/regex/
  Complete output (17 lines):
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.macosx-10.15-x86_64-3.8
  creating build/lib.macosx-10.15-x86_64-3.8/regex
  copying regex_3/__init__.py -> build/lib.macosx-10.15-x86_64-3.8/regex
  copying regex_3/regex.py -> build/lib.macosx-10.15-x86_64-3.8/regex
  copying regex_3/_regex_core.py -> build/lib.macosx-10.15-x86_64-3.8/regex
  copying regex_3/test_regex.py -> build/lib.macosx-10.15-x86_64-3.8/regex
  running build_ext
  building 'regex._regex' extension
  creating build/temp.macosx-10.15-x86_64-3.8
  creating build/temp.macosx-10.15-x86_64-3.8/regex_3
  clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c regex_3/_regex.c -o build/temp.macosx-10.15-x86_64-3.8/regex_3/_regex.o
  xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun
  error: command 'clang' failed with exit status 1
  ----------------------------------------
  ERROR: Failed building wheel for regex
  Running setup.py clean for regex
Failed to build regex
Installing collected packages: regex, nltk
    Running setup.py install for regex ... error
    ERROR: Command errored out with exit status 1:
     command: /usr/local/opt/python@3.8/bin/python3.8 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-install-kns0dngq/regex/setup.py'""'""'; __file__='""'""'/private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-install-kns0dngq/regex/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-record-04g_xr2i/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.8/regex
         cwd: /private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-install-kns0dngq/regex/
    Complete output (17 lines):
    running install
    running build
    running build_py
    creating build
    creating build/lib.macosx-10.15-x86_64-3.8
    creating build/lib.macosx-10.15-x86_64-3.8/regex
    copying regex_3/__init__.py -> build/lib.macosx-10.15-x86_64-3.8/regex
    copying regex_3/regex.py -> build/lib.macosx-10.15-x86_64-3.8/regex
    copying regex_3/_regex_core.py -> build/lib.macosx-10.15-x86_64-3.8/regex
    copying regex_3/test_regex.py -> build/lib.macosx-10.15-x86_64-3.8/regex
    running build_ext
    building 'regex._regex' extension
    creating build/temp.macosx-10.15-x86_64-3.8
    creating build/temp.macosx-10.15-x86_64-3.8/regex_3
    clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c regex_3/_regex.c -o build/temp.macosx-10.15-x86_64-3.8/regex_3/_regex.o
    xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun
    error: command 'clang' failed with exit status 1
    ----------------------------------------
ERROR: Command errored out with exit status 1: /usr/local/opt/python@3.8/bin/python3.8 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-install-kns0dngq/regex/setup.py'""'""'; __file__='""'""'/private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-install-kns0dngq/regex/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/sl/w8b8rbg577z9c1j9mk6lf6tr0000gn/T/pip-record-04g_xr2i/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.8/regex Check the logs for full command output.

*****I don't know why I cannot download nltk. Thank you for your help***"
770,https://github.com/nltk/nltk/issues/2613,2613,[],closed,2020-10-12 17:15:36+00:00,,2,bug in ntlk.word_tokenize,"The word `cannot` is split into `can` and `not`, when it shouldn't be.

To reproduce -  
1. Install `nltk` using pip.
2. Download as little as possible for `word_tokenize` to work (I think `punkt` is enough).
3. Run the following using the python REPL (only the first line, without the `>`).
```
> import nltk.word_tokenize(""cannot"")
['can', 'not']
```"
771,https://github.com/nltk/nltk/issues/2614,2614,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2020-10-17 11:45:10+00:00,,1,Raising a non implemented error,"Hello,

I was implementing some code for text analysis. Created the following class (everything is well defined):

```
class NGramTagChunker(ChunkParserI):
  def __init__(self,train_sentences,tagger_classes=[UnigramTagger,BigramTagger]):
    train_sent_tags=conll_tag_chunks(train_sentences)
    self.chunk_tagger=combined_tagger(train_sent_tags,tagger_classes)
def parse(self,tagged_sentence):
    if not tagged_sentence:
      return None
    pos_tags=[tag for word, tag in tagged_sentence]
    chunk_pos_tags=self.chunk_tagger.tag(pos_tags)
    chunk_tags=[chunk_tag for (pos_tag,chunk_tag) in chunk_pos_tags]
    wpc_tags=[(word,pos_tag,chunk_tag) for ((word,pos_tag),chunk_tag) in zip(tagged_sentence,chunk_tags)]
    return conlltags2tree(wpc_tags)

#train chunker model
ntc=NGramTagChunker(train_data)
#evaluate chunker model performance
print(ntc.evaluate(test_data))
```

The problem is specifically in the last line of the code (ntc.evaluate). It raises a not implemented error as follow:
```
     32         :rtype: Tree
     33         """"""
---> 34         raise NotImplementedError()
     35 
     36     def evaluate(self, gold):

NotImplementedError: 
```"
772,https://github.com/nltk/nltk/issues/2616,2616,[],closed,2020-10-22 03:22:20+00:00,,3,`framenet.annotations()` raises generator-related RuntimeError,"To reproduce:

```python
Python 3.7.4 (default, Aug 13 2019, 20:35:49) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from nltk.corpus import framenet as fn
>>> list(fn.annotations())
Traceback (most recent call last):
  File ""/home/luke/.anaconda3/lib/python3.7/site-packages/nltk/collections.py"", line 602, in iterate_from
    v = next(self._it)
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/luke/.anaconda3/lib/python3.7/site-packages/nltk/collections.py"", line 587, in __len__
    for x in self.iterate_from(len(self._cache)):
  File ""/home/luke/.anaconda3/lib/python3.7/site-packages/nltk/collections.py"", line 602, in iterate_from
    v = next(self._it)
  File ""/home/luke/.anaconda3/lib/python3.7/site-packages/nltk/collections.py"", line 602, in iterate_from
    v = next(self._it)
  File ""/home/luke/.anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/framenet.py"", line 2362, in <genexpr>
    sent.frameAnnotation for sent in self.exemplars(luNamePattern)
RuntimeError: generator raised StopIteration
```

I unfortunately can't dig into the code to find out what's going on right now, but [this related stack overflow](https://stackoverflow.com/questions/51700960/runtimeerror-generator-raised-stopiteration-every-time-i-try-to-run-app) post suggests this is being caused by [PEP 479](https://www.python.org/dev/peps/pep-0479/), which changed the way the `StopIteration` exception works inside generators.

To any reading this facing the same issue: in the meantime, you can do this:

```python
>>> annotations = []
>>> try:
...     for annotation in fn.annotations():
...         annotations.append(annotation)
... except RuntimeError:
...     pass
... 
>>> len(annotations)
401502
```"
773,https://github.com/nltk/nltk/issues/2618,2618,[],closed,2020-10-26 23:09:48+00:00,,0,Meteor score does not use `preprocess` input argument,"Hello,

The `meteor_score` function defined [here](https://github.com/nltk/nltk/blob/385fb3f22031ebfa87b5c5571f32008f967264fa/nltk/translate/meteor_score.py#L421) takes the argument `preprocess` as an input, but does not pass it to the `single_meteor_score` function call. As a result, the function always uses the default `preprocess` function (`str.lower`). "
774,https://github.com/nltk/nltk/issues/2622,2622,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2020-11-09 12:56:01+00:00,,1,[wiki] SENNA binary link is outdated,"On this page: 
https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software 

The link to SENNA toolkit is no longer accessible. It was most likely taken from [here](https://www.nec-labs.com/research-departments/machine-learning/machine-learning-software/Senna) where it also leads nowhere.

The only other place I found this distribution author's website: https://ronan.collobert.com/senna/"
775,https://github.com/nltk/nltk/issues/2623,2623,"[{'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2020-11-10 21:29:34+00:00,,2,Potential bug in Porter stemmer,"Hi, I accidentally noticed that at line [504](https://github.com/nltk/nltk/blob/385fb3f22031ebfa87b5c5571f32008f967264fa/nltk/stem/porter.py#L504) of porter.py there is a potential bug:
```python
rules.append(
    (""logi"", ""log"", lambda stem: self._has_positive_measure(word[:-3]))
)
```
The `lambda` function's body refers to `word` and not to `stem`.
Is it correct or should it refer to `stem` instead?
"
776,https://github.com/nltk/nltk/issues/2624,2624,[],closed,2020-11-15 11:09:25+00:00,,0,nltk.corpus.wordnet.synset_from_pos_and_offset broken in NLTK 3.5?,"Not an NLTK expert here, but this seems wrong. On a fresh install of NLTK 3.5 (Python 3.8.5) the following

```
from nltk.corpus import wordnet as wn
wn.synset_from_pos_and_offset(wn.NOUN, 1)
```

gives

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/.local/lib/python3.8/site-packages/nltk/corpus/reader/wordnet.py in _synset_from_pos_and_line(self, pos, data_file_line)
   1395             # parse out the definitions and examples from the gloss
-> 1396             columns_str, gloss = data_file_line.strip().split(""|"")
   1397             definition = re.sub(r""[\""].*?[\""]"", """", gloss).strip()

ValueError: not enough values to unpack (expected 2, got 1)

During handling of the above exception, another exception occurred:

WordNetError                              Traceback (most recent call last)
<ipython-input-66-a20d7aaa0d8c> in <module>
----> 1 wn.synset_from_pos_and_offset(wn.NOUN, 1)

~/.local/lib/python3.8/site-packages/nltk/corpus/reader/wordnet.py in synset_from_pos_and_offset(self, pos, offset)
   1372         data_file.seek(offset)
   1373         data_file_line = data_file.readline()
-> 1374         synset = self._synset_from_pos_and_line(pos, data_file_line)
   1375         assert synset._offset == offset
   1376         self._synset_offset_cache[pos][offset] = synset

~/.local/lib/python3.8/site-packages/nltk/corpus/reader/wordnet.py in _synset_from_pos_and_line(self, pos, data_file_line)
   1478         # raise a more informative error with line text
   1479         except ValueError as e:
-> 1480             raise WordNetError(""line %r: %s"" % (data_file_line, e))
   1481 
   1482         # set sense keys for Lemma objects - note that this has to be

WordNetError: line ' 1 This software and database is being provided to you, the LICENSEE, by  \n': not enough values to unpack (expected 2, got 1)
```

When looking at the file that is accessed internally there seems to be some license info before the actual data:

```
wn.open(""data.noun"").read()[:5000].split(""\n"")
```

```
['  1 This software and database is being provided to you, the LICENSEE, by  ',
 '  2 Princeton University under the following license.  By obtaining, using  ',
 '  3 and/or copying this software and database, you agree that you have  ',
 '  4 read, understood, and will comply with these terms and conditions.:  ',
 '  5   ',
 '  6 Permission to use, copy, modify and distribute this software and  ',
 '  7 database and its documentation for any purpose and without fee or  ',
 '  8 royalty is hereby granted, provided that you agree to comply with  ',
 '  9 the following copyright notice and statements, including the disclaimer,  ',
 '  10 and that the same appear on ALL copies of the software, database and  ',
 '  11 documentation, including modifications that you make for internal  ',
 '  12 use or for distribution.  ',
 '  13   ',
 '  14 WordNet 3.0 Copyright 2006 by Princeton University.  All rights reserved.  ',
 '  15   ',
 '  16 THIS SOFTWARE AND DATABASE IS PROVIDED ""AS IS"" AND PRINCETON  ',
 '  17 UNIVERSITY MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR  ',
 '  18 IMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, PRINCETON  ',
 '  19 UNIVERSITY MAKES NO REPRESENTATIONS OR WARRANTIES OF MERCHANT-  ',
 '  20 ABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE USE  ',
 '  21 OF THE LICENSED SOFTWARE, DATABASE OR DOCUMENTATION WILL NOT  ',
 '  22 INFRINGE ANY THIRD PARTY PATENTS, COPYRIGHTS, TRADEMARKS OR  ',
 '  23 OTHER RIGHTS.  ',
 '  24   ',
 '  25 The name of Princeton University or Princeton may not be used in  ',
 '  26 advertising or publicity pertaining to distribution of the software  ',
 '  27 and/or database.  Title to copyright in this software, database and  ',
 '  28 any associated documentation shall at all times remain with  ',
 '  29 Princeton University and LICENSEE agrees to preserve same.  ',
 '00001740 03 n 01 entity 0 003 ~ 00001930 n 0000 ~ 00002137 n 0000 ~ 04424418 n 0000 | that which is perceived or known or inferred to have its own distinct existence (living or nonliving)  ',
 '00001930 03 n 01 physical_entity 0 007 @ 00001740 n 0000 ~ 00002452 n 0000 ~ 00002684 n 0000 ~ 00007347 n 0000 ~ 00020827 n 0000 ~ 00029677 n 0000 ~ 14580597 n 0000 | an entity that has physical existence  ',
 '00002137 03 n 02 abstraction 0 abstract_entity 0 010 @ 00001740 n 0000 + 00692329 v 0101 ~ 00023100 n 0000 ~ 00024264 n 0000 ~ 00031264 n 0000 ~ 00031921 n 0000 ~ 00033020 n 0000 ~ 00033615 n 0000 ~ 05810143 n 0000 ~ 07999699 n 0000 | a general concept formed by extracting common features from specific examples  ',
 '00002452 03 n 01 thing 0 009 @ 00001930 n 0000 ~ 04347225 n 0000 ~ 09225146 n 0000 ~ 09312645 n 0000 ~ 09367203 n 0000 ~ 09385911 n 0000 ~ 09407867 n 0000 ~ 09465459 n 0000 ~ 09468959 n 0000 | a separate and self-contained entity  ',
 '00002684 03 n 02 object 0 physical_object 0 039 @ 00001930 n 0000 + 00532607 v 0105 ~ 00003553 n 0000 ~ 00027167 n 0000 ~ 03009633 n 0000 ~ 03149951 n 0000 ~ 03233423 n 0000 ~ 03338648 n 0000 ~ 03532080 n 0000 ~ 03595179 n 0000 ~ 03610270 n 0000 ~ 03714721 n 0000 ~ 03892891 n 0000 ~ 04012260 n 0000 ~ 04248010 n 0000 ~ 04345288 n 0000 ~ 04486445 n 0000 ~ 07851054 n 0000 ~ 09238143 n 0000 ~ 09251689 n 0000 ~ 09267490 n 0000 ~ 09279458 n 0000 ~ 09281777 n 0000 ~ 09283193 n 0000 ~ 09287968 n 0000 ~ 09295338 n 0000 ~ 09300905 n 0000 ~ 09302031 n 0000 ~ 09308398 n 0000 ~ 09334396 n 0000 ~ 09335240 n 0000 ~ 09358550 n 0000 ~ 09368224 n 0000 ~ 09407346 n 0000 ~ 09409203 n 0000 ~ 09432990 n 0000 ~ 09468237 n 0000 ~ 09474162 n 0000 ~ 09477037 n 0000 | a tangible and visible entity; an entity that can cast a shadow; ""it was full of rackets, balls and other objects""  ',
 '00003553 03 n 02 whole 0 unit 0 015 @ 00002684 n 0000 + 01462005 v 0204 + 00367685 v 0201 + 01385458 v 0201 + 00368109 v 0201 + 00784215 a 0103 ~ 00003993 n 0000 ~ 00004258 n 0000 ~ 00019128 n 0000 ~ 00021939 n 0000 ~ 02749953 n 0000 ~ 03588414 n 0000 %p 03892891 n 0000 %p 04164989 n 0000 ~ 04353803 n 0000 | an assemblage of parts that is regarded as a single entity; ""how big is that part compared to the whole?""; ""the team is a unit""  ',
 '00003993 03 n 01 congener 0 001 @ 00003553 n 0000 | a whole (a thing or person) of the same kind or category as another; ""lard was also used, though its congener, butter, was more frequently employed""; ""the American shopkeeper differs from his European congener""  ',
 '00004258 03 n 02 living_thing 0 animate_thing 0 007 @ 00003553 n 0000 -c 01646941 a 0000 ~ 00004475 n 0000 ~ 00006269 n 0000 ~ 00006400 n 0000 ~ 00006484 n 0000 -c 05056234 n 0000 | a living (or once living) entity  ',
 '00004475 03 n 02 organism 0 being 0 065 @ 00004258 n 0000 + 02614181 v 0201 + 02986509 a 0102 + 01679459 a 0101 + 01093142 a 0101 -c 00270856 a 0000 -c 00327031 a 0000 -c 01665816 a 0000 ~ 00005787 n 0000 ~ 00005930 n 0000 ~ 00006024 n 0000 ~ 00006150 n 0000 %p 00006484 n 0000 ~ 00007846 n 0000 ~ 00015388 n 0000 ~ 00017222 n 0000 ~ 00019046 n 0000 ~ 01313888 n 0000 ~ 01314026 n 0000 ~ 01314145 n 0000 ~ 01315062 n 0000 ~ 01319932 n 0000 ~ 01320093 n 0000 ~ 01320314 n 0000 ~ 01320479 n 0000 ~ 01320692 n 0000 ~ 01324019 n ']
```"
777,https://github.com/nltk/nltk/issues/2627,2627,[],closed,2020-11-23 21:10:26+00:00,,1,Does feature-based CFG support generation like CFG does?,"with `nltk.CFG`, it is possible to generate sentences recognized by a CFG grammar:
http://www.nltk.org/howto/generate.html

But is it possible to do the similar generation for feature-based CFG (https://www.nltk.org/book/ch09.html#ref-load_parser1)?

And if not, is there a relatively-easy workaround to enable the generation?
What comes in my mind is to use CFG to generate sentences and then use more detailed F-CFG to keep only recognized sentences.

I don't have in mind more complex language to generate than the grammars offered here:
https://www.nltk.org/book/ch09.html#ref-load_parser1
Just with a larger lexicon.
 

 "
778,https://github.com/nltk/nltk/issues/2628,2628,[],open,2020-11-24 14:35:01+00:00,,0,Feature-based CFG overgenerates (apparently ignores features),"**MWE**

```
from nltk.parse.generate import generate
from nltk.grammar import FeatStructNonterminal, FeatureGrammar

grammar = '''
## NLTK-style feature-based CFG

% start NP

#####################
# Grammar Rules

NP[NUM=?n] -> DT[NUM=?n] N[NUM=?n]

# ###################
# Lexical Rules

DT[NUM=sg]  -> 'this'
DT[NUM=pl]  -> 'these'

N[NUM=sg] -> 'dog'
N[NUM=pl] -> 'dogs'
'''

gr = FeatureGrammar.fromstring(grammar) 

for s in generate(gr, n=200):
    print(s)
```

Outputs all 4 possibilities instead of only two grammatical NPs.
```
['this', 'dog']
['this', 'dogs'] # this should be blocked
['these', 'dog'] # this should be blocked
['these', 'dogs']
```

It seems the features are not taken into account during generation.
Though parsing does take features into account.

One workaround is to generate and then parse.

Maybe at least mention in http://www.nltk.org/howto or https://www.nltk.org/book/ch09.html that generation ignores features.

"
779,https://github.com/nltk/nltk/issues/2631,2631,[],closed,2020-11-29 17:41:53+00:00,,1,Can we delete CoreNLP test code? (It's currently skipped),"This originally was part of https://github.com/nltk/nltk/pull/2629, but it's probably best to wait and let folks chat about it.

- https://github.com/nltk/nltk/pull/1942 first marked it as skipped, and here's the relevant comment: `skipped all corenlp tests since we're dropping official support after stanford release their official python wrapper`
- Original implementation: https://github.com/nltk/nltk/pull/1249

cc @alvations @dimazest "
780,https://github.com/nltk/nltk/issues/2633,2633,[],closed,2020-11-30 13:06:57+00:00,,2,CrubadanCorpusReader doesn't close open file handle,"In the code below, for the function `_load_lang_ngrams`, a file is opened but is never closed.

https://github.com/nltk/nltk/blob/develop/nltk/corpus/reader/crubadan.py#L97

This raises a warning

```
ResourceWarning: unclosed file <_io.TextIOWrapper name='/Users/user/nltk_data/corpora/crubadan/ab-3grams.txt' mode='r' encoding='utf-8'>
  self._all_lang_freq[lang] = self._load_lang_ngrams(lang)
ResourceWarning: Enable tracemalloc to get the object allocation traceback
```"
781,https://github.com/nltk/nltk/issues/2634,2634,[],open,2020-12-02 11:57:09+00:00,,0,DRT is not correctly resolved with nltk.sem.drt.resolve_anaphore(),"I created a small example to visualize my findings. If I set my discourse referent with character `e` like here:

```
drs_a = dexpr(r'([a], [zzz(a)])')
drs_not_b = dexpr(r'([], [-([b],[yyy(b,a)])])')
drs_and = dexpr(r'([], [([c],[xxx(c), -([d],[www(d)])]) -> ([e], [e=Entity, ([f], [vvv(f)]) | ([g], [vvv(g), PRO(g)])])])')
drs = drs_a + drs_not_b + drs_and
print(drs.simplify().resolve_anaphora().pretty_format())
```

And simplifying and resolving my anaphoras, I get this following result:

```
 ____________________________________________________________ 
| a                                                          |
|------------------------------------------------------------|
| zzz(a)                                                     |
|      __________                                            |
|     | b        |                                           |
| __  |----------|                                           |
|   | | yyy(b,a) |                                           |
|     |__________|                                           |
|   ________________      ________________________________   |
|  | c              |    | e                              |  |
| (|----------------| -> |--------------------------------|) |
|  | xxx(c)         |    | (e = Entity)                   |  |
|  |      ________  |    |   ________     _____________   |  |
|  |     | d      | |    |  | f      |   | g           |  |  |
|  | __  |--------| |    | (|--------| | |-------------|) |  |
|  |   | | www(d) | |    |  | vvv(f) |   | vvv(g)      |  |  |
|  |     |________| |    |  |________|   | (g = [a,c]) |  |  |
|  |________________|    |               |_____________|  |  |
|                        |________________________________|  |
|____________________________________________________________|
```

Somehow, my `g` is missing my `e`. I wonder if `e` is a restricted character? I've checked [NLTK DRT How to](http://www.nltk.org/howto/drt.html) and couldn't find any information about it. Nevermind, after changing my discourse referent to `p`, like here:

```
drs_and = dexpr(r'([], [([c],[xxx(c), -([d],[www(d)])]) -> ([p], [p=Entity, ([f], [vvv(f)]) | ([g], [vvv(g), PRO(g)])])])')
```

My `g` has now my missing `p`:

```
 ______________________________________________________________ 
| a                                                            |
|--------------------------------------------------------------|
| zzz(a)                                                       |
|      __________                                              |
|     | b        |                                             |
| __  |----------|                                             |
|   | | yyy(b,a) |                                             |
|     |__________|                                             |
|   ________________      __________________________________   |
|  | c              |    | p                                |  |
| (|----------------| -> |----------------------------------|) |
|  | xxx(c)         |    | (p = Entity)                     |  |
|  |      ________  |    |   ________     _______________   |  |
|  |     | d      | |    |  | f      |   | g             |  |  |
|  | __  |--------| |    | (|--------| | |---------------|) |  |
|  |   | | www(d) | |    |  | vvv(f) |   | vvv(g)        |  |  |
|  |     |________| |    |  |________|   | (g = [a,c,p]) |  |  |
|  |________________|    |               |_______________|  |  |
|                        |__________________________________|  |
|______________________________________________________________|
```

Maybe I overlook something, why I can not use `e` as a discourse referent. Does someone have an idea why I get this behavior?

Best regards
Stefan"
782,https://github.com/nltk/nltk/issues/2636,2636,[],closed,2020-12-03 22:49:55+00:00,,0,FutureWarning: Possible nested set,"/usr/local/lib/python3.8/site-packages/nltk/tokenize/regexp.py:120: FutureWarning: Possible nested set at position 48
  self._regexp = re.compile(self._pattern, self._flags)"
783,https://github.com/nltk/nltk/issues/2637,2637,[],open,2020-12-08 18:09:16+00:00,,0,SentimentIntensityAnalyzer() from nltk.sentiment.vader does not respond to hashtags. ,"Example: 
```
SentimentIntensityAnalyzer.polarity_scores( 'Strings with hashtag #stupid #useless #BAD' )  
... compound: 0.0, 
... neg: 0.0, 
... neu: 1.0, 
... pos: 0.0,
```"
784,https://github.com/nltk/nltk/issues/2638,2638,[],closed,2020-12-14 11:48:46+00:00,,4,"WordNetLemmatizer does not lemmatize ""matroids""","```
from nltk.stem import WordNetLemmatizer
wnl=WordNetLemmatizer()
print(wnl.lemmatize('matroids'))
```

returns ""matroids"", but should return ""matroid""."
785,https://github.com/nltk/nltk/issues/2639,2639,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2020-12-14 18:55:48+00:00,,2,NLTK and pattern.en incorrectly classifying some proper nouns,"I am using nltk and pattern.en to analyze some text. In a test run, I used the text from Little Red Riding Hood. I noticed that pattern.en did recognize 'Little', 'Red', and 'Hood' as proper nouns (probably due to capitalization?), but 'Riding' was seen as a verb and not part of the proper noun 'Little Red Riding Hood'.

For example, the sentence in the story:

`text = ""Little Red Riding Hood, was so long until the large, large trough was quite full.""
`
was tagged as

`[('""', '""'), ('Little', 'NNP'), ('Red', 'NNP'), ('Riding', 'VBG'), ('Hood', 'NNP'), (',', ','), ('was', 'VBD'), ('so', 'RB'), ('long', 'RB'), ('until', 'IN'), ('the', 'DT'), ('large', 'JJ'), (',', ','), ('large', 'JJ'), ('trough', 'NN'), ('was', 'VBD'), ('quite', 'RB'), ('full', 'JJ'), ('.', '.')]
`

My code:

```
import nltk
from pattern.en import tag
tagged_text = tag(text)
```
Is this just a problem with the pattern.en, or am I assuming nltk should do something it doesn't? If the latter, how do I improve on the rendering of proper nouns so 'Riding' is tagged in this instance as a NNP and not a VBG so it is not converted to 'rid'?

A final question for the linguists in the crowd. Why is the lemma of 'riding' 'rid', and not 'ride'? 

Thanks!

Mark
"
786,https://github.com/nltk/nltk/issues/2642,2642,[],closed,2020-12-21 16:57:30+00:00,,6,adding amharic አማርኛ support,"Greetings,

Where do I go to contribute to the nltk corpus to add amharic support?

```
from nltk.corpus import stopwords
```

Thanks"
787,https://github.com/nltk/nltk/issues/2644,2644,[],open,2020-12-26 15:00:57+00:00,,1,How to generate bigram language model with Katz Backoff smoothing(nltk version 3.5.0),"Python: 3.6.8
nltk: 3.5.0

I am new to nltk and also a NLP newbie. Recently I am trying to generate a bigram language model from a corpus with **Katz Backoff smoothing**, with which I can calculate the text's probability in this corpus.
I noticed that there is some possible methods in NLTK 3.0.0 documentation (http://www.nltk.org/_modules/nltk/model/ngram.html#NgramModel), which is abandoned in version 3.5.0.

Since I want to generate the bigram language model with Katz Backoff smoothing with nltk’s latest version, can anyone give me some help or suggestions on how to do this?"
788,https://github.com/nltk/nltk/issues/2647,2647,"[{'id': 718743421, 'node_id': 'MDU6TGFiZWw3MTg3NDM0MjE=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/parsing', 'name': 'parsing', 'color': '94e06d', 'default': False, 'description': None}, {'id': 719346674, 'node_id': 'MDU6TGFiZWw3MTkzNDY2NzQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/plot', 'name': 'plot', 'color': 'eddf76', 'default': False, 'description': None}, {'id': 719374994, 'node_id': 'MDU6TGFiZWw3MTkzNzQ5OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/GUI', 'name': 'GUI', 'color': 'f9b3d9', 'default': False, 'description': None}, {'id': 739423158, 'node_id': 'MDU6TGFiZWw3Mzk0MjMxNTg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tkinter', 'name': 'tkinter', 'color': '03c4a7', 'default': False, 'description': None}]",closed,2020-12-28 21:14:16+00:00,,1,RegexpParser and tree.draw() crashes Python 3.8.6,"Per nltk's requirement faq, I installed a separate environment running Python 3.8.6. 
I am running Mac OS 11.1, nltk 3.5, and Jupyter Labs 3.0.0.
I have no issue tokenizing words or sentences. Neither when I tag those tokens. But when I want to parse those tags, Python crashes, and I get an error message that pops up. Might this be an issue with OS 11? Anyone have any ideas of what could be causing this? 
```

import pandas as pd, os, string, re
from nltk.corpus import stopwords
from nltk import RegexpParser, Tree
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer
from nltk import *

os.listdir('/Users/kylereaves/Desktop/python_scripts/job_requirements')
path = '/Users/kylereaves/Desktop/python_scripts/job_requirements'
os.chdir(path)

stop_words = set(stopwords.words('english'))
punct = string.punctuation

job_reqs = []
for file in os.listdir(path):
    with open(file, 'r') as f:
        job_reqs.append(f.read().strip())
        f.close()

# concactenate all strings into one string
job_reqs = ''.join(job_reqs)

# tokenize job_reqs
sents = sent_tokenize(job_reqs)
tokens = word_tokenize(job_reqs)
pos_tokens = pos_tag(tokens)

stopped = [word for word in tokens if word not in stop_words]
pos_tags = pos_tag(stopped)

grammar = r'''NP: {<DT>?<JJ>*<NN>}'''
chunker = RegexpParser(grammar)
```

trying to run:
`
chunker.parse(pos_tokens) `

crashes my Python kernel and brings up the following crash report

```

Here is the crash report: 
Process:               Python [1920]
Path:                  /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/Resources/Python.app/Contents/MacOS/Python
Identifier:            org.python.python
Version:               3.8.6 (3.8.6)
Code Type:             X86-64 (Native)
Parent Process:        Python [1738]
Responsible:           Terminal [419]
User ID:               501

Date/Time:             2020-12-28 16:13:20.027 -0500
OS Version:            macOS 11.1 (20C69)
Report Version:        12
Anonymous UUID:        8EEE2257-0986-3569-AA83-52641AF02282


Time Awake Since Boot: 1800 seconds

System Integrity Protection: enabled

Crashed Thread:        0  Dispatch queue: com.apple.main-thread

Exception Type:        EXC_CRASH (SIGABRT)
Exception Codes:       0x0000000000000000, 0x0000000000000000
Exception Note:        EXC_CORPSE_NOTIFY

Application Specific Information:
abort() called

Thread 0 Crashed:: Dispatch queue: com.apple.main-thread
0   libsystem_kernel.dylib        	0x00007fff20354462 __pthread_kill + 10
1   libsystem_pthread.dylib       	0x00007fff20382610 pthread_kill + 263
2   libsystem_c.dylib             	0x00007fff202d5720 abort + 120
3   Tcl                           	0x00007fff6fe53b55 Tcl_PanicVA + 398
4   Tcl                           	0x00007fff6fe53bd5 Tcl_Panic + 128
5   Tk                            	0x00007fff6ff53ad5 TkpInit + 385
6   Tk                            	0x00007fff6fed3788 0x7fff6fea2000 + 202632
7   _tkinter.cpython-38-darwin.so 	0x000000012559a709 Tcl_AppInit + 84
8   _tkinter.cpython-38-darwin.so 	0x00000001255959e2 _tkinter_create + 975
9   org.python.python             	0x000000010623b3a9 cfunction_vectorcall_FASTCALL + 169
10  org.python.python             	0x00000001062ad67d call_function + 346
11  org.python.python             	0x00000001062aa257 _PyEval_EvalFrameDefault + 29861
12  org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
13  org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
14  org.python.python             	0x000000010620c93c _PyObject_FastCallDict + 236
15  org.python.python             	0x000000010620dc28 _PyObject_Call_Prepend + 131
16  org.python.python             	0x00000001062503b9 slot_tp_init + 80
17  org.python.python             	0x000000010624d011 type_call + 172
18  org.python.python             	0x000000010620ca7d _PyObject_MakeTpCall + 274
19  org.python.python             	0x00000001062ad847 call_function + 804
20  org.python.python             	0x00000001062aa314 _PyEval_EvalFrameDefault + 30050
21  org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
22  org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
23  org.python.python             	0x000000010620c93c _PyObject_FastCallDict + 236
24  org.python.python             	0x000000010620dc28 _PyObject_Call_Prepend + 131
25  org.python.python             	0x00000001062503b9 slot_tp_init + 80
26  org.python.python             	0x000000010624d011 type_call + 172
27  org.python.python             	0x000000010620ca7d _PyObject_MakeTpCall + 274
28  org.python.python             	0x00000001062ad847 call_function + 804
29  org.python.python             	0x00000001062aa314 _PyEval_EvalFrameDefault + 30050
30  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
31  org.python.python             	0x000000010620f377 method_vectorcall + 135
32  org.python.python             	0x00000001062ad67d call_function + 346
33  org.python.python             	0x00000001062aa314 _PyEval_EvalFrameDefault + 30050
34  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
35  org.python.python             	0x000000010620cd09 PyVectorcall_Call + 108
36  org.python.python             	0x00000001062aa5d4 _PyEval_EvalFrameDefault + 30754
37  org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
38  org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
39  org.python.python             	0x00000001062ad67d call_function + 346
40  org.python.python             	0x00000001062aa314 _PyEval_EvalFrameDefault + 30050
41  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
42  org.python.python             	0x000000010620c93c _PyObject_FastCallDict + 236
43  org.python.python             	0x000000010620dc28 _PyObject_Call_Prepend + 131
44  org.python.python             	0x000000010624f9d0 slot_tp_call + 71
45  org.python.python             	0x000000010620ca7d _PyObject_MakeTpCall + 274
46  org.python.python             	0x00000001062ad847 call_function + 804
47  org.python.python             	0x00000001062aa314 _PyEval_EvalFrameDefault + 30050
48  org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
49  org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
50  org.python.python             	0x00000001062ad67d call_function + 346
51  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
52  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
53  org.python.python             	0x00000001062ad67d call_function + 346
54  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
55  org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
56  org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
57  org.python.python             	0x000000010620c93c _PyObject_FastCallDict + 236
58  org.python.python             	0x000000010620dc28 _PyObject_Call_Prepend + 131
59  org.python.python             	0x000000010624f9d0 slot_tp_call + 71
60  org.python.python             	0x000000010620ca7d _PyObject_MakeTpCall + 274
61  org.python.python             	0x000000010620e77e object_vacall + 388
62  org.python.python             	0x000000010620e965 PyObject_CallFunctionObjArgs + 136
63  org.python.python             	0x00000001062a5719 _PyEval_EvalFrameDefault + 10599
64  org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
65  org.python.python             	0x00000001062a2d0f PyEval_EvalCode + 51
66  org.python.python             	0x00000001062a05b8 builtin_exec + 581
67  org.python.python             	0x000000010623b3a9 cfunction_vectorcall_FASTCALL + 169
68  org.python.python             	0x00000001062ad67d call_function + 346
69  org.python.python             	0x00000001062aa314 _PyEval_EvalFrameDefault + 30050
70  org.python.python             	0x000000010621a516 gen_send_ex + 244
71  org.python.python             	0x00000001062a5b8f _PyEval_EvalFrameDefault + 11741
72  org.python.python             	0x000000010621a516 gen_send_ex + 244
73  org.python.python             	0x00000001062a5b8f _PyEval_EvalFrameDefault + 11741
74  org.python.python             	0x000000010621a516 gen_send_ex + 244
75  org.python.python             	0x0000000106214679 method_vectorcall_O + 244
76  org.python.python             	0x00000001062ad67d call_function + 346
77  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
78  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
79  org.python.python             	0x00000001062ad67d call_function + 346
80  org.python.python             	0x00000001062aa314 _PyEval_EvalFrameDefault + 30050
81  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
82  org.python.python             	0x00000001062ad67d call_function + 346
83  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
84  org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
85  org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
86  org.python.python             	0x000000010620f464 method_vectorcall + 372
87  org.python.python             	0x000000010620cd09 PyVectorcall_Call + 108
88  org.python.python             	0x00000001062aa5d4 _PyEval_EvalFrameDefault + 30754
89  org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
90  org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
91  org.python.python             	0x000000010620f377 method_vectorcall + 135
92  org.python.python             	0x00000001062ad67d call_function + 346
93  org.python.python             	0x00000001062aa3df _PyEval_EvalFrameDefault + 30253
94  org.python.python             	0x000000010621a516 gen_send_ex + 244
95  org.python.python             	0x00000001062a1120 builtin_next + 89
96  org.python.python             	0x000000010623b3a9 cfunction_vectorcall_FASTCALL + 169
97  org.python.python             	0x00000001062bb8de context_run + 92
98  org.python.python             	0x000000010623b46f cfunction_vectorcall_FASTCALL_KEYWORDS + 122
99  org.python.python             	0x00000001062ad67d call_function + 346
100 org.python.python             	0x00000001062aa314 _PyEval_EvalFrameDefault + 30050
101 org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
102 org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
103 org.python.python             	0x00000001062ad67d call_function + 346
104 org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
105 org.python.python             	0x000000010621a516 gen_send_ex + 244
106 org.python.python             	0x00000001062a1120 builtin_next + 89
107 org.python.python             	0x000000010623b3a9 cfunction_vectorcall_FASTCALL + 169
108 org.python.python             	0x00000001062bb8de context_run + 92
109 org.python.python             	0x000000010623b46f cfunction_vectorcall_FASTCALL_KEYWORDS + 122
110 org.python.python             	0x00000001062ad67d call_function + 346
111 org.python.python             	0x00000001062aa314 _PyEval_EvalFrameDefault + 30050
112 org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
113 org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
114 org.python.python             	0x000000010620f377 method_vectorcall + 135
115 org.python.python             	0x00000001062ad67d call_function + 346
116 org.python.python             	0x00000001062aa314 _PyEval_EvalFrameDefault + 30050
117 org.python.python             	0x000000010621a516 gen_send_ex + 244
118 org.python.python             	0x00000001062a1120 builtin_next + 89
119 org.python.python             	0x000000010623b3a9 cfunction_vectorcall_FASTCALL + 169
120 org.python.python             	0x00000001062bb8de context_run + 92
121 org.python.python             	0x000000010623b46f cfunction_vectorcall_FASTCALL_KEYWORDS + 122
122 org.python.python             	0x00000001062ad67d call_function + 346
123 org.python.python             	0x00000001062aa314 _PyEval_EvalFrameDefault + 30050
124 org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
125 org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
126 org.python.python             	0x000000010620f464 method_vectorcall + 372
127 org.python.python             	0x000000010620cd09 PyVectorcall_Call + 108
128 org.python.python             	0x00000001062aa5d4 _PyEval_EvalFrameDefault + 30754
129 org.python.python             	0x000000010621a516 gen_send_ex + 244
130 org.python.python             	0x0000000106214679 method_vectorcall_O + 244
131 org.python.python             	0x00000001062ad67d call_function + 346
132 org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
133 org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
134 org.python.python             	0x000000010620f3f0 method_vectorcall + 256
135 org.python.python             	0x00000001062bb8de context_run + 92
136 org.python.python             	0x000000010623b46f cfunction_vectorcall_FASTCALL_KEYWORDS + 122
137 org.python.python             	0x00000001062ad67d call_function + 346
138 org.python.python             	0x00000001062aa257 _PyEval_EvalFrameDefault + 29861
139 org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
140 org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
141 org.python.python             	0x000000010620c93c _PyObject_FastCallDict + 236
142 org.python.python             	0x000000010631240e partial_call + 367
143 org.python.python             	0x000000010620ca7d _PyObject_MakeTpCall + 274
144 org.python.python             	0x00000001062ad847 call_function + 804
145 org.python.python             	0x00000001062aa314 _PyEval_EvalFrameDefault + 30050
146 org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
147 org.python.python             	0x00000001062ad67d call_function + 346
148 org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
149 org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
150 org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
151 org.python.python             	0x00000001062bb8de context_run + 92
152 org.python.python             	0x000000010623b46f cfunction_vectorcall_FASTCALL_KEYWORDS + 122
153 org.python.python             	0x000000010620cd09 PyVectorcall_Call + 108
154 org.python.python             	0x00000001062aa7dc _PyEval_EvalFrameDefault + 31274
155 org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
156 org.python.python             	0x00000001062ad67d call_function + 346
157 org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
158 org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
159 org.python.python             	0x00000001062ad67d call_function + 346
160 org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
161 org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
162 org.python.python             	0x00000001062ad67d call_function + 346
163 org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
164 org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
165 org.python.python             	0x00000001062ad67d call_function + 346
166 org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
167 org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
168 org.python.python             	0x00000001062ad67d call_function + 346
169 org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
170 org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
171 org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
172 org.python.python             	0x000000010620f377 method_vectorcall + 135
173 org.python.python             	0x00000001062ad67d call_function + 346
174 org.python.python             	0x00000001062aa257 _PyEval_EvalFrameDefault + 29861
175 org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
176 org.python.python             	0x00000001062a2d0f PyEval_EvalCode + 51
177 org.python.python             	0x00000001062a05b8 builtin_exec + 581
178 org.python.python             	0x000000010623b3a9 cfunction_vectorcall_FASTCALL + 169
179 org.python.python             	0x00000001062ad67d call_function + 346
180 org.python.python             	0x00000001062aa314 _PyEval_EvalFrameDefault + 30050
181 org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
182 org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
183 org.python.python             	0x00000001062ad67d call_function + 346
184 org.python.python             	0x00000001062aa314 _PyEval_EvalFrameDefault + 30050
185 org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
186 org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
187 org.python.python             	0x000000010620cd09 PyVectorcall_Call + 108
188 org.python.python             	0x00000001062f17ef pymain_run_module + 179
189 org.python.python             	0x00000001062f10a2 Py_RunMain + 1209
190 org.python.python             	0x00000001062f167e pymain_main + 306
191 org.python.python             	0x00000001062f16cc Py_BytesMain + 42
192 libdyld.dylib                 	0x00007fff2039d621 start + 1

Thread 1:: ZMQbg/Reaper
0   libsystem_kernel.dylib        	0x00007fff203527e2 kevent + 10
1   libzmq.cpython-38-darwin.so   	0x0000000107ff7cc5 zmq::kqueue_t::loop() + 181
2   libzmq.cpython-38-darwin.so   	0x0000000107fedd3d thread_routine(void*) + 61
3   libsystem_pthread.dylib       	0x00007fff20382950 _pthread_start + 224
4   libsystem_pthread.dylib       	0x00007fff2037e47b thread_start + 15

Thread 2:: ZMQbg/IO/0
0   libsystem_kernel.dylib        	0x00007fff203527e2 kevent + 10
1   libzmq.cpython-38-darwin.so   	0x0000000107ff7cc5 zmq::kqueue_t::loop() + 181
2   libzmq.cpython-38-darwin.so   	0x0000000107fedd3d thread_routine(void*) + 61
3   libsystem_pthread.dylib       	0x00007fff20382950 _pthread_start + 224
4   libsystem_pthread.dylib       	0x00007fff2037e47b thread_start + 15

Thread 3:
0   libsystem_kernel.dylib        	0x00007fff203508e2 __psynch_cvwait + 10
1   libsystem_pthread.dylib       	0x00007fff20382e6f _pthread_cond_wait + 1254
2   org.python.python             	0x00000001062a2281 take_gil + 145
3   org.python.python             	0x00000001062a281f PyEval_RestoreThread + 48
4   socket.cpython-38-darwin.so   	0x000000010813f5f4 __pyx_f_3zmq_7backend_6cython_6socket__send_copy + 612
5   socket.cpython-38-darwin.so   	0x000000010813a4d1 __pyx_f_3zmq_7backend_6cython_6socket_6Socket_send + 369
6   socket.cpython-38-darwin.so   	0x000000010813ee5c __pyx_pw_3zmq_7backend_6cython_6socket_6Socket_25send + 236
7   error.cpython-38-darwin.so    	0x00000001080f563c __Pyx_CyFunction_CallAsMethod + 92
8   org.python.python             	0x000000010620ca7d _PyObject_MakeTpCall + 274
9   org.python.python             	0x000000010620f397 method_vectorcall + 167
10  org.python.python             	0x00000001062ad67d call_function + 346
11  org.python.python             	0x00000001062aa3df _PyEval_EvalFrameDefault + 30253
12  org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
13  org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
14  org.python.python             	0x000000010620f377 method_vectorcall + 135
15  org.python.python             	0x00000001062ad67d call_function + 346
16  org.python.python             	0x00000001062aa3df _PyEval_EvalFrameDefault + 30253
17  org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
18  org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
19  org.python.python             	0x000000010620f464 method_vectorcall + 372
20  org.python.python             	0x000000010620cd09 PyVectorcall_Call + 108
21  org.python.python             	0x00000001062aa5d4 _PyEval_EvalFrameDefault + 30754
22  org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
23  org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
24  org.python.python             	0x000000010620f464 method_vectorcall + 372
25  org.python.python             	0x000000010620cd09 PyVectorcall_Call + 108
26  org.python.python             	0x00000001062aa5d4 _PyEval_EvalFrameDefault + 30754
27  org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
28  org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
29  org.python.python             	0x00000001062ad67d call_function + 346
30  org.python.python             	0x00000001062aa314 _PyEval_EvalFrameDefault + 30050
31  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
32  org.python.python             	0x000000010620f464 method_vectorcall + 372
33  org.python.python             	0x000000010620cd09 PyVectorcall_Call + 108
34  org.python.python             	0x00000001062aa5d4 _PyEval_EvalFrameDefault + 30754
35  org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
36  org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
37  org.python.python             	0x00000001062ad67d call_function + 346
38  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
39  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
40  org.python.python             	0x00000001062ad67d call_function + 346
41  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
42  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
43  org.python.python             	0x000000010620f377 method_vectorcall + 135
44  org.python.python             	0x00000001062ad67d call_function + 346
45  org.python.python             	0x00000001062aa314 _PyEval_EvalFrameDefault + 30050
46  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
47  org.python.python             	0x000000010620f464 method_vectorcall + 372
48  org.python.python             	0x00000001062bb8de context_run + 92
49  org.python.python             	0x000000010623b46f cfunction_vectorcall_FASTCALL_KEYWORDS + 122
50  org.python.python             	0x000000010620cd09 PyVectorcall_Call + 108
51  org.python.python             	0x00000001062aa7dc _PyEval_EvalFrameDefault + 31274
52  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
53  org.python.python             	0x00000001062ad67d call_function + 346
54  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
55  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
56  org.python.python             	0x00000001062ad67d call_function + 346
57  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
58  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
59  org.python.python             	0x00000001062ad67d call_function + 346
60  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
61  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
62  org.python.python             	0x00000001062ad67d call_function + 346
63  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
64  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
65  org.python.python             	0x000000010620f3f0 method_vectorcall + 256
66  org.python.python             	0x000000010620cd09 PyVectorcall_Call + 108
67  org.python.python             	0x00000001062aa5d4 _PyEval_EvalFrameDefault + 30754
68  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
69  org.python.python             	0x00000001062ad67d call_function + 346
70  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
71  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
72  org.python.python             	0x00000001062ad67d call_function + 346
73  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
74  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
75  org.python.python             	0x000000010620f3f0 method_vectorcall + 256
76  org.python.python             	0x000000010620cd09 PyVectorcall_Call + 108
77  org.python.python             	0x0000000106323465 t_bootstrap + 74
78  org.python.python             	0x00000001062e4ced pythread_wrapper + 25
79  libsystem_pthread.dylib       	0x00007fff20382950 _pthread_start + 224
80  libsystem_pthread.dylib       	0x00007fff2037e47b thread_start + 15

Thread 4:
0   libsystem_kernel.dylib        	0x00007fff203544fe poll + 10
1   libzmq.cpython-38-darwin.so   	0x000000010801566e zmq_poll + 430
2   libzmq.cpython-38-darwin.so   	0x0000000108018202 zmq::proxy(zmq::socket_base_t*, zmq::socket_base_t*, zmq::socket_base_t*, zmq::socket_base_t*) + 386
3   _device.cpython-38-darwin.so  	0x00000001081a46d0 __pyx_pw_3zmq_7backend_6cython_7_device_3proxy + 368
4   _device.cpython-38-darwin.so  	0x00000001081a40e5 __Pyx_PyObject_Call + 85
5   _device.cpython-38-darwin.so  	0x00000001081a3375 __pyx_pw_3zmq_7backend_6cython_7_device_1device + 421
6   org.python.python             	0x000000010620ca7d _PyObject_MakeTpCall + 274
7   org.python.python             	0x00000001062ad847 call_function + 804
8   org.python.python             	0x00000001062aa257 _PyEval_EvalFrameDefault + 29861
9   org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
10  org.python.python             	0x00000001062ad67d call_function + 346
11  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
12  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
13  org.python.python             	0x00000001062ad67d call_function + 346
14  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
15  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
16  org.python.python             	0x000000010620f3f0 method_vectorcall + 256
17  org.python.python             	0x000000010620cd09 PyVectorcall_Call + 108
18  org.python.python             	0x0000000106323465 t_bootstrap + 74
19  org.python.python             	0x00000001062e4ced pythread_wrapper + 25
20  libsystem_pthread.dylib       	0x00007fff20382950 _pthread_start + 224
21  libsystem_pthread.dylib       	0x00007fff2037e47b thread_start + 15

Thread 5:: ZMQbg/Reaper
0   libsystem_kernel.dylib        	0x00007fff203527e2 kevent + 10
1   libzmq.cpython-38-darwin.so   	0x0000000107ff7cc5 zmq::kqueue_t::loop() + 181
2   libzmq.cpython-38-darwin.so   	0x0000000107fedd3d thread_routine(void*) + 61
3   libsystem_pthread.dylib       	0x00007fff20382950 _pthread_start + 224
4   libsystem_pthread.dylib       	0x00007fff2037e47b thread_start + 15

Thread 6:: ZMQbg/IO/0
0   libsystem_kernel.dylib        	0x00007fff203527e2 kevent + 10
1   libzmq.cpython-38-darwin.so   	0x0000000107ff7cc5 zmq::kqueue_t::loop() + 181
2   libzmq.cpython-38-darwin.so   	0x0000000107fedd3d thread_routine(void*) + 61
3   libsystem_pthread.dylib       	0x00007fff20382950 _pthread_start + 224
4   libsystem_pthread.dylib       	0x00007fff2037e47b thread_start + 15

Thread 7:
0   libsystem_kernel.dylib        	0x00007fff203508e2 __psynch_cvwait + 10
1   libsystem_pthread.dylib       	0x00007fff20382e6f _pthread_cond_wait + 1254
2   org.python.python             	0x00000001062a2281 take_gil + 145
3   org.python.python             	0x00000001062a281f PyEval_RestoreThread + 48
4   _sqlite3.cpython-38-darwin.so 	0x0000000107224680 pysqlite_statement_reset + 57
5   _sqlite3.cpython-38-darwin.so 	0x0000000107222236 _pysqlite_query_execute + 1902
6   org.python.python             	0x000000010620d022 cfunction_call_varargs + 319
7   _sqlite3.cpython-38-darwin.so 	0x0000000107220b00 pysqlite_connection_execute + 74
8   org.python.python             	0x0000000106213ebe method_vectorcall_VARARGS + 270
9   org.python.python             	0x00000001062ad67d call_function + 346
10  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
11  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
12  org.python.python             	0x00000001062ad67d call_function + 346
13  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
14  org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
15  org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
16  org.python.python             	0x000000010620cd09 PyVectorcall_Call + 108
17  org.python.python             	0x00000001062aa5d4 _PyEval_EvalFrameDefault + 30754
18  org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
19  org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
20  org.python.python             	0x00000001062ad67d call_function + 346
21  org.python.python             	0x00000001062aa314 _PyEval_EvalFrameDefault + 30050
22  org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
23  org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
24  org.python.python             	0x00000001062ad67d call_function + 346
25  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
26  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
27  org.python.python             	0x000000010620cd09 PyVectorcall_Call + 108
28  org.python.python             	0x00000001062aa5d4 _PyEval_EvalFrameDefault + 30754
29  org.python.python             	0x00000001062ae197 _PyEval_EvalCodeWithName + 1947
30  org.python.python             	0x000000010620d427 _PyFunction_Vectorcall + 227
31  org.python.python             	0x00000001062ad67d call_function + 346
32  org.python.python             	0x00000001062aa314 _PyEval_EvalFrameDefault + 30050
33  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
34  org.python.python             	0x00000001062ad67d call_function + 346
35  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
36  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
37  org.python.python             	0x00000001062ad67d call_function + 346
38  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
39  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
40  org.python.python             	0x000000010620f3f0 method_vectorcall + 256
41  org.python.python             	0x000000010620cd09 PyVectorcall_Call + 108
42  org.python.python             	0x0000000106323465 t_bootstrap + 74
43  org.python.python             	0x00000001062e4ced pythread_wrapper + 25
44  libsystem_pthread.dylib       	0x00007fff20382950 _pthread_start + 224
45  libsystem_pthread.dylib       	0x00007fff2037e47b thread_start + 15

Thread 8:
0   libsystem_pthread.dylib       	0x00007fff2037e458 start_wqthread + 0

Thread 9:
0   libsystem_kernel.dylib        	0x00007fff203561fe __select + 10
1   org.python.python             	0x000000010632102f time_sleep + 116
2   org.python.python             	0x000000010623b6a2 cfunction_vectorcall_O + 206
3   org.python.python             	0x00000001062ad67d call_function + 346
4   org.python.python             	0x00000001062aa257 _PyEval_EvalFrameDefault + 29861
5   org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
6   org.python.python             	0x00000001062ad67d call_function + 346
7   org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
8   org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
9   org.python.python             	0x00000001062ad67d call_function + 346
10  org.python.python             	0x00000001062aa23b _PyEval_EvalFrameDefault + 29833
11  org.python.python             	0x000000010620d2c2 function_code_fastcall + 106
12  org.python.python             	0x000000010620f3f0 method_vectorcall + 256
13  org.python.python             	0x000000010620cd09 PyVectorcall_Call + 108
14  org.python.python             	0x0000000106323465 t_bootstrap + 74
15  org.python.python             	0x00000001062e4ced pythread_wrapper + 25
16  libsystem_pthread.dylib       	0x00007fff20382950 _pthread_start + 224
17  libsystem_pthread.dylib       	0x00007fff2037e47b thread_start + 15

Thread 10:
0   libsystem_kernel.dylib        	0x00007fff203508e2 __psynch_cvwait + 10
1   libsystem_pthread.dylib       	0x00007fff20382e6f _pthread_cond_wait + 1254
2   libopenblas.0.dylib           	0x000000010de3e27b blas_thread_server + 507
3   libsystem_pthread.dylib       	0x00007fff20382950 _pthread_start + 224
4   libsystem_pthread.dylib       	0x00007fff2037e47b thread_start + 15

Thread 11:
0   libsystem_kernel.dylib        	0x00007fff203508e2 __psynch_cvwait + 10
1   libsystem_pthread.dylib       	0x00007fff20382e6f _pthread_cond_wait + 1254
2   libopenblas.0.dylib           	0x000000010de3e27b blas_thread_server + 507
3   libsystem_pthread.dylib       	0x00007fff20382950 _pthread_start + 224
4   libsystem_pthread.dylib       	0x00007fff2037e47b thread_start + 15

Thread 12:
0   libsystem_kernel.dylib        	0x00007fff203508e2 __psynch_cvwait + 10
1   libsystem_pthread.dylib       	0x00007fff20382e6f _pthread_cond_wait + 1254
2   libopenblas.0.dylib           	0x000000010de3e27b blas_thread_server + 507
3   libsystem_pthread.dylib       	0x00007fff20382950 _pthread_start + 224
4   libsystem_pthread.dylib       	0x00007fff2037e47b thread_start + 15

Thread 13:
0   libsystem_kernel.dylib        	0x00007fff203508e2 __psynch_cvwait + 10
1   libsystem_pthread.dylib       	0x00007fff20382e6f _pthread_cond_wait + 1254
2   libopenblas.0.dylib           	0x000000011a0f52eb blas_thread_server + 619
3   libsystem_pthread.dylib       	0x00007fff20382950 _pthread_start + 224
4   libsystem_pthread.dylib       	0x00007fff2037e47b thread_start + 15

Thread 14:
0   libsystem_kernel.dylib        	0x00007fff203508e2 __psynch_cvwait + 10
1   libsystem_pthread.dylib       	0x00007fff20382e6f _pthread_cond_wait + 1254
2   libopenblas.0.dylib           	0x000000011a0f52eb blas_thread_server + 619
3   libsystem_pthread.dylib       	0x00007fff20382950 _pthread_start + 224
4   libsystem_pthread.dylib       	0x00007fff2037e47b thread_start + 15

Thread 15:
0   libsystem_kernel.dylib        	0x00007fff203508e2 __psynch_cvwait + 10
1   libsystem_pthread.dylib       	0x00007fff20382e6f _pthread_cond_wait + 1254
2   libopenblas.0.dylib           	0x000000011a0f52eb blas_thread_server + 619
3   libsystem_pthread.dylib       	0x00007fff20382950 _pthread_start + 224
4   libsystem_pthread.dylib       	0x00007fff2037e47b thread_start + 15

Thread 16:
0   libsystem_pthread.dylib       	0x00007fff2037e458 start_wqthread + 0

Thread 17:
0   libsystem_pthread.dylib       	0x00007fff2037e458 start_wqthread + 0

Thread 18:
0   libsystem_pthread.dylib       	0x00007fff2037e458 start_wqthread + 0

Thread 0 crashed with X86 Thread State (64-bit):
  rax: 0x0000000000000000  rbx: 0x000000010dac4e00  rcx: 0x00007ffee9a19d38  rdx: 0x0000000000000000
  rdi: 0x0000000000000307  rsi: 0x0000000000000006  rbp: 0x00007ffee9a19d60  rsp: 0x00007ffee9a19d38
   r8: 0x00000000000130a8   r9: 0x00007fff889e20e8  r10: 0x000000010dac4e00  r11: 0x0000000000000246
  r12: 0x0000000000000307  r13: 0x00007f9eb8292450  r14: 0x0000000000000006  r15: 0x0000000000000016
  rip: 0x00007fff20354462  rfl: 0x0000000000000246  cr2: 0x00007f9eb11ff000
  
Logical CPU:     0
Error Code:      0x02000148
Trap Number:     133

Thread 0 instruction stream not available.

Thread 0 last branch register state not available.


Binary Images:
       0x1061de000 -        0x1061e1fff +org.python.python (3.8.6 - 3.8.6) <5C1C8C7B-2140-3750-A25D-FF9BFFC095FD> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/Resources/Python.app/Contents/MacOS/Python
       0x1061f0000 -        0x1063c7fff +org.python.python (3.8.6, [c] 2001-2019 Python Software Foundation. - 3.8.6) <1008FC47-FD97-3569-9906-DA7EEC97CED5> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/Python
       0x106661000 -        0x106664fff +_heapq.cpython-38-darwin.so (0) <6E58CDBA-74DC-3E67-A6FB-912811C03410> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_heapq.cpython-38-darwin.so
       0x106731000 -        0x106738fff +_json.cpython-38-darwin.so (0) <8A438B79-DC34-37FE-A6C6-2A0D9AC7A71D> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_json.cpython-38-darwin.so
       0x106785000 -        0x106788fff +_posixsubprocess.cpython-38-darwin.so (0) <F9929C21-A825-377F-A8D0-9138E80F99F4> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_posixsubprocess.cpython-38-darwin.so
       0x106795000 -        0x10679cfff +select.cpython-38-darwin.so (0) <BDC9848D-957D-3C4D-A9AC-DAC62ECD2466> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/select.cpython-38-darwin.so
       0x1067a9000 -        0x1067b0fff +math.cpython-38-darwin.so (0) <3D4C8916-2937-3D34-B428-6FB58D6F3BA4> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/math.cpython-38-darwin.so
       0x10683d000 -        0x106844fff +zlib.cpython-38-darwin.so (0) <62B9F358-A8C8-3291-A8BC-12890616D714> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/zlib.cpython-38-darwin.so
       0x106851000 -        0x106854fff +_bz2.cpython-38-darwin.so (0) <3A6ED412-B6B1-3607-8738-3B03CEEDF43A> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_bz2.cpython-38-darwin.so
       0x106861000 -        0x106868fff +_lzma.cpython-38-darwin.so (0) <813F76FC-8225-3879-B5B2-D5E54FDDAF87> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_lzma.cpython-38-darwin.so
       0x106875000 -        0x106890fff +liblzma.5.dylib (0) <E4406E42-7BC4-3945-A1A4-E9B6874EF052> /usr/local/opt/xz/lib/liblzma.5.dylib
       0x106897000 -        0x10689afff +grp.cpython-38-darwin.so (0) <FA54BD47-B346-38D8-945C-D9B74389B226> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/grp.cpython-38-darwin.so
       0x106967000 -        0x10696afff +_opcode.cpython-38-darwin.so (0) <264D1D68-7C58-34FF-95F6-819745993277> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_opcode.cpython-38-darwin.so
       0x106a37000 -        0x106a3afff +_bisect.cpython-38-darwin.so (0) <D93F085A-0119-39E8-9BAB-8FF049FE6F0C> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_bisect.cpython-38-darwin.so
       0x106a47000 -        0x106a4efff +_sha512.cpython-38-darwin.so (0) <651EE381-DC85-306B-BB3B-E6182F2BB50A> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_sha512.cpython-38-darwin.so
       0x106a5b000 -        0x106a5efff +_random.cpython-38-darwin.so (0) <2F48B700-D617-3FAB-BB66-274578898444> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_random.cpython-38-darwin.so
       0x106bab000 -        0x106baefff +termios.cpython-38-darwin.so (0) <A0940FA9-ED0E-3ABC-9A9B-4930EE8B23FD> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/termios.cpython-38-darwin.so
       0x106bbb000 -        0x106bbefff +fcntl.cpython-38-darwin.so (0) <E0C45298-DDF6-3BEC-A01C-1B88AEBF5B74> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/fcntl.cpython-38-darwin.so
       0x106bcb000 -        0x106bcefff +resource.cpython-38-darwin.so (0) <94A18715-A709-3C12-8F9D-A60600BA68FC> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/resource.cpython-38-darwin.so
       0x106bdb000 -        0x106be2fff +_struct.cpython-38-darwin.so (0) <B63B5D8C-1A3C-3F48-AEDA-F4F6A8B33CAF> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_struct.cpython-38-darwin.so
       0x106c2f000 -        0x106c36fff +_hashlib.cpython-38-darwin.so (0) <633AC073-3973-3B04-B8FD-38C51FAEBE52> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_hashlib.cpython-38-darwin.so
       0x106c43000 -        0x106c92fff +libssl.1.1.dylib (0) <378E1E35-93EE-34E0-83B3-B93A875BEFF3> /usr/local/opt/openssl@1.1/lib/libssl.1.1.dylib
       0x106cbf000 -        0x106e7efff +libcrypto.1.1.dylib (0) <46792733-D3D4-3DBD-9347-8C0C7EC6D50C> /usr/local/opt/openssl@1.1/lib/libcrypto.1.1.dylib
       0x106f17000 -        0x106f1efff +_blake2.cpython-38-darwin.so (0) <7A94A834-6C1A-3796-BFF6-E0B0F45DBD97> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_blake2.cpython-38-darwin.so
       0x106f2b000 -        0x106f3efff +_sha3.cpython-38-darwin.so (0) <CBCEF30E-E3F3-3E1D-936B-D9FCC84D18E7> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_sha3.cpython-38-darwin.so
       0x10700b000 -        0x107012fff +binascii.cpython-38-darwin.so (0) <7B3D1E44-0D17-3176-B558-BEFE87469E55> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/binascii.cpython-38-darwin.so
       0x10705f000 -        0x10706efff +_datetime.cpython-38-darwin.so (0) <D4F68A00-76F6-3BC1-9457-A48984DB1869> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_datetime.cpython-38-darwin.so
       0x1071bb000 -        0x1071cafff +_pickle.cpython-38-darwin.so (0) <13F8BAD6-60DF-3CF9-BCE9-899776FDE51B> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_pickle.cpython-38-darwin.so
       0x10721b000 -        0x107226fff +_sqlite3.cpython-38-darwin.so (0) <79541592-0162-3705-9D64-04383F0D9E1D> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_sqlite3.cpython-38-darwin.so
       0x107237000 -        0x107316fff +libsqlite3.0.dylib (0) <9D24AB1D-B6E9-3CA5-B49E-7880DEECE26F> /usr/local/opt/sqlite/lib/libsqlite3.0.dylib
       0x10743b000 -        0x10744afff +_socket.cpython-38-darwin.so (0) <DF7CEA77-467E-3E67-AA8A-0FDBAF90088C> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_socket.cpython-38-darwin.so
       0x107457000 -        0x10746afff +_ssl.cpython-38-darwin.so (0) <FEFB2210-936E-3D38-A369-DD73635A2B37> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_ssl.cpython-38-darwin.so
       0x1074ff000 -        0x107502fff +_contextvars.cpython-38-darwin.so (0) <3426E9B3-49B1-31F5-B9BE-C94CE4EC14D3> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_contextvars.cpython-38-darwin.so
       0x10750f000 -        0x107516fff +_asyncio.cpython-38-darwin.so (0) <B9A141F7-6FFF-3313-80FC-DD1D3D8A0C63> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_asyncio.cpython-38-darwin.so
       0x1076e7000 -        0x1076eefff +array.cpython-38-darwin.so (0) <F655A4C6-14E1-3DF0-B11F-909E684B344D> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/array.cpython-38-darwin.so
       0x10797b000 -        0x107a82fff +unicodedata.cpython-38-darwin.so (0) <34CEFBF0-EBE2-376B-9666-B11DE11B871F> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/unicodedata.cpython-38-darwin.so
       0x107d0f000 -        0x107d12fff +_queue.cpython-38-darwin.so (0) <4E52E2B8-5458-32A9-A883-BE456FBFB3E1> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_queue.cpython-38-darwin.so
       0x107edf000 -        0x107ee2fff +_scproxy.cpython-38-darwin.so (0) <36FC1863-9840-3DCE-9E4F-9CD6DAD05204> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_scproxy.cpython-38-darwin.so
       0x107eef000 -        0x107ef2fff +_lsprof.cpython-38-darwin.so (0) <B62E6583-9836-3C7C-BB99-28D612D90CED> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_lsprof.cpython-38-darwin.so
       0x107fbf000 -        0x107fd2fff +_ctypes.cpython-38-darwin.so (0) <6FA02EE0-CF5F-3FF8-BB00-26624873AA39> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_ctypes.cpython-38-darwin.so
       0x107fe3000 -        0x108052fff +libzmq.cpython-38-darwin.so (0) <2C8C5F0F-64A1-3BF0-827F-04ECD4D505CC> /Users/USER/Desktop/*/libzmq.cpython-38-darwin.so
       0x1080d7000 -        0x1080e2fff +constants.cpython-38-darwin.so (0) <48D708F2-F9F0-3CB8-9BAB-F80594D8DFBF> /Users/USER/Desktop/*/constants.cpython-38-darwin.so
       0x1080f3000 -        0x1080f6fff +error.cpython-38-darwin.so (0) <0BE9D0C1-CDD0-3967-8F98-F6A186188798> /Users/USER/Desktop/*/error.cpython-38-darwin.so
       0x1080ff000 -        0x10810efff +message.cpython-38-darwin.so (0) <391F92F4-7D88-330F-9900-6F4975CDCCEC> /Users/USER/Desktop/*/message.cpython-38-darwin.so
       0x10811f000 -        0x108126fff +context.cpython-38-darwin.so (0) <240B7633-A5B7-3149-A821-1AB413006C55> /Users/USER/Desktop/*/context.cpython-38-darwin.so
       0x108133000 -        0x10814afff +socket.cpython-38-darwin.so (0) <B2EA7CAE-27E6-3306-9E47-CBF7B335DBA9> /Users/USER/Desktop/*/socket.cpython-38-darwin.so
       0x108167000 -        0x10816efff +utils.cpython-38-darwin.so (0) <086096D0-CC37-3CB7-88EF-288E01ED5869> /Users/USER/Desktop/*/utils.cpython-38-darwin.so
       0x10817b000 -        0x108186fff +_poll.cpython-38-darwin.so (0) <2DDAE451-79DA-3345-911A-0A377697F836> /Users/USER/Desktop/*/_poll.cpython-38-darwin.so
       0x108193000 -        0x108196fff +_version.cpython-38-darwin.so (0) <B563A9ED-1D0B-32DE-BEE4-4F1DAAE9D5BF> /Users/USER/Desktop/*/_version.cpython-38-darwin.so
       0x10819f000 -        0x1081a6fff +_device.cpython-38-darwin.so (0) <6ED0CF5F-9C53-3EC2-AC9F-DCB5822CE731> /Users/USER/Desktop/*/_device.cpython-38-darwin.so
       0x1081b3000 -        0x1081bafff +_proxy_steerable.cpython-38-darwin.so (0) <A811E86D-8675-37F0-9C40-63AED4477E7A> /Users/USER/Desktop/*/_proxy_steerable.cpython-38-darwin.so
       0x108287000 -        0x10828afff +_uuid.cpython-38-darwin.so (0) <42BEB4BD-D438-3E28-80D3-269691D07C43> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_uuid.cpython-38-darwin.so
       0x1082d7000 -        0x1082d7fff +speedups.cpython-38-darwin.so (0) <A8701C12-1FE5-31F8-8187-0A7B27CFD4B2> /Users/USER/Desktop/*/speedups.cpython-38-darwin.so
       0x1082da000 -        0x1082edfff +_curses.cpython-38-darwin.so (0) <4B91DD18-EFB1-3249-8A4B-8212AA1A190F> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_curses.cpython-38-darwin.so
       0x10837e000 -        0x1083b1fff +_decimal.cpython-38-darwin.so (0) <98DE7914-7B33-378D-B2F8-33CDB0B1A062> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_decimal.cpython-38-darwin.so
       0x108546000 -        0x108569fff +pyexpat.cpython-38-darwin.so (0) <11D8828D-C1BF-3A68-87B2-C3629FCBE864> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/pyexpat.cpython-38-darwin.so
       0x1085fa000 -        0x108605fff +_elementtree.cpython-38-darwin.so (0) <397201B9-DC8D-338B-A4B6-F0899A3EC048> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_elementtree.cpython-38-darwin.so
       0x108696000 -        0x1086ccfff +libquadmath.0.dylib (0) <7FFA409F-FB04-3B64-BE9A-3E3A494C975E> /Users/USER/Desktop/*/libquadmath.0.dylib
       0x1086db000 -        0x1086f0fff +libgcc_s.1.dylib (0) <7C6D7CB7-82DB-3290-8181-07646FEA1F80> /Users/USER/Desktop/*/libgcc_s.1.dylib
       0x1087fb000 -        0x1087fcfff +lapack_lite.cpython-38-darwin.so (0) <05C1AF9A-3287-3383-96DC-BAD8ED729EB8> /Users/USER/Desktop/*/lapack_lite.cpython-38-darwin.so
       0x10c800000 -        0x10cb75fff +_multiarray_umath.cpython-38-darwin.so (0) <5D412372-1F89-39F2-A5FF-75D51617AE6D> /Users/USER/Desktop/*/_multiarray_umath.cpython-38-darwin.so
       0x10cc8a000 -        0x10cda1fff +libgfortran.3.dylib (0) <9ABE5EDE-AD43-391A-9E54-866711FAC32A> /Users/USER/Desktop/*/libgfortran.3.dylib
       0x10ce05000 -        0x10ce12fff +_multiarray_tests.cpython-38-darwin.so (0) <5AE1476B-B3E7-3D4F-8759-028738754A37> /Users/USER/Desktop/*/_multiarray_tests.cpython-38-darwin.so
       0x10cea3000 -        0x10cebcfff +_umath_linalg.cpython-38-darwin.so (0) <6AEC7776-A41B-32A4-98DA-B39831C47013> /Users/USER/Desktop/*/_umath_linalg.cpython-38-darwin.so
       0x10cf8b000 -        0x10cf9cfff +_pocketfft_internal.cpython-38-darwin.so (0) <5CEA9A11-0256-309D-B6FC-59D70300E328> /Users/USER/Desktop/*/_pocketfft_internal.cpython-38-darwin.so
       0x10cfe0000 -        0x10d04bfff +mtrand.cpython-38-darwin.so (0) <0D35A33C-70C6-3780-9FE4-1A474BC23707> /Users/USER/Desktop/*/mtrand.cpython-38-darwin.so
       0x10d09f000 -        0x10d0bdfff +bit_generator.cpython-38-darwin.so (0) <CA64EA30-F60D-3CF8-AB2F-4D2BA2E95B81> /Users/USER/Desktop/*/bit_generator.cpython-38-darwin.so
       0x10d0d8000 -        0x10d10afff +_common.cpython-38-darwin.so (0) <CD1241F4-4FA7-3496-BC1A-F4959D807D2A> /Users/USER/Desktop/*/_common.cpython-38-darwin.so
       0x10d121000 -        0x10d170fff +_bounded_integers.cpython-38-darwin.so (0) <8DF6F66E-1CA6-3CCB-BC3F-FB2E4447D7B4> /Users/USER/Desktop/*/_bounded_integers.cpython-38-darwin.so
       0x10d191000 -        0x10d19ffff +_mt19937.cpython-38-darwin.so (0) <2938D60D-F2D2-365B-9AEA-5A705CC9E87C> /Users/USER/Desktop/*/_mt19937.cpython-38-darwin.so
       0x10d1ab000 -        0x10d1b7fff +_philox.cpython-38-darwin.so (0) <FE5DDBC1-4248-3491-B590-14DEDC922673> /Users/USER/Desktop/*/_philox.cpython-38-darwin.so
       0x10d1c2000 -        0x10d1cbfff +_pcg64.cpython-38-darwin.so (0) <5AA55D52-BCC7-39C0-AFFB-0C4EE4E838D5> /Users/USER/Desktop/*/_pcg64.cpython-38-darwin.so
       0x10d1d5000 -        0x10d1dcfff +_sfc64.cpython-38-darwin.so (0) <233D9009-AF99-35DB-AF24-33A0EAA164B5> /Users/USER/Desktop/*/_sfc64.cpython-38-darwin.so
       0x10d1e4000 -        0x10d265fff +_generator.cpython-38-darwin.so (0) <DD94F935-6520-32AB-A84D-556026A1C9C8> /Users/USER/Desktop/*/_generator.cpython-38-darwin.so
       0x10d3be000 -        0x10d3c9fff +_ccallback_c.cpython-38-darwin.so (0) <EED40C26-4770-3566-9E50-70930693E60B> /Users/USER/Desktop/*/_ccallback_c.cpython-38-darwin.so
       0x10d3d5000 -        0x10d3ddfff +_uarray.cpython-38-darwin.so (0) <6205ADF1-EB0B-32E8-B6AE-2C6F4EADDD7E> /Users/USER/Desktop/*/_uarray.cpython-38-darwin.so
       0x10d3e5000 -        0x10d4bbfff +pypocketfft.cpython-38-darwin.so (0) <2EC56584-003A-32AA-AD17-9865DAC6206C> /Users/USER/Desktop/*/pypocketfft.cpython-38-darwin.so
       0x10d601000 -        0x10d679fff +_csparsetools.cpython-38-darwin.so (0) <F6803384-6BFE-3403-86DE-EE1F669508A8> /Users/USER/Desktop/*/_csparsetools.cpython-38-darwin.so
       0x10d6ee000 -        0x10d73dfff +_shortest_path.cpython-38-darwin.so (0) <700034A9-CCE1-39D2-8CA0-27EE439E8449> /Users/USER/Desktop/*/_shortest_path.cpython-38-darwin.so
       0x10d766000 -        0x10d781fff +_tools.cpython-38-darwin.so (0) <A93695A3-1607-36C3-8A6B-44E7331906F1> /Users/USER/Desktop/*/_tools.cpython-38-darwin.so
       0x10d796000 -        0x10d7aefff +_traversal.cpython-38-darwin.so (0) <C055C5F2-C2F1-3625-8B40-5A684235BC28> /Users/USER/Desktop/*/_traversal.cpython-38-darwin.so
       0x10d7be000 -        0x10d7dafff +_min_spanning_tree.cpython-38-darwin.so (0) <20D969E7-827D-398B-BA4D-1DF88AFFE4BE> /Users/USER/Desktop/*/_min_spanning_tree.cpython-38-darwin.so
       0x10d7ef000 -        0x10d816fff +_flow.cpython-38-darwin.so (0) <E2B192F7-2DBB-3F6C-9F4F-8EFD991701A8> /Users/USER/Desktop/*/_flow.cpython-38-darwin.so
       0x10d832000 -        0x10d850fff +_matching.cpython-38-darwin.so (0) <AA9778C6-D83D-3CC1-BE3E-4EC7E5100FE5> /Users/USER/Desktop/*/_matching.cpython-38-darwin.so
       0x10d866000 -        0x10d88ffff +_reordering.cpython-38-darwin.so (0) <1A73F1E1-DC1C-30C2-A7F7-4B74398EF782> /Users/USER/Desktop/*/_reordering.cpython-38-darwin.so
       0x10d8ac000 -        0x10d936fff +ckdtree.cpython-38-darwin.so (0) <E46E42C0-81D7-385D-A94D-78DC94E2E3F4> /Users/USER/Desktop/*/ckdtree.cpython-38-darwin.so
       0x10d9ba000 -        0x10d9cffff +libgcc_s.1.dylib (0) <7C6D7CB7-82DB-3290-8181-07646FEA1F80> /Users/USER/Desktop/*/libgcc_s.1.dylib
       0x10d9da000 -        0x10d9dffff +messagestream.cpython-38-darwin.so (0) <09D44A47-6684-3497-933E-81786BB7BA35> /Users/USER/Desktop/*/messagestream.cpython-38-darwin.so
       0x10d9e6000 -        0x10d9e7fff +_zeros.cpython-38-darwin.so (0) <87F4DED4-00E3-3B4D-AB03-7719BC3F5416> /Users/USER/Desktop/*/_zeros.cpython-38-darwin.so
       0x10d9ed000 -        0x10da88fff  dyld (832.7.1) <DEA51514-B4E8-3368-979B-89D0F8397ABC> /usr/lib/dyld
       0x10db05000 -        0x11155cfff +libopenblas.0.dylib (0) <07E4ABF0-E967-375A-90D9-1C54B8AF2F58> /Users/USER/Desktop/*/libopenblas.0.dylib
       0x119788000 -        0x119bc0fff +_sparsetools.cpython-38-darwin.so (0) <303E65C6-B42E-3672-A28A-0641E6467A3F> /Users/USER/Desktop/*/_sparsetools.cpython-38-darwin.so
       0x119c9f000 -        0x119d74fff +qhull.cpython-38-darwin.so (0) <432B74FF-04E7-321A-80D8-CD7491B2F2F3> /Users/USER/Desktop/*/qhull.cpython-38-darwin.so
       0x119dbc000 -        0x11d7fdfff +libopenblas.0.dylib (0) <6286B1A0-1FC3-3620-A830-7AF7902EAD7F> /Users/USER/Desktop/*/libopenblas.0.dylib
       0x11da29000 -        0x11db40fff +libgfortran.3.dylib (0) <9ABE5EDE-AD43-391A-9E54-866711FAC32A> /Users/USER/Desktop/*/libgfortran.3.dylib
       0x11dba4000 -        0x11dbdafff +libquadmath.0.dylib (0) <7FFA409F-FB04-3B64-BE9A-3E3A494C975E> /Users/USER/Desktop/*/libquadmath.0.dylib
       0x123be9000 -        0x123c03fff +_voronoi.cpython-38-darwin.so (0) <AEDFC55F-B8FC-378F-B795-4A503858A082> /Users/USER/Desktop/*/_voronoi.cpython-38-darwin.so
       0x123c16000 -        0x123c6ffff +_fblas.cpython-38-darwin.so (0) <C6765802-4919-370F-A911-117B611D6BF0> /Users/USER/Desktop/*/_fblas.cpython-38-darwin.so
       0x123ca7000 -        0x123db9fff +_flapack.cpython-38-darwin.so (0) <D7C74527-3D66-3358-A3C2-F1496F6C94ED> /Users/USER/Desktop/*/_flapack.cpython-38-darwin.so
       0x123eb2000 -        0x123ebdfff +_flinalg.cpython-38-darwin.so (0) <3AE04E14-9C7D-3484-ACDE-EB4E1FD3EE8B> /Users/USER/Desktop/*/_flinalg.cpython-38-darwin.so
       0x123ec5000 -        0x123ee8fff +_solve_toeplitz.cpython-38-darwin.so (0) <D590EE5F-10A6-3549-8C5C-2044149A99A6> /Users/USER/Desktop/*/_solve_toeplitz.cpython-38-darwin.so
       0x123f41000 -        0x123f75fff +_decomp_update.cpython-38-darwin.so (0) <B5E126A8-4EFB-3983-BC7E-1085EFAFA4A4> /Users/USER/Desktop/*/_decomp_update.cpython-38-darwin.so
       0x123f8b000 -        0x123faffff +cython_blas.cpython-38-darwin.so (0) <C7C19AF1-7F30-3F62-9344-4A706C991746> /Users/USER/Desktop/*/cython_blas.cpython-38-darwin.so
       0x123fcc000 -        0x124035fff +cython_lapack.cpython-38-darwin.so (0) <11FB7F29-FDE2-3D91-B015-62FBF7DD28D6> /Users/USER/Desktop/*/cython_lapack.cpython-38-darwin.so
       0x1240d3000 -        0x1240e0fff +_distance_wrap.cpython-38-darwin.so (0) <6CABFE31-DEFE-3E52-8334-44220109DA66> /Users/USER/Desktop/*/_distance_wrap.cpython-38-darwin.so
       0x1240ea000 -        0x124104fff +_hausdorff.cpython-38-darwin.so (0) <AB699014-CDD3-3FC2-9E91-7954C5486B8C> /Users/USER/Desktop/*/_hausdorff.cpython-38-darwin.so
       0x124117000 -        0x124295fff +_ufuncs.cpython-38-darwin.so (0) <37E9B188-ABA9-356D-AD08-9CBCD3F66C0E> /Users/USER/Desktop/*/_ufuncs.cpython-38-darwin.so
       0x124308000 -        0x12431cfff +_ufuncs_cxx.cpython-38-darwin.so (0) <528BCE62-850E-3EF7-A2E9-ECC3275B332D> /Users/USER/Desktop/*/_ufuncs_cxx.cpython-38-darwin.so
       0x12432b000 -        0x1243e4fff +specfun.cpython-38-darwin.so (0) <E4ABD1FB-5D2E-3B1D-B682-E0E57B89D9E3> /Users/USER/Desktop/*/specfun.cpython-38-darwin.so
       0x124433000 -        0x124436fff +_comb.cpython-38-darwin.so (0) <6A30E84C-0543-3547-9A32-BF55668A5BE0> /Users/USER/Desktop/*/_comb.cpython-38-darwin.so
       0x12443b000 -        0x124447fff +_ellip_harm_2.cpython-38-darwin.so (0) <7615AE13-C8E6-3F31-A0BB-27A53BD5F0A2> /Users/USER/Desktop/*/_ellip_harm_2.cpython-38-darwin.so
       0x1244d2000 -        0x1244f3fff +_nd_image.cpython-38-darwin.so (0) <DE71B7E1-BD8B-305E-B52E-739F607C4DCB> /Users/USER/Desktop/*/_nd_image.cpython-38-darwin.so
       0x1244fa000 -        0x124533fff +_ni_label.cpython-38-darwin.so (0) <38C2B0F5-D0A8-34C5-97D8-3D2AE1E8726D> /Users/USER/Desktop/*/_ni_label.cpython-38-darwin.so
       0x12458f000 -        0x124596fff +minpack2.cpython-38-darwin.so (0) <BFCFEB74-5BC6-3996-BD13-9A7BB49D0E11> /Users/USER/Desktop/*/minpack2.cpython-38-darwin.so
       0x1245db000 -        0x124608fff +_iterative.cpython-38-darwin.so (0) <83A26803-BA4C-3FE1-A90A-8BC0B06EC8BD> /Users/USER/Desktop/*/_iterative.cpython-38-darwin.so
       0x124620000 -        0x124670fff +_superlu.cpython-38-darwin.so (0) <CCEA30D2-BFB2-3381-A1A4-FBE2481FAA4B> /Users/USER/Desktop/*/_superlu.cpython-38-darwin.so
       0x1246c7000 -        0x12474cfff +_arpack.cpython-38-darwin.so (0) <4369DEAA-6A2B-39DB-AE98-B3A6724D466B> /Users/USER/Desktop/*/_arpack.cpython-38-darwin.so
       0x12476c000 -        0x124787fff +_group_columns.cpython-38-darwin.so (0) <89E0D5F3-88F4-30A3-B95C-0D842DF0F6F8> /Users/USER/Desktop/*/_group_columns.cpython-38-darwin.so
       0x1247da000 -        0x12480dfff +_trlib.cpython-38-darwin.so (0) <A0402E56-E6B1-32F4-8B74-B23A1DFFC0EE> /Users/USER/Desktop/*/_trlib.cpython-38-darwin.so
       0x1248a9000 -        0x1248c5fff +_lbfgsb.cpython-38-darwin.so (0) <4AB8D944-E888-31DA-A0E1-9936548AC0BA> /Users/USER/Desktop/*/_lbfgsb.cpython-38-darwin.so
       0x1248cb000 -        0x1248d5fff +moduleTNC.cpython-38-darwin.so (0) <04305437-2C7E-3B28-8190-C01859E7A6F2> /Users/USER/Desktop/*/moduleTNC.cpython-38-darwin.so
       0x1248d8000 -        0x1248f5fff +_cobyla.cpython-38-darwin.so (0) <31196678-1FDF-3BC6-81BD-3BC9F8A23B42> /Users/USER/Desktop/*/_cobyla.cpython-38-darwin.so
       0x12493a000 -        0x124958fff +_slsqp.cpython-38-darwin.so (0) <3A7819E5-C4C9-3C51-84C6-BB534B55D715> /Users/USER/Desktop/*/_slsqp.cpython-38-darwin.so
       0x12495d000 -        0x12497afff +_minpack.cpython-38-darwin.so (0) <B03A50A2-3951-3A40-9FC9-635CC411D50C> /Users/USER/Desktop/*/_minpack.cpython-38-darwin.so
       0x12497f000 -        0x124997fff +givens_elimination.cpython-38-darwin.so (0) <67429DE9-23EE-3EED-B85E-891EFB7675A5> /Users/USER/Desktop/*/givens_elimination.cpython-38-darwin.so
       0x1249e8000 -        0x1249f0fff +__nnls.cpython-38-darwin.so (0) <D87A26CE-0108-39E4-977A-291ADB551EE9> /Users/USER/Desktop/*/__nnls.cpython-38-darwin.so
       0x1249f5000 -        0x124a21fff +_bglu_dense.cpython-38-darwin.so (0) <DB9F5CE4-8055-3353-A1A1-BC80770D868E> /Users/USER/Desktop/*/_bglu_dense.cpython-38-darwin.so
       0x124a3f000 -        0x124a41fff +_lsap_module.cpython-38-darwin.so (0) <64545500-18DD-3634-8211-7C8C37F47CA5> /Users/USER/Desktop/*/_lsap_module.cpython-38-darwin.so
       0x124a85000 -        0x124a9afff +_odepack.cpython-38-darwin.so (0) <003B0C71-2C09-3208-B68E-ED6F42A54CA3> /Users/USER/Desktop/*/_odepack.cpython-38-darwin.so
       0x124adf000 -        0x124af7fff +_quadpack.cpython-38-darwin.so (0) <E88E6FA6-3506-3636-9EE6-356D9A7C5634> /Users/USER/Desktop/*/_quadpack.cpython-38-darwin.so
       0x124afd000 -        0x124b33fff +vode.cpython-38-darwin.so (0) <9680834C-032F-3870-A06F-0B14717008D5> /Users/USER/Desktop/*/vode.cpython-38-darwin.so
       0x124b3b000 -        0x124b53fff +_dop.cpython-38-darwin.so (0) <9D8B5ECC-BFA4-3404-B20C-09B25073929A> /Users/USER/Desktop/*/_dop.cpython-38-darwin.so
       0x124b5a000 -        0x124b71fff +lsoda.cpython-38-darwin.so (0) <C05CF6C3-180F-303D-AA17-E74A9BF8B69D> /Users/USER/Desktop/*/lsoda.cpython-38-darwin.so
       0x124c39000 -        0x124c6dfff +_fitpack.cpython-38-darwin.so (0) <AC3C3393-E8F0-35EC-A47D-D5EBED2F18A1> /Users/USER/Desktop/*/_fitpack.cpython-38-darwin.so
       0x124c74000 -        0x124cd1fff +dfitpack.cpython-38-darwin.so (0) <FC23D58A-5601-3485-ACF3-6CE133CA40E5> /Users/USER/Desktop/*/dfitpack.cpython-38-darwin.so
       0x124d22000 -        0x124d4cfff +_bspl.cpython-38-darwin.so (0) <A382F257-C8CE-3890-BB67-99F0E7CDF43B> /Users/USER/Desktop/*/_bspl.cpython-38-darwin.so
       0x124d67000 -        0x124da3fff +_ppoly.cpython-38-darwin.so (0) <7556E6CE-0B89-38ED-ADE1-2C21B1DBC6FE> /Users/USER/Desktop/*/_ppoly.cpython-38-darwin.so
       0x124dc0000 -        0x124dfbfff +interpnd.cpython-38-darwin.so (0) <975C9F1B-BDF4-3224-A379-B549E25CA7D6> /Users/USER/Desktop/*/interpnd.cpython-38-darwin.so
       0x124e1d000 -        0x124e82fff +_stats.cpython-38-darwin.so (0) <5AB5F7A9-2797-3F07-A1D2-044DADC178EF> /Users/USER/Desktop/*/_stats.cpython-38-darwin.so
       0x124eb2000 -        0x1250a4fff +cython_special.cpython-38-darwin.so (0) <06BF8531-93A5-38CD-847A-19214E954653> /Users/USER/Desktop/*/cython_special.cpython-38-darwin.so
       0x1252b3000 -        0x1252bffff +statlib.cpython-38-darwin.so (0) <5312E161-B0B4-3AF2-9EFB-135559684285> /Users/USER/Desktop/*/statlib.cpython-38-darwin.so
       0x125304000 -        0x125316fff +mvn.cpython-38-darwin.so (0) <2CA18312-0999-3E73-ABBC-957DF84BCD44> /Users/USER/Desktop/*/mvn.cpython-38-darwin.so
       0x125594000 -        0x12559bfff +_tkinter.cpython-38-darwin.so (0) <4B394C76-4AD4-37A0-9A6F-FFFAF3F9CD0A> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_tkinter.cpython-38-darwin.so
       0x1256ec000 -        0x125786fff +_regex.cpython-38-darwin.so (0) <147F83FC-833D-3004-863C-056759671B95> /Users/USER/Desktop/*/_regex.cpython-38-darwin.so
       0x125921000 -        0x125924fff +_check_build.cpython-38-darwin.so (0) <0F4C9939-6E1B-33EA-9926-B8807491F556> /Users/USER/Desktop/*/_check_build.cpython-38-darwin.so
       0x12592d000 -        0x125988fff +libomp.dylib (0) <E15A8558-585A-32DD-88D0-8FFAD9B4DC85> /Users/USER/Desktop/*/libomp.dylib
       0x125a01000 -        0x125a0cfff +murmurhash.cpython-38-darwin.so (0) <3D4C888C-3396-38AE-8279-5CECA8741383> /Users/USER/Desktop/*/murmurhash.cpython-38-darwin.so
       0x125a59000 -        0x125a5cfff +_multiprocessing.cpython-38-darwin.so (0) <ACC95E67-FFA1-3CFD-8202-2A6BA49AA850> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_multiprocessing.cpython-38-darwin.so
       0x125aa9000 -        0x125aacfff +mmap.cpython-38-darwin.so (0) <8B97516D-228B-3FA4-AD8D-1D4167253EC1> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/mmap.cpython-38-darwin.so
       0x125d39000 -        0x125d3cfff +_openmp_helpers.cpython-38-darwin.so (0) <8652C37C-AC46-3734-AAB9-98FD91A7BE89> /Users/USER/Desktop/*/_openmp_helpers.cpython-38-darwin.so
       0x125d45000 -        0x125d4cfff +_csv.cpython-38-darwin.so (0) <BCFE6BF2-1250-35ED-AA14-380699416421> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/_csv.cpython-38-darwin.so
       0x125d99000 -        0x125da4fff +_hashing_fast.cpython-38-darwin.so (0) <982A27A3-173A-32BE-A9A7-649624AF2CA4> /Users/USER/Desktop/*/_hashing_fast.cpython-38-darwin.so
       0x125df1000 -        0x125e08fff +_logistic_sigmoid.cpython-38-darwin.so (0) <0F94CB01-3BFE-3BF1-AF2E-24A2924EAD5C> /Users/USER/Desktop/*/_logistic_sigmoid.cpython-38-darwin.so
       0x125e1d000 -        0x125ea4fff +sparsefuncs_fast.cpython-38-darwin.so (0) <19E027A9-8DCE-324F-914E-E06BF4F1BAA4> /Users/USER/Desktop/*/sparsefuncs_fast.cpython-38-darwin.so
       0x125ed5000 -        0x125efcfff +_csr_polynomial_expansion.cpython-38-darwin.so (0) <F83C41B5-26A4-31B0-8FA6-48C11E26792B> /Users/USER/Desktop/*/_csr_polynomial_expansion.cpython-38-darwin.so
       0x125f99000 -        0x125fa4fff +_random.cpython-38-darwin.so (0) <D01AC1E4-A741-3C48-A471-594E3FDD7EE9> /Users/USER/Desktop/*/_random.cpython-38-darwin.so
       0x125fb1000 -        0x125fbcfff +_svmlight_format_fast.cpython-38-darwin.so (0) <50717879-307A-3069-842C-B4FBD15C574A> /Users/USER/Desktop/*/_svmlight_format_fast.cpython-38-darwin.so
       0x126009000 -        0x12600dfff +mio_utils.cpython-38-darwin.so (0) <28B4EF3A-522E-37DB-9C52-91A072A228F8> /Users/USER/Desktop/*/mio_utils.cpython-38-darwin.so
       0x126014000 -        0x126034fff +mio5_utils.cpython-38-darwin.so (0) <11D56E09-9C71-3031-B9FA-5AEB64E8D10F> /Users/USER/Desktop/*/mio5_utils.cpython-38-darwin.so
       0x12604c000 -        0x126059fff +streams.cpython-38-darwin.so (0) <04BC1417-121B-3F97-8E75-515CC90A1B15> /Users/USER/Desktop/*/streams.cpython-38-darwin.so
       0x1260a6000 -        0x1260e5fff +_libsvm.cpython-38-darwin.so (0) <4D19D10F-7FF3-302D-A590-AFBDE906441D> /Users/USER/Desktop/*/_libsvm.cpython-38-darwin.so
       0x126106000 -        0x126145fff +_cython_blas.cpython-38-darwin.so (0) <7A6024CF-3C2F-3BD2-AE1B-728E17274687> /Users/USER/Desktop/*/_cython_blas.cpython-38-darwin.so
       0x12616a000 -        0x126181fff +_liblinear.cpython-38-darwin.so (0) <F86A9C70-DD84-340F-9C54-AD4BBA541016> /Users/USER/Desktop/*/_liblinear.cpython-38-darwin.so
       0x1261d2000 -        0x126209fff +_libsvm_sparse.cpython-38-darwin.so (0) <23EF1566-5D7B-30EB-9E06-AFE541830D25> /Users/USER/Desktop/*/_libsvm_sparse.cpython-38-darwin.so
       0x126226000 -        0x12623dfff +_seq_dataset.cpython-38-darwin.so (0) <F17EAA2C-637F-31DE-B980-2A23D582BA7C> /Users/USER/Desktop/*/_seq_dataset.cpython-38-darwin.so
       0x126252000 -        0x126271fff +arrayfuncs.cpython-38-darwin.so (0) <ADF1654B-CB36-3B99-8A7C-A15B7772EB2E> /Users/USER/Desktop/*/arrayfuncs.cpython-38-darwin.so
       0x12630a000 -        0x12631dfff +_expected_mutual_info_fast.cpython-38-darwin.so (0) <F2703F79-F2C7-328E-9AAF-50A6C6155568> /Users/USER/Desktop/*/_expected_mutual_info_fast.cpython-38-darwin.so
       0x12632e000 -        0x126351fff +_pairwise_fast.cpython-38-darwin.so (0) <609C21B6-6257-3EF1-A38A-06D29FD5B6D9> /Users/USER/Desktop/*/_pairwise_fast.cpython-38-darwin.so
       0x1263aa000 -        0x1263f5fff +_cd_fast.cpython-38-darwin.so (0) <4C166128-169D-3D01-AFF0-8AE0981D6CC4> /Users/USER/Desktop/*/_cd_fast.cpython-38-darwin.so
       0x12645e000 -        0x126485fff +_sgd_fast.cpython-38-darwin.so (0) <C0E02C1F-BDB8-3A92-828D-0C4F1FC908DE> /Users/USER/Desktop/*/_sgd_fast.cpython-38-darwin.so
       0x1264aa000 -        0x1264c5fff +_weight_vector.cpython-38-darwin.so (0) <128CF2D7-84F4-36C9-A511-720F3296B556> /Users/USER/Desktop/*/_weight_vector.cpython-38-darwin.so
       0x12651a000 -        0x126535fff +_sag_fast.cpython-38-darwin.so (0) <5C48FAB9-B4CC-3B6C-B7E5-B19CC9AFED57> /Users/USER/Desktop/*/_sag_fast.cpython-38-darwin.so
       0x126a0a000 -        0x126af9fff +interval.cpython-38-darwin.so (0) <18E0B06C-0CB2-3F57-BC80-27AF66DAEC89> /Users/USER/Desktop/*/interval.cpython-38-darwin.so
       0x126b5c000 -        0x126c40fff +hashtable.cpython-38-darwin.so (0) <1C510A12-09C2-30A1-982A-FAEA9B433763> /Users/USER/Desktop/*/hashtable.cpython-38-darwin.so
       0x126cb8000 -        0x126cdbfff +missing.cpython-38-darwin.so (0) <DC89B520-651D-3432-9B1E-BCA149596D5E> /Users/USER/Desktop/*/missing.cpython-38-darwin.so
       0x126cf9000 -        0x126d0bfff +dtypes.cpython-38-darwin.so (0) <94CC39E5-6DCE-3154-BFFE-823C5A3B6FE9> /Users/USER/Desktop/*/dtypes.cpython-38-darwin.so
       0x126d21000 -        0x126d52fff +conversion.cpython-38-darwin.so (0) <AEBC59A6-71C4-3C0D-9B9C-21A866286599> /Users/USER/Desktop/*/conversion.cpython-38-darwin.so
       0x126d70000 -        0x126d74fff +base.cpython-38-darwin.so (0) <52AAEEE3-8301-3C98-9B32-5EEEEB8E9006> /Users/USER/Desktop/*/base.cpython-38-darwin.so
       0x126d7b000 -        0x126d9ffff +nattype.cpython-38-darwin.so (0) <DDD444CB-B4BB-39CE-9CC2-8DEE60B48B61> /Users/USER/Desktop/*/nattype.cpython-38-darwin.so
       0x126dc1000 -        0x126dc9fff +np_datetime.cpython-38-darwin.so (0) <9C1336EB-6A4A-3305-A0A9-030D066D26AF> /Users/USER/Desktop/*/np_datetime.cpython-38-darwin.so
       0x126e11000 -        0x126e38fff +timezones.cpython-38-darwin.so (0) <92A13212-62D2-3133-BF83-CFC913FBE4EC> /Users/USER/Desktop/*/timezones.cpython-38-darwin.so
       0x126e53000 -        0x126e8ffff +tzconversion.cpython-38-darwin.so (0) <1C0A81A9-1C37-398F-B068-C669A44B1616> /Users/USER/Desktop/*/tzconversion.cpython-38-darwin.so
       0x126ead000 -        0x126eb5fff +ccalendar.cpython-38-darwin.so (0) <BFFA36A2-095E-3485-A338-E7C0379EBA33> /Users/USER/Desktop/*/ccalendar.cpython-38-darwin.so
       0x126ec0000 -        0x126f08fff +parsing.cpython-38-darwin.so (0) <713AD079-8C65-3378-9EFE-0A084A89A7E0> /Users/USER/Desktop/*/parsing.cpython-38-darwin.so
       0x126f37000 -        0x126ff0fff +offsets.cpython-38-darwin.so (0) <01E85DDB-FDFA-3EF0-A27E-4E67D9842512> /Users/USER/Desktop/*/offsets.cpython-38-darwin.so
       0x12705c000 -        0x1270a8fff +timedeltas.cpython-38-darwin.so (0) <E5579839-6314-30A9-BD2F-0096B8FE2CAB> /Users/USER/Desktop/*/timedeltas.cpython-38-darwin.so
       0x1270dc000 -        0x12712afff +timestamps.cpython-38-darwin.so (0) <3CF74AB7-D2C1-3CF4-B258-DAE5E90DF1A5> /Users/USER/Desktop/*/timestamps.cpython-38-darwin.so
       0x1271a2000 -        0x1271cdfff +fields.cpython-38-darwin.so (0) <36ADAC57-5E9D-32B9-B219-DFC21E5CC40E> /Users/USER/Desktop/*/fields.cpython-38-darwin.so
       0x1271ea000 -        0x12722cfff +strptime.cpython-38-darwin.so (0) <B19FCF85-5E50-31EB-9ECC-946671C32505> /Users/USER/Desktop/*/strptime.cpython-38-darwin.so
       0x127259000 -        0x127261fff +properties.cpython-38-darwin.so (0) <9AA35AFA-C1B6-375A-BB7F-46F39E238818> /Users/USER/Desktop/*/properties.cpython-38-darwin.so
       0x12726b000 -        0x1272b1fff +period.cpython-38-darwin.so (0) <98124E6B-DA10-3959-9E06-01660A0B8DE8> /Users/USER/Desktop/*/period.cpython-38-darwin.so
       0x1272e2000 -        0x127307fff +vectorized.cpython-38-darwin.so (0) <E3FD991F-F289-3E69-9F0F-27B243248798> /Users/USER/Desktop/*/vectorized.cpython-38-darwin.so
       0x12731e000 -        0x127324fff +ops_dispatch.cpython-38-darwin.so (0) <061CC90B-8410-31E5-ADCB-6E6F06BA56D3> /Users/USER/Desktop/*/ops_dispatch.cpython-38-darwin.so
       0x12736e000 -        0x1274d0fff +algos.cpython-38-darwin.so (0) <5AF2FB83-245F-35C5-900D-20ECA1BBA7BA> /Users/USER/Desktop/*/algos.cpython-38-darwin.so
       0x12753d000 -        0x1275a0fff +lib.cpython-38-darwin.so (0) <16F948EE-D260-379B-8E65-78BD7B720778> /Users/USER/Desktop/*/lib.cpython-38-darwin.so
       0x1275e6000 -        0x127605fff +tslib.cpython-38-darwin.so (0) <872810E7-C27A-3EF9-9756-52D247CB4FCE> /Users/USER/Desktop/*/tslib.cpython-38-darwin.so
       0x127697000 -        0x1276b1fff +hashing.cpython-38-darwin.so (0) <ED1D4B58-8231-3FC3-B08E-A078221D162E> /Users/USER/Desktop/*/hashing.cpython-38-darwin.so
       0x127705000 -        0x127729fff +ops.cpython-38-darwin.so (0) <1555925D-97D4-3167-9B35-FFBB5B70A3EF> /Users/USER/Desktop/*/ops.cpython-38-darwin.so
       0x127900000 -        0x127968fff +index.cpython-38-darwin.so (0) <995C96B6-F8B4-3E59-A321-D4A17D980A62> /Users/USER/Desktop/*/index.cpython-38-darwin.so
       0x1279a2000 -        0x127c15fff +join.cpython-38-darwin.so (0) <71D2E5A4-97EC-32A5-BC2A-8827222B45B6> /Users/USER/Desktop/*/join.cpython-38-darwin.so
       0x127c9f000 -        0x127d66fff +sparse.cpython-38-darwin.so (0) <66B1CE9E-F2E0-31FF-8C05-62D0B3248828> /Users/USER/Desktop/*/sparse.cpython-38-darwin.so
       0x127e1e000 -        0x127e52fff +reduction.cpython-38-darwin.so (0) <29BA0854-D827-32AD-AAF4-6893C20F75C4> /Users/USER/Desktop/*/reduction.cpython-38-darwin.so
       0x127ff1000 -        0x127ff6fff +indexing.cpython-38-darwin.so (0) <D5DDAB17-AE5D-3536-94B3-5A889D2FCD50> /Users/USER/Desktop/*/indexing.cpython-38-darwin.so
       0x127ffd000 -        0x128026fff +internals.cpython-38-darwin.so (0) <CFA67B52-3479-3A25-97A0-CA54A1816696> /Users/USER/Desktop/*/internals.cpython-38-darwin.so
       0x128041000 -        0x128060fff +writers.cpython-38-darwin.so (0) <36EE55CC-9F83-3ACA-AA1E-FFD16F8DA68E> /Users/USER/Desktop/*/writers.cpython-38-darwin.so
       0x1280b8000 -        0x1280f3fff +aggregations.cpython-38-darwin.so (0) <7C2588BC-5016-3269-BA1C-89A634270960> /Users/USER/Desktop/*/aggregations.cpython-38-darwin.so
       0x12815a000 -        0x128173fff +indexers.cpython-38-darwin.so (0) <CF768FB7-5C52-386F-BFAD-A3FF474FE572> /Users/USER/Desktop/*/indexers.cpython-38-darwin.so
       0x128246000 -        0x128272fff +reshape.cpython-38-darwin.so (0) <788E6D71-C735-3512-981D-A6133894964E> /Users/USER/Desktop/*/reshape.cpython-38-darwin.so
       0x12830b000 -        0x128417fff +groupby.cpython-38-darwin.so (0) <77241D65-B3D4-3FBC-8F95-B58078A02B83> /Users/USER/Desktop/*/groupby.cpython-38-darwin.so
       0x128521000 -        0x12857efff +parsers.cpython-38-darwin.so (0) <FCAF7178-D72A-31FB-AA8C-71DA7C72DA4D> /Users/USER/Desktop/*/parsers.cpython-38-darwin.so
       0x1285f4000 -        0x128602fff +json.cpython-38-darwin.so (0) <F542426B-5C56-30EF-9DD8-5752BD0CEE03> /Users/USER/Desktop/*/json.cpython-38-darwin.so
       0x12874c000 -        0x12875bfff +testing.cpython-38-darwin.so (0) <D9F6CABD-9E0C-313D-B6A4-0F6653A0B8D0> /Users/USER/Desktop/*/testing.cpython-38-darwin.so
       0x128766000 -        0x12876dfff +cmath.cpython-38-darwin.so (0) <1000C3C0-0774-3BD8-B702-B351F65578D9> /usr/local/Cellar/python@3.8/3.8.6_2/Frameworks/Python.framework/Versions/3.8/lib/python3.8/lib-dynload/cmath.cpython-38-darwin.so
    0x7fff200b8000 -     0x7fff200b9fff  libsystem_blocks.dylib (78) <9CF131C6-16FB-3DD0-B046-9E0B6AB99935> /usr/lib/system/libsystem_blocks.dylib
    0x7fff200ba000 -     0x7fff200effff  libxpc.dylib (2038.40.38) <003A027D-9CE3-3794-A319-88495844662D> /usr/lib/system/libxpc.dylib
    0x7fff200f0000 -     0x7fff20107fff  libsystem_trace.dylib (1277.50.1) <48C14376-626E-3C81-B0F5-7416E64580C7> /usr/lib/system/libsystem_trace.dylib
    0x7fff20108000 -     0x7fff201a6fff  libcorecrypto.dylib (1000.60.19) <92F0211E-506E-3760-A3C2-808BF3905C07> /usr/lib/system/libcorecrypto.dylib
    0x7fff201a7000 -     0x7fff201d3fff  libsystem_malloc.dylib (317.40.8) <2EF43B96-90FB-3C50-B73E-035238504E33> /usr/lib/system/libsystem_malloc.dylib
    0x7fff201d4000 -     0x7fff20218fff  libdispatch.dylib (1271.40.12) <CEF1460B-1362-381A-AE69-6BCE2D8C215B> /usr/lib/system/libdispatch.dylib
    0x7fff20219000 -     0x7fff20251fff  libobjc.A.dylib (818.2) <45EA2DE2-B612-3486-B156-2359CE279159> /usr/lib/libobjc.A.dylib
    0x7fff20252000 -     0x7fff20254fff  libsystem_featureflags.dylib (28.60.1) <7B4EBDDB-244E-3F78-8895-566FE22288F3> /usr/lib/system/libsystem_featureflags.dylib
    0x7fff20255000 -     0x7fff202ddfff  libsystem_c.dylib (1439.40.11) <06D9F593-C815-385D-957F-2B5BCC223A8A> /usr/lib/system/libsystem_c.dylib
    0x7fff202de000 -     0x7fff20333fff  libc++.1.dylib (904.4) <AE3A940A-7A9C-3F99-B175-3511528D8DFE> /usr/lib/libc++.1.dylib
    0x7fff20334000 -     0x7fff2034cfff  libc++abi.dylib (904.4) <DDFCBF9C-432D-3B8A-8641-578D2EDDCAD8> /usr/lib/libc++abi.dylib
    0x7fff2034d000 -     0x7fff2037bfff  libsystem_kernel.dylib (7195.60.75) <4BD61365-29AF-3234-8002-D989D295FDBB> /usr/lib/system/libsystem_kernel.dylib
    0x7fff2037c000 -     0x7fff20387fff  libsystem_pthread.dylib (454.60.1) <8DD3A0BC-2C92-31E3-BBAB-CE923A4342E4> /usr/lib/system/libsystem_pthread.dylib
    0x7fff20388000 -     0x7fff203c2fff  libdyld.dylib (832.7.1) <2F8A14F5-7CB8-3EDD-85EA-7FA960BBC04E> /usr/lib/system/libdyld.dylib
    0x7fff203c3000 -     0x7fff203ccfff  libsystem_platform.dylib (254.60.1) <3F7F6461-7B5C-3197-ACD7-C8A0CFCC6F55> /usr/lib/system/libsystem_platform.dylib
    0x7fff203cd000 -     0x7fff203f8fff  libsystem_info.dylib (542.40.3) <0979757C-5F0D-3F5A-9E0E-EBF234B310AF> /usr/lib/system/libsystem_info.dylib
    0x7fff203f9000 -     0x7fff20894fff  com.apple.CoreFoundation (6.9 - 1770.300) <EAC298C4-CE3E-3551-A832-42ED9A13EF74> /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation
    0x7fff20895000 -     0x7fff20ac4fff  com.apple.LaunchServices (1122.11 - 1122.11) <CAEEC254-68AE-39B5-8452-EC3E1EE8577B> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/LaunchServices.framework/Versions/A/LaunchServices
    0x7fff20ac5000 -     0x7fff20b98fff  com.apple.gpusw.MetalTools (1.0 - 1) <C235D5FA-0B9D-3E72-A8CE-67174E1B9E7C> /System/Library/PrivateFrameworks/MetalTools.framework/Versions/A/MetalTools
    0x7fff20b99000 -     0x7fff20dfcfff  libBLAS.dylib (1336.40.1) <AD2D155C-1294-3D10-817A-F6A581E6ACF1> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
    0x7fff20dfd000 -     0x7fff20e4afff  com.apple.Lexicon-framework (1.0 - 86.1) <D54364A6-1C4C-33D7-8B24-C753777B3654> /System/Library/PrivateFrameworks/Lexicon.framework/Versions/A/Lexicon
    0x7fff20e4b000 -     0x7fff20eb9fff  libSparse.dylib (106) <60559226-6E4B-3601-B6CA-E3B85B5EB27B> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libSparse.dylib
    0x7fff20eba000 -     0x7fff20f37fff  com.apple.SystemConfiguration (1.20 - 1.20) <8524EE4C-628F-315A-9531-44DD83CE275E> /System/Library/Frameworks/SystemConfiguration.framework/Versions/A/SystemConfiguration
    0x7fff20f38000 -     0x7fff20f6dfff  libCRFSuite.dylib (50) <6CA29EAA-0585-3682-9AD2-DFD3D87A74D4> /usr/lib/libCRFSuite.dylib
    0x7fff20f6e000 -     0x7fff211a5fff  libmecabra.dylib (929.1.1) <39F5AD50-3AF2-3CFB-BD21-2DC45AA92A91> /usr/lib/libmecabra.dylib
    0x7fff211a6000 -     0x7fff21509fff  com.apple.Foundation (6.9 - 1770.300) <44A7115B-7FF0-3300-B61B-0FA71B63C715> /System/Library/Frameworks/Foundation.framework/Versions/C/Foundation
    0x7fff2150a000 -     0x7fff215f6fff  com.apple.LanguageModeling (1.0 - 247.1) <BCB1F8A7-54B9-36D1-B742-70DF7657BF0B> /System/Library/PrivateFrameworks/LanguageModeling.framework/Versions/A/LanguageModeling
    0x7fff215f7000 -     0x7fff2172dfff  com.apple.CoreDisplay (231.3 - 231.3) <229BF97A-1D56-3CB4-8338-E0D464F73A33> /System/Library/Frameworks/CoreDisplay.framework/Versions/A/CoreDisplay
    0x7fff2172e000 -     0x7fff219a3fff  com.apple.audio.AudioToolboxCore (1.0 - 1180.23) <56821802-07B9-3FA9-AF73-D943BAE0DE57> /System/Library/PrivateFrameworks/AudioToolboxCore.framework/Versions/A/AudioToolboxCore
    0x7fff219a4000 -     0x7fff21b8cfff  com.apple.CoreText (677.2.0.5 - 677.2.0.5) <B0B2A8DD-A6F1-3EF7-9351-1BA604353A11> /System/Library/Frameworks/CoreText.framework/Versions/A/CoreText
    0x7fff21b8d000 -     0x7fff22230fff  com.apple.audio.CoreAudio (5.0 - 5.0) <DF623EC9-FC55-3B3C-94FF-6A5C50A981B3> /System/Library/Frameworks/CoreAudio.framework/Versions/A/CoreAudio
    0x7fff22231000 -     0x7fff22582fff  com.apple.security (7.0 - 59754.60.13) <A20AB68D-51DA-340B-B813-F2AFC81F7143> /System/Library/Frameworks/Security.framework/Versions/A/Security
    0x7fff22583000 -     0x7fff227e4fff  libicucore.A.dylib (66109) <6C0A0196-2778-3035-81CE-7CA48D6C0628> /usr/lib/libicucore.A.dylib
    0x7fff227e5000 -     0x7fff227eefff  libsystem_darwin.dylib (1439.40.11) <BD269412-C9D0-32EE-B42B-B09A187A9B95> /usr/lib/system/libsystem_darwin.dylib
    0x7fff227ef000 -     0x7fff22ad6fff  com.apple.CoreServices.CarbonCore (1307 - 1307) <9C615967-6D8E-307F-B028-6278A4FA7C8C> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/CarbonCore.framework/Versions/A/CarbonCore
    0x7fff22ad7000 -     0x7fff22b15fff  com.apple.CoreServicesInternal (476 - 476) <FD1692F7-A4B4-3FE5-B9C8-E0840D53C7D0> /System/Library/PrivateFrameworks/CoreServicesInternal.framework/Versions/A/CoreServicesInternal
    0x7fff22b16000 -     0x7fff22b50fff  com.apple.CSStore (1122.11 - 1122.11) <088D0108-AA14-3610-86A0-89D0C605384F> /System/Library/PrivateFrameworks/CoreServicesStore.framework/Versions/A/CoreServicesStore
    0x7fff22b51000 -     0x7fff22bfefff  com.apple.framework.IOKit (2.0.2 - 1845.60.2) <F2299682-5884-363F-9069-AA804E712C74> /System/Library/Frameworks/IOKit.framework/Versions/A/IOKit
    0x7fff22bff000 -     0x7fff22c0afff  libsystem_notify.dylib (279.40.4) <98D74EEF-60D9-3665-B877-7BE1558BA83E> /usr/lib/system/libsystem_notify.dylib
    0x7fff22c57000 -     0x7fff239b9fff  com.apple.AppKit (6.9 - 2022.20.119) <4CB42914-672D-3AF0-A0A5-2209088A3DA0> /System/Library/Frameworks/AppKit.framework/Versions/C/AppKit
    0x7fff239ba000 -     0x7fff23c0dfff  com.apple.UIFoundation (1.0 - 726.11) <71C63CE5-094D-34AF-B538-8DCAB3B66DE9> /System/Library/PrivateFrameworks/UIFoundation.framework/Versions/A/UIFoundation
    0x7fff23c0e000 -     0x7fff23c20fff  com.apple.UniformTypeIdentifiers (633.0.2 - 633.0.2) <7BEC7DDC-2B7A-3B5D-B994-5FA352FC485A> /System/Library/Frameworks/UniformTypeIdentifiers.framework/Versions/A/UniformTypeIdentifiers
    0x7fff24078000 -     0x7fff246bbfff  libnetwork.dylib (2288.60.5) <180FE916-8DD6-3385-B231-0C423B7D2BD3> /usr/lib/libnetwork.dylib
    0x7fff246bc000 -     0x7fff24b59fff  com.apple.CFNetwork (1209.1 - 1209.1) <60DE4CD6-B5AF-3E0E-8AF1-39ECFC1B8C98> /System/Library/Frameworks/CFNetwork.framework/Versions/A/CFNetwork
    0x7fff24b5a000 -     0x7fff24b68fff  libsystem_networkextension.dylib (1295.60.5) <F476B1CB-3561-30C5-A78E-44E99B1720A3> /usr/lib/system/libsystem_networkextension.dylib
    0x7fff24b69000 -     0x7fff24b69fff  libenergytrace.dylib (22) <9BE5E51A-F531-3D59-BBBC-486FFF97BD30> /usr/lib/libenergytrace.dylib
    0x7fff24b6a000 -     0x7fff24bc5fff  libMobileGestalt.dylib (978.60.2) <F721B185-0E6A-396B-A50F-0F55810D9A67> /usr/lib/libMobileGestalt.dylib
    0x7fff24bc6000 -     0x7fff24bdcfff  libsystem_asl.dylib (385) <940C5BB9-4928-3A63-97F2-132797C8B7E5> /usr/lib/system/libsystem_asl.dylib
    0x7fff24bdd000 -     0x7fff24bf4fff  com.apple.TCC (1.0 - 1) <457D5F24-A346-38FC-8FA1-43B0C835E035> /System/Library/PrivateFrameworks/TCC.framework/Versions/A/TCC
    0x7fff24bf5000 -     0x7fff24f5afff  com.apple.SkyLight (1.600.0 - 569.6) <35876384-45F9-3C62-995B-38EC31BE75D7> /System/Library/PrivateFrameworks/SkyLight.framework/Versions/A/SkyLight
    0x7fff24f5b000 -     0x7fff255eefff  com.apple.CoreGraphics (2.0 - 1463.2.2) <323F725F-CB03-3AAD-AFBC-37B430B3FD4E> /System/Library/Frameworks/CoreGraphics.framework/Versions/A/CoreGraphics
    0x7fff255ef000 -     0x7fff256e5fff  com.apple.ColorSync (4.13.0 - 3472) <7387EBC7-CBD9-34FE-B4A3-345E4750FD81> /System/Library/Frameworks/ColorSync.framework/Versions/A/ColorSync
    0x7fff256e6000 -     0x7fff25741fff  com.apple.HIServices (1.22 - 713) <9AF2CDD9-8B68-3606-8C9E-1842420ACDA7> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/HIServices.framework/Versions/A/HIServices
    0x7fff25aed000 -     0x7fff25f0bfff  com.apple.CoreData (120 - 1044.3) <76179A55-CA89-3967-A0A7-C419DB735983> /System/Library/Frameworks/CoreData.framework/Versions/A/CoreData
    0x7fff25f0c000 -     0x7fff25f22fff  com.apple.ProtocolBuffer (1 - 285.20.8.8.1) <8EE538E7-2BB1-3E29-8FC3-938335998B22> /System/Library/PrivateFrameworks/ProtocolBuffer.framework/Versions/A/ProtocolBuffer
    0x7fff25f23000 -     0x7fff260e2fff  libsqlite3.dylib (321.1) <D7017429-8D46-3ECB-8B70-4625C74918F3> /usr/lib/libsqlite3.dylib
    0x7fff26160000 -     0x7fff26178fff  com.apple.commonutilities (8.0 - 900) <76711775-FF46-38CA-88F3-B4201C285C7F> /System/Library/PrivateFrameworks/CommonUtilities.framework/Versions/A/CommonUtilities
    0x7fff26179000 -     0x7fff261fafff  com.apple.BaseBoard (526 - 526) <38C24B3A-8226-3FD5-8C28-B11D02747B56> /System/Library/PrivateFrameworks/BaseBoard.framework/Versions/A/BaseBoard
    0x7fff261fb000 -     0x7fff26246fff  com.apple.RunningBoardServices (1.0 - 505.60.2) <F99A0D0C-D063-3E3F-8D1F-0E0B35E7CE2C> /System/Library/PrivateFrameworks/RunningBoardServices.framework/Versions/A/RunningBoardServices
    0x7fff26247000 -     0x7fff262bcfff  com.apple.AE (918.0.1 - 918.0.1) <3A298716-A130-345E-B8FF-74194849015E> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/AE.framework/Versions/A/AE
    0x7fff262bd000 -     0x7fff262c3fff  libdns_services.dylib (1310.60.4) <61EB26AD-C09E-3140-955E-16BF7DD2D6E3> /usr/lib/libdns_services.dylib
    0x7fff262c4000 -     0x7fff262cbfff  libsystem_symptoms.dylib (1431.60.1) <88F35AAC-746F-3176-81DF-49CE3D285636> /usr/lib/system/libsystem_symptoms.dylib
    0x7fff262cc000 -     0x7fff26450fff  com.apple.Network (1.0 - 1) <EED4099E-B17C-3E0B-AA8F-78A2D4F26CBF> /System/Library/Frameworks/Network.framework/Versions/A/Network
    0x7fff26451000 -     0x7fff26475fff  com.apple.analyticsd (1.0 - 1) <99FE0234-454F-36FF-9DE9-36B94D8753F9> /System/Library/PrivateFrameworks/CoreAnalytics.framework/Versions/A/CoreAnalytics
    0x7fff26476000 -     0x7fff26478fff  libDiagnosticMessagesClient.dylib (112) <1014A32B-89EE-3ADD-971F-9CB973172F69> /usr/lib/libDiagnosticMessagesClient.dylib
    0x7fff26479000 -     0x7fff264c5fff  com.apple.spotlight.metadata.utilities (1.0 - 2150.7.2) <37A1E760-2006-366C-9FAC-FB70227393FB> /System/Library/PrivateFrameworks/MetadataUtilities.framework/Versions/A/MetadataUtilities
    0x7fff264c6000 -     0x7fff26560fff  com.apple.Metadata (10.7.0 - 2150.7.2) <509C6597-ABB2-3B81-8E09-C51A755CCDA2> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/Metadata.framework/Versions/A/Metadata
    0x7fff26561000 -     0x7fff26567fff  com.apple.DiskArbitration (2.7 - 2.7) <83DED679-BE65-3475-8AFF-D664BBAFA60A> /System/Library/Frameworks/DiskArbitration.framework/Versions/A/DiskArbitration
    0x7fff26568000 -     0x7fff26c0efff  com.apple.vImage (8.1 - 544) <305D97CC-B47C-32FD-9EC5-43259A469A14> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vImage.framework/Versions/A/vImage
    0x7fff26c0f000 -     0x7fff26edcfff  com.apple.QuartzCore (1.11 - 925.5) <D59138DC-10CD-3DF8-9F04-CCDB6102C370> /System/Library/Frameworks/QuartzCore.framework/Versions/A/QuartzCore
    0x7fff26edd000 -     0x7fff26f1efff  libFontRegistry.dylib (309) <790676A3-2B74-3239-A60D-429069933542> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/Resources/libFontRegistry.dylib
    0x7fff26f1f000 -     0x7fff27060fff  com.apple.coreui (2.1 - 689.4) <0DA8F4E0-9473-374E-8B48-F0A40AEC63CE> /System/Library/PrivateFrameworks/CoreUI.framework/Versions/A/CoreUI
    0x7fff2714d000 -     0x7fff27158fff  com.apple.PerformanceAnalysis (1.275 - 275) <2F811EE6-D4D4-347E-B4A0-961F0DF050E5> /System/Library/PrivateFrameworks/PerformanceAnalysis.framework/Versions/A/PerformanceAnalysis
    0x7fff27159000 -     0x7fff27168fff  com.apple.OpenDirectory (11.1 - 230.40.1) <7710743E-6F55-342E-88FA-18796CF83700> /System/Library/Frameworks/OpenDirectory.framework/Versions/A/OpenDirectory
    0x7fff27169000 -     0x7fff27188fff  com.apple.CFOpenDirectory (11.1 - 230.40.1) <32ECCB06-56D8-3704-935B-7D5363B2988E> /System/Library/Frameworks/OpenDirectory.framework/Versions/A/Frameworks/CFOpenDirectory.framework/Versions/A/CFOpenDirectory
    0x7fff27189000 -     0x7fff27191fff  com.apple.CoreServices.FSEvents (1290.40.2 - 1290.40.2) <FB18B8D7-C7F5-3CAB-B538-3F4B4E85D1F1> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/FSEvents.framework/Versions/A/FSEvents
    0x7fff27192000 -     0x7fff271b6fff  com.apple.coreservices.SharedFileList (144 - 144) <93D2192D-7A27-3FD4-B3AB-A4DCBF8419B7> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/SharedFileList.framework/Versions/A/SharedFileList
    0x7fff271b7000 -     0x7fff271b9fff  libapp_launch_measurement.dylib (14.1) <9E2700C3-E993-3695-988E-FEF798B75E34> /usr/lib/libapp_launch_measurement.dylib
    0x7fff271ba000 -     0x7fff27202fff  com.apple.CoreAutoLayout (1.0 - 21.10.1) <998BC461-F4F5-396E-9798-1C8126AD61DA> /System/Library/PrivateFrameworks/CoreAutoLayout.framework/Versions/A/CoreAutoLayout
    0x7fff27203000 -     0x7fff272e5fff  libxml2.2.dylib (34.8) <68396181-8100-390C-8886-EFB79F5B484C> /usr/lib/libxml2.2.dylib
    0x7fff272e6000 -     0x7fff27332fff  com.apple.CoreVideo (1.8 - 408.4) <0D5AD16E-A871-3ACB-B910-39B87928E937> /System/Library/Frameworks/CoreVideo.framework/Versions/A/CoreVideo
    0x7fff27333000 -     0x7fff27335fff  com.apple.loginsupport (1.0 - 1) <4F860927-F6F5-3A99-A103-744CF365634F> /System/Library/PrivateFrameworks/login.framework/Versions/A/Frameworks/loginsupport.framework/Versions/A/loginsupport
    0x7fff28316000 -     0x7fff28326fff  libsystem_containermanager.dylib (318.60.1) <4ED09A19-04CC-3464-9EFB-F674932020B5> /usr/lib/system/libsystem_containermanager.dylib
    0x7fff28327000 -     0x7fff28338fff  com.apple.IOSurface (289.3 - 289.3) <A3B10665-5909-30EE-BE34-F3284D6D5975> /System/Library/Frameworks/IOSurface.framework/Versions/A/IOSurface
    0x7fff28339000 -     0x7fff28341fff  com.apple.IOAccelerator (439.52 - 439.52) <3944C92D-7838-3D2F-A453-9DB15C815D7B> /System/Library/PrivateFrameworks/IOAccelerator.framework/Versions/A/IOAccelerator
    0x7fff28342000 -     0x7fff28467fff  com.apple.Metal (244.32.7 - 244.32.7) <413B81AE-653F-3CF7-B5A4-A4391436E6D1> /System/Library/Frameworks/Metal.framework/Versions/A/Metal
    0x7fff28468000 -     0x7fff28484fff  com.apple.audio.caulk (1.0 - 70) <952BA9D4-BAD3-3319-8C17-F7BB2655F80C> /System/Library/PrivateFrameworks/caulk.framework/Versions/A/caulk
    0x7fff28485000 -     0x7fff2856efff  com.apple.CoreMedia (1.0 - 2760.6.4.6) <CBCD783B-B3C9-37B8-835C-A3BACEC35BB5> /System/Library/Frameworks/CoreMedia.framework/Versions/A/CoreMedia
    0x7fff2856f000 -     0x7fff286cbfff  libFontParser.dylib (305.2.0.6) <76C6C92A-1B16-3FB7-9EA2-7227D379C20F> /System/Library/PrivateFrameworks/FontServices.framework/libFontParser.dylib
    0x7fff286cc000 -     0x7fff289cbfff  com.apple.HIToolbox (2.1.1 - 1060.4) <93518490-429F-3E31-8344-15D479C2F4CE> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/HIToolbox.framework/Versions/A/HIToolbox
    0x7fff289cc000 -     0x7fff289dffff  com.apple.framework.DFRFoundation (1.0 - 265) <FB85651D-6221-38AF-BD6D-29BFF5830D36> /System/Library/PrivateFrameworks/DFRFoundation.framework/Versions/A/DFRFoundation
    0x7fff289e0000 -     0x7fff289e3fff  com.apple.dt.XCTTargetBootstrap (1.0 - 17500) <13ADD312-F6F5-3C03-BD3B-9331B3851285> /System/Library/PrivateFrameworks/XCTTargetBootstrap.framework/Versions/A/XCTTargetBootstrap
    0x7fff289e4000 -     0x7fff28a0dfff  com.apple.CoreSVG (1.0 - 149) <A0DAE6AE-9DDA-37B4-A087-545A242CF982> /System/Library/PrivateFrameworks/CoreSVG.framework/Versions/A/CoreSVG
    0x7fff28a0e000 -     0x7fff28c47fff  com.apple.ImageIO (3.3.0 - 2130.2.7) <0FE3D51B-EC76-3558-BD56-7BFF61A6793D> /System/Library/Frameworks/ImageIO.framework/Versions/A/ImageIO
    0x7fff28c48000 -     0x7fff28fc5fff  com.apple.CoreImage (16.1.0 - 1120.10) <46F1E4F5-DF8F-32D4-8D0C-6FCF2C27A5CD> /System/Library/Frameworks/CoreImage.framework/Versions/A/CoreImage
    0x7fff28fc6000 -     0x7fff29021fff  com.apple.MetalPerformanceShaders.MPSCore (1.0 - 1) <E2377275-53D7-31A0-AEAF-0A0273B99B92> /System/Library/Frameworks/MetalPerformanceShaders.framework/Versions/A/Frameworks/MPSCore.framework/Versions/A/MPSCore
    0x7fff29022000 -     0x7fff29025fff  libsystem_configuration.dylib (1109.60.2) <C57B346B-0A03-3F87-BCAC-87B702FA0719> /usr/lib/system/libsystem_configuration.dylib
    0x7fff29026000 -     0x7fff2902afff  libsystem_sandbox.dylib (1441.60.4) <8CE27199-D633-31D2-AB08-56380A1DA9FB> /usr/lib/system/libsystem_sandbox.dylib
    0x7fff2902b000 -     0x7fff2902cfff  com.apple.AggregateDictionary (1.0 - 1) <7F2AFEBB-FF06-3194-B691-B411F3456962> /System/Library/PrivateFrameworks/AggregateDictionary.framework/Versions/A/AggregateDictionary
    0x7fff2902d000 -     0x7fff29030fff  com.apple.AppleSystemInfo (3.1.5 - 3.1.5) <250CD2CA-E796-3CB0-9ADD-054998903B1D> /System/Library/PrivateFrameworks/AppleSystemInfo.framework/Versions/A/AppleSystemInfo
    0x7fff29031000 -     0x7fff29032fff  liblangid.dylib (136) <224DC045-2B60-39AF-B89E-E524175667F5> /usr/lib/liblangid.dylib
    0x7fff29033000 -     0x7fff290d3fff  com.apple.CoreNLP (1.0 - 245) <F876FD71-F077-3CF7-B94D-9E05A17E03D7> /System/Library/PrivateFrameworks/CoreNLP.framework/Versions/A/CoreNLP
    0x7fff290d4000 -     0x7fff290dafff  com.apple.LinguisticData (1.0 - 399) <D1B7F1D5-EB9E-3555-BA57-3611FA153C44> /System/Library/PrivateFrameworks/LinguisticData.framework/Versions/A/LinguisticData
    0x7fff290db000 -     0x7fff29797fff  libBNNS.dylib (288.60.2) <E3FF47D5-7DD9-3A9E-A819-C79B0CC17C03> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBNNS.dylib
    0x7fff29798000 -     0x7fff2996bfff  libvDSP.dylib (760.40.6) <9434101D-E001-357F-9503-9896C6011F52> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvDSP.dylib
    0x7fff2996c000 -     0x7fff2997efff  com.apple.CoreEmoji (1.0 - 128) <7CCFC59A-8746-3E52-AF1D-1B67798E940C> /System/Library/PrivateFrameworks/CoreEmoji.framework/Versions/A/CoreEmoji
    0x7fff2997f000 -     0x7fff29989fff  com.apple.IOMobileFramebuffer (343.0.0 - 343.0.0) <9A6F913C-EC79-3FC1-A92C-3A1BA96D8DFB> /System/Library/PrivateFrameworks/IOMobileFramebuffer.framework/Versions/A/IOMobileFramebuffer
    0x7fff29c80000 -     0x7fff29c90fff  com.apple.AssertionServices (1.0 - 505.60.2) <9F8620BD-A58D-3A42-9B9E-DEC21517EF1A> /System/Library/PrivateFrameworks/AssertionServices.framework/Versions/A/AssertionServices
    0x7fff29c91000 -     0x7fff29d1dfff  com.apple.securityfoundation (6.0 - 55240.40.4) <5F06D141-62F4-3405-BA72-24673B170A16> /System/Library/Frameworks/SecurityFoundation.framework/Versions/A/SecurityFoundation
    0x7fff29d1e000 -     0x7fff29d27fff  com.apple.coreservices.BackgroundTaskManagement (1.0 - 104) <C5E4B35C-FFDA-3423-890F-06DAD1F684F5> /System/Library/PrivateFrameworks/BackgroundTaskManagement.framework/Versions/A/BackgroundTaskManagement
    0x7fff29d28000 -     0x7fff29d2cfff  com.apple.xpc.ServiceManagement (1.0 - 1) <2C03BEB7-915C-3A3A-A44F-A77775E1BFD5> /System/Library/Frameworks/ServiceManagement.framework/Versions/A/ServiceManagement
    0x7fff29d2d000 -     0x7fff29d2ffff  libquarantine.dylib (119.40.2) <19D42B9D-3336-3543-AF75-6E605EA31599> /usr/lib/system/libquarantine.dylib
    0x7fff29d30000 -     0x7fff29d3bfff  libCheckFix.dylib (31) <3381FC93-F188-348C-9345-5567A7116CEF> /usr/lib/libCheckFix.dylib
    0x7fff29d3c000 -     0x7fff29d53fff  libcoretls.dylib (169) <9C244029-6B45-3583-B27F-BB7BBF84D814> /usr/lib/libcoretls.dylib
    0x7fff29d54000 -     0x7fff29d64fff  libbsm.0.dylib (68.40.1) <DC652D50-FA69-3801-9361-004D4D6832D0> /usr/lib/libbsm.0.dylib
    0x7fff29d65000 -     0x7fff29daefff  libmecab.dylib (929.1.1) <B5D8C96C-D3B8-32F8-84F9-A432CEAD4E5C> /usr/lib/libmecab.dylib
    0x7fff29daf000 -     0x7fff29db4fff  libgermantok.dylib (24) <F9772A76-7AFA-3E0B-A02C-A61FC6CA8D8B> /usr/lib/libgermantok.dylib
    0x7fff29db5000 -     0x7fff29dcafff  libLinearAlgebra.dylib (1336.40.1) <D2826FAB-174C-3CD6-A765-06D83A9A0EDB> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLinearAlgebra.dylib
    0x7fff29dcb000 -     0x7fff29ff2fff  com.apple.MetalPerformanceShaders.MPSNeuralNetwork (1.0 - 1) <231CF580-952A-32BC-A423-9B9756AC9744> /System/Library/Frameworks/MetalPerformanceShaders.framework/Versions/A/Frameworks/MPSNeuralNetwork.framework/Versions/A/MPSNeuralNetwork
    0x7fff29ff3000 -     0x7fff2a042fff  com.apple.MetalPerformanceShaders.MPSRayIntersector (1.0 - 1) <65A993E4-3DC2-3152-98D5-A1DF3DB4573F> /System/Library/Frameworks/MetalPerformanceShaders.framework/Versions/A/Frameworks/MPSRayIntersector.framework/Versions/A/MPSRayIntersector
    0x7fff2a043000 -     0x7fff2a189fff  com.apple.MLCompute (1.0 - 1) <BCEA1149-197E-398F-9424-E29B0AD0829F> /System/Library/Frameworks/MLCompute.framework/Versions/A/MLCompute
    0x7fff2a18a000 -     0x7fff2a1c0fff  com.apple.MetalPerformanceShaders.MPSMatrix (1.0 - 1) <F719DA57-EAAA-3527-B859-21025722932F> /System/Library/Frameworks/MetalPerformanceShaders.framework/Versions/A/Frameworks/MPSMatrix.framework/Versions/A/MPSMatrix
    0x7fff2a1c1000 -     0x7fff2a1fefff  com.apple.MetalPerformanceShaders.MPSNDArray (1.0 - 1) <FCCC0D3F-74D2-3107-82B3-E2B500E36AAE> /System/Library/Frameworks/MetalPerformanceShaders.framework/Versions/A/Frameworks/MPSNDArray.framework/Versions/A/MPSNDArray
    0x7fff2a1ff000 -     0x7fff2a28ffff  com.apple.MetalPerformanceShaders.MPSImage (1.0 - 1) <21527A17-2D6F-3BDF-9A74-F90FA6E26BB3> /System/Library/Frameworks/MetalPerformanceShaders.framework/Versions/A/Frameworks/MPSImage.framework/Versions/A/MPSImage
    0x7fff2a290000 -     0x7fff2a29ffff  com.apple.AppleFSCompression (125 - 1.0) <D1E7DC71-1929-30A8-B73E-268387110608> /System/Library/PrivateFrameworks/AppleFSCompression.framework/Versions/A/AppleFSCompression
    0x7fff2a2a0000 -     0x7fff2a2adfff  libbz2.1.0.dylib (44) <0575C0D0-B107-3E53-857F-DEC55998197B> /usr/lib/libbz2.1.0.dylib
    0x7fff2a2ae000 -     0x7fff2a2b2fff  libsystem_coreservices.dylib (127) <A2D875B9-8BA8-33AD-BE92-ADAB915A8D5B> /usr/lib/system/libsystem_coreservices.dylib
    0x7fff2a2b3000 -     0x7fff2a2e0fff  com.apple.CoreServices.OSServices (1122.11 - 1122.11) <870F34BE-C0ED-318B-858D-5F1E4757D552> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/OSServices.framework/Versions/A/OSServices
    0x7fff2a4b6000 -     0x7fff2a4c8fff  libz.1.dylib (76) <9F89FD60-03F7-3175-AB34-5112B99E2B8A> /usr/lib/libz.1.dylib
    0x7fff2a4c9000 -     0x7fff2a510fff  libsystem_m.dylib (3186.40.2) <79820D9E-0FF1-3F20-AF4F-F87EE20CE8C9> /usr/lib/system/libsystem_m.dylib
    0x7fff2a511000 -     0x7fff2a511fff  libcharset.1.dylib (59) <414F6A1C-1EBC-3956-AC2D-CCB0458F31AF> /usr/lib/libcharset.1.dylib
    0x7fff2a512000 -     0x7fff2a517fff  libmacho.dylib (973.4) <28AE1649-22ED-3C4D-A232-29D37F821C39> /usr/lib/system/libmacho.dylib
    0x7fff2a518000 -     0x7fff2a533fff  libkxld.dylib (7195.60.75) <3600A314-332A-343D-B45D-D9D8B302545D> /usr/lib/system/libkxld.dylib
    0x7fff2a534000 -     0x7fff2a53ffff  libcommonCrypto.dylib (60178.40.2) <1D0A75A5-DEC5-39C6-AB3D-E789B8866712> /usr/lib/system/libcommonCrypto.dylib
    0x7fff2a540000 -     0x7fff2a54afff  libunwind.dylib (200.10) <C5792A9C-DF0F-3821-BC14-238A78462E8A> /usr/lib/system/libunwind.dylib
    0x7fff2a54b000 -     0x7fff2a552fff  liboah.dylib (203.13.2) <FF72E19B-3B02-34D4-A821-3397BB28AC02> /usr/lib/liboah.dylib
    0x7fff2a553000 -     0x7fff2a55dfff  libcopyfile.dylib (173.40.2) <89483CD4-DA46-3AF2-AE78-FC37CED05ACC> /usr/lib/system/libcopyfile.dylib
    0x7fff2a55e000 -     0x7fff2a565fff  libcompiler_rt.dylib (102.2) <0DB26EC8-B4CD-3268-B865-C2FC07E4D2AA> /usr/lib/system/libcompiler_rt.dylib
    0x7fff2a566000 -     0x7fff2a568fff  libsystem_collections.dylib (1439.40.11) <D40D8097-0ABF-3645-B065-168F43ACFF4C> /usr/lib/system/libsystem_collections.dylib
    0x7fff2a569000 -     0x7fff2a56bfff  libsystem_secinit.dylib (87.60.1) <99B5FD99-1A8B-37C1-BD70-04990FA33B1C> /usr/lib/system/libsystem_secinit.dylib
    0x7fff2a56c000 -     0x7fff2a56efff  libremovefile.dylib (49.40.3) <750012C2-7097-33C3-B796-2766E6CDE8C1> /usr/lib/system/libremovefile.dylib
    0x7fff2a56f000 -     0x7fff2a56ffff  libkeymgr.dylib (31) <2C7B58B0-BE54-3A50-B399-AA49C19083A9> /usr/lib/system/libkeymgr.dylib
    0x7fff2a570000 -     0x7fff2a577fff  libsystem_dnssd.dylib (1310.60.4) <81EFC44D-450E-3AA3-AC8F-D7EF68F464B4> /usr/lib/system/libsystem_dnssd.dylib
    0x7fff2a578000 -     0x7fff2a57dfff  libcache.dylib (83) <2F7F7303-DB23-359E-85CD-8B2F93223E2A> /usr/lib/system/libcache.dylib
    0x7fff2a57e000 -     0x7fff2a57ffff  libSystem.B.dylib (1292.60.1) <A7FB4899-9E04-37ED-9DD8-8FFF0400879C> /usr/lib/libSystem.B.dylib
    0x7fff2a580000 -     0x7fff2a583fff  libfakelink.dylib (3) <34B6DC95-E19A-37C0-B9D0-558F692D85F5> /usr/lib/libfakelink.dylib
    0x7fff2a584000 -     0x7fff2a584fff  com.apple.SoftLinking (1.0 - 1) <90D679B3-DFFD-3604-B89F-1BCF70B3EBA4> /System/Library/PrivateFrameworks/SoftLinking.framework/Versions/A/SoftLinking
    0x7fff2a585000 -     0x7fff2a5bcfff  libpcap.A.dylib (98.40.1) <E1995A1C-7EEB-3340-B1E1-DD45FA625C12> /usr/lib/libpcap.A.dylib
    0x7fff2a5bd000 -     0x7fff2a6adfff  libiconv.2.dylib (59) <3E53F735-1D7E-3ABB-BC45-AAA37F535830> /usr/lib/libiconv.2.dylib
    0x7fff2a6ae000 -     0x7fff2a6bffff  libcmph.dylib (8) <865FA425-831D-3E49-BD1B-14188D2A98AA> /usr/lib/libcmph.dylib
    0x7fff2a6c0000 -     0x7fff2a731fff  libarchive.2.dylib (83.40.4) <76B2F421-5335-37FB-9CD5-1018878B9E74> /usr/lib/libarchive.2.dylib
    0x7fff2a732000 -     0x7fff2a799fff  com.apple.SearchKit (1.4.1 - 1.4.1) <7BDD2800-BDDC-3DE0-A4A8-B1E855130E3B> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/SearchKit.framework/Versions/A/SearchKit
    0x7fff2a79a000 -     0x7fff2a79bfff  libThaiTokenizer.dylib (3) <513547CD-5C7F-37BE-A2AD-55A22F279588> /usr/lib/libThaiTokenizer.dylib
    0x7fff2a79c000 -     0x7fff2a7c3fff  com.apple.applesauce (1.0 - 16.26) <AE525243-2CE7-373E-994E-C2457611EB3C> /System/Library/PrivateFrameworks/AppleSauce.framework/Versions/A/AppleSauce
    0x7fff2a7c4000 -     0x7fff2a7dbfff  libapple_nghttp2.dylib (1.41) <CC004768-6E3B-3D80-9431-61149EBE2E10> /usr/lib/libapple_nghttp2.dylib
    0x7fff2a7dc000 -     0x7fff2a7eefff  libSparseBLAS.dylib (1336.40.1) <CEBD7B0F-A54D-3A43-BD7E-E8BC2C7B7F0C> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libSparseBLAS.dylib
    0x7fff2a7ef000 -     0x7fff2a7f0fff  com.apple.MetalPerformanceShaders.MetalPerformanceShaders (1.0 - 1) <1BFEB124-CF05-342F-BC65-B233EAB661D9> /System/Library/Frameworks/MetalPerformanceShaders.framework/Versions/A/MetalPerformanceShaders
    0x7fff2a7f1000 -     0x7fff2a7f5fff  libpam.2.dylib (28.40.1) <AE84F5FA-DDB0-3028-AF25-D6B6A12DBA6A> /usr/lib/libpam.2.dylib
    0x7fff2a7f6000 -     0x7fff2a80efff  libcompression.dylib (96.40.6) <45B8B821-8EB6-34FE-92E9-5CBA474499E2> /usr/lib/libcompression.dylib
    0x7fff2a80f000 -     0x7fff2a814fff  libQuadrature.dylib (7) <FB21F53D-4A40-327F-BD3B-C7C8D08C6A86> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libQuadrature.dylib
    0x7fff2a815000 -     0x7fff2abb1fff  libLAPACK.dylib (1336.40.1) <509FBCC6-4ECB-3192-98A6-D0C030E4E9D8> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib
    0x7fff2abb2000 -     0x7fff2ac00fff  com.apple.DictionaryServices (1.2 - 341) <83CDCE83-6B48-35F1-BACF-83240D940777> /System/Library/Frameworks/CoreServices.framework/Versions/A/Frameworks/DictionaryServices.framework/Versions/A/DictionaryServices
    0x7fff2ac01000 -     0x7fff2ac19fff  liblzma.5.dylib (16) <A45348BC-AA9C-39D6-A7C3-2246A3EFA34C> /usr/lib/liblzma.5.dylib
    0x7fff2ac1a000 -     0x7fff2ac1bfff  libcoretls_cfhelpers.dylib (169) <C0F19E92-DACB-3100-8610-62DEC5E5FB81> /usr/lib/libcoretls_cfhelpers.dylib
    0x7fff2ac1c000 -     0x7fff2ad15fff  com.apple.APFS (1677.60.23 - 1677.60.23) <8271EE40-CDF5-3E0B-9F42-B49DC7C46C98> /System/Library/PrivateFrameworks/APFS.framework/Versions/A/APFS
    0x7fff2ad16000 -     0x7fff2ad23fff  libxar.1.dylib (452) <3F3DA942-DC7B-31EF-BCF1-38F99F59A660> /usr/lib/libxar.1.dylib
    0x7fff2ad24000 -     0x7fff2ad27fff  libutil.dylib (58.40.2) <85CF2B3B-6BEB-381D-8683-1DE2B0167ECC> /usr/lib/libutil.dylib
    0x7fff2ad28000 -     0x7fff2ad50fff  libxslt.1.dylib (17.2) <2C881E82-6E2C-3E92-8DC5-3C2D05FE7C95> /usr/lib/libxslt.1.dylib
    0x7fff2ad51000 -     0x7fff2ad5bfff  libChineseTokenizer.dylib (37) <36891BB5-4A83-33A3-9995-CC5DB2AB53CE> /usr/lib/libChineseTokenizer.dylib
    0x7fff2ad5c000 -     0x7fff2ae1afff  libvMisc.dylib (760.40.6) <219319E1-BDBD-34D1-97B7-E46256785D3C> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib
    0x7fff2ae1b000 -     0x7fff2aeb3fff  libate.dylib (3.0.4) <51D50D08-F614-3929-AFB1-BF4ED9BE4751> /usr/lib/libate.dylib
    0x7fff2aeb4000 -     0x7fff2aebbfff  libIOReport.dylib (64) <3C26FBDC-931E-3318-8225-C10849CF1D60> /usr/lib/libIOReport.dylib
    0x7fff2b06b000 -     0x7fff2b0befff  com.apple.AppleVAFramework (6.1.3 - 6.1.3) <8A5B1C42-DD83-303B-85DE-754FB6C10E1A> /System/Library/PrivateFrameworks/AppleVA.framework/Versions/A/AppleVA
    0x7fff2b0bf000 -     0x7fff2b0d8fff  libexpat.1.dylib (26) <4408FC72-BDAA-33AE-BE14-4008642794ED> /usr/lib/libexpat.1.dylib
    0x7fff2b0d9000 -     0x7fff2b0e2fff  libheimdal-asn1.dylib (597.40.10) <032931C8-B042-3B3D-93D3-5B3E27431FEA> /usr/lib/libheimdal-asn1.dylib
    0x7fff2b0e3000 -     0x7fff2b0f7fff  com.apple.IconFoundation (479.3 - 479.3) <650C91C9-D6A1-3FF7-964B-DE1065F2243C> /System/Library/PrivateFrameworks/IconFoundation.framework/Versions/A/IconFoundation
    0x7fff2b0f8000 -     0x7fff2b165fff  com.apple.IconServices (479.3 - 479.3) <63CAB1AB-C485-382A-9088-F6E3937BB8E9> /System/Library/PrivateFrameworks/IconServices.framework/Versions/A/IconServices
    0x7fff2b166000 -     0x7fff2b203fff  com.apple.MediaExperience (1.0 - 1) <A7A754CE-61AB-39B8-AA31-3AEB14695F55> /System/Library/PrivateFrameworks/MediaExperience.framework/Versions/A/MediaExperience
    0x7fff2b204000 -     0x7fff2b22dfff  com.apple.persistentconnection (1.0 - 1.0) <C3F975D3-A87C-353C-BA1F-072825E60E8C> /System/Library/PrivateFrameworks/PersistentConnection.framework/Versions/A/PersistentConnection
    0x7fff2b22e000 -     0x7fff2b23cfff  com.apple.GraphVisualizer (1.0 - 100.1) <7035CCDF-5B9D-365C-A1FA-1D961EBEE44D> /System/Library/PrivateFrameworks/GraphVisualizer.framework/Versions/A/GraphVisualizer
    0x7fff2b23d000 -     0x7fff2b658fff  com.apple.vision.FaceCore (4.3.2 - 4.3.2) <E0518821-B65D-31A4-8C37-DF3569CF8867> /System/Library/PrivateFrameworks/FaceCore.framework/Versions/A/FaceCore
    0x7fff2b659000 -     0x7fff2b6a3fff  com.apple.OTSVG (1.0 - 677.2.0.5) <D2722431-6C71-3144-A024-6ED06334AEE0> /System/Library/PrivateFrameworks/OTSVG.framework/Versions/A/OTSVG
    0x7fff2b6a4000 -     0x7fff2b6aafff  com.apple.xpc.AppServerSupport (1.0 - 2038.40.38) <27B96AA0-421E-3E5A-B9D8-9BA3F0D133E9> /System/Library/PrivateFrameworks/AppServerSupport.framework/Versions/A/AppServerSupport
    0x7fff2b6ab000 -     0x7fff2b6bcfff  libhvf.dylib (1.0 - $[CURRENT_PROJECT_VERSION]) <CAD78803-0F56-316E-A7F1-D2BF26CA2DD6> /System/Library/PrivateFrameworks/FontServices.framework/libhvf.dylib
    0x7fff2b6bd000 -     0x7fff2b6bffff  libspindump.dylib (295) <C6F804A3-5682-3766-A324-76667364873D> /usr/lib/libspindump.dylib
    0x7fff2b6c0000 -     0x7fff2b780fff  com.apple.Heimdal (4.0 - 2.0) <8BB18335-5DD3-3154-85C8-0145C64556A2> /System/Library/PrivateFrameworks/Heimdal.framework/Versions/A/Heimdal
    0x7fff2b91f000 -     0x7fff2b989fff  com.apple.bom (14.0 - 233) <A62EEEE5-3027-3F25-BCD9-32D36922106E> /System/Library/PrivateFrameworks/Bom.framework/Versions/A/Bom
    0x7fff2b98a000 -     0x7fff2b9d4fff  com.apple.AppleJPEG (1.0 - 1) <A2E9E2A4-AEDC-3481-BDC9-05D9AD84FC25> /System/Library/PrivateFrameworks/AppleJPEG.framework/Versions/A/AppleJPEG
    0x7fff2b9d5000 -     0x7fff2bab3fff  libJP2.dylib (2130.2.7) <9D837C01-3D6C-3D71-8E92-3673CE06A21F> /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libJP2.dylib
    0x7fff2bab4000 -     0x7fff2bab7fff  com.apple.WatchdogClient.framework (1.0 - 98.60.1) <8374BBBB-65CB-3D46-9AD6-0DD1FB99AD88> /System/Library/PrivateFrameworks/WatchdogClient.framework/Versions/A/WatchdogClient
    0x7fff2bab8000 -     0x7fff2baebfff  com.apple.MultitouchSupport.framework (4400.28 - 4400.28) <E9A95272-5E84-3B64-8263-8C7F84456269> /System/Library/PrivateFrameworks/MultitouchSupport.framework/Versions/A/MultitouchSupport
    0x7fff2baec000 -     0x7fff2bc3efff  com.apple.VideoToolbox (1.0 - 2760.6.4.6) <35098775-A188-3BE0-B0B1-7CE0027BA295> /System/Library/Frameworks/VideoToolbox.framework/Versions/A/VideoToolbox
    0x7fff2bc3f000 -     0x7fff2bc71fff  libAudioToolboxUtility.dylib (1180.23) <58B4505B-F0EA-37FC-9F5A-6F9F05B0F2A5> /usr/lib/libAudioToolboxUtility.dylib
    0x7fff2bc72000 -     0x7fff2bc98fff  libPng.dylib (2130.2.7) <1F3FED3B-FB07-3F43-8EAD-6100017FBAB5> /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libPng.dylib
    0x7fff2bc99000 -     0x7fff2bcf6fff  libTIFF.dylib (2130.2.7) <27E9A2D3-003D-3D97-AD85-BE595EA0516F> /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libTIFF.dylib
    0x7fff2bcf7000 -     0x7fff2bd11fff  com.apple.IOPresentment (53 - 37) <070919DC-978E-3DB3-80FD-FB0C1BAAE80A> /System/Library/PrivateFrameworks/IOPresentment.framework/Versions/A/IOPresentment
    0x7fff2bd12000 -     0x7fff2bd18fff  com.apple.GPUWrangler (6.2.2 - 6.2.2) <F4B3905F-C024-33C1-82C8-F1744AF8516E> /System/Library/PrivateFrameworks/GPUWrangler.framework/Versions/A/GPUWrangler
    0x7fff2bd19000 -     0x7fff2bd1cfff  libRadiance.dylib (2130.2.7) <7ABF94D2-5281-363F-A613-9C945D77AAE8> /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libRadiance.dylib
    0x7fff2bd1d000 -     0x7fff2bd22fff  com.apple.DSExternalDisplay (3.1 - 380) <BA802582-F1EB-35B8-902F-3D0F426124E0> /System/Library/PrivateFrameworks/DSExternalDisplay.framework/Versions/A/DSExternalDisplay
    0x7fff2bd23000 -     0x7fff2bd47fff  libJPEG.dylib (2130.2.7) <FDD55379-6673-31E4-B916-7949459B60AE> /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libJPEG.dylib
    0x7fff2bd48000 -     0x7fff2bd77fff  com.apple.ATSUI (1.0 - 1) <B82D099B-4F53-3B60-8BAA-975C41EFD356> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATSUI.framework/Versions/A/ATSUI
    0x7fff2bd78000 -     0x7fff2bd7cfff  libGIF.dylib (2130.2.7) <C51FB0BA-E5C0-335B-9C64-185B1DDC9166> /System/Library/Frameworks/ImageIO.framework/Versions/A/Resources/libGIF.dylib
    0x7fff2bd7d000 -     0x7fff2bd86fff  com.apple.CMCaptureCore (1.0 - 80.17.1.1) <A0D43E58-B960-3A80-8807-4115F0E1EF74> /System/Library/PrivateFrameworks/CMCaptureCore.framework/Versions/A/CMCaptureCore
    0x7fff2bd87000 -     0x7fff2bdcefff  com.apple.print.framework.PrintCore (16 - 531) <FC56A643-F502-3578-9EFF-375BE6B87691> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/PrintCore.framework/Versions/A/PrintCore
    0x7fff2bdcf000 -     0x7fff2be9bfff  com.apple.TextureIO (3.10.9 - 3.10.9) <0AC15003-4B6A-3FB3-9B41-3EF61A2BD430> /System/Library/PrivateFrameworks/TextureIO.framework/Versions/A/TextureIO
    0x7fff2be9c000 -     0x7fff2bea4fff  com.apple.InternationalSupport (1.0 - 60) <5485FFDC-CE44-37F4-865F-91B2EFBC6CAF> /System/Library/PrivateFrameworks/InternationalSupport.framework/Versions/A/InternationalSupport
    0x7fff2bea5000 -     0x7fff2bf20fff  com.apple.datadetectorscore (8.0 - 674) <A2DEEF63-7643-37AA-9420-ED875629D1B2> /System/Library/PrivateFrameworks/DataDetectorsCore.framework/Versions/A/DataDetectorsCore
    0x7fff2bf21000 -     0x7fff2bf7ffff  com.apple.UserActivity (435 - 435) <075FD354-28FD-3A13-881C-955FA9106D5C> /System/Library/PrivateFrameworks/UserActivity.framework/Versions/A/UserActivity
    0x7fff2cbcd000 -     0x7fff2cbfefff  libSessionUtility.dylib (76.7) <95615EDE-46B9-32AE-96EC-7F6E5EB6A932> /System/Library/PrivateFrameworks/AudioSession.framework/libSessionUtility.dylib
    0x7fff2cbff000 -     0x7fff2cd2ffff  com.apple.audio.toolbox.AudioToolbox (1.14 - 1.14) <D0F9F628-F241-3FA2-A785-7B9DCBB2FEC4> /System/Library/Frameworks/AudioToolbox.framework/Versions/A/AudioToolbox
    0x7fff2cd30000 -     0x7fff2cd97fff  com.apple.audio.AudioSession (1.0 - 76.7) <C0B1C9EB-A594-31E3-ADDF-118583840E6F> /System/Library/PrivateFrameworks/AudioSession.framework/Versions/A/AudioSession
    0x7fff2cd98000 -     0x7fff2cdaafff  libAudioStatistics.dylib (25.1) <1D07EA54-BE7C-37C4-AA73-5224D402F0C3> /usr/lib/libAudioStatistics.dylib
    0x7fff2cdab000 -     0x7fff2cdbafff  com.apple.speech.synthesis.framework (9.0.51 - 9.0.51) <B86A2136-8DD7-395D-BB9F-9416C56DD2D6> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/SpeechSynthesis.framework/Versions/A/SpeechSynthesis
    0x7fff2cdbb000 -     0x7fff2ce26fff  com.apple.ApplicationServices.ATS (377 - 516) <3A435648-CC5F-387E-AB37-391AAEABE314> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ATS.framework/Versions/A/ATS
    0x7fff2ce27000 -     0x7fff2ce3ffff  libresolv.9.dylib (68) <9957A6F4-8B66-3429-86CD-6DF4993EB6F5> /usr/lib/libresolv.9.dylib
    0x7fff2cf72000 -     0x7fff2d051fff  libSMC.dylib (20) <CE5162B7-379E-3DF0-9D1E-44BC98BD2422> /usr/lib/libSMC.dylib
    0x7fff2d052000 -     0x7fff2d0b1fff  libcups.2.dylib (494.1) <04A4801E-E1B5-3919-9F14-100F0C2D049B> /usr/lib/libcups.2.dylib
    0x7fff2d0b2000 -     0x7fff2d0c1fff  com.apple.LangAnalysis (1.7.0 - 254) <120945D9-B74D-3A6F-B160-2678E6B6481D> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/LangAnalysis.framework/Versions/A/LangAnalysis
    0x7fff2d0c2000 -     0x7fff2d0ccfff  com.apple.NetAuth (6.2 - 6.2) <C65B2F54-67EA-3E4D-B84A-BBA94998BD6B> /System/Library/PrivateFrameworks/NetAuth.framework/Versions/A/NetAuth
    0x7fff2d0cd000 -     0x7fff2d0d4fff  com.apple.ColorSyncLegacy (4.13.0 - 1) <33DA9348-EADF-36D2-B999-56854481D272> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/ColorSyncLegacy.framework/Versions/A/ColorSyncLegacy
    0x7fff2d0d5000 -     0x7fff2d0e0fff  com.apple.QD (4.0 - 416) <7FFC9049-7E42-372B-9105-1C4C94DE0110> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/Frameworks/QD.framework/Versions/A/QD
    0x7fff2d0e1000 -     0x7fff2d74ffff  com.apple.audio.AudioResourceArbitration (1.0 - 1) <098FD431-D302-3DD5-9AD1-453615A73E68> /System/Library/PrivateFrameworks/AudioResourceArbitration.framework/Versions/A/AudioResourceArbitration
    0x7fff2d750000 -     0x7fff2d75cfff  com.apple.perfdata (1.0 - 67.40.1) <85A57A67-8721-3035-BCEE-D4AC98332D2C> /System/Library/PrivateFrameworks/perfdata.framework/Versions/A/perfdata
    0x7fff2d75d000 -     0x7fff2d76bfff  libperfcheck.dylib (41) <67113817-A463-360A-B321-9286DC50FEDA> /usr/lib/libperfcheck.dylib
    0x7fff2d76c000 -     0x7fff2d77bfff  com.apple.Kerberos (3.0 - 1) <2E872705-0841-3695-AF79-4160D2A436AB> /System/Library/Frameworks/Kerberos.framework/Versions/A/Kerberos
    0x7fff2d77c000 -     0x7fff2d7cbfff  com.apple.GSS (4.0 - 2.0) <2A38D59F-5F3A-3779-A421-2F8128F22B95> /System/Library/Frameworks/GSS.framework/Versions/A/GSS
    0x7fff2d7cc000 -     0x7fff2d7dcfff  com.apple.CommonAuth (4.0 - 2.0) <D9431F22-A16B-3237-9676-B6159B36F5EA> /System/Library/PrivateFrameworks/CommonAuth.framework/Versions/A/CommonAuth
    0x7fff2d9b1000 -     0x7fff2d9b1fff  liblaunch.dylib (2038.40.38) <05A7EFDD-4111-3E4D-B668-239B69DE3D0F> /usr/lib/system/liblaunch.dylib
    0x7fff2fbd7000 -     0x7fff2fc02fff  com.apple.RemoteViewServices (2.0 - 163) <AC6E2D2F-8131-3A40-97D7-E24E2A45CD66> /System/Library/PrivateFrameworks/RemoteViewServices.framework/Versions/A/RemoteViewServices
    0x7fff2fc03000 -     0x7fff2fc12fff  com.apple.SpeechRecognitionCore (6.1.12 - 6.1.12) <F2A0E41A-7976-3175-959A-98DC24AAFFCC> /System/Library/PrivateFrameworks/SpeechRecognitionCore.framework/Versions/A/SpeechRecognitionCore
    0x7fff2fc13000 -     0x7fff2fc1afff  com.apple.speech.recognition.framework (6.0.3 - 6.0.3) <9C14FA0A-D905-375B-8C32-E311ED59B6AD> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/SpeechRecognition.framework/Versions/A/SpeechRecognition
    0x7fff2fe5e000 -     0x7fff2fe5efff  libsystem_product_info_filter.dylib (8.40.1) <7CCAF1A8-F570-341E-B275-0C80B092F8E0> /usr/lib/system/libsystem_product_info_filter.dylib
    0x7fff2ff39000 -     0x7fff2ff39fff  com.apple.Accelerate.vecLib (3.11 - vecLib 3.11) <510A463F-5CA5-3585-969F-2D44583B71C8> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/vecLib
    0x7fff2ff60000 -     0x7fff2ff60fff  com.apple.CoreServices (1122.11 - 1122.11) <5DDB040C-6E92-3DBE-9049-873F510F26E2> /System/Library/Frameworks/CoreServices.framework/Versions/A/CoreServices
    0x7fff3022e000 -     0x7fff3022efff  com.apple.Accelerate (1.11 - Accelerate 1.11) <F2FFCC7B-EE3D-3768-A73B-342851B53741> /System/Library/Frameworks/Accelerate.framework/Versions/A/Accelerate
    0x7fff32fae000 -     0x7fff32fb1fff  com.apple.help (1.3.8 - 71) <599F7E42-DEF1-3B70-83AB-C3BDF727CF93> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Help.framework/Versions/A/Help
    0x7fff331f3000 -     0x7fff331f3fff  com.apple.ApplicationServices (48 - 50) <7B536871-3F10-3138-B06B-9C2A3C07EC1E> /System/Library/Frameworks/ApplicationServices.framework/Versions/A/ApplicationServices
    0x7fff334f3000 -     0x7fff334f3fff  libHeimdalProxy.dylib (79) <1BD94BF6-8E63-3B21-95DC-E5EEEBFB8AE8> /System/Library/Frameworks/Kerberos.framework/Versions/A/Libraries/libHeimdalProxy.dylib
    0x7fff34ff4000 -     0x7fff34ff7fff  com.apple.Cocoa (6.11 - 23) <E44AC98B-5BEA-3087-AB41-C73CEB8A98C8> /System/Library/Frameworks/Cocoa.framework/Versions/A/Cocoa
    0x7fff36437000 -     0x7fff36452fff  com.apple.openscripting (1.7 - 190) <D0B98DF9-7A61-3810-AE81-2F870DCC2AC0> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/OpenScripting.framework/Versions/A/OpenScripting
    0x7fff36453000 -     0x7fff36456fff  com.apple.securityhi (9.0 - 55008) <DD7770F7-661C-363B-A1F4-8B69EB0FFB6A> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/SecurityHI.framework/Versions/A/SecurityHI
    0x7fff36457000 -     0x7fff3645afff  com.apple.ink.framework (10.15 - 227) <E10C40B6-2656-36D1-882C-2091CE02883A> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Ink.framework/Versions/A/Ink
    0x7fff3645b000 -     0x7fff3645efff  com.apple.CommonPanels (1.2.6 - 101) <101582BA-E64F-391A-BD23-50DCC3CF8939> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/CommonPanels.framework/Versions/A/CommonPanels
    0x7fff3645f000 -     0x7fff36466fff  com.apple.ImageCapture (1708 - 1708) <FE9D13DD-D733-3B2A-B4A6-D3C8313005F5> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/ImageCapture.framework/Versions/A/ImageCapture
    0x7fff3d151000 -     0x7fff3d154fff  com.apple.print.framework.Print (15 - 271) <8411879F-7E3E-3882-BD06-68E797A3B9D6> /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/Print.framework/Versions/A/Print
    0x7fff3d155000 -     0x7fff3d158fff  com.apple.Carbon (160 - 164) <5683716A-5610-3B97-B473-B4652067E7A6> /System/Library/Frameworks/Carbon.framework/Versions/A/Carbon
    0x7fff3d3dd000 -     0x7fff3d3fcfff  com.apple.private.SystemPolicy (1.0 - 1) <CFE0B0C6-DF5B-31EB-9D79-B23E00A80B05> /System/Library/PrivateFrameworks/SystemPolicy.framework/Versions/A/SystemPolicy
    0x7fff3dd47000 -     0x7fff3dd59fff  libmis.dylib (274.60.2) <54387457-A60B-3390-AD6D-3B380792CD79> /usr/lib/libmis.dylib
    0x7fff414dd000 -     0x7fff4150cfff  libncurses.5.4.dylib (57) <05652959-10ED-3D39-8E0E-C6FC5A68B045> /usr/lib/libncurses.5.4.dylib
    0x7fff6c851000 -     0x7fff6c857fff  libCoreFSCache.dylib (177.22) <4ECE128D-5E79-3ADF-8FE7-4FE8F565F8AA> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libCoreFSCache.dylib
    0x7fff6c858000 -     0x7fff6c85cfff  libCoreVMClient.dylib (177.22) <E0DBED1D-39B4-3E51-9EA8-D1ECAED93EAB> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libCoreVMClient.dylib
    0x7fff6c85d000 -     0x7fff6c86cfff  com.apple.opengl (18.1.1 - 18.1.1) <D8EE3AD0-C0D0-32F7-9C6D-39341099EB55> /System/Library/Frameworks/OpenGL.framework/Versions/A/OpenGL
    0x7fff6c86d000 -     0x7fff6c86ffff  libCVMSPluginSupport.dylib (18.1.1) <5F020D32-8663-3CB8-A50C-F939D4D4C31F> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libCVMSPluginSupport.dylib
    0x7fff6c870000 -     0x7fff6c878fff  libGFXShared.dylib (18.1.1) <2271532D-E2B3-3D4D-ADF0-0935F8DCE89B> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGFXShared.dylib
    0x7fff6c879000 -     0x7fff6c8acfff  libGLImage.dylib (18.1.1) <528E53A3-33E1-34C7-8EE3-C42AE5255553> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGLImage.dylib
    0x7fff6c8ad000 -     0x7fff6c8e9fff  libGLU.dylib (18.1.1) <15CBDF20-8A87-3D84-90F8-D19F4A2B06E2> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGLU.dylib
    0x7fff6ca7f000 -     0x7fff6ca89fff  libGL.dylib (18.1.1) <157B74E1-F30D-3F9D-9AF8-AAA333D2812D> /System/Library/Frameworks/OpenGL.framework/Versions/A/Libraries/libGL.dylib
    0x7fff6dec0000 -     0x7fff6df18fff  com.apple.opencl (4.5 - 4.5) <8A3D06D5-4E82-355C-AE1B-E2C91DB58233> /System/Library/Frameworks/OpenCL.framework/Versions/A/OpenCL
    0x7fff6fdd6000 -     0x7fff6fea1fff  Tcl (8.5.9 - 8.5.9) <51600BFF-7BDA-3446-9BAA-5800C815A91E> /System/Library/Frameworks/Tcl.framework/Versions/8.5/Tcl
    0x7fff6fea2000 -     0x7fff6ff86fff  Tk (8.5.9 - 8.5.9) <DA21CF89-6AE9-3430-8085-83522AA426A7> /System/Library/Frameworks/Tk.framework/Versions/8.5/Tk

External Modification Summary:
  Calls made by other processes targeting this process:
    task_for_pid: 0
    thread_create: 0
    thread_set_state: 0
  Calls made by this process:
    task_for_pid: 0
    thread_create: 0
    thread_set_state: 0
  Calls made by all processes on this machine:
    task_for_pid: 8724
    thread_create: 0
    thread_set_state: 0

VM Region Summary:
ReadOnly portion of Libraries: Total=854.1M resident=0K(0%) swapped_out_or_unallocated=854.1M(100%)
Writable regions: Total=1.6G written=0K(0%) resident=0K(0%) swapped_out=0K(0%) unallocated=1.6G(100%)
 
                                VIRTUAL   REGION 
REGION TYPE                        SIZE    COUNT (non-coalesced) 
===========                     =======  ======= 
Activity Tracing                   256K        1 
Dispatch continuations            64.0M        1 
Kernel Alloc Once                    8K        1 
MALLOC                           154.1M       41 
MALLOC guard page                   24K        5 
MALLOC_MEDIUM (reserved)         960.0M        8         reserved VM address space (unallocated)
STACK GUARD                         76K       19 
Stack                             87.1M       19 
VM_ALLOCATE                      192.7M      400 
VM_ALLOCATE (reserved)           128.0M        2         reserved VM address space (unallocated)
__DATA                            17.2M      528 
__DATA_CONST                      11.9M      201 
__DATA_DIRTY                       509K       86 
__FONT_DATA                          4K        1 
__LINKEDIT                       511.4M      283 
__OBJC_RO                         60.5M        1 
__OBJC_RW                         2452K        2 
__TEXT                           343.3M      473 
__UNICODE                          588K        1 
mapped file                       51.5M        9 
shared memory                       40K        4 
===========                     =======  ======= 
TOTAL                              2.5G     2086 
TOTAL, minus reserved VM space     1.5G     2086 

Model: iMac18,2, BootROM 429.60.3.0.0, 4 processors, Quad-Core Intel Core i7, 3.6 GHz, 32 GB, SMC 2.40f1
Graphics: kHW_AMDRadeonPro560Item, Radeon Pro 560, spdisplays_pcie_device, 4 GB
Memory Module: BANK 0/DIMM0, 16 GB, DDR4 SO-DIMM, 2400 MHz, 0x802C, 0x313641544632473634485A2D3247334232202020
Memory Module: BANK 1/DIMM0, 16 GB, DDR4 SO-DIMM, 2400 MHz, 0x802C, 0x313641544632473634485A2D3247334232202020
AirPort: spairport_wireless_card_type_airport_extreme (0x14E4, 0x16E), Broadcom BCM43xx 1.0 (7.77.111.1 AirPortDriverBrcmNIC-1675.1)
Bluetooth: Version 8.0.2f9, 3 services, 27 devices, 1 incoming serial ports
Network Service: Wi-Fi, AirPort, en1
USB Device: USB 3.0 Bus
USB Device: AS2105
USB Device: USB 2.0 BILLBOARD
USB Device: Bluetooth USB Host Controller
USB Device: FaceTime HD Camera (Built-in)
USB Device: Scarlett 2i4 USB
USB Device: My Passport 0827
Thunderbolt Bus: iMac, Apple Inc., 41.4
```
 "
789,https://github.com/nltk/nltk/issues/2648,2648,"[{'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2020-12-29 19:09:37+00:00,,2,Pos-tag returns strange output for 'as',"Any insights on how to handle 'as'?
Inltk.pos_tag('as')
[('a', DT), ('s','NN')

I also tried to check wordnet synsets, most of the synsets were chemical terms and only the last one was my desired synset: Synset('equally.r.01) r , any insights on how i can isolate this specific meaning and use it for pos_tag, etc?"
790,https://github.com/nltk/nltk/issues/2651,2651,[],closed,2020-12-30 09:23:40+00:00,,2,Synset._needs_root returns None for pos other than noun and verb,"`Synset._needs_root` returns None for pos other than noun and verb. I think it is a bit strange for `Synset._needs_root` to return a boolean for noun and verb, but None for any other part of speech.

![image](https://user-images.githubusercontent.com/35712574/103341379-c9e70180-4a54-11eb-90ae-e6f35364d449.png)

This causes Synset.path_similarity, and Synset.wup_similarity to not work for parts of speech other than noun, and verb.  
If `Synset._needs_root` returns `True` instead, then `Synset.path_similarity` and `Synset.wup_similarity` will work for other parts of speech.

Should these methods work for other parts of speech?"
791,https://github.com/nltk/nltk/issues/2655,2655,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 975531894, 'node_id': 'MDU6TGFiZWw5NzU1MzE4OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/metrics', 'name': 'metrics', 'color': 'f442c8', 'default': False, 'description': ''}]",open,2021-01-10 16:36:31+00:00,,3,METEOR scores very different from Meteor-1.5,"I'd like to use NLTK METEOR in a Python project, but I'm getting very different scores from the [""official"" Java implementation](http://www.cs.cmu.edu/~alavie/METEOR/). I would expect slight differences due to tokenization etc., but the difference is too big for that.

For example, this hypothesis:
```
Alimentum is located in the city centre. It is not family-friendly.
```
And this reference (from the [E2E NLG](https://github.com/tuetschek/e2e-dataset) data):
```
There is a place in the city centre, Alimentum, that is not family-friendly.
```
Get a score of 0.4249 from the Java implementation running with `-l en -norm`, but I'm getting 0.6838 from NLTK.

Do you have any idea why this happens?"
792,https://github.com/nltk/nltk/issues/2656,2656,[],closed,2021-01-11 19:05:17+00:00,,4,Wordnet 503 error,"Attempting to download wordnet from nltk produced a 503 error earlier today around 10am PST.
Did anyone else experience this and was there a resolution?
Error detail snippets:

import nltk
nltk.download('wordnet')

[nltk_data] Downloading package wordnet to /root/nltk_data...
[nltk_data] Error downloading 'wordnet' from
[nltk_data]     <https://raw.githubusercontent.com/nltk/nltk_data/gh-...
[nltk_data]     pages/packages/corpora/wordnet.zip>:   HTTP Error 503:
[nltk_data]     between bytes timeout

Other nltk packages like perceptron tagger seemed to work."
793,https://github.com/nltk/nltk/issues/2658,2658,[],closed,2021-01-21 13:29:35+00:00,,0,Grammar doesn't provide any warnings for missing parse,"If words aren't covered by context-free grammars, then NLTK produces an error. But if a phrase isn't covered by the rules of the grammar, even though all the words are in the vocab, then NLTK just silently produces an empty parse. This means that users may not notice the lack of coverage of their parser.

Would it make sense to add a warning for such cases?"
794,https://github.com/nltk/nltk/issues/2660,2660,[],open,2021-01-27 11:48:50+00:00,,0,"Introduce a new feature in the FCFG phrasal rules, but not in the lexical rules","I find it unintuitive from a unification point of view that it is impossible to introduce new features in the grammar's phrase rules.

For example, I want to introduce a Boolean head feature `h` for phrases but this seems impossible unless I introduce it for lexical rules.

```
import nltk
from nltk.grammar import FeatStructNonterminal, FeatureGrammar
from nltk.parse.featurechart import FeatureChart, FeatureChartParser

gr1 = FeatureGrammar.fromstring('''
% start S
# phrasal rules
NP -> DT N[+ani, +h]

# lexical rules
DT -> 'the'
N[+ani] -> 'animal'
N[-ani] -> 'table'
''')
parser1 = FeatureChartParser(gr1, trace=0, chart_class=FeatureChart)

chart1 = parser1.chart_parse('the animal'.split())

list(chart1.parses(FeatStructNonterminal('NP')))
# [Tree(NP[], [Tree(DT[], ['the']), Tree(N[+ani], ['animal'])])] <---- there is no 'h' feature in the tree
```

On the other hand, if I modify the grammar by doubling the lexical rules, then 'h' appears in the parse tree.
```
gr2 = FeatureGrammar.fromstring('''
% start S
# phrasal rules
NP -> DT N[+ani, +h]

# lexical rules
DT -> 'the'
N[+ani,-h] -> 'animal'
N[+ani,+h] -> 'animal'
N[-ani,-h] -> 'table'
N[-ani,+h] -> 'table'
''')
```

Colab link: https://colab.research.google.com/drive/1p4Q_2LAnJql8a6V2fdYQX-1yod7u9P22?usp=sharing

**Meanwhile, I also wonder what is a workaround to introduce head feature without doubling the lexical rules.
For example, can I define `h` feature for each word without specifying its value?**

"
795,https://github.com/nltk/nltk/issues/2662,2662,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2021-01-29 23:32:41+00:00,,1,Incorrect tagging of a phrase,"```
tok=nltk.word_tokenize(""define and formalize a common starting application for graph data"")
tags=nltk.pos_tag(tok)
print(tags)
```
produces
```
[('define', 'NN'), ('and', 'CC'), ('formalize', 'VB'), ('a', 'DT'), ('common', 'JJ'), ('starting', 'VBG'), ('application', 'NN'), ('for', 'IN'), ('graph', 'NN'), ('data', 'NNS')]
```
""define"" here is marked as ""NN"" which I think means ""noun"" while I expect it to be ""VB"", same as ""formalize"".

I am not sure if this phrase is a proper english though. I found it in some marketing materials.
"
796,https://github.com/nltk/nltk/issues/2663,2663,[],closed,2021-02-01 01:41:50+00:00,,0,Remove plt.show() from FreqDist.plot(),"I think it would be a good idea to remove plt.show() from the FreqDist.plot() function here: https://github.com/nltk/nltk/blob/3d7960008dd9f3802a318f5ed20fa4a63584721e/nltk/probability.py#L302

This is because you cannot customize the plot further after calling plt.show(). For example, I'd prefer to change the x-axis label to 'word', save the figure with plt.savefig(), use plt.tight_layout(), and so on. The only way to currently do this is to run the code in IPython after using the %matplotlib magic command.

I'd be happy to complete a pull request but it's such a tiny change, maybe it's best to do it another way?"
797,https://github.com/nltk/nltk/issues/2665,2665,[],open,2021-02-03 16:59:45+00:00,,0,Incorrect semantics for ccg >T when the argument semantics is a lambda abstraction,"This demonstrates the problem and a solution using [https://github.com/MatsRooth/nltk/blob/develop/nltk/ccg/prescoped.py](prescoped.py) and [https://github.com/MatsRooth/nltk/blob/develop/nltk/ccg/logic.py](logic.py).

Lexicon
```
Justin => NP {\P.P(j)}
Keisha => NP {\P.P(k)}
admires => ((S\NP)/NP) {\Y Z.Z(\z.Y(\y.admire(z,y)))}
complains => (S\NP) {complain}
everybody => NP {\P.all x.(person(x) -> P(x))}
somebody => NP {\P.exists x.(person(x) & P(x))}
```
 
Derivation for 'somebody admires everybody' obtained with ApplicationRuleSet 
The semantics is the expected one.   

```
              somebody                                 admires                                everybody
 NP {\P.exists x.(person(x) & P(x))}  ((S\NP)/NP) {\Y Z.Z(\z.Y(\y.admire(z,y)))}  NP {\P.all x.(person(x) -> P(x))}
                                     ------------------------------------------------------------------------------->
                                                   (S\NP) {\Z.Z(\z.all x.(person(x) -> admire(z,x)))}
--------------------------------------------------------------------------------------------------------------------<
                           S {exists x.(person(x) & all z1.(person(z1) -> admire(x,z1)))}
```

Derivation for 'somebody admires everybody' obtained with  ForwardTypeRaiseRule + ForwardApplication.                                      The result has scrambled scopes.       
```
                somebody                                  admires                                everybody
 NP {\P.F(exists x.(person(x) & P(x)))}  ((S\NP)/NP) {\Y Z.Z(\z.Y(\y.admire(z,y)))}  NP {\P.all x.(person(x) -> P(x))}
---------------------------------------->T
(S/(S\NP)) {\F P.F(exists x.(person(x) & P(x)))}
                                        ------------------------------------------------------------------------------->
                                                      (S\NP) {\Z.Z(\z.all x.(person(x) -> admire(z,x)))}
----------------------------------------------------------------------------------------------------------------------->
                        S {\P.exists x.(person(x) & P(x))(\z.all x.(person(x) -> admire(z,x)))}
```

Derivation for 'Justin admires Justin' obtained with  ForwardTypeRaiseRule + ForwardApplication. 
The result is ill-formed, and a free F appears.                       
```
     Justin                        admires                        Justin
 NP {\P.F(P(j))}  ((S\NP)/NP) {\Y Z.Z(\z.Y(\y.admire(z,y)))}  NP {\P.F(P(j))}
----------------->T
(S/(S\NP)) {\F P.F(P(j))}
                 ------------------------------------------------------------->
                               (S\NP) {\Z.Z(\z.F(admire(z,j)))}
------------------------------------------------------------------------------>
                        S {\P.P(j,\z.F(admire(z,j)))}
```
The problem is fixed by this change in nltk/ccg/logic.py.  I'm not clear enough about
the chart design to say if the change has any unwelcome consequences. This constructs
a closed type raiser, applies it to the input semantics, and normalizes.

```
def compute_type_raised_semantics(semantics):
    varf = Variable('F')
    vara = Variable('z')
    expf = FunctionVariableExpression(varf)
    expa = FunctionVariableExpression(vara)

    raiser = LambdaExpression(vara,LambdaExpression(varf,ApplicationExpression(expf, expa)))

    semantics2 = ApplicationExpression(raiser,semantics)
    return semantics2.simplify()
```
 
Derivation for 'somebody admires everybody' obtained with  ForwardTypeRaiseRule + ForwardApplication.                                       
```
              somebody                                 admires                                everybody
 NP {\P.exists x.(person(x) & P(x))}  ((S\NP)/NP) {\Y Z.Z(\z.Y(\y.admire(z,y)))}  NP {\P.all x.(person(x) -> P(x))}
------------------------------------->T
(S/(S\NP)) {\F.F(\P.exists x.(person(x) & P(x)))}
                                     ------------------------------------------------------------------------------->
                                                   (S\NP) {\Z.Z(\z.all x.(person(x) -> admire(z,x)))}
-------------------------------------------------------------------------------------------------------------------->
                           S {exists x.(person(x) & all z2.(person(z2) -> admire(x,z2)))}
```
 Derivation for 'Justin admires Justin' obtained with  ForwardTypeRaiseRule + ForwardApplication. 
```
    Justin                      admires                       Justin
 NP {\P.P(j)}  ((S\NP)/NP) {\Y Z.Z(\z.Y(\y.admire(z,y)))}  NP {\P.P(j)}
-------------->T
(S/(S\NP)) {\F.F(\P.P(j))}
              ---------------------------------------------------------->
                            (S\NP) {\Z.Z(\z.admire(z,j))}
------------------------------------------------------------------------>
                            S {admire(j,j)}
```
 "
798,https://github.com/nltk/nltk/issues/2666,2666,[],open,2021-02-08 14:02:44+00:00,,1,Encountered an old issue #1387 with the latest version,"I've got the same error after doing similar stuff. (See #1387.)

```
---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
G:\Anaconda3\lib\site-packages\IPython\core\formatters.py in __call__(self, obj)
    343             method = get_real_method(obj, self.print_method)
    344             if method is not None:
--> 345                 return method()
    346             return None
    347         else:

G:\Anaconda3\lib\site-packages\nltk\tree.py in _repr_png_(self)
    817                 raise LookupError
    818 
--> 819             with open(out_path, ""rb"") as sr:
    820                 res = sr.read()
    821             os.remove(in_path)

FileNotFoundError: [Errno 2] No such file or directory: 'G:\\Users\\01\\AppData\\Local\\Temp\\tmpkyszdc9g.png'
```

(Python)

```
Error: /syntaxerror in (binary token, type=155)
Operand stack:
   --nostringval--   寰蒋?Execution stack:
   %interp_exit   .runexec2   --nostringval--   --nostringval--   --nostringval-
-   2   %stopped_push   --nostringval--   --nostringval--   --nostringval--   fa
lse   1   %stopped_push   1926   1   3   %oparray_pop   1925   1   3   %oparray_
pop   --nostringval--   1909   1   3   %oparray_pop   1803   1   3   %oparray_po
p   --nostringval--   %errorexec_pop   .runexec2   --nostringval--   --nostringv
al--   --nostringval--   2   %stopped_push
Dictionary stack:
   --dict:1173/1684(ro)(G)--   --dict:0/20(G)--   --dict:82/200(L)--   --dict:23
/50(L)--
Current allocation mode is local
Last OS error: No such file or directory
GPL Ghostscript 9.05: Unrecoverable error, exit code 1
```

(commmand line)

Seems the problem is that ghostscript failed - strange.

It is reproducible as https://github.com/nltk/nltk/issues/1387#issuecomment-216426399:

(in ipython notebook)

```
import nltk

nltk.tree.Tree.fromstring(""(test (this tree))"")
```

This old issue is closed and referenced by some PRs, so I thought it was fixed - or this is a problem of Jupyter Notebook?"
799,https://github.com/nltk/nltk/issues/2667,2667,[],open,2021-02-09 14:52:58+00:00,,0,Not being able to append path to nltk_data,"`print(os.path.join(DATA_DIR, ""nltk_data""))
 nltk.data.path.append(os.path.join(DATA_DIR, ""nltk_data""))
 print(nltk.data.path)`

The first print correctly returns the path to the nltk_data folder. Then, I try to append it to the path list, but the print returns the list without the previously appended path. 

Also, it is weird, but if I append a wrong path to nltk_data, it correctly appends the wrong path.

I can do it with a previously set environment variable, which is not ideal. And I also cannot do it by setting environment variables programmatically."
800,https://github.com/nltk/nltk/issues/2669,2669,"[{'id': 113906330, 'node_id': 'MDU6TGFiZWwxMTM5MDYzMzA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/SMT', 'name': 'SMT', 'color': 'bfdadc', 'default': False, 'description': None}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2021-03-02 17:56:39+00:00,,2,nltk.translate.sentence_bleu function not working ,"For sentences of length less than 4, sentence_bleu is not being calculated properly. If length is less than 4, BLEU value is greater than 1. Refer image below
![Bleu_Score](https://user-images.githubusercontent.com/58718023/109692397-7b69f680-7bae-11eb-8a5c-dc3ab9e5507a.png)



"
801,https://github.com/nltk/nltk/issues/2671,2671,[],closed,2021-03-20 22:15:42+00:00,,1,nltk.translate.sentence_bleu not working as expected,"According to http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf 

> Because it computes a geometric mean of n-gram precisions, if a higher order n-gram precision (eg. n= 4) of a sentence is 0, then the BLEU score of the entire sentence is 0, no matter how many 1-grams or 2-grams are matched.

The following test shows that it doesn't work as expected 

```python
hypothesis = ['a', 'b', 'c', 'e']

reference = ['a', 'b', 'c', 'd']

print(sentence_bleu([reference], hypothesis))
```

Result is `1.0` and should be `0.0` because 4-gram precision is zero.

As far as I understand the implementation, it seems that the error comes from following generator
https://github.com/nltk/nltk/blob/aed0fbb87e29f8ce983217663b87c7ac286eaca6/nltk/translate/bleu_score.py#L219

Replacing by
```python
s = [w_i * math.log(p_i) for w_i, p_i in zip(weights, p_n)]
```
gives good results.
 "
802,https://github.com/nltk/nltk/issues/2672,2672,[],closed,2021-03-22 09:38:10+00:00,,1,Issue #1523 not yet resolved,Issue hasn't been resolved yet #1523
803,https://github.com/nltk/nltk/issues/2675,2675,[],closed,2021-03-22 21:07:49+00:00,,1,Type Hinting,"Is Type Hinting still a no-go for NLTK? - I see in setup.py that the minimum Python version required is 3.5, which supports Type Hinting.

I have some type hints created for the classification files for better understanding them myself. If possible, I can create a PR."
804,https://github.com/nltk/nltk/issues/2676,2676,[],closed,2021-03-24 01:22:16+00:00,,3,Issue about BLEU smoothing method 4,"NLTK3.5 implements smoothing method 4 proposed in the paper [https://www.aclweb.org/anthology/W14-3346/ ].
However, we suspect that the implementation is incorrect. The code at https://github.com/nltk/nltk/blob/d0f54c29b3f65ea13dd6e824e08eb9f193bcb958/nltk/translate/bleu_score.py#L582-L585 is problematic.
This leads to the p_n[i] greater than 100% in some case. When hyp_len < 4 in 4-gram, p_n[i] can be assigned with a percentage number that is much greater than 100% (or even > 500%).
In fact, the p_n[i] should not be greater than 100%.
The correct implementation is p_n[i] = (1 / incvnt) / p_i.denominator in line585."
805,https://github.com/nltk/nltk/issues/2677,2677,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2021-03-24 23:42:31+00:00,,1,[Question] How can we use do unsupervised document tagging with NLTK?,"Hello everyone!

I wished to use the `HiddenMarkovModelTrainer` from NLTK to try to do **fully** unsupervised document tagging of a low resource language. Apparently, from an S.O answer, we need to feed tuples of `states` and `symbols`.

Assuming that it refers to the ground labels, isn't that a semi-supervised approach? can we do a fully supervised and local approach with NLTK?

if that is indeed possible, can someone clarify what exactly `states` and `symbols` are?"
806,https://github.com/nltk/nltk/issues/2682,2682,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2021-03-30 12:09:44+00:00,,1,Segfault with for fresh ubuntu 20.04 install using conda,"The python interpreter segfaults when running in a miniconda environment on a fresh install of ubuntu 20.04.2. This seems to happen intermittently, both while running ""pip"" during the conda setup of an environment and during the execution of code like below. 

The segfault always occurs when running the following code, which reads texts from files and tokenizes the result. The segfault location changes from run to run. Also the exact same code can run on another computer with the same conda environment on a ubuntu 18.04.

The core dumps always points to some function in the unicodeobject.c file in python but the exact function changes from crash to crash. At least one crash has a clear dereferenced pointer 0x0 where the ""unicode object"" should be.

My guess is that something causes the python interpreter to throw away the pointed to unicode object while it is still being worked on causing a segfault. But any bug in the interpreter or NLTK should have been noticed by more users, and I cannot find anyone with similar issues. This issue was also reported to the python development team which replied that the usage of the c-API is used in an incorrect way https://bugs.python.org/issue43668

Things tried that didn't fix the issue:
1. Reformatting and reinstalling ubuntu
2. Switched to ubuntu 18.04 (on this computer, another computer with 18.04 can run the code just fine)
3. Replacing hardware, to ensure that RAM, or SSD disk isn't broken
4. Changing to python versions 3.8.6, 3.8.8, 3.9.2
5. Cloning the conda environment from a working computer to the broken one

Attached is one stacktrace of the fault handler along with it's corresponding core dump stack trace from gdb.

```
(eo) axel@minimind:~/test$ python tokenizer_mini.py 
2021-03-30 11:10:15.588399: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-03-30 11:10:15.588426: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Fatal Python error: Segmentation fault

Current thread 0x00007faa73bbe740 (most recent call first):
  File ""tokenizer_mini.py"", line 36 in preprocess_string
  File ""tokenizer_mini.py"", line 51 in <module>
Segmentation fault (core dumped)
```

```
#0  raise (sig=<optimized out>) at ../sysdeps/unix/sysv/linux/raise.c:50
#1  <signal handler called>
#2  find_maxchar_surrogates (num_surrogates=<synthetic pointer>, maxchar=<synthetic pointer>, 
    end=0x4 <error: Cannot access memory at address 0x4>, begin=0x0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/unicodeobject.c:1703
#3  _PyUnicode_Ready (unicode=0x7f7e4e04d7f0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/unicodeobject.c:1742
#4  0x000055cd65f6df6a in PyUnicode_RichCompare (left=0x7f7e4cf43fb0, right=<optimized out>, op=2)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/unicodeobject.c:11205
#5  0x000055cd6601712a in do_richcompare (op=2, w=0x7f7e4e04d7f0, v=0x7f7e4cf43fb0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/object.c:726
#6  PyObject_RichCompare (op=2, w=0x7f7e4e04d7f0, v=0x7f7e4cf43fb0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/object.c:774
#7  PyObject_RichCompareBool (op=2, w=0x7f7e4e04d7f0, v=0x7f7e4cf43fb0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/object.c:796
#8  list_contains (a=0x7f7e4e04b4c0, el=0x7f7e4cf43fb0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/listobject.c:455
#9  0x000055cd660be41b in PySequence_Contains (ob=0x7f7e4cf43fb0, seq=0x7f7e4e04b4c0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/abstract.c:2083
#10 cmp_outcome (w=0x7f7e4e04b4c0, v=0x7f7e4cf43fb0, op=<optimized out>, tstate=<optimized out>)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ceval.c:5082
#11 _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ceval.c:2977
#12 0x000055cd6609f706 in PyEval_EvalFrameEx (throwflag=0, f=0x7f7e4f4d3c40)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ceval.c:738
#13 function_code_fastcall (globals=<optimized out>, nargs=<optimized out>, args=<optimized out>, co=<optimized out>)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/call.c:284
#14 _PyFunction_Vectorcall (func=<optimized out>, stack=<optimized out>, nargsf=<optimized out>, kwnames=<optimized out>)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Objects/call.c:411
#15 0x000055cd660be54f in _PyObject_Vectorcall (kwnames=0x0, nargsf=<optimized out>, args=0x7f7f391985b8, callable=0x7f7f39084160)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Include/cpython/abstract.h:115
#16 call_function (kwnames=0x0, oparg=<optimized out>, pp_stack=<synthetic pointer>, tstate=0x55cd66c2e880)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ceval.c:4963
#17 _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ceval.c:3500
#18 0x000055cd6609e503 in PyEval_EvalFrameEx (throwflag=0, f=0x7f7f39198440)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ceval.c:4298
#19 _PyEval_EvalCodeWithName (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, 
    argcount=<optimized out>, kwnames=<optimized out>, kwargs=<optimized out>, kwcount=<optimized out>, kwstep=<optimized out>, 
    defs=<optimized out>, defcount=<optimized out>, kwdefs=<optimized out>, closure=<optimized out>, name=<optimized out>, 
    qualname=<optimized out>) at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ceval.c:4298
#20 0x000055cd6609f559 in PyEval_EvalCodeEx (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, 
    args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, kwdefs=0x0, closure=0x0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ceval.c:4327
#21 0x000055cd661429ab in PyEval_EvalCode (co=<optimized out>, globals=<optimized out>, locals=<optimized out>)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ceval.c:718
#22 0x000055cd66142a43 in run_eval_code_obj (co=0x7f7f3910f240, globals=0x7f7f391fad80, locals=0x7f7f391fad80)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/pythonrun.c:1165
#23 0x000055cd6615c6b3 in run_mod (mod=<optimized out>, filename=<optimized out>, globals=0x7f7f391fad80, locals=0x7f7f391fad80, 
    flags=<optimized out>, arena=<optimized out>)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/pythonrun.c:1187
--Type <RET> for more, q to quit, c to continue without paging--
#24 0x000055cd661615b2 in pyrun_file (fp=0x55cd66c2cdf0, filename=0x7f7f391bbee0, start=<optimized out>, globals=0x7f7f391fad80, 
    locals=0x7f7f391fad80, closeit=1, flags=0x7ffe3ee6f8e8)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/pythonrun.c:1084
#25 0x000055cd66161792 in pyrun_simple_file (flags=0x7ffe3ee6f8e8, closeit=1, filename=0x7f7f391bbee0, fp=0x55cd66c2cdf0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/pythonrun.c:439
#26 PyRun_SimpleFileExFlags (fp=0x55cd66c2cdf0, filename=<optimized out>, closeit=1, flags=0x7ffe3ee6f8e8)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/pythonrun.c:472
#27 0x000055cd66161d0d in pymain_run_file (cf=0x7ffe3ee6f8e8, config=0x55cd66c2da70)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Modules/main.c:391
#28 pymain_run_python (exitcode=0x7ffe3ee6f8e0)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Modules/main.c:616
#29 Py_RunMain () at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Modules/main.c:695
#30 0x000055cd66161ec9 in Py_BytesMain (argc=<optimized out>, argv=<optimized out>)
    at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Modules/main.c:1127
#31 0x00007f7f3a3620b3 in __libc_start_main (main=0x55cd65fe3490 <main>, argc=2, argv=0x7ffe3ee6fae8, init=<optimized out>, 
    fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7ffe3ee6fad8) at ../csu/libc-start.c:308
#32 0x000055cd660d7369 in _start () at /home/conda/feedstock_root/build_artifacts/python-split_1613835706476/work/Python/ast.c:937
```

The conda environment used is below, using Miniconda3-py38_4.9.2-Linux-x86_64.sh (note that the segfault does sometimes occur during the setup of a conda environment so it's probably not related to the env)
```
name: eo
channels:
  - conda-forge
  - defaults
dependencies:
  - python=3.8.8
  - pip=20.3.1
  - pip:
    - transformers==4.3.2
    - tensorflow_gpu==2.4.0
    - scikit-learn==0.23.2
    - nltk==3.5
    - matplotlib==3.2.1
    - seaborn==0.11.0
    - tensorflow-addons==0.11.2
    - tf-models-official==2.4.0
    - gspread==3.6.0
    - oauth2client==4.1.3
    - ipykernel==5.4.2
    - autopep8==1.5.4
    - torch==1.7.1
```


The code below consistently reproduces the problem, the files read are simple text files containing unicode text:

```python
from nltk.tokenize import wordpunct_tokenize
from tensorflow.keras.preprocessing.text import Tokenizer
from nltk.stem.snowball import SnowballStemmer
from nltk.corpus import stopwords
import pickle
from pathlib import Path
import faulthandler
faulthandler.enable()


def load_data(root_path, feature, index):
    feature_root = root_path / feature
    dir1 = str(index // 10_000)
    base_path = feature_root / dir1 / str(index)
    full_path = base_path.with_suffix('.txt')
    data = None
    with open(full_path, 'r', encoding='utf-8') as f:
        data = f.read()
    return data


def preprocess_string(text, stemmer, stop_words):
    word_tokens = wordpunct_tokenize(text.lower())
    alpha_tokens = []
    for w in word_tokens:
        try:
            if (w.isalpha() and w not in stop_words):
                alpha_tokens.append(w)
        except:
            print(""Something went wrong when handling the word: "", w)

    clean_tokens = []
    for w in alpha_tokens:
        try:
            word = stemmer.stem(w)
            clean_tokens.append(word)
        except:
            print(""Something went wrong when stemming the word: "", w)
            clean_tokens.append(w)
    return clean_tokens


stop_words = stopwords.words('english')
stemmer = SnowballStemmer(language='english')
tokenizer = Tokenizer()

root_path = '/srv/patent/EbbaOtto/E'
for idx in range(0, 57454):
    print(f'Processed {idx}/57454', end='\r')
    desc = str(load_data(Path(root_path), 'clean_description', idx))
    desc = preprocess_string(desc, stemmer, stop_words)
    tokenizer.fit_on_texts([desc])

```"
807,https://github.com/nltk/nltk/issues/2684,2684,[],closed,2021-04-04 06:18:59+00:00,,0,Wordnet tree() is not directly compatible with tree.Tree(),"The trees built by the tree() function in the _wordnet_ module have a slightly different structure than those from NLTK's  Tree class from the _tree_ module. This is regrettable, since the Tree class provides additional functionality that would be nice to use for Wordnet relation trees also, such as the ability do draw trees in Postscript or Latex formats."
808,https://github.com/nltk/nltk/issues/2686,2686,[],closed,2021-04-04 08:00:45+00:00,,1,Travis CI test fails with Python 3.9,"Travis CI now also tests with Python 3.9.

The output reveals that the XML Parser used by NLTK's corpus.reader is no longer compatible with the newest Python 3.9 version.

So all NLTK builds will fail Travis CI testing with Python 3.9 until this is fixed:

=================================== FAILURES ===================================
___________________________ [doctest] corpus.doctest ___________________________
615 
616 semcor
617 ------
618 The Brown Corpus, annotated with WordNet senses.
619 
620     >>> from nltk.corpus import semcor
621     >>> semcor.words('brown2/tagfiles/br-n12.xml')  # doctest: +ELLIPSIS
622     ['When', 'several', 'minutes', 'had', 'passed', ...]
623     >>> sent = semcor.xml('brown2/tagfiles/br-n12.xml').findall('context/p/s')[0]
624     >>> for wordform in sent.getchildren():
UNEXPECTED EXCEPTION: AttributeError(""'xml.etree.ElementTree.Element' object has no attribute 'getchildren'"")
Traceback (most recent call last):
  File ""/opt/python/3.9.1/lib/python3.9/doctest.py"", line 1336, in __run
    exec(compile(example.source, filename, ""single"",
  File ""<doctest corpus.doctest[135]>"", line 1, in <module>
AttributeError: 'xml.etree.ElementTree.Element' object has no attribute 'getchildren'
/home/travis/build/nltk/nltk/nltk/test/corpus.doctest[0m:624: UnexpectedException
920 rte
921 ---
922 The RTE (Recognizing Textual Entailment) corpus was derived from the
923 RTE1, RTE2 and RTE3 datasets (dev and test data), and consists of a
924 list of XML-formatted 'text'/'hypothesis' pairs.
925 
926     >>> from nltk.corpus import rte
927     >>> print(rte.fileids()) # doctest: +ELLIPSIS
928     ['rte1_dev.xml', 'rte1_test.xml', 'rte2_dev.xml', ..., 'rte3_test.xml']
929     >>> rtepairs = rte.pairs(['rte2_test.xml', 'rte3_test.xml'])
UNEXPECTED EXCEPTION: AttributeError(""'xml.etree.ElementTree.Element' object has no attribute 'getiterator'"")
"
809,https://github.com/nltk/nltk/issues/2689,2689,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 2889145530, 'node_id': 'MDU6TGFiZWwyODg5MTQ1NTMw', 'url': 'https://api.github.com/repos/nltk/nltk/labels/python4', 'name': 'python4', 'color': '122159', 'default': False, 'description': ''}]",closed,2021-04-06 06:47:30+00:00,,2,More Python3.9 deprecations,"Running pytest with the new Python 3.9.4 shows a number of deprecation warnings about future failures to anticipate:

============================= test session starts ==============================
platform linux -- Python 3.9.4, pytest-6.2.0, py-1.10.0, pluggy-0.13.1

[....]
=============================== warnings summary ===============================
chunk.doctest::chunk.doctest
  <doctest chunk.doctest[11]>:1: DeprecationWarning: invalid escape sequence \.

classify.doctest::classify.doctest
classify.doctest::classify.doctest
classify.doctest::classify.doctest
classify.doctest::classify.doctest
classify.doctest::classify.doctest
nltk/util.py:64: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly
    argspec = inspect.formatargspec(args, varargs, varkw, defaults)

corpus.doctest::corpus.doctest
  <doctest corpus.doctest[232]>:2: DeprecationWarning: invalid escape sequence \.

corpus.doctest::corpus.doctest
  <doctest corpus.doctest[272]>:1: DeprecationWarning: invalid escape sequence \.

data.doctest::data.doctest
  <doctest data.doctest[63]>:1: DeprecationWarning: 
    Function BufferedGzipFile() has been deprecated.  Use gzip.GzipFile
    instead as it also uses a buffer.

data.doctest::data.doctest
  <doctest data.doctest[67]>:1: DeprecationWarning: 
    Function BufferedGzipFile() has been deprecated.  Use gzip.GzipFile
    instead as it also uses a buffer.

gensim.doctest::gensim.doctest
  <doctest gensim.doctest[7]>:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).

gensim.doctest::gensim.doctest
  /usr/lib64/python3.9/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a ""sequence"" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
    vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)

gluesemantics.doctest::gluesemantics.doctest
  <doctest gluesemantics.doctest[55]>:1: DeprecationWarning: invalid escape sequence \P

gluesemantics.doctest::gluesemantics.doctest
  <doctest gluesemantics.doctest[65]>:1: DeprecationWarning: invalid escape sequence \P

gluesemantics.doctest::gluesemantics.doctest
  <doctest gluesemantics.doctest[71]>:1: DeprecationWarning: invalid escape sequence \P

probability.doctest: 410 warnings
nltk/tag/hmm.py:396: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
  Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
    B = -np.ones((T, N), np.int)

probability.doctest::probability.doctest
nltk/probability.py:1456: UserWarning: SimpleGoodTuring did not find a proper best fit line for smoothing probabilities of occurrences. The probability estimates are likely to be unreliable.
    warnings.warn(

relextract.doctest::relextract.doctest
  <doctest relextract.doctest[15]>:1: DeprecationWarning: invalid escape sequence \s

toolbox.doctest::toolbox.doctest
  /usr/lib64/python3.9/codecs.py:905: DeprecationWarning: 'U' mode is deprecated
    file = builtins.open(filename, mode, buffering)

toolbox.doctest::toolbox.doctest
  <doctest toolbox.doctest[90]>:1: DeprecationWarning: invalid escape sequence \|

[....]"
810,https://github.com/nltk/nltk/issues/2690,2690,[],closed,2021-04-07 13:34:22+00:00,,10,nltk 3.6 has pytest as unlisted requirement,"From version 3.6 pytest is a requirement, e.g. it's imported in https://github.com/nltk/nltk/blob/24d6e0d4123a5cfea3ede7dbdfe77fe0ec54b83c/nltk/parse/corenlp.py line 23. However it is not listed as requirement in setup.py. This probably works fine if you set up the dev environment, but for a normal user install it results in an error.

Reproduce (in fresh env):
```
$ pip install nltk
...
$ python -m nltk
...
ModuleNotFoundError: No module named 'pytest'
```"
811,https://github.com/nltk/nltk/issues/2691,2691,[],closed,2021-04-07 15:45:24+00:00,,0,nltk.TrigramAssocMeasures.likelihood_ratio wrong factor,"When computing a likelihood ratio for a bigram with `nltk.NgramAssocMeasures.likelihood_ratio`, `cls._n` evaluates to 2 in the code below, which is correct I believe.
```python
def likelihood_ratio(cls, *marginals):
    cont = cls._contingency(*marginals)
    return cls._n * sum(
        obs * _ln(obs / (exp + _SMALL) + _SMALL)
        for obs, exp in zip(cont, cls._expected_values(cont))
    )
```
For a 2x2 table, `nltk` returns the same likelihood ratio result as `scipy.stats`.
```python
import nltk
from scipy.stats import chi2_contingency

nltk.BigramAssocMeasures.likelihood_ratio(2, (4, 4), 20)
>>> 2.4142743368419755
nltk.BigramAssocMeasures._contingency(2, (4, 4), 20)
>>> (2, 2, 2, 14)
g, p, dof, expctd = chi2_contingency(np.array(((2, 2), (2, 14))), correction=False, lambda_=""log-likelihood"")
g
>>> 2.4142743368419755
```

However, when computing a likelihood ratio for a trigram, `cls._n` in nltk.NgramAssocMeasures.likelihood_ratio evaluates to 3 (since `TrigramAssocMeasures._n = 3`). So what is returned is 3 * the sum of the product of the observed count with the log of the observed/expected ratio. I believe this is not correct, I think it should still be 2 * the sum of ... (assuming that what we want to test is full independence of the 3 words in the trigram). I am not an expert, but you can refer to wikipedia on the [Likelihood-ratio test](https://en.wikipedia.org/wiki/Likelihood-ratio_test) and on the [G-test](https://en.wikipedia.org/wiki/G-test). 

For a 2x2x2 table, `nltk` does not return the same likelihood ratio as `scipy.stats`. Instead, it returns the same result multiplied by 3/2.
```python
nltk.TrigramAssocMeasures.likelihood_ratio(2, (2, 2, 2), (4, 4, 4), 20)
>>> 16.420671549441057
nltk.TrigramAssocMeasures._contingency(2, (2, 2, 2), (4, 4, 4), 20)
>>> (2, 0, 0, 2, 0, 2, 2, 12)
g, p, dof, expctd = chi2_contingency(np.array((((2, 0), (0, 2)), ((0, 2), (2, 12)))), correction=False, lambda_=""log-likelihood"")
g
>>> 10.947114366294038
g * 3/2
>>> 16.420671549441057
```
Maybe I am overlooking something? I can make a PR to fix this."
812,https://github.com/nltk/nltk/issues/2694,2694,[],closed,2021-04-07 16:47:25+00:00,,1,Move all pytest code into `nltk/test/`,A better way to solve https://github.com/nltk/nltk/issues/2690 may be to remove all pytest imports that are outside of `nltk/test/`
813,https://github.com/nltk/nltk/issues/2695,2695,[],closed,2021-04-08 05:36:07+00:00,,1,How to obtain the universal POS for tokens when using Stanford CoreNLP?,"When using the original NLTK package, we can obtain the universal POS via using the following demo commend:
`parsed_results = nltk.pos_tag(nltk.word_tokenize(sentence), tagset='universal')`

While the tutorial in this project to achieve the same target using _Stanford CoreNLP_ is:
`pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')`
`list(pos_tagger.tag('What is the airspeed of an unladen swallow ?'.split()))`

However, this could not return the universal result. I'd like to know to achieve my goal. Anybody can help me?"
814,https://github.com/nltk/nltk/issues/2699,2699,[],open,2021-04-14 22:44:35+00:00,,1,ResolutionProver is unsound,"Following: https://stackoverflow.com/questions/43507770/basic-first-order-logic-inference-fails-for-symmetric-binary-predicate

ResolutionProver appears to not implement the resolution rule correctly, a fix is described by (not myself) above."
815,https://github.com/nltk/nltk/issues/2703,2703,"[{'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",open,2021-04-18 05:14:37+00:00,,0,Spanish sentence tokenizer does not take in account added abbreviations,"Hello! 

I am trying to split a spanish text into sentences using the punkt tokenizer. The code is:

```
>> es_tokenizer = nltk.data.load(""tokenizers/punkt/spanish.pickle"")
>> chile_abrevs = {""ord"", ""num"", ""sra"", ""no"", ""corp""}
>> es_tokenizer._params.abbrev_types.update(chile_abrevs)
>> sents = es_tokenizer.tokenize(""EL MONUMENTO NATURAL ALERCE COSTERO Y AMPLÍA EL PARQUE NACIONAL ALERCE COSTERO, EN LAS COMUNAS DE CORRAL Y LA UNIÓN, PROVINCIAS DE VALDIVIA Y DEL RANCO, REGIÓN DE LOS RÍOS\n      \n     Núm. 60.- Santiago, 7 de junio de 2013.- Vistos: Estos antecedentes, lo solicitado por la Sra. Ministra de Bienes Nacionales en oficio ord. Nº 916, de 3 de octubre de 2012; lo informado por la Secretaría Regional Ministerial de Los Ríos, mediante oficio ord. Nº 2.957, de 18 de octubre de 2012; lo informado por la Corporación Nacional Forestal (CONAF) y el Ministerio del Medio Ambiente; lo informado por el Asesor Legal de Asuntos Indígenas del Ministerio de Desarrollo Social, mediante oficio ord. Nº 78, de 14 de octubre de 2012; el Acuerdo Nº 26, de 2012, del Consejo de Ministros para la Sustentabilidad; la ley 19.300; el DL Nº 1.939, de 1977;"")
>> print(sents)
['EL MONUMENTO NATURAL ALERCE COSTERO Y AMPLÍA EL PARQUE NACIONAL ALERCE COSTERO, EN LAS COMUNAS DE CORRAL Y LA UNIÓN, PROVINCIAS DE VALDIVIA Y DEL RANCO, REGIÓN DE LOS RÍOS\n      \n     Núm.',
 '60.- Santiago, 7 de junio de 2013.- Vistos: Estos antecedentes, lo solicitado por la Sra.',
 'Ministra de Bienes Nacionales en oficio ord. Nº 916, de 3 de octubre de 2012; lo informado por la Secretaría Regional Ministerial de Los Ríos, mediante oficio ord. Nº 2.957, de 18 de octubre de 2012; lo informado por la Corporación Nacional Forestal (CONAF) y el Ministerio del Medio Ambiente; lo informado por el Asesor Legal de Asuntos Indígenas del Ministerio de Desarrollo Social, mediante oficio ord. Nº 78, de 14 de octubre de 2012; el Acuerdo Nº 26, de 2012, del Consejo de Ministros para la Sustentabilidad; la ley 19.300; el DL Nº 1.939, de 1977;']
```

As you can see, I am trying to extend the dictionary of possible abbreviations to include ""num"", ""ord"" and ""sra"", but while it works with ""ord."", the sentences still get separated at ""num"" and ""sra."", which shouldn't happen. Any insights on what I should be doing differently? 

Thank you in advance!

P.S: The problem persists even if I remove the accent in ""Núm"" before attempting the split."
816,https://github.com/nltk/nltk/issues/2705,2705,"[{'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}, {'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2021-04-25 13:11:01+00:00,,2,urlopen error [Errno 60] Operation   timed out,"[nltk_data] Error loading stopwords: <urlopen error [Errno 60]
[nltk_data]     Operation timed out>
[nltk_data] Error loading words: <urlopen error [Errno 60] Operation
[nltk_data]     timed out>

getting the above error when tried to do nltk.download('all)"
817,https://github.com/nltk/nltk/issues/2707,2707,[],closed,2021-04-27 21:04:24+00:00,,1,Do you provide a object.inv file for sphinx documentation link?,"I've seen that you provide documentation on readthedocs and I'm building a software (for academic pourposes) that uses NLTK and I'm documenting it using Sphinx. I'd like to cross reference your documentation using sphinx.ext.intersphinx. But for that, I need to configure the location of the object.inv file in your documentation. Do you provide it? Thanks."
818,https://github.com/nltk/nltk/issues/2708,2708,[],closed,2021-05-01 07:36:59+00:00,,3,Wrong default value in aline.py,"I believe the value of `C_skip` in https://github.com/nltk/nltk/blob/develop/nltk/metrics/aline.py#L51 should be negative. In particular, the value in the original paper seems to be `-10`, not `10` (see https://webdocs.cs.ualberta.ca/~kondrak/papers/thesis.pdf, p. 54)."
819,https://github.com/nltk/nltk/issues/2710,2710,[],closed,2021-05-07 16:59:39+00:00,,0,Empty README file,#2514 added an empty `README` file next to the existing `README.md`
820,https://github.com/nltk/nltk/issues/2714,2714,"[{'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}, {'id': 975531894, 'node_id': 'MDU6TGFiZWw5NzU1MzE4OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/metrics', 'name': 'metrics', 'color': 'f442c8', 'default': False, 'description': ''}]",closed,2021-05-10 14:14:33+00:00,,2,Wrong unit tests for aline. Aline does not function as specified.,"Some of the [aline unit tests](https://github.com/nltk/nltk/blob/develop/nltk/test/unit/test_aline.py) are not correct when compared to Kondrak's original algorithms. 
For instance, in the unit test,
`aline.align('tuwθ', 'dentis') == [[('t', 'd'), ('u', 'e'), ('w', '-'), ('-', 'n'), ('-', 't'), ('-', 'i'), ('θ', 's')]]`
but the actual output according to p63 of Kondrak's Thesis is
`aline.align('tuwθ', 'dentis') == [[('t', 't'), ('uw', 'i'), '('θ', 's')]]`

I suspect the other unit tests are also written using the output of the implementation itself rather than known outputs. I am not sure what the behavior of the original python implementation is to cause this deviation, nor do I know where the source code for the original C++ implementation is to fix it. Would be nice if someone can look into this.

A list of other alignments are also given in Kondrak's Thesis which can also be used as unit tests.

(see [Kondrak's Thesis](https://webdocs.cs.ualberta.ca/~kondrak/papers/thesis.pdf), p. 63 and 151-158)."
821,https://github.com/nltk/nltk/issues/2717,2717,[],closed,2021-05-25 13:09:59+00:00,,3,FreqDist Relative Frequencies (Corpus Linguistics),"`FreqDist` is a very useful tool for corpus linguists trying to generate frequency tables. However, in many cases, relative frequencies (i.e., normalized frequencies) are desired in these cases. 

There are two common ways of doing this: Either the frequency of an item is divided by the total number of items (e.g., words), or the frequency is normalized to something like ""X per million."" This allows us to compare frequencies between corpora of different sizes.

I believe that this could be a useful addition to the `FreqDist` that is relatively straightforward to implement. If I haven't overlooked this functionality and if there's interest, I'd be happy to work on a PR."
822,https://github.com/nltk/nltk/issues/2718,2718,[],closed,2021-05-28 06:52:23+00:00,,2,Error when using nltk.stem.arlstem2 module,"I was trying to use the stemming function in arlstem2 object and always found an error that there is no object called adject but, i tried to fix adject and replaced it with adjective in the source code it worked perfectly.

I just wanted to mention that it works for me :)

![SharedScreenshot](https://user-images.githubusercontent.com/64975785/119942357-02522380-bf92-11eb-98ee-0fbfe7d8d5a1.jpg)
"
823,https://github.com/nltk/nltk/issues/2721,2721,[],closed,2021-06-02 03:45:27+00:00,,0,WordNetCorpusReader.ic() does not add smoothing to N,"The `WordNetCorpusReader.ic()` function, which computes the concept-frequency mapping used in information content calculations, does not add the smoothing value to what Resnik 1995 ([arXiv link](https://arxiv.org/pdf/cmp-lg/9511007.pdf)) calls *N*, i.e., the total number of occurrences of words in a category, which is used in the denominator of the concept probability. For example, here I build the concept-frequency mapping (*freq(c)* in Resnik 1995) with a corpus of 1 word, *word*:

```python
>>> from nltk.corpus import wordnet as wn
>>> class FakeCorpus:
...   def words(self): return ['word']
... 
>>> freq = wn.ic(FakeCorpus(), weight_senses_equally=False, smoothing=1.0)
```

This dictionary is structured `freq[pos][synset_offset] = count` (where `count` is not strictly a count if `weight_senses_equally` is False). The special offset `0` is used for counting the total, *N*. Since *word* occurs once and it has 11 senses (1 verb, 10 nouns), each synset will accumulate 1/11 == 0.0909... for that one occurrence, in addition to the 1.0 for smoothing:

```python
>>> word = wn.synset('word.n.01')  # one of the 11 synsets
>>> freq['n'][word._offset]
1.0909090909090908
```

But the total for nouns and verbs is less than that, which means the probability of a synset is > 1.0 and the information content is less than 0:

```python
>>> freq['v'][0]  # 0.0909... * 1
0.09090909090909091
>>> freq['n'][0]  # 0.0909... * 10
0.9090909090909093
>>> freq['n'][word._offset] / freq['n'][0]  # p(c) = freq(c) / N
1.1999999999999997
>>> from nltk.corpus.reader.wordnet import information_content
>>> information_content(wn.synset('word.n.01'), freq)
-0.1823215567939544
```

When `weight_senses_equally` is True, each synset gets the full counts and not the counts divided by the number of synsets, so this situation would only come up if the smoothing value were greater than the number of counts:

```python
>>> freq = wn.ic(FakeCorpus(), weight_senses_equally=True, smoothing=1.0)
>>> freq['n'][0]
10.0
>>> freq['n'][word._offset]
2.0
>>> freq = wn.ic(FakeCorpus(), weight_senses_equally=True, smoothing=10.0)
>>> freq['n'][0]
10.0
>>> freq['n'][word._offset]
11.0
```

In conclusion, I think `WordNetCorpusReader.ic()` should initialize the *N* values with the smoothing argument. This will change the values of the similarity functions for generated IC frequency mappings, but the `wordnet.doctest` tests still pass because the `smoothing` value is set to 0.0, so we might need a new test.
"
824,https://github.com/nltk/nltk/issues/2724,2724,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 1026900607, 'node_id': 'MDU6TGFiZWwxMDI2OTAwNjA3', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wontfix', 'name': 'wontfix', 'color': 'f4b7ca', 'default': True, 'description': ''}]",closed,2021-06-07 21:35:58+00:00,,2,TweetTokenizer wrongly tokenizes emoticons,"For the following sentence ""it's a:::person"" the TweetTokenizer output is:
`[""it's"", 'a', '::', ':p', 'erson']`

Where the expected output should be:
`[""it's"", 'a', ':::', 'person']`"
825,https://github.com/nltk/nltk/issues/2725,2725,"[{'id': 718746024, 'node_id': 'MDU6TGFiZWw3MTg3NDYwMjQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/stem/lemma', 'name': 'stem/lemma', 'color': '53bab8', 'default': False, 'description': None}]",open,2021-06-09 11:15:29+00:00,,1,"Add suffix ""acion"" to Spanish SnowballStemmer","Hi, 

I'm a native Spanish speaker and I've noticed words with suffix ""acion"" don't get stemmed. (You got right the plural version ""aciones"", but not the singular one)

Please, add suffix ""acion"" to Spanish SnowballStemmer

Thank you for your work!"
826,https://github.com/nltk/nltk/issues/2727,2727,"[{'id': 81645781, 'node_id': 'MDU6TGFiZWw4MTY0NTc4MQ==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/language-model', 'name': 'language-model', 'color': 'd4c5f9', 'default': False, 'description': ''}]",open,2021-06-11 17:03:54+00:00,,3,Unable to evaluate perplexity and entropy of KneserNeyInterpolated and WittenBellInterpolated Language Model,"On nltk 3.6.2 google colab environment
The code
```python
lm = KneserNeyInterpolated(n)
lm.fit(train_ngrams, vocab)

test_sentences = list(flatten(pad_both_ends(sent, n=n) for sent in test_corpus))
entropy = lm.entropy(test_sentences)
```
gives error: 

> /usr/local/lib/python3.7/dist-packages/nltk/lm/smoothing.py in alpha_gamma(self, word, context)
>      49     def alpha_gamma(self, word, context):
>      50         prefix_counts = self.counts[context]
> ---> 51         prefix_total_ngrams = prefix_counts.N()
>      52         alpha = max(prefix_counts[word] - self.discount, 0.0) / prefix_total_ngrams
>      53         gamma = (
> 
> AttributeError: 'int' object has no attribute 'N'  

MLE and Laplace works fine. Thank you."
827,https://github.com/nltk/nltk/issues/2729,2729,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 975531894, 'node_id': 'MDU6TGFiZWw5NzU1MzE4OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/metrics', 'name': 'metrics', 'color': 'f442c8', 'default': False, 'description': ''}]",closed,2021-06-15 02:41:51+00:00,,0,Meteor unmatched-lists calculation,"Hi, 

I think [unmatched-lists calculation by _enum_stem_match()](https://github.com/nltk/nltk/blob/e4444c9b15762e6d86f5f6c4f5faadb87632c72a/nltk/translate/meteor_score.py#L109) is wrong,
```
enum_unmat_hypo_list = (
        list(zip(*enum_unmat_hypo_list)) if len(enum_unmat_hypo_list) > 0 else []
    )
enum_hypothesis_list = list(
        filter(lambda x: x[0] not in enum_unmat_hypo_list, enum_hypothesis_list)
    )
```
because the upper zip(*) generates [(unmatched indices), (unmatched stems)]
but what we want is to filter by (unmatched indices).
Also, ""not in"" would filter **out** unmatched, not the opposite.

When I fix the calculation to:
```
enum_unmat_hypo_list = (
        list(zip(*enum_unmat_hypo_list))[0] if len(enum_unmat_hypo_list) > 0 else []
    )
enum_hypothesis_list = list(
       filter(lambda x: x[0] in enum_unmat_hypo_list, enum_hypothesis_list)
    )
```

the score below is 0.655 with matches = [(0, 0), (1, 2), (2, 3)]
(Before fixing, it was 0.809 with matches = [(0, 0), (1, 2), (1, 2), (2, 3)] which contains duplicated elements)
```
hypothesis = 'I likes pizza'
reference = 'I really like pizza'
single_meteor_score(reference, hypothesis)
```


I would like to know if this is wrong.
Thanks."
828,https://github.com/nltk/nltk/issues/2731,2731,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}]",open,2021-06-15 09:29:43+00:00,,2,"""Smart"" issue","Whenever I write smart/smarter/smartest, nltk always defines it as a noun

Examples:

```py
import nltk


def part_of_speech(word):
    token = nltk.word_tokenize(word)
    pos = nltk.pos_tag(token)
    return pos


print(part_of_speech('smart'))  # Output: [('smart', 'NN')]
print(part_of_speech('smarter'))  # Output: [('smarter', 'NN')]
print(part_of_speech('smartest'))  # Output: [('smartest', 'NN')]
```"
829,https://github.com/nltk/nltk/issues/2732,2732,"[{'id': 975531894, 'node_id': 'MDU6TGFiZWw5NzU1MzE4OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/metrics', 'name': 'metrics', 'color': 'f442c8', 'default': False, 'description': ''}]",open,2021-06-16 02:58:55+00:00,,1,Question - Agreement Krippendorff's alpha handling missing values,"Documentation does not specify how to handle missing values:

https://github.com/nltk/nltk/blob/e4444c9b15762e6d86f5f6c4f5faadb87632c72a/nltk/metrics/agreement.py#L35-L44

In theory, Krippendorff's alpha CAN handle missing data (coders that do not assign an annotation to an item).

However, the documentation states:

```
Note that the data list needs to contain the same number of triples for each
individual coder, containing category values for the same set of items.
```

So, if we omit a triplet for missing data. Will the metrics be computed correctly? Do we have to put a place holder in the `labels` field of the triplet like np.nan or None?  

Something like?

`(""coder1"", ""item1"",  None)`

I am particularly interested in the multi-label case with the MASI distance and with labels defined in a `frozenset(['l1','l2'])`.
"
830,https://github.com/nltk/nltk/issues/2734,2734,[],closed,2021-06-22 21:02:27+00:00,,0,edit_distance() does not compute the actual Damerau-Levenshtein distance when transpositions is set to True,"I have recently noticed that `edit_distance(s1, s2, transpositions=True)` in `nltk.metrics.distance` does not compute the Damerau-Levenshtein distance between `s1` and `s2` -- i.e. the minimal number of operations among insertion,  deletion, substitution and transpositions required to transform `s1` into `s2`. As an example, 

`edit_distance(""ca"", ""abc"", transpositions=True)` returns 3, where the distance is actually 2 (it can be achieved by first transposing c and a, then inserting b in the middle).

I am unsure whether this is intended behavior (apologies if it is). It seems that what the function computes in its present state, is what is referred to as 'Optimal String Alignment Distance' on Wikipedia (https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance) -- which imho does not really have a natural and appealing interpretation. As a side note, despite its name, Optimal String Alignment is not a distance in the mathematical sense of the term (it does not satisfies the triangle inequality) which contradicts a comment in the heading of `nltk.metrics.distance`. 

Best,
Antoine"
831,https://github.com/nltk/nltk/issues/2735,2735,[],closed,2021-06-22 21:04:34+00:00,,2,Importing words throws numpy deprecation warning,"Warning: 

DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps"
832,https://github.com/nltk/nltk/issues/2738,2738,"[{'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",closed,2021-06-26 08:50:54+00:00,,1,Use Graphviz for Wordnet graph visualisations,"Currently, NLTK can draw the tree of hypernyms of one sense of ""person"" at a time, but it would be difficult to merge those trees into one graphic like the following interesting ""Figure 1: Hypernyms of the word person (all senses)"", extracted from the article ""WordNet and Prolog: why not?"",  Julian-Iranzo and Saenz-Perez, 2019 Conference of the International Fuzzy Systems Association and the European Society for Fuzzy Logic and Technology (EUSFLAT 2019).

![person-dot](https://user-images.githubusercontent.com/4782556/123507839-0abe7c80-d66c-11eb-8691-5909fa987436.png)

However, it turns out that the authors used a pretty simple approach: collect the set of edges of this graph, print that set in the format expected by the ""dot"" program from the ""graphviz"" package, and then rely on ""dot"" to draw the graph.

The same approach is quite easy to implement in NLTK, since a few NLTK modules (api, chart, dependencygraph) already interface with the ""dot"" program. Wouldn'it be nice to have this kind of more powerful graphical capabilities available for Wordnets also?"
833,https://github.com/nltk/nltk/issues/2739,2739,[],closed,2021-06-29 06:00:53+00:00,,1,"NLTK RegexpTokenizer('/s+', gaps = True) not working","I’m getting a weird output in NLTK.

```
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer('/s+', gaps = True)
tokenizer.tokenize(""won't is a contraction."")
```

Should ouput as
```
[""won't"", 'is', 'a', 'contraction']
```
Instead it outputs
```
[""won't is a contraction.""]
```"
834,https://github.com/nltk/nltk/issues/2742,2742,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}]",closed,2021-07-07 13:20:04+00:00,,2,3.6.2: sphinx apidoc integration,"I see that to generate documentation is used web/Makefile.
Theoretically this file is not necessary.

- setutool does not know where it is `copy.py` and it fails on generate documentation using `build_sphinx` command
```console
+ /usr/bin/python3 setup.py build_sphinx -b man --build-dir build/sphinx
running build_sphinx

Application error:
config directory doesn't contain a conf.py file (.)
```
Thi can be fixed by patch:
```patch
--- a/setup.cfg~        2021-04-20 05:49:36.000000000 +0100
+++ b/setup.cfg 2021-07-07 13:42:31.594240880 +0100
@@ -3,3 +3,6 @@
     LICENSE.txt
     AUTHORS.md
     README.md
+
+[build_sphinx]
+source-dir = web
```
After apply above patch:
```console
+ /usr/bin/python3 setup.py build_sphinx -b man --build-dir build/sphinx
running build_sphinx
Running Sphinx v4.0.3
making output directory... done
WARNING: html_static_path entry '_static' does not exist
building [mo]: targets for 0 po files that are out of date
building [man]: all manpages
updating environment: [new config] 6 added, 0 changed, 0 removed
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/app/__init__.py:45: UserWarning: nltk.app.wordfreq not loaded (requires the matplotlib library).
  warnings.warn(
reading sources... [100%] news
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/grammar.py:docstring of nltk.grammar.CFG.binarize:5: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/grammar.py:docstring of nltk.grammar.CFG.binarize:6: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/grammar.py:docstring of nltk.grammar.CFG.chomsky_normal_form:3: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/text.py:docstring of nltk.text.Text.generate:11: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.rst:130: WARNING: toctree contains reference to nonexisting document 'api/nltk.app'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.rst:130: WARNING: toctree contains reference to nonexisting document 'api/nltk.ccg'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.rst:130: WARNING: toctree contains reference to nonexisting document 'api/nltk.chat'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.rst:130: WARNING: toctree contains reference to nonexisting document 'api/nltk.chunk'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.rst:130: WARNING: toctree contains reference to nonexisting document 'api/nltk.classify'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.rst:130: WARNING: toctree contains reference to nonexisting document 'api/nltk.cluster'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.rst:130: WARNING: toctree contains reference to nonexisting document 'api/nltk.corpus'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.rst:130: WARNING: toctree contains reference to nonexisting document 'api/nltk.draw'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.rst:130: WARNING: toctree contains reference to nonexisting document 'api/nltk.inference'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.rst:130: WARNING: toctree contains reference to nonexisting document 'api/nltk.metrics'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.rst:130: WARNING: toctree contains reference to nonexisting document 'api/nltk.misc'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.rst:130: WARNING: toctree contains reference to nonexisting document 'api/nltk.parse'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.rst:130: WARNING: toctree contains reference to nonexisting document 'api/nltk.sem'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.rst:130: WARNING: toctree contains reference to nonexisting document 'api/nltk.stem'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.rst:130: WARNING: toctree contains reference to nonexisting document 'api/nltk.tag'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.rst:130: WARNING: toctree contains reference to nonexisting document 'api/nltk.test'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.rst:130: WARNING: toctree contains reference to nonexisting document 'api/nltk.tokenize'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/install.rst:9: WARNING: Title underline too short.

Setting up a Python Environment (Mac/Unix/Windows)
--------
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/news.rst:2: WARNING: Duplicate explicit target name: ""here"".
looking for now-outdated files... none found
pickling environment... done
checking consistency... done
writing... python-nltk.3 { news install data contribute api/nltk } done
build succeeded, 24 warnings.
```
So in this case in your web.Makefile is used `sphinx-apidoc -o api ../nltk` to generate `web/api` files however that can be done as well automatically by sphix whem below patch is applied:
```patch
--- a/web/conf.py~      2021-07-07 13:43:20.000000000 +0100
+++ b/web/conf.py       2021-07-07 14:00:03.084484177 +0100
@@ -29,11 +29,16 @@
 # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
 extensions = [
     'sphinx.ext.autodoc',
+    'sphinxcontrib.apidoc',
     'sphinx.ext.coverage',
     'sphinx.ext.imgmath',
     'sphinx.ext.viewcode',
 ]

+apidoc_module_dir = '../nltk'
+apidoc_output_dir = 'api'
+apidoc_separate_modules = False
+
 # Add any paths that contain templates here, relative to this directory.
 templates_path = ['_templates']
```
However wit that patch it is a lot of new warnings and some other issues
```console
+ /usr/bin/python3 setup.py build_sphinx -b man --build-dir build/sphinx
running build_sphinx
Running Sphinx v4.0.3
making output directory... done
WARNING: html_static_path entry '_static' does not exist
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.app.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.ccg.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.chat.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.chunk.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.classify.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.cluster.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.corpus.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.corpus.reader.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.draw.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.inference.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.lm.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.metrics.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.misc.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.parse.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.sem.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.sentiment.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.stem.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.tag.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.tbl.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.test.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.test.unit.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.test.unit.lm.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.test.unit.translate.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.tokenize.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.translate.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/nltk.twitter.rst.
Creating file /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/api/modules.rst.
building [mo]: targets for 0 po files that are out of date
building [man]: all manpages
updating environment: [new config] 32 added, 0 changed, 0 removed
*** Introductory Examples for the NLTK Book ***
Loading text1, ..., text9 and sent1, ..., sent9
Type the name of the text or sentence to view it.
Type: 'texts()' or 'sents()' to list the materials.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/app/__init__.py:45: UserWarning: nltk.app.wordfreq not loaded (requires the matplotlib library).
  warnings.warn(
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/twitter/__init__.py:21: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.
  warnings.warn(
reading sources... [100%] news
WARNING: autodoc: failed to import module 'book' from module 'nltk'; the following exception was raised:
Traceback (most recent call last):
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 83, in __load
    root = nltk.data.find(""{}/{}"".format(self.subdir, zip_name))
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/data.py"", line 583, in find
    raise LookupError(resource_not_found)
LookupError:
**********************************************************************
  Resource gutenberg not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('gutenberg')

  For more information see: https://www.nltk.org/data.html

  Attempted to load corpora/gutenberg.zip/gutenberg/

  Searched in:
    - '/home/tkloczko/nltk_data'
    - '/usr/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/sphinx/ext/autodoc/importer.py"", line 70, in import_module
    return importlib.import_module(modname)
  File ""/usr/lib64/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 843, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/book.py"", line 27, in <module>
    text1 = Text(gutenberg.words(""melville-moby_dick.txt""))
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 120, in __getattr__
    self.__load()
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 85, in __load
    raise e
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 80, in __load
    root = nltk.data.find(""{}/{}"".format(self.subdir, self.__name))
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/data.py"", line 583, in find
    raise LookupError(resource_not_found)
LookupError:
**********************************************************************
  Resource gutenberg not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('gutenberg')

  For more information see: https://www.nltk.org/data.html

  Attempted to load corpora/gutenberg

  Searched in:
    - '/home/tkloczko/nltk_data'
    - '/usr/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************


WARNING: autodoc: failed to import module 'cli' from module 'nltk'; the following exception was raised:
No module named 'tqdm'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/grammar.py:docstring of nltk.grammar.CFG.binarize:5: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/grammar.py:docstring of nltk.grammar.CFG.binarize:6: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/grammar.py:docstring of nltk.grammar.CFG.chomsky_normal_form:3: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/internals.py:docstring of nltk.internals.overridden:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/internals.py:docstring of nltk.internals.read_int:22: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/internals.py:docstring of nltk.internals.read_number:22: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/internals.py:docstring of nltk.internals.read_str:26: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/text.py:docstring of nltk.text.Text.generate:11: WARNING: Field list ends without a blank line; unexpected unindent.
WARNING: autodoc: failed to import module 'wordfreq_app' from module 'nltk.app'; the following exception was raised:
No module named 'matplotlib'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/ccg/chart.py:docstring of nltk.ccg.chart.CCGChartParser.parse:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/classify/senna.py:docstring of nltk.classify.senna:17: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/classify/senna.py:docstring of nltk.classify.senna:18: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/cluster/util.py:docstring of nltk.cluster.util.Dendrogram.show:3: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/cluster/util.py:docstring of nltk.cluster.util.Dendrogram.show:4: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/cluster/__init__.py:docstring of nltk.cluster:43: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/cluster/util.py:docstring of nltk.cluster.util.cosine_distance:1: WARNING: Undefined substitution referenced: ""u||v"".
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py:docstring of nltk.corpus.util.LazyCorpusLoader:26: WARNING: Inline emphasis start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py:docstring of nltk.corpus.util.LazyCorpusLoader:27: WARNING: Inline emphasis start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/cmudict.py:docstring of nltk.corpus.reader.cmudict.CMUDictCorpusReader.dict:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/cmudict.py:docstring of nltk.corpus.reader.cmudict.CMUDictCorpusReader.entries:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.AttrDict:3: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.doc:31: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.frame:36: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.frame_relations:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.lu:75: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.lu:76: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.lu:82: WARNING: Definition list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.lu:84: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.lu:85: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.lu:85: WARNING: Inline emphasis start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.lu:86: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/ieer.py:docstring of nltk.corpus.reader.ieer:6: WARNING: Inline emphasis start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/knbc.py:docstring of nltk.corpus.reader.knbc.KNBCorpusReader:16: WARNING: Unexpected section title.

Usage example
-------------
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.lemma_paras:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.lemma_sents:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.lemma_words:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.paras:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.raw:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.sents:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.tagged_paras:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.tagged_sents:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.tagged_words:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.words:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/nombank.py:docstring of nltk.corpus.reader.nombank.NombankCorpusReader.instances:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/nombank.py:docstring of nltk.corpus.reader.nombank.NombankCorpusReader.lines:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/nombank.py:docstring of nltk.corpus.reader.nombank.NombankCorpusReader.nouns:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/opinion_lexicon.py:docstring of nltk.corpus.reader.opinion_lexicon:4: WARNING: Bullet list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/opinion_lexicon.py:docstring of nltk.corpus.reader.opinion_lexicon:15: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/panlex_lite.py:docstring of nltk.corpus.reader.panlex_lite.PanLexLiteCorpusReader.translations:9: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/panlex_lite.py:docstring of nltk.corpus.reader.panlex_lite.PanLexLiteCorpusReader.translations:11: WARNING: Definition list ends without a blank line; unexpected unindent.
WARNING: error while formatting arguments for nltk.corpus.reader.plaintext.EuroparlCorpusReader:
WARNING: error while formatting arguments for nltk.corpus.reader.plaintext.PlaintextCorpusReader:
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/plaintext.py:docstring of nltk.corpus.reader.plaintext.EuroparlCorpusReader:6: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/propbank.py:docstring of nltk.corpus.reader.propbank.PropbankCorpusReader.instances:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/propbank.py:docstring of nltk.corpus.reader.propbank.PropbankCorpusReader.lines:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/propbank.py:docstring of nltk.corpus.reader.propbank.PropbankCorpusReader.verbs:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/reviews.py:docstring of nltk.corpus.reader.reviews:4: WARNING: Bullet list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/reviews.py:docstring of nltk.corpus.reader.reviews:36: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/reviews.py:docstring of nltk.corpus.reader.reviews:38: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/timit.py:docstring of nltk.corpus.reader.timit:12: WARNING: Bullet list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/timit.py:docstring of nltk.corpus.reader.timit.TimitCorpusReader.spkrutteranceids:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/timit.py:docstring of nltk.corpus.reader.timit.TimitCorpusReader.transcription_dict:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/timit.py:docstring of nltk.corpus.reader.timit.TimitCorpusReader.utteranceids:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/twitter.py:docstring of nltk.corpus.reader.twitter.TwitterCorpusReader.docs:6: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/twitter.py:docstring of nltk.corpus.reader.twitter.TwitterCorpusReader.tokenized:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/verbnet.py:docstring of nltk.corpus.reader.verbnet.VerbnetCorpusReader.pprint:7: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordlist.py:docstring of nltk.corpus.reader.wordlist.MWAPPDBCorpusReader:4: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.Synset.max_depth:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.Synset.min_depth:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.custom_lemmas:9: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.ic:5: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.synset_from_pos_and_offset:5: WARNING: Bullet list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.synset_from_sense_key:6: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.synset_from_sense_key:7: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.synset_from_sense_key:12: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.synset_from_sense_key:18: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.synset_from_sense_key:21: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.synset_from_sense_key:22: WARNING: Block quote ends without a blank line; unexpected unindent.
WARNING: error while formatting arguments for nltk.corpus.reader.EuroparlCorpusReader:
WARNING: error while formatting arguments for nltk.corpus.reader.PlaintextCorpusReader:
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/cmudict.py:docstring of nltk.corpus.reader.cmudict.CMUDictCorpusReader.dict:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/cmudict.py:docstring of nltk.corpus.reader.cmudict.CMUDictCorpusReader.entries:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/plaintext.py:docstring of nltk.corpus.reader.plaintext.EuroparlCorpusReader:6: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.doc:31: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.frame:36: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.frame_relations:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.lu:75: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.lu:76: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.lu:82: WARNING: Definition list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.lu:84: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.lu:85: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.lu:85: WARNING: Inline emphasis start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/framenet.py:docstring of nltk.corpus.reader.framenet.FramenetCorpusReader.lu:86: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/knbc.py:docstring of nltk.corpus.reader.knbc.KNBCorpusReader:16: WARNING: Unexpected section title.

Usage example
-------------
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.lemma_paras:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.lemma_sents:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.lemma_words:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.paras:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.raw:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.sents:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.tagged_paras:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.tagged_sents:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.tagged_words:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/mte.py:docstring of nltk.corpus.reader.mte.MTECorpusReader.words:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordlist.py:docstring of nltk.corpus.reader.wordlist.MWAPPDBCorpusReader:4: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/nombank.py:docstring of nltk.corpus.reader.nombank.NombankCorpusReader.instances:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/nombank.py:docstring of nltk.corpus.reader.nombank.NombankCorpusReader.lines:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/nombank.py:docstring of nltk.corpus.reader.nombank.NombankCorpusReader.nouns:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/panlex_lite.py:docstring of nltk.corpus.reader.panlex_lite.PanLexLiteCorpusReader.translations:9: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/panlex_lite.py:docstring of nltk.corpus.reader.panlex_lite.PanLexLiteCorpusReader.translations:11: WARNING: Definition list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/propbank.py:docstring of nltk.corpus.reader.propbank.PropbankCorpusReader.instances:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/propbank.py:docstring of nltk.corpus.reader.propbank.PropbankCorpusReader.lines:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/propbank.py:docstring of nltk.corpus.reader.propbank.PropbankCorpusReader.verbs:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/timit.py:docstring of nltk.corpus.reader.timit.TimitCorpusReader.spkrutteranceids:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/timit.py:docstring of nltk.corpus.reader.timit.TimitCorpusReader.transcription_dict:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/timit.py:docstring of nltk.corpus.reader.timit.TimitCorpusReader.utteranceids:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/twitter.py:docstring of nltk.corpus.reader.twitter.TwitterCorpusReader.docs:6: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/twitter.py:docstring of nltk.corpus.reader.twitter.TwitterCorpusReader.tokenized:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/verbnet.py:docstring of nltk.corpus.reader.verbnet.VerbnetCorpusReader.pprint:7: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.custom_lemmas:9: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.ic:5: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.synset_from_pos_and_offset:5: WARNING: Bullet list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.synset_from_sense_key:6: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.synset_from_sense_key:7: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.synset_from_sense_key:12: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.synset_from_sense_key:18: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.synset_from_sense_key:21: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.synset_from_sense_key:22: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/bracket_parse.py:docstring of nltk.corpus.reader.bracket_parse.AlpinoCorpusReader:1: WARNING: Unknown target name: ""tag"".
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/bracket_parse.py:docstring of nltk.corpus.reader.bracket_parse.AlpinoCorpusReader:1: WARNING: Unknown target name: ""word"".
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/bracket_parse.py:docstring of nltk.corpus.reader.bracket_parse.AlpinoCorpusReader:1: WARNING: Unknown target name: ""tag"".
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/bracket_parse.py:docstring of nltk.corpus.reader.bracket_parse.AlpinoCorpusReader:1: WARNING: Unknown target name: ""word"".
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/draw/util.py:docstring of nltk.draw.util.CanvasWidget:57: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/draw/util.py:docstring of nltk.draw.util.ColorizedList:3: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/draw/util.py:docstring of nltk.draw.util.ColorizedList:8: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/inference/api.py:docstring of nltk.inference.api.BaseTheoremToolCommand.retract_assumptions:4: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/inference/api.py:docstring of nltk.inference.api.TheoremToolCommand.retract_assumptions:4: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/inference/api.py:docstring of nltk.inference.api.TheoremToolCommandDecorator.retract_assumptions:4: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/inference/nonmonotonic.py:docstring of nltk.inference.nonmonotonic.ClosedDomainProver.replace_quants:6: WARNING: Definition list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/inference/prover9.py:docstring of nltk.inference.prover9.Prover9Parent.prover9_input:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/inference/resolution.py:docstring of nltk.inference.resolution.Clause.substitute_bindings:4: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/inference/resolution.py:docstring of nltk.inference.resolution.Clause.unify:6: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/lm/api.py:docstring of nltk.lm.api.LanguageModel.generate:6: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/lm/api.py:docstring of nltk.lm.api.LanguageModel.unmasked_score:9: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/lm/models.py:docstring of nltk.lm.models.InterpolatedLanguageModel.unmasked_score:9: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/lm/preprocessing.py:docstring of nltk.lm.preprocessing.padded_everygram_pipeline:9: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/lm/vocabulary.py:docstring of nltk.lm.vocabulary.Vocabulary:5: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/lm/vocabulary.py:docstring of nltk.lm.vocabulary.Vocabulary:6: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/lm/vocabulary.py:docstring of nltk.lm.vocabulary.Vocabulary:5: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/lm/vocabulary.py:docstring of nltk.lm.vocabulary.Vocabulary:6: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/association.py:docstring of nltk.metrics.association.BigramAssocMeasures:11: WARNING: Inline emphasis start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/association.py:docstring of nltk.metrics.association.BigramAssocMeasures:11: WARNING: Inline emphasis start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/association.py:docstring of nltk.metrics.association.QuadgramAssocMeasures:10: WARNING: Inline emphasis start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/association.py:docstring of nltk.metrics.association.QuadgramAssocMeasures:10: WARNING: Inline emphasis start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/association.py:docstring of nltk.metrics.association.QuadgramAssocMeasures:10: WARNING: Inline emphasis start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/association.py:docstring of nltk.metrics.association.TrigramAssocMeasures:9: WARNING: Inline emphasis start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/association.py:docstring of nltk.metrics.association.TrigramAssocMeasures:9: WARNING: Inline emphasis start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/distance.py:docstring of nltk.metrics.distance.edit_distance:22: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/distance.py:docstring of nltk.metrics.distance.edit_distance_align:24: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/distance.py:docstring of nltk.metrics.distance.jaro_winkler_similarity:7: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/distance.py:docstring of nltk.metrics.distance.jaro_winkler_similarity:14: WARNING: Bullet list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/distance.py:docstring of nltk.metrics.distance.jaro_winkler_similarity:16: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/distance.py:docstring of nltk.metrics.distance.jaro_winkler_similarity:18: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/distance.py:docstring of nltk.metrics.distance.jaro_winkler_similarity:34: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/distance.py:docstring of nltk.metrics.distance.jaro_winkler_similarity:63: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/paice.py:docstring of nltk.metrics.paice.get_words_from_dictionary:4: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/segmentation.py:docstring of nltk.metrics.segmentation:7: WARNING: Definition list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/segmentation.py:docstring of nltk.metrics.segmentation.ghd:37: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/distance.py:docstring of nltk.metrics.distance.jaro_similarity:11: WARNING: Undefined substitution referenced: ""s_1"".
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/distance.py:docstring of nltk.metrics.distance.jaro_similarity:14: WARNING: Undefined substitution referenced: ""s_i"".
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/metrics/segmentation.py:docstring of nltk.metrics.segmentation.ghd:37: WARNING: Undefined substitution referenced: ""i - j"".
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/misc/minimalset.py:docstring of nltk.misc.minimalset.MinimalSet.contexts:5: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/api.py:docstring of nltk.parse.api.ParserI.parse:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/bllip.py:docstring of nltk.parse.bllip.BllipParser.from_unified_model_dir:8: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/bllip.py:docstring of nltk.parse.bllip.BllipParser.from_unified_model_dir:13: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/bllip.py:docstring of nltk.parse.bllip.BllipParser.parse:6: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/bllip.py:docstring of nltk.parse.bllip.BllipParser.tagged_parse:9: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/chart.py:docstring of nltk.parse.chart.AbstractChartRule:5: WARNING: Inline literal start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/chart.py:docstring of nltk.parse.chart.ChartParser.parse:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/chart.py:docstring of nltk.parse.chart.SteppingChartParser.parse:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/corenlp.py:docstring of nltk.parse.corenlp.CoreNLPDependencyParser:105: WARNING: Unexpected section title.

Special cases
-------------
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/corenlp.py:docstring of nltk.parse.corenlp.CoreNLPParser:123: WARNING: Unexpected section title.

Special cases
-------------
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/dependencygraph.py:docstring of nltk.parse.dependencygraph.DependencyGraph.load:3: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/featurechart.py:docstring of nltk.parse.featurechart.FeatureSingleEdgeFundamentalRule:1: WARNING: Inline literal start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/featurechart.py:docstring of nltk.parse.featurechart.FeatureTopDownPredictRule:1: WARNING: Inline literal start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/featurechart.py:docstring of nltk.parse.featurechart.InstantiateVarsChart:1: WARNING: Inline literal start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/malt.py:docstring of nltk.parse.malt.MaltParser.parse_tagged_sents:8: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/nonprojectivedependencyparser.py:docstring of nltk.parse.nonprojectivedependencyparser.DemoScorer.score:3: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/nonprojectivedependencyparser.py:docstring of nltk.parse.nonprojectivedependencyparser.DemoScorer.score:14: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/nonprojectivedependencyparser.py:docstring of nltk.parse.nonprojectivedependencyparser.DemoScorer.score:18: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/nonprojectivedependencyparser.py:docstring of nltk.parse.nonprojectivedependencyparser.DemoScorer.train:3: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/nonprojectivedependencyparser.py:docstring of nltk.parse.nonprojectivedependencyparser.DependencyScorerI.score:3: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/nonprojectivedependencyparser.py:docstring of nltk.parse.nonprojectivedependencyparser.DependencyScorerI.score:14: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/nonprojectivedependencyparser.py:docstring of nltk.parse.nonprojectivedependencyparser.DependencyScorerI.score:18: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/nonprojectivedependencyparser.py:docstring of nltk.parse.nonprojectivedependencyparser.DependencyScorerI.train:3: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/nonprojectivedependencyparser.py:docstring of nltk.parse.nonprojectivedependencyparser.ProbabilisticNonprojectiveParser:31: WARNING: Unexpected section title.

Rule based example
------------------
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/nonprojectivedependencyparser.py:docstring of nltk.parse.nonprojectivedependencyparser.ProbabilisticNonprojectiveParser.best_incoming_arc:6: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/nonprojectivedependencyparser.py:docstring of nltk.parse.nonprojectivedependencyparser.ProbabilisticNonprojectiveParser.compute_max_subtract_score:7: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/nonprojectivedependencyparser.py:docstring of nltk.parse.nonprojectivedependencyparser.ProbabilisticNonprojectiveParser.compute_original_indexes:9: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/pchart.py:docstring of nltk.parse.pchart.BottomUpProbabilisticChartParser.parse:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/projectivedependencyparser.py:docstring of nltk.parse.projectivedependencyparser.ProbabilisticProjectiveDependencyParser:11: WARNING: Unexpected section title.

Usage example
-------------
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/recursivedescent.py:docstring of nltk.parse.recursivedescent.RecursiveDescentParser.parse:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/recursivedescent.py:docstring of nltk.parse.recursivedescent.SteppingRecursiveDescentParser.parse:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/shiftreduce.py:docstring of nltk.parse.shiftreduce.ShiftReduceParser.parse:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/shiftreduce.py:docstring of nltk.parse.shiftreduce.SteppingShiftReduceParser.parse:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/transitionparser.py:docstring of nltk.parse.transitionparser.Transition.left_arc:3: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/transitionparser.py:docstring of nltk.parse.transitionparser.Transition.reduce:3: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/transitionparser.py:docstring of nltk.parse.transitionparser.Transition.right_arc:3: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/transitionparser.py:docstring of nltk.parse.transitionparser.Transition.shift:3: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/util.py:docstring of nltk.parse.util.TestGrammar.run:4: WARNING: Definition list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/util.py:docstring of nltk.parse.util.extract_test_sentences:3: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/util.py:docstring of nltk.parse.util.extract_test_sentences:5: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/parse/viterbi.py:docstring of nltk.parse.viterbi.ViterbiParser.parse:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/sem/boxer.py:docstring of nltk.sem.boxer:10: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/sem/boxer.py:docstring of nltk.sem.boxer.BoxerOutputDrsParser.parse:5: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/sem/glue.py:docstring of nltk.sem.glue.GlueDict.get_meaning_formula:2: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/sem/logic.py:docstring of nltk.sem.logic.LogicParser.parse:5: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/sem/util.py:docstring of nltk.sem.util.parse_sents:8: WARNING: Inline literal start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/sentiment/sentiment_analyzer.py:docstring of nltk.sentiment.sentiment_analyzer.SentimentAnalyzer.all_words:4: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/sentiment/sentiment_analyzer.py:docstring of nltk.sentiment.sentiment_analyzer.SentimentAnalyzer.all_words:6: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/sentiment/util.py:docstring of nltk.sentiment.util.demo_movie_reviews:5: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/sentiment/util.py:docstring of nltk.sentiment.util.demo_tweets:4: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/stem/porter.py:docstring of nltk.stem.porter.PorterStemmer:33: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/stem/porter.py:docstring of nltk.stem.porter.PorterStemmer:39: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/stem/snowball.py:docstring of nltk.stem.snowball.ArabicStemmer:4: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/stem/snowball.py:docstring of nltk.stem.snowball.ArabicStemmer:6: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/stem/snowball.py:docstring of nltk.stem.snowball.ArabicStemmer.stem:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/stem/util.py:docstring of nltk.stem.util.prefix_replace:2: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tag/brill_trainer.py:docstring of nltk.tag.brill_trainer.BrillTaggerTrainer.train:100: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tag/brill_trainer.py:docstring of nltk.tag.brill_trainer.BrillTaggerTrainer.train:101: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tag/brill_trainer.py:docstring of nltk.tag.brill_trainer.BrillTaggerTrainer.train:103: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tag/crf.py:docstring of nltk.tag.crf.CRFTagger.tag:2: WARNING: Inline literal start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tag/crf.py:docstring of nltk.tag.crf.CRFTagger.tag:3: WARNING: Inline literal start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tag/crf.py:docstring of nltk.tag.crf.CRFTagger.tag:4: WARNING: Definition list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tag/crf.py:docstring of nltk.tag.crf.CRFTagger.tag_sents:2: WARNING: Inline literal start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tag/crf.py:docstring of nltk.tag.crf.CRFTagger.tag_sents:3: WARNING: Inline literal start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tag/crf.py:docstring of nltk.tag.crf.CRFTagger.tag_sents:4: WARNING: Definition list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tag/perceptron.py:docstring of nltk.tag.perceptron.PerceptronTagger:3: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tag/senna.py:docstring of nltk.tag.senna:5: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tag/senna.py:docstring of nltk.tag.senna:6: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tbl/feature.py:docstring of nltk.tbl.feature.Feature.expand:34: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tbl/feature.py:docstring of nltk.tbl.feature.Feature.expand:36: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tbl/feature.py:docstring of nltk.tbl.feature.Feature.expand:37: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tbl/rule.py:docstring of nltk.tbl.rule.Rule:14: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tbl/template.py:docstring of nltk.tbl.template.BrillTemplateI.applicable_rules:1: WARNING: Inline emphasis start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tbl/template.py:docstring of nltk.tbl.template.BrillTemplateI.applicable_rules:6: WARNING: Inline emphasis start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tbl/template.py:docstring of nltk.tbl.template.BrillTemplateI.applicable_rules:14: WARNING: Inline emphasis start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tbl/template.py:docstring of nltk.tbl.template.Template.applicable_rules:1: WARNING: Inline emphasis start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tbl/template.py:docstring of nltk.tbl.template.Template.applicable_rules:6: WARNING: Inline emphasis start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tbl/template.py:docstring of nltk.tbl.template.Template.applicable_rules:14: WARNING: Inline emphasis start-string without end-string.
WARNING: autodoc: failed to import module 'test_corpora' from module 'nltk.test.unit'; the following exception was raised:
Traceback (most recent call last):
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 83, in __load
    root = nltk.data.find(""{}/{}"".format(self.subdir, zip_name))
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/data.py"", line 583, in find
    raise LookupError(resource_not_found)
LookupError:
**********************************************************************
  Resource ptb not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('ptb')

  For more information see: https://www.nltk.org/data.html

  Attempted to load corpora/ptb.zip/ptb/

  Searched in:
    - '/home/tkloczko/nltk_data'
    - '/usr/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/sphinx/ext/autodoc/importer.py"", line 70, in import_module
    return importlib.import_module(modname)
  File ""/usr/lib64/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 843, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/test/unit/test_corpora.py"", line 187, in <module>
    not ptb.fileids(),
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 120, in __getattr__
    self.__load()
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 85, in __load
    raise e
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 80, in __load
    root = nltk.data.find(""{}/{}"".format(self.subdir, self.__name))
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/data.py"", line 583, in find
    raise LookupError(resource_not_found)
LookupError:
**********************************************************************
  Resource ptb not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('ptb')

  For more information see: https://www.nltk.org/data.html

  Attempted to load corpora/ptb

  Searched in:
    - '/home/tkloczko/nltk_data'
    - '/usr/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************


WARNING: autodoc: failed to import module 'test_nombank' from module 'nltk.test.unit'; the following exception was raised:
Traceback (most recent call last):
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 83, in __load
    root = nltk.data.find(""{}/{}"".format(self.subdir, zip_name))
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/data.py"", line 583, in find
    raise LookupError(resource_not_found)
LookupError:
**********************************************************************
  Resource nombank.1.0 not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('nombank.1.0')

  For more information see: https://www.nltk.org/data.html

  Attempted to load corpora/nombank.1.0.zip/nombank.1.0/

  Searched in:
    - '/home/tkloczko/nltk_data'
    - '/usr/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/sphinx/ext/autodoc/importer.py"", line 70, in import_module
    return importlib.import_module(modname)
  File ""/usr/lib64/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 843, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/test/unit/test_nombank.py"", line 10, in <module>
    nombank.nouns()
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 120, in __getattr__
    self.__load()
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 85, in __load
    raise e
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 80, in __load
    root = nltk.data.find(""{}/{}"".format(self.subdir, self.__name))
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/data.py"", line 583, in find
    raise LookupError(resource_not_found)
LookupError:
**********************************************************************
  Resource nombank.1.0 not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('nombank.1.0')

  For more information see: https://www.nltk.org/data.html

  Attempted to load corpora/nombank.1.0

  Searched in:
    - '/home/tkloczko/nltk_data'
    - '/usr/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************


/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/test/unit/test_stem.py:docstring of nltk.test.unit.test_stem.PorterTest.test_vocabulary_martin_mode:6: WARNING: Definition list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/test/unit/test_stem.py:docstring of nltk.test.unit.test_stem.PorterTest.test_vocabulary_martin_mode:8: WARNING: Unexpected indentation.
WARNING: autodoc: failed to import module 'test_twitter_auth' from module 'nltk.test.unit'; the following exception was raised:
Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/sphinx/ext/autodoc/importer.py"", line 70, in import_module
    return importlib.import_module(modname)
  File ""/usr/lib64/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 843, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/test/unit/test_twitter_auth.py"", line 9, in <module>
    pytest.importorskip(""twython"")
  File ""/usr/lib/python3.8/site-packages/_pytest/outcomes.py"", line 212, in importorskip
    raise Skipped(reason, allow_module_level=True) from None
Skipped: could not import 'twython': No module named 'twython'

WARNING: autodoc: failed to import module 'test_wordnet' from module 'nltk.test.unit'; the following exception was raised:
Traceback (most recent call last):
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 83, in __load
    root = nltk.data.find(""{}/{}"".format(self.subdir, zip_name))
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/data.py"", line 583, in find
    raise LookupError(resource_not_found)
LookupError:
**********************************************************************
  Resource wordnet not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('wordnet')

  For more information see: https://www.nltk.org/data.html

  Attempted to load corpora/wordnet.zip/wordnet/

  Searched in:
    - '/home/tkloczko/nltk_data'
    - '/usr/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/sphinx/ext/autodoc/importer.py"", line 70, in import_module
    return importlib.import_module(modname)
  File ""/usr/lib64/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 843, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/test/unit/test_wordnet.py"", line 12, in <module>
    wn.ensure_loaded()
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 120, in __getattr__
    self.__load()
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 85, in __load
    raise e
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 80, in __load
    root = nltk.data.find(""{}/{}"".format(self.subdir, self.__name))
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/data.py"", line 583, in find
    raise LookupError(resource_not_found)
LookupError:
**********************************************************************
  Resource wordnet not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('wordnet')

  For more information see: https://www.nltk.org/data.html

  Attempted to load corpora/wordnet

  Searched in:
    - '/home/tkloczko/nltk_data'
    - '/usr/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************


/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tokenize/legality_principle.py:docstring of nltk.tokenize.legality_principle:1: WARNING: Inline literal start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tokenize/legality_principle.py:docstring of nltk.tokenize.legality_principle:1: WARNING: Inline literal start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tokenize/legality_principle.py:docstring of nltk.tokenize.legality_principle:1: WARNING: Inline literal start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tokenize/legality_principle.py:docstring of nltk.tokenize.legality_principle:1: WARNING: Inline literal start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tokenize/legality_principle.py:docstring of nltk.tokenize.legality_principle:8: WARNING: Inline literal start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tokenize/legality_principle.py:docstring of nltk.tokenize.legality_principle:8: WARNING: Inline literal start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tokenize/legality_principle.py:docstring of nltk.tokenize.legality_principle:24: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tokenize/legality_principle.py:docstring of nltk.tokenize.legality_principle:25: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tokenize/legality_principle.py:docstring of nltk.tokenize.legality_principle:25: WARNING: Inline literal start-string without end-string.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tokenize/legality_principle.py:docstring of nltk.tokenize.legality_principle:26: WARNING: Inline literal start-string without end-string.
WARNING: autodoc: failed to import module 'nist' from module 'nltk.tokenize'; the following exception was raised:
Traceback (most recent call last):
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 83, in __load
    root = nltk.data.find(""{}/{}"".format(self.subdir, zip_name))
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/data.py"", line 583, in find
    raise LookupError(resource_not_found)
LookupError:
**********************************************************************
  Resource perluniprops not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('perluniprops')

  For more information see: https://www.nltk.org/data.html

  Attempted to load misc/perluniprops.zip/perluniprops/

  Searched in:
    - '/home/tkloczko/nltk_data'
    - '/usr/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/sphinx/ext/autodoc/importer.py"", line 70, in import_module
    return importlib.import_module(modname)
  File ""/usr/lib64/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 843, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tokenize/nist.py"", line 27, in <module>
    class NISTTokenizer(TokenizerI):
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tokenize/nist.py"", line 94, in NISTTokenizer
    pup_number = str("""".join(set(perluniprops.chars(""Number""))))  # i.e. \p{N}
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 120, in __getattr__
    self.__load()
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 85, in __load
    raise e
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/util.py"", line 80, in __load
    root = nltk.data.find(""{}/{}"".format(self.subdir, self.__name))
  File ""/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/data.py"", line 583, in find
    raise LookupError(resource_not_found)
LookupError:
**********************************************************************
  Resource perluniprops not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('perluniprops')

  For more information see: https://www.nltk.org/data.html

  Attempted to load misc/perluniprops

  Searched in:
    - '/home/tkloczko/nltk_data'
    - '/usr/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************


/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tokenize/sonority_sequencing.py:docstring of nltk.tokenize.sonority_sequencing:17: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tokenize/sonority_sequencing.py:docstring of nltk.tokenize.sonority_sequencing:18: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tokenize/treebank.py:docstring of nltk.tokenize.treebank.TreebankWordDetokenizer:6: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/tokenize/treebank.py:docstring of nltk.tokenize.treebank.TreebankWordDetokenizer:7: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/bleu_score.py:docstring of nltk.translate.bleu_score.SmoothingFunction.method3:6: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/bleu_score.py:docstring of nltk.translate.bleu_score.SmoothingFunction.method3:8: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/bleu_score.py:docstring of nltk.translate.bleu_score.brevity_penalty:74: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm1.py:docstring of nltk.translate.ibm1:25: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm1.py:docstring of nltk.translate.ibm1:34: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm1.py:docstring of nltk.translate.ibm1:35: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm1.py:docstring of nltk.translate.ibm1:37: WARNING: Definition list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm2.py:docstring of nltk.translate.ibm2:9: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm2.py:docstring of nltk.translate.ibm2:11: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm2.py:docstring of nltk.translate.ibm2:12: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm2.py:docstring of nltk.translate.ibm2:21: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm2.py:docstring of nltk.translate.ibm2:22: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm2.py:docstring of nltk.translate.ibm2:24: WARNING: Definition list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm3.py:docstring of nltk.translate.ibm3:24: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm3.py:docstring of nltk.translate.ibm3:26: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm3.py:docstring of nltk.translate.ibm3:27: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm3.py:docstring of nltk.translate.ibm3:44: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm3.py:docstring of nltk.translate.ibm3:45: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm3.py:docstring of nltk.translate.ibm3:47: WARNING: Definition list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm3.py:docstring of nltk.translate.ibm3:53: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm3.py:docstring of nltk.translate.ibm3:54: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm4.py:docstring of nltk.translate.ibm4:18: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm4.py:docstring of nltk.translate.ibm4:20: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm4.py:docstring of nltk.translate.ibm4:42: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm4.py:docstring of nltk.translate.ibm4:45: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm4.py:docstring of nltk.translate.ibm4:50: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm4.py:docstring of nltk.translate.ibm4:52: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm4.py:docstring of nltk.translate.ibm4:53: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm4.py:docstring of nltk.translate.ibm4:71: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm4.py:docstring of nltk.translate.ibm4:72: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm4.py:docstring of nltk.translate.ibm4:74: WARNING: Definition list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm4.py:docstring of nltk.translate.ibm4:80: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm4.py:docstring of nltk.translate.ibm4:81: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm5.py:docstring of nltk.translate.ibm5:25: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm5.py:docstring of nltk.translate.ibm5:33: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm5.py:docstring of nltk.translate.ibm5:43: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm5.py:docstring of nltk.translate.ibm5:44: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm5.py:docstring of nltk.translate.ibm5:46: WARNING: Enumerated list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm5.py:docstring of nltk.translate.ibm5:50: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm5.py:docstring of nltk.translate.ibm5:52: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm5.py:docstring of nltk.translate.ibm5:53: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm5.py:docstring of nltk.translate.ibm5:56: WARNING: Enumerated list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm5.py:docstring of nltk.translate.ibm5:59: WARNING: Enumerated list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm5.py:docstring of nltk.translate.ibm5:72: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm5.py:docstring of nltk.translate.ibm5:73: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm5.py:docstring of nltk.translate.ibm5:75: WARNING: Definition list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm5.py:docstring of nltk.translate.ibm5:81: WARNING: Unexpected indentation.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/ibm5.py:docstring of nltk.translate.ibm5:82: WARNING: Block quote ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/stack_decoder.py:docstring of nltk.translate.stack_decoder.StackDecoder.compute_future_scores:10: WARNING: Field list ends without a blank line; unexpected unindent.
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/twitter/common.py:docstring of nltk.twitter.common:1: WARNING: Unknown interpreted text role ""module"".
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/twitter/common.py:docstring of nltk.twitter.common.json2csv_entities:16: WARNING: Field list ends without a blank line; unexpected unindent.
WARNING: autodoc: failed to import module 'twitter_demo' from module 'nltk.twitter'; the following exception was raised:
cannot import name 'Query' from 'nltk.twitter' (/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/twitter/__init__.py)
WARNING: autodoc: failed to import module 'twitterclient' from module 'nltk.twitter'; the following exception was raised:
No module named 'twython'
WARNING: autodoc: failed to import module 'util' from module 'nltk.twitter'; the following exception was raised:
No module named 'twython'
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/install.rst:9: WARNING: Title underline too short.

Setting up a Python Environment (Mac/Unix/Windows)
--------
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/web/news.rst:2: WARNING: Duplicate explicit target name: ""here"".
looking for now-outdated files... none found
pickling environment... done
checking consistency... done
writing... python-nltk.3 { news install data contribute api/nltk api/nltk.app api/nltk.ccg api/nltk.chat api/nltk.chunk api/nltk.classify api/nltk.cluster api/nltk.corpus api/nltk.corpus.reader api/nltk.draw api/nltk.inference api/nltk.lm api/nltk.metrics api/nltk.misc api/nltk.parse api/nltk.sem api/nltk.sentiment api/nltk.stem api/nltk.tag api/nltk.tbl api/nltk.test api/nltk.test.unit api/nltk.test.unit.lm api/nltk.test.unit.translate api/nltk.tokenize api/nltk.translate api/nltk.twitter } /home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/corpus/reader/wordnet.py:docstring of nltk.corpus.reader.wordnet.WordNetCorpusReader.ic:: WARNING: more than one target found for cross-reference 'CorpusReader': nltk.corpus.reader.api.CorpusReader, nltk.corpus.reader.CorpusReader
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/inference/discourse.py:docstring of nltk.inference.discourse.DiscourseTester.multiply:: WARNING: more than one target found for cross-reference 'Expression': nltk.sem.linearlogic.Expression, nltk.sem.logic.Expression
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/inference/discourse.py:docstring of nltk.inference.discourse.ReadingCommand.combine_readings:: WARNING: more than one target found for cross-reference 'Expression': nltk.sem.linearlogic.Expression, nltk.sem.logic.Expression
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/inference/discourse.py:docstring of nltk.inference.discourse.ReadingCommand.combine_readings:: WARNING: more than one target found for cross-reference 'Expression': nltk.sem.linearlogic.Expression, nltk.sem.logic.Expression
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/inference/discourse.py:docstring of nltk.inference.discourse.ReadingCommand.process_thread:: WARNING: more than one target found for cross-reference 'Expression': nltk.sem.linearlogic.Expression, nltk.sem.logic.Expression
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/inference/discourse.py:docstring of nltk.inference.discourse.ReadingCommand.process_thread:: WARNING: more than one target found for cross-reference 'Expression': nltk.sem.linearlogic.Expression, nltk.sem.logic.Expression
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/inference/discourse.py:docstring of nltk.inference.discourse.ReadingCommand.to_fol:: WARNING: more than one target found for cross-reference 'Expression': nltk.sem.linearlogic.Expression, nltk.sem.logic.Expression
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/inference/discourse.py:docstring of nltk.inference.discourse.ReadingCommand.to_fol:: WARNING: more than one target found for cross-reference 'Expression': nltk.sem.linearlogic.Expression, nltk.sem.logic.Expression
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/inference/discourse.py:docstring of nltk.inference.discourse.load_fol:: WARNING: more than one target found for cross-reference 'Expression': nltk.sem.linearlogic.Expression, nltk.sem.logic.Expression
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/sem/evaluate.py:docstring of nltk.sem.evaluate.Model.satisfiers:: WARNING: more than one target found for cross-reference 'Expression': nltk.sem.linearlogic.Expression, nltk.sem.logic.Expression
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/meteor_score.py:docstring of nltk.translate.meteor_score.allign_words:: WARNING: more than one target found for cross-reference 'WordNetCorpusReader': nltk.corpus.reader.wordnet.WordNetCorpusReader, nltk.corpus.reader.WordNetCorpusReader
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/meteor_score.py:docstring of nltk.translate.meteor_score.meteor_score:: WARNING: more than one target found for cross-reference 'WordNetCorpusReader': nltk.corpus.reader.wordnet.WordNetCorpusReader, nltk.corpus.reader.WordNetCorpusReader
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/meteor_score.py:docstring of nltk.translate.meteor_score.single_meteor_score:: WARNING: more than one target found for cross-reference 'WordNetCorpusReader': nltk.corpus.reader.wordnet.WordNetCorpusReader, nltk.corpus.reader.WordNetCorpusReader
/home/tkloczko/rpmbuild/BUILD/nltk-3.6.2/nltk/translate/meteor_score.py:docstring of nltk.translate.meteor_score.wordnetsyn_match:: WARNING: more than one target found for cross-reference 'WordNetCorpusReader': nltk.corpus.reader.wordnet.WordNetCorpusReader, nltk.corpus.reader.WordNetCorpusReader
done
build succeeded, 344 warnings.
```

"
835,https://github.com/nltk/nltk/issues/2744,2744,[],closed,2021-07-07 17:21:10+00:00,,6,Add pre-commit hooks?,"Generally curious if we've given thought into this. A lot of repos are now using pre-commit hooks to keep code nice and consistent.

- We could dip our toes in the water with some of these https://github.com/pre-commit/pre-commit-hooks -- they generally don't change the code much

The following two are noisier to set up but I really like them both:
- [Pyupgrade](https://github.com/asottile/pyupgrade/)
- [isort](https://github.com/PyCQA/isort/issues)"
836,https://github.com/nltk/nltk/issues/2749,2749,"[{'id': 910625822, 'node_id': 'MDU6TGFiZWw5MTA2MjU4MjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/CI', 'name': 'CI', 'color': '76dbed', 'default': False, 'description': ''}, {'id': 1026900607, 'node_id': 'MDU6TGFiZWwxMDI2OTAwNjA3', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wontfix', 'name': 'wontfix', 'color': 'f4b7ca', 'default': True, 'description': ''}]",closed,2021-07-21 11:30:21+00:00,,1,travis-ci.org no longer runs builds,"
![travis_ci_org_eol](https://user-images.githubusercontent.com/4669013/126481562-e9d7475f-3c1f-499c-9416-c04b8514d793.png)
The above message is currently displayed on https://travis-ci.org/github/nltk/nltk

travis-ci.org no longer runs ci pipeline for nltk, they suggest migrating to travis-ci.com
At the time of writing there are no CI builds running for latest pull requests and merges.

Possible courses of action:
1. Migrate from travis-ci.org to travis-ci.com
2. Use github actions to run CI pipeline and builds"
837,https://github.com/nltk/nltk/issues/2751,2751,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 975531894, 'node_id': 'MDU6TGFiZWw5NzU1MzE4OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/metrics', 'name': 'metrics', 'color': 'f442c8', 'default': False, 'description': ''}]",closed,2021-07-22 08:21:34+00:00,,1,Is not the max of meteor score 100?  (nltk.translate.meteor_score),"I used [nltk.translate.meteor_score](https://github.com/nltk/nltk/blob/develop/nltk/translate/meteor_score.py) to calculate the meteor score of a example, strangely I got a score of 105.6. 
Is not the max of meteor score 100?

The example is following:

> **candidate:**   creates or updates a virtual machine scale set
**reference:**   create or update a vm scale set
**METEOR score:**   105.6

Is there anyone know that? Thank you very much!"
838,https://github.com/nltk/nltk/issues/2758,2758,[],closed,2021-07-26 06:28:51+00:00,,1,TypeError: expected string or bytes-like object,"Hello!

I have an error `TypeError: expected string or bytes-like object` after running this code in google colaboratory:

```
from nltk import tokenize
text_data =  tokenize.sent_tokenize(data_list)
```

`data_list` is the list by type, contains textual strings and was getting like this:

```
df = pd.read_csv(io.BytesIO('data.csv', header=None)
data_list = df['text'].tolist()
```

What is the problem and how can I solve it?"
839,https://github.com/nltk/nltk/issues/2759,2759,"[{'id': 1350810881, 'node_id': 'MDU6TGFiZWwxMzUwODEwODgx', 'url': 'https://api.github.com/repos/nltk/nltk/labels/invalid', 'name': 'invalid', 'color': 'cecece', 'default': True, 'description': ''}]",closed,2021-07-26 12:05:47+00:00,,1,issue in  'tokenize' is not defined,"i think there is some issue in new update ""tokenize"" is down. "
840,https://github.com/nltk/nltk/issues/2766,2766,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}]",closed,2021-07-29 17:37:25+00:00,,3,NTLK invalid package metdata breaks install via poetry,"It appears poetry is enforcing PEP440 on package metadata.  The NLTK uses package metadata that does not conform and so poetry fails to install (with poetry 1.2.0 and later, it seems - 1.1.4 doesn't seem affected).

```
[[package]]
name = ""nltk""
version = ""3.6.2""
description = ""Natural Language Toolkit""
category = ""main""
optional = false
python-versions = "">=3.5.*""
```

Error message (in docker)
```
STEP 14: RUN poetry install
Creating virtualenv science-lens-BYYjfetP-py3.8 in /home/1001/.cache/pypoetry/virtualenvs
Installing dependencies from lock file

  InvalidVersion

  Invalid PEP 440 version: '3.5.'

  at /usr/local/lib/python3.8/site-packages/poetry/core/version/pep440/parser.py:67 in parse
       63│     @classmethod
       64│     def parse(cls, value: str, version_class: Optional[Type[""PEP440Version""]] = None):
       65│         match = cls._regex.search(value) if value else None
       66│         if not match:
    →  67│             raise InvalidVersion(f""Invalid PEP 440 version: '{value}'"")
       68│ 
       69│         if version_class is None:
       70│             from poetry.core.version.pep440.version import PEP440Version
       71│ 

The following error occurred when trying to handle this error:


  ValueError

  Could not parse version constraint: >=3.5.*

  at /usr/local/lib/python3.8/site-packages/poetry/core/semver/helpers.py:139 in parse_single_constraint
      135│ 
      136│         try:
      137│             version = Version.parse(version)
      138│         except ValueError:
    → 139│             raise ValueError(
      140│                 ""Could not parse version constraint: {}"".format(constraint)
      141│             )
      142│ 
      143│         if op == ""<"":
subprocess exited with status 1
subprocess exited with status 1
error building at STEP ""RUN poetry install"": exit status 1
level=error msg=""exit status 1""
```"
841,https://github.com/nltk/nltk/issues/2770,2770,"[{'id': 910625822, 'node_id': 'MDU6TGFiZWw5MTA2MjU4MjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/CI', 'name': 'CI', 'color': '76dbed', 'default': False, 'description': ''}]",open,2021-07-31 11:46:02+00:00,,6,GitHub Actions CI does not download third party dependencies,"Hello!

The Travis CI used to use https://github.com/nltk/nltk/blob/develop/tools/travis/third-party.sh to ensure that the Stanford CoreNLP, Parser, POSTagger and SENNA are downloaded before running the CI. The current Github Actions CI workflow does not do this, and more files are lacking too.

These are all the skipped and xfailed tests on the current develop branch:
```
=========================== short test summary info ============================
SKIPPED [2] nltk/test/childes_fixt.py:9: The CHILDES corpus is not found. It should be manually downloaded and saved/unpacked to [NLTK_Data_Dir]/corpora/childes/
SKIPPED [1] nltk/test/inference_fixt.py:10: Mace4/Prover9 is not available so inference.doctest was skipped
SKIPPED [1] nltk/test/nonmonotonic_fixt.py:10: Mace4/Prover9 is not available so nonmonotonic.doctest was skipped
SKIPPED [1] nltk/test/unit/test_brill.py:33: Should be tested in __main__ of nltk.tbl.demo
SKIPPED [1] nltk/test/unit/test_classify.py:35: 

===========================================================================
NLTK was unable to find the megam file!
Use software specific configuration parameters or set the MEGAM environment variable.

  For more information on megam, see:
    <http://www.umiacs.umd.edu/~hal/megam/index.html>
===========================================================================
SKIPPED [6] nltk/test/unit/test_corenlp.py: Skipping all CoreNLP tests.
SKIPPED [1] nltk/test/unit/test_corpora.py:213: A full installation of the Penn Treebank is not available
SKIPPED [1] nltk/test/unit/test_corpora.py:190: A full installation of the Penn Treebank is not available
SKIPPED [1] nltk/test/unit/test_corpora.py:207: A full installation of the Penn Treebank is not available
SKIPPED [2] nltk/test/unit/test_corpora.py: Skipping test for mwa_ppdb.
SKIPPED [1] nltk/test/unit/test_rte_classify.py:88: Skipping tests with dependencies on MEGAM
SKIPPED [4] ../../../../../opt/hostedtoolcache/Python/3.6.14/x64/lib/python3.6/site-packages/_pytest/unittest.py:153: Requires Senna executable
SKIPPED [1] nltk/test/gluesemantics_malt_fixt.py:8: MaltParser is not available
SKIPPED [16] nltk/test/unit/test_tokenize.py:26: Tests for nltk.tokenize.stanford_segmenter skipped: 

===========================================================================
  NLTK was unable to find stanford-segmenter.jar! Set the CLASSPATH
  environment variable.

  For more information, on stanford-segmenter.jar, see:
    <https://nlp.stanford.edu/software>
===========================================================================
SKIPPED [1] nltk/test/portuguese_en_fixt.py:5: portuguese_en.doctest imports nltk.examples.pt which doesn't exist!
SKIPPED [1] nltk/test/unit/lm/test_vocabulary.py:141: Test is known to be flaky as it compares (runtime) performance.
SKIPPED [1] nltk/test/unit/test_classify.py:35: 

===========================================================================
NLTK was unable to find the tadm file!
Use software specific configuration parameters or set the TADM environment variable.

  For more information on tadm, see:
    <http://tadm.sf.net>
===========================================================================
SKIPPED [1] nltk/test/unit/test_corpora.py:235: A full installation of the Penn Treebank is not available
SKIPPED [1] nltk/test/unit/test_corpora.py:229: A full installation of the Penn Treebank is not available
SKIPPED [1] nltk/test/unit/test_corpora.py:201: A full installation of the Penn Treebank is not available
XFAIL nltk/test/unit/lm/test_models.py::test_sums_to_1[a-stupid_backoff_trigram_model]
  Stupid Backoff is not a valid distribution
XFAIL nltk/test/unit/lm/test_models.py::test_sums_to_1[c-stupid_backoff_trigram_model]
  Stupid Backoff is not a valid distribution
XFAIL nltk/test/unit/lm/test_models.py::test_sums_to_1[<s>-stupid_backoff_trigram_model]
  Stupid Backoff is not a valid distribution
XFAIL nltk/test/unit/lm/test_models.py::test_sums_to_1[b-stupid_backoff_trigram_model]
  Stupid Backoff is not a valid distribution
XFAIL nltk/test/unit/lm/test_models.py::test_sums_to_1[<UNK>-stupid_backoff_trigram_model]
  Stupid Backoff is not a valid distribution
XFAIL nltk/test/unit/lm/test_models.py::test_sums_to_1[d-stupid_backoff_trigram_model]
  Stupid Backoff is not a valid distribution
XFAIL nltk/test/unit/lm/test_models.py::test_sums_to_1[e-stupid_backoff_trigram_model]
  Stupid Backoff is not a valid distribution
XFAIL nltk/test/unit/lm/test_models.py::test_sums_to_1[r-stupid_backoff_trigram_model]
  Stupid Backoff is not a valid distribution
XFAIL nltk/test/unit/lm/test_models.py::test_sums_to_1[w-stupid_backoff_trigram_model]
  Stupid Backoff is not a valid distribution
```

(Note, some of these will be removed if #2769 is merged)

So, these dependencies still need to be installed as a part of the CI, preferably with a cache.

- Tom Aarsen"
842,https://github.com/nltk/nltk/issues/2771,2771,[],closed,2021-08-02 12:28:42+00:00,,0,Some words does not have correct stem,experimental
843,https://github.com/nltk/nltk/issues/2775,2775,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 3375727332, 'node_id': 'LA_kwDOAASTVs7JNYLk', 'url': 'https://api.github.com/repos/nltk/nltk/labels/discussion', 'name': 'discussion', 'color': '71909E', 'default': False, 'description': ''}]",open,2021-08-03 13:32:46+00:00,,9,Suggestions and discussions regarding `pre-commit`,"Hello!

Now that `pre-commit` has been in place since #2753, we should take a moment to reflect on it, and consider whether we want to remove or add certain hooks to it. This issue will be a place for hook suggestions and discussions.

Previously, discussion about `pre-commit` has occurred in #2744.
WIthin this issue, the following hooks were mentioned:
- [PyUpgrade](https://github.com/asottile/pyupgrade/): A hook to automatically upgrade syntax for newer verions of Python. I like the sound of this one.
- [isort](https://github.com/PyCQA/isort): Import sorting. I like the sound of this one.
- [black](https://github.com/psf/black): Python formatter, implemented if #2774 gets merged.

Beyond these, the `pre-commit` team has provided some more hooks, which can be found [here](https://github.com/pre-commit/pre-commit-hooks). Some of them I find particularly interesting, such as:
- `debug_statements`: Check for debugger imports and py37+ `breakpoint()` calls in python source.
- `no-commit-to-branch`: Protect specific branches from direct checkins.
- Hooks that attempt to load specific files to verify syntax. (`check-xml`, `check-yaml`, `check-toml`, `check-json`)

Some other hooks provided by the `pre-commit` team have niche uses, but might be nice, such as:
- `check-case-conflict`: Check for files with names that would conflict on a case-insensitive filesystem like MacOS HFS+ or Windows FAT.
- `check-merge-conflict`: Check for files that contain merge conflict strings.

Other custom hooks I've used myself are:
- [Bandit](https://github.com/PyCQA/bandit): For security vulnerabilities. Might not be very applicable for nltk.
- [mypy](https://github.com/pre-commit/mirrors-mypy): Add type hints ([PEP 484](https://www.python.org/dev/peps/pep-0484/)) (e.g. `def func(n: int) -> bool`) and statically check correctness of these type hints. I like this one, but it might require solving a bunch of warnings before it can be incorporated into `pre-commit`.
- [pylint](https://github.com/PyCQA/pylint): Very interesting if you're very interested in improving code quality, but this hook will only pass if there are no warnings, meaning that a developer can only push their commit if all warnings are resolved. Currently, there are roughly 15000 warnings.
- [codespell](https://github.com/codespell-project/codespell): Points out potential spelling errors in code comments, with the option to automatically resolve them. Interesting, but often marks correct words as incorrect.

Once again, this issue is meant for suggestions and discussion, so feel free to voice your opinions and recommendations. Together we can figure out what works best for nltk and its contributors."
844,https://github.com/nltk/nltk/issues/2776,2776,"[{'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}]",closed,2021-08-03 22:27:57+00:00,,5,Communication between NLTK team,"I would like to resume our use of slack for communication between members of the NLTK team (nltk.slack.com). To participate, please [email me](mailto:steven.bird@cdu.edu.au), so I can email an invitation, thanks.

@iliakur @tomaarsen @dannysepler @purificant"
845,https://github.com/nltk/nltk/issues/2778,2778,[],closed,2021-08-06 08:54:35+00:00,,0,There is a word in hindi language which is giving wrong meaning.,experimental
846,https://github.com/nltk/nltk/issues/2781,2781,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}, {'id': 718773983, 'node_id': 'MDU6TGFiZWw3MTg3NzM5ODM=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/classifier', 'name': 'classifier', 'color': '60d2ff', 'default': False, 'description': None}, {'id': 1346305519, 'node_id': 'MDU6TGFiZWwxMzQ2MzA1NTE5', 'url': 'https://api.github.com/repos/nltk/nltk/labels/third-party', 'name': 'third-party', 'color': '42e5b7', 'default': False, 'description': ''}]",open,2021-08-11 08:14:35+00:00,,2,Building own classifier based POS tagger using SklearnClassifier and ClassifierBasedPOSTagger,"I'm trying to build my own classifier based POS tagger using `SklearnClassifier` and `ClassifierBasedPOSTagger`. The code that I've tried is given below.

```
from nltk.corpus import treebank
nltk.download('treebank')

data = treebank.tagged_sents()
train_data = data[:3500]
test_data = data[3500:]
```

```
from nltk.classify import SklearnClassifier
from sklearn.naive_bayes import BernoulliNB
from nltk.tag.sequential import ClassifierBasedPOSTagger

bnb = SklearnClassifier(BernoulliNB())
bnb_tagger = ClassifierBasedPOSTagger(train=train_data,
                                      classifier_builder=bnb.train)

# evaluate tagger on test data and sample sentence
print(bnb_tagger.evaluate(test_data))

# see results on our previously defined sentence
print(bnb_tagger.tag(nltk.word_tokenize(sentence)))
```

This code is yielding the following error:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
C:\Users\ABDULL~1.IMR\AppData\Local\Temp/ipykernel_6580/266992580.py in <module>
      4 
      5 bnb = SklearnClassifier(BernoulliNB())
----> 6 bnb_tagger = ClassifierBasedPOSTagger(train=train_data,
      7                                       classifier_builder=bnb.train)
      8 

~\Miniconda3\envs\nlp_course\lib\site-packages\nltk\tag\sequential.py in __init__(self, feature_detector, train, classifier_builder, classifier, backoff, cutoff_prob, verbose)
    637 
    638         if train:
--> 639             self._train(train, classifier_builder, verbose)
    640 
    641     def choose_tag(self, tokens, index, history):

~\Miniconda3\envs\nlp_course\lib\site-packages\nltk\tag\sequential.py in _train(self, tagged_corpus, classifier_builder, verbose)
    673         if verbose:
    674             print(""Training classifier ({} instances)"".format(len(classifier_corpus)))
--> 675         self._classifier = classifier_builder(classifier_corpus)
    676 
    677     def __repr__(self):

~\Miniconda3\envs\nlp_course\lib\site-packages\nltk\classify\scikitlearn.py in train(self, labeled_featuresets)
    110 
    111         X, y = list(zip(*labeled_featuresets))
--> 112         X = self._vectorizer.fit_transform(X)
    113         y = self._encoder.fit_transform(y)
    114         self._clf.fit(X, y)

~\Miniconda3\envs\nlp_course\lib\site-packages\sklearn\feature_extraction\_dict_vectorizer.py in fit_transform(self, X, y)
    288             Feature vectors; always 2-d.
    289         """"""
--> 290         return self._transform(X, fitting=True)
    291 
    292     def inverse_transform(self, X, dict_type=dict):

~\Miniconda3\envs\nlp_course\lib\site-packages\sklearn\feature_extraction\_dict_vectorizer.py in _transform(self, X, fitting)
    233                     if feature_name in vocab:
    234                         indices.append(vocab[feature_name])
--> 235                         values.append(self.dtype(v))
    236 
    237             indptr.append(len(indices))

TypeError: float() argument must be a string or a number, not 'NoneType'
```
How to do it right?"
847,https://github.com/nltk/nltk/issues/2783,2783,"[{'id': 25396640, 'node_id': 'MDU6TGFiZWwyNTM5NjY0MA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/book', 'name': 'book', 'color': 'e102d8', 'default': False, 'description': None}]",closed,2021-08-12 12:34:26+00:00,,3,Can't find nltk.examples module,"I'm trying to follow the instructions at [Examples for Portuguese Processing](http://www.nltk.org/howto/portuguese_en.html) but trying to import from `nltk.examples.pt` gives me the following error:
`ModuleNotFoundError: No module named 'nltk.examples'`

Is there an alternative way to import all portuguese corpora? Is there a list somewhere listing all of them?
Do I need to import by id from `nltk.corpus`?

Thank you in advance"
848,https://github.com/nltk/nltk/issues/2788,2788,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 719346674, 'node_id': 'MDU6TGFiZWw3MTkzNDY2NzQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/plot', 'name': 'plot', 'color': 'eddf76', 'default': False, 'description': None}]",open,2021-08-26 07:38:36+00:00,,0,"Only return `ax` object by default in `plot` methods, instead of plotting by default","In the following locations:
* https://github.com/nltk/nltk/blob/f989fe65d421e7ea4d1037a00f07eaeee3ad6a29/nltk/probability.py#L247-L249
* https://github.com/nltk/nltk/blob/f989fe65d421e7ea4d1037a00f07eaeee3ad6a29/nltk/probability.py#L1922-L1932
* https://github.com/nltk/nltk/blob/f989fe65d421e7ea4d1037a00f07eaeee3ad6a29/nltk/text.py#L607

We should by default merely return the matplotlib `ax` object, rather than showing the plot. That way, users can still modify the graph themselves, or save it rather than plotting it. As @iliakur rightfully mentioned in the discussion regarding this issue in https://github.com/nltk/nltk/pull/2786#issuecomment-905283065, a Jupyter notebook would still render the plot even if we only return it. In the future, people will have to invoke `.plot()`, and then perform `plt.show()`, where `plt` imported as `import matplotlib.pyplot as plt`. 

These changes can easily be implemented by setting `show=False` as default in the first two method, and adding `return` to the third method.

This issue is planned to be resolved for the next major release.

- Tom Aarsen"
849,https://github.com/nltk/nltk/issues/2790,2790,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}]",open,2021-08-29 17:20:34+00:00,,0,Indian Languages extension,I'm a NLP developer from India and I'd like to add some recognized Indian languages libraries including a morphological analyzer and a parser in a single package. Is any one interested ? 
850,https://github.com/nltk/nltk/issues/2792,2792,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",closed,2021-08-29 19:18:20+00:00,,14,TweetTokenizer fails to remove user handles in text.,"The below input should remove all user handles which start with ""@"".

input: @remy:This is waaaaayyyy too much for you!!!!!!@adam 
output : [':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!', '@adam']

The TweetTokenizer fail to remove the user handle of Adam.

I would like to open a pull request that solves the following issues:- 

- Improve the regular expression pattern to detect in-text handles properly.
- Adjust the length of characters to be 15 instead of 20. "
851,https://github.com/nltk/nltk/issues/2794,2794,"[{'id': 718740438, 'node_id': 'MDU6TGFiZWw3MTg3NDA0Mzg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tagger', 'name': 'tagger', 'color': 'a2d4e8', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2021-09-01 18:57:09+00:00,,2,BigramTagger not tagging tokens,"I am trying to create a custom BigramTagger the following way:

```
>>> from nltk.tag.sequential import BigramTagger
>>> model = {'abcd abcd': 'y', 'efgh efgh': 'm'}
>>> bitager = BigramTagger(model=model)
>>> bitager.tag(['abcd abcd', 'efgh efgh'])
[('abcd abcd', None), ('efgh efgh', None)]
```

I'm using nltk version 3.6.2. Am I doing something wrong or this might be a bug?"
852,https://github.com/nltk/nltk/issues/2796,2796,"[{'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",closed,2021-09-04 02:51:20+00:00,,2,NLTK installation failed,"![image](https://user-images.githubusercontent.com/88143204/132080065-cbec8e76-252a-4654-b7fa-fb31b0bc29e8.png)
"
853,https://github.com/nltk/nltk/issues/2800,2800,"[{'id': 29356672, 'node_id': 'MDU6TGFiZWwyOTM1NjY3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/corpus', 'name': 'corpus', 'color': '0b02e1', 'default': False, 'description': None}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 1032922666, 'node_id': 'MDU6TGFiZWwxMDMyOTIyNjY2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nltk_data', 'name': 'nltk_data', 'color': 'ed6fd1', 'default': False, 'description': ''}]",open,2021-09-08 20:21:22+00:00,,3,Swahili stopwords are missing ,"When I try to access the swahili stopwords using the below feature, I'm getting a traceback that the swahili stopwords are missing in the documentation, I'm currently working on building a swahili language model with data gathered from Wikipedia swahili articles and popular swahili blogs,  I was thinking of contributing the stopwords to the library to allow others users to easily load them. 

Best regards
Kalebu"
854,https://github.com/nltk/nltk/issues/2802,2802,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2021-09-11 05:28:25+00:00,,3,TreebankWordDetokenizer does not detokenize to original string,"Hi, I am trying to tokenize replace a token and then detokenize a string. I am noticing that detokenize does not ""undo"" the tokenization correctly. Here is a example:
```
>>> import nltk
>>> _tokenizer = nltk.tokenize.treebank.TreebankWordTokenizer()
>>> _detokenizer = nltk.tokenize.treebank.TreebankWordDetokenizer()
>>> f = ""here, is an email: me@mail.com""
>>> _detokenizer.detokenize(_tokenizer.tokenize(f))
'here, is an email: me @ mail.com'
```

I was expecting the email to be detokenized back to me@mail.com. This seems like a bug.
I'm using nltk version 3.6.2"
855,https://github.com/nltk/nltk/issues/2805,2805,"[{'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}]",closed,2021-09-14 12:30:15+00:00,,2,proxy error,"i used to get my output through nltk.set_proxy('http://user:password@ip:port') for like one year, but now this method is giving HTTP error, even by using the rule given by the nltk library which is ('http://ip:port',user,password) 
![image_2021_09_14T05_20_15_646Z](https://user-images.githubusercontent.com/83581864/133257750-6a3de092-92ef-49c1-93aa-335bbcaf5a3a.png)
"
856,https://github.com/nltk/nltk/issues/2807,2807,"[{'id': 3375726484, 'node_id': 'LA_kwDOAASTVs7JNX-U', 'url': 'https://api.github.com/repos/nltk/nltk/labels/deprecation', 'name': 'deprecation', 'color': '201B0C', 'default': False, 'description': ''}, {'id': 3375727332, 'node_id': 'LA_kwDOAASTVs7JNYLk', 'url': 'https://api.github.com/repos/nltk/nltk/labels/discussion', 'name': 'discussion', 'color': '71909E', 'default': False, 'description': ''}]",closed,2021-09-16 21:10:24+00:00,,3,usage function: repair or drop?,"Do we need to support this usage method?
https://github.com/nltk/nltk/blob/develop/nltk/util.py#L40

Here's a discussion of it's shortcomings:
https://github.com/nltk/nltk/pull/2801#discussion_r706567025"
857,https://github.com/nltk/nltk/issues/2809,2809,[],closed,2021-09-19 19:26:09+00:00,,2,Trying to get in touch regarding a security issue,"Hey there!

I'd like to report a security issue but cannot find contact instructions on your repository.

If not a hassle, might you kindly add a `SECURITY.md` file with an email, or another contact method? GitHub [recommends](https://docs.github.com/en/code-security/getting-started/adding-a-security-policy-to-your-repository) this best practice to ensure security issues are responsibly disclosed, and it would serve as a simple instruction for security researchers in the future.

Thank you for your consideration, and I look forward to hearing from you!

(cc @huntr-helper)"
858,https://github.com/nltk/nltk/issues/2812,2812,"[{'id': 3375726484, 'node_id': 'LA_kwDOAASTVs7JNX-U', 'url': 'https://api.github.com/repos/nltk/nltk/labels/deprecation', 'name': 'deprecation', 'color': '201B0C', 'default': False, 'description': ''}, {'id': 3375727332, 'node_id': 'LA_kwDOAASTVs7JNYLk', 'url': 'https://api.github.com/repos/nltk/nltk/labels/discussion', 'name': 'discussion', 'color': '71909E', 'default': False, 'description': ''}]",open,2021-09-21 16:16:07+00:00,,3,Finishing up Stanford Deprecation,"Hello!

As some of you might be aware, several Stanford related classes have been deprecated back in 2017. They are the following:
* [`nltk.tag.StanfordTagger`](https://github.com/nltk/nltk/blob/develop/nltk/tag/stanford.py#L31)
* [`nltk.tag.StanfordPOSTagger`](https://github.com/nltk/nltk/blob/develop/nltk/tag/stanford.py#L139)
* [`nltk.tag.StanfordNERTagger`](https://github.com/nltk/nltk/blob/develop/nltk/tag/stanford.py#L176)
* [`nltk.parse.GenericStanfordParser`](https://github.com/nltk/nltk/blob/develop/nltk/parse/stanford.py#L28)
* [`nltk.parse.StanfordParser`](https://github.com/nltk/nltk/blob/develop/nltk/parse/stanford.py#L274)
* [`nltk.parse.StanfordDependencyParser`](https://github.com/nltk/nltk/blob/develop/nltk/parse/stanford.py#L341)
* [`nltk.parse.StanfordNeuralDependencyParser`](https://github.com/nltk/nltk/blob/develop/nltk/parse/stanford.py#L407)
* [`nltk.tokenize.StanfordTokenizer`](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/stanford.py#L22)
* [`nltk.tokenize.StanfordSegmenter`](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/stanford_segmenter.py#L32)

These have been replaced by the following newer classes:<sup>[1](#f1)</sup>
* [`nltk.parse.GenericCoreNLPParser`](https://github.com/nltk/nltk/blob/develop/nltk/parse/corenlp.py#L176)
* [`nltk.parse.CoreNLPParser`](https://github.com/nltk/nltk/blob/develop/nltk/parse/corenlp.py#L394)
* [`nltk.parse.CoreNLPDependencyParser`](https://github.com/nltk/nltk/blob/develop/nltk/parse/corenlp.py#L546)

Note that each of these new classes rely on a `CoreNLPServer` running. One of the ways to get this to run is directly from the source using Java, as mentioned in https://github.com/nltk/nltk/pull/1735#issuecomment-306091826 by the author of most of these changes, @alvations. He used:
```
wget http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip
unzip stanford-corenlp-full-2016-10-31.zip && cd stanford-corenlp-full-2016-10-31

java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \
-preload tokenize,ssplit,pos,lemma,parse,depparse \
-status_port 9000 -port 9000 -timeout 15000
```
Note that newer versions of the stanford-corenlp package are available nowadays.
Alternatively, the [`CoreNLPServer`](https://github.com/nltk/nltk/blob/develop/nltk/parse/corenlp.py#L38) class can also be used to run the server in Python, though I haven't gotten that to work on Windows.

---

### What now?

All of these Stanford classes contain DeprecationWarnings placed back in 2017, such as this one:
https://github.com/nltk/nltk/blob/d21646dbd547cdd02d0c60f8e23d1d28a9fd1266/nltk/tokenize/stanford_segmenter.py#L71-L82

Clearly, we need to make some changes here. We're on v3.6.3 now.

With this issue I invite some discussion on the following options (among others):
1. Remove the deprecated classes in their entirety.
2. Remove the bodies of the methods, and point to a documentation reference of porting these methods to the newer CoreNLP equivalents.
3. Keep them, but don't maintain them if we have issues in the future. 

Personally I'm leaning towards either 1 or 2.

However, before simply removing potentially often used code, I went over each of the deprecated classes to see if there are indeed new equivalents, and for adding to the documentation somewhere.

---

### Stanford updating reference

The following table contains the deprecated classes with their main methods, and the equivalent newer classes and methods. Each line on the left column is equivalent to a line on the right column.

<table>
<tr>
<th> Old </th>
<th> New </th>
</tr>
<tr>
<td colspan=""2"" align=""center""><b>POS Tagger</b></td>
</tr>
<tr>
<td>

```python
>>> from nltk.tag.stanford import StanfordPOSTagger
>>> tagger = StanfordPOSTagger()
>>> tagger.tag(...)
>>> tagger.tag_sents(...)
>>> tagger.parse_output(...)
>>> ...
```

</td>
<td>

```python
>>> from nltk.parse import CoreNLPParser
>>> parser = CoreNLPParser(tagtype=""pos"")
>>> parser.tag(...)
>>> parser.tag_sents(...)
>>> *deprecated*
>>> parser.raw_tag_sents(...)
```

</td>
</tr>
<tr>
<td colspan=""2"" align=""center""><b>NER Tagger</b></td>
</tr>
<tr>
<td>

```python
>>> from nltk.tag.stanford import StanfordNERTagger
>>> tagger = StanfordNERTagger()
>>> tagger.tag(...)
>>> tagger.tag_sents(...)
>>> tagger.parse_output(...)
>>> ...
```

</td>
<td>

```python
>>> from nltk.parse import CoreNLPParser
>>> parser = CoreNLPParser(tagtype=""ner"")
>>> parser.tag(...)
>>> parser.tag_sents(...)
>>> *deprecated*
>>> parser.raw_tag_sents(...)
```

</td>
</tr>
<tr>
<td colspan=""2"" align=""center""><b>StanfordParser</b></td>
</tr>
<tr>
<td>

```python
>>> from nltk.parse.stanford import StanfordParser
>>> parser = StanfordParser()
>>> parser.parse_sents(...)
>>> parser.raw_parse(...)
>>> parser.raw_parse_sents(...)
>>> parser.tagged_parse(...)
>>> parser.tagged_parse_sents(...)
>>> ...
```

</td>
<td>

```python
>>> from nltk.parse import CoreNLPParser
>>> parser = CoreNLPParser()
>>> parser.parse_sents(...)
>>> parser.raw_parse(...)
>>> parser.raw_parse_sents(...)
>>> *deprecated*
>>> *deprecated*
>>> parser.parse_text()
```

</td>
</tr>
<tr>
<td colspan=""2"" align=""center""><b>StanfordTokenizer</b></td>
</tr>
<tr>
<td>

```python
>>> from nltk.tokenize.stanford import StanfordTokenizer
>>> tokenizer = StanfordTokenizer()
>>> tokenizer.tokenize(...)
>>> tokenizer.tokenize_sents(...)
```

</td>
<td>

```python
>>> from nltk.parse import CoreNLPParser
>>> parser = CoreNLPParser()
>>> parser.tokenize(...)
>>> parser.tokenize_sents(...)
```

</td>
</tr>
<tr>
<td colspan=""2"" align=""center""><b>StanfordSegmenter</b></td>
</tr>
<tr>
<td>

```python
>>> from nltk.tokenize import StanfordSegmenter
>>> segmenter = StanfordSegmenter()
>>> segmenter.tokenize(...)
>>> segmenter.tokenize_sents(...)
>>> segmenter.segment_file(...)
>>> segmenter.segment(...)
>>> segmenter.segment_sents(...)
```

</td>
<td>

```python
>>> from nltk.parse import CoreNLPParser
>>> parser = CoreNLPParser()
>>> parser.tokenize(...)
>>> parser.tokenize_sents(...)
>>> *deprecated*
>>> *deprecated*
>>> *deprecated*
```

</td>
</tr>
</table>

### Notes

* `StanfordDependencyParser` used to have the same methods as `StanfordParser`. Nowadays, you should use `CoreNLPDependencyParser` instead, which has the same methods as `CoreNLPParser`.

---

My goal with this PR is to reach a consensus on how to move forwards, and then create a PR with those agreed upon changes, so feel free to share your opinion.

- Tom Aarsen

---

### Footnotes
<a name=""f1"">1</a>: `StanfordNeuralDependencyParser` was never fully implemented, and as a result does not exist in the newer `CoreNLP...` format."
859,https://github.com/nltk/nltk/issues/2817,2817,[],closed,2021-09-25 12:10:59+00:00,,2,RegexpTagger fails to handle exception.,"There is a bug in the RegexpTagger. It fails to handle exception in case of missing tag or regular expression. 

```
    def __init__(self, regexps, backoff=None):
        """""" """"""
        super().__init__(backoff)
        try:
            self._regexps = [
                (
                    re.compile(regexp),
                    tag,
                )
                for regexp, tag in regexps
            ]
        except Exception as e:
            raise Exception(
                ""Invalid RegexpTagger regexp:"", str(e), ""regexp:"", regexp, ""tag:"", tag
            ) from e
```

The problem that both regexp and tag are not defined anywhere in the class, which is weird because it implies that the user should be notified with the missing tag or regular expression which is missing!. 

The possible solutions from POV are: 

- Simply remove the regexp and tag variables from the exception messae. 
- Check for the length of each tuple in the list and throw exception if length of one tuple is not two.

Maybe the Exception should be more meaningful like, ""RegexpTagger expects tuple of size two(regexp, tag). 

"
860,https://github.com/nltk/nltk/issues/2818,2818,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}]",closed,2021-09-26 02:44:43+00:00,,3,WordNetLemmatizer in nltk.stem module,"What's the parameter of WordNetLemmatizer.lemmatize() in nltk.stem module?
Turn to the document, what are the candidate value of the parameter **'pos'**?
![image](https://user-images.githubusercontent.com/62245023/134791412-1ff85ba5-5eb9-4859-a3f1-3b48bdd5a6fa.png)
The default value is 'Noun'. But use the function pos_tag() to get the pos of the word, the value appears to come from several options."
861,https://github.com/nltk/nltk/issues/2824,2824,"[{'id': 486810128, 'node_id': 'MDU6TGFiZWw0ODY4MTAxMjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tests', 'name': 'tests', 'color': 'fbca04', 'default': False, 'description': None}, {'id': 910625822, 'node_id': 'MDU6TGFiZWw5MTA2MjU4MjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/CI', 'name': 'CI', 'color': '76dbed', 'default': False, 'description': ''}]",open,2021-09-27 21:38:37+00:00,,0,Support for CI testing involving corpora and third party tools,"From #2820 (@tomaarsen)

The number of skipped tests has been reduced from 31 to 16, but can be reduced even further if more tools are automatically downloaded, such as:

- TADM
- Penn Treebank
- CHILDES corpus
"
862,https://github.com/nltk/nltk/issues/2825,2825,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 719083862, 'node_id': 'MDU6TGFiZWw3MTkwODM4NjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/installation', 'name': 'installation', 'color': 'f7731b', 'default': False, 'description': None}, {'id': 1118709607, 'node_id': 'MDU6TGFiZWwxMTE4NzA5NjA3', 'url': 'https://api.github.com/repos/nltk/nltk/labels/internals', 'name': 'internals', 'color': '77359e', 'default': False, 'description': ''}]",open,2021-09-27 21:39:24+00:00,,1,Expand ~ in paths,"From #2820 (@tomaarsen)

Expand ~ in paths used in environment variables to the home path of the user's operating system. This is as simple as wrapping a path with path = os.path.expanduser(path). This probably ought to be done in some functions in internals.py. However, files on linux are allowed to have ~ in them. In some of those cases, you wouldn't want to expand the ~."
863,https://github.com/nltk/nltk/issues/2826,2826,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 1346305519, 'node_id': 'MDU6TGFiZWwxMzQ2MzA1NTE5', 'url': 'https://api.github.com/repos/nltk/nltk/labels/third-party', 'name': 'third-party', 'color': '42e5b7', 'default': False, 'description': ''}]",closed,2021-09-27 21:42:05+00:00,,0,MaltParser is fixed to 1.7.2,"From #2802 (@tomaarsen)

The tests for MaltParser are fixed to version 1.7.2. Perhaps this means that users who have another version for personal use won't have passing tests? If so, that is a problem. Perhaps the best solution is to not even have to specify a version in the constructor. I'm not even sure it gets used - as I believe the MaltParser executable is actually loaded through the `MALT_PARSER` environment variable."
864,https://github.com/nltk/nltk/issues/2827,2827,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 910625822, 'node_id': 'MDU6TGFiZWw5MTA2MjU4MjI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/CI', 'name': 'CI', 'color': '76dbed', 'default': False, 'description': ''}, {'id': 1346305519, 'node_id': 'MDU6TGFiZWwxMzQ2MzA1NTE5', 'url': 'https://api.github.com/repos/nltk/nltk/labels/third-party', 'name': 'third-party', 'color': '42e5b7', 'default': False, 'description': ''}]",closed,2021-09-27 21:43:59+00:00,,0,Update third party tools to newer versions,"From #2820 (@tomaarsen)

The Stanford and MaltParser tools have had newer versions released than the ones used in `third-party.sh`. This may be as simple as modifying the download link to point to the new versions.
"
865,https://github.com/nltk/nltk/issues/2829,2829,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",closed,2021-09-30 05:09:30+00:00,,2,emojis are not tokenized very well,"Maybe add something like this as a pre or post processing step?

It might make sense to download the emoji list and store it as part of the build, so people do not need to load the emojis module, ...

```
import emoji
from emoji import unicode_codes
import re


EMOJI_UNICODE = unicode_codes.EMOJI_UNICODE['en']
emojis = sorted(EMOJI_UNICODE.values(), key=len, reverse=True)
print (emojis, sep='\n')
emoji_regexp = f""({'|'.join(re.escape(u) for u in emojis)})""
EMOJI_XL = re.compile(rf""\B({emoji_regexp})"", flags=re.UNICODE)
EMOJI_XR = re.compile(rf""({emoji_regexp})\B"", flags=re.UNICODE)
EMOJI_WL = re.compile(rf""(\w)({emoji_regexp})"", flags=re.UNICODE)
EMOJI_WR = re.compile(rf""({emoji_regexp})(\w)"", flags=re.UNICODE)
EMOJI_REGEX = re.compile(rf""({emoji_regexp})"", flags=re.UNICODE)

def split_emoji(text):
    text = EMOJI_REGEX.sub(r' \1 ', text)
    text = text.replace('  ', ' ')
    return text

test =  ""🤔 🙈 me así, se😌 ds 💕👭👙 hello 👩🏾‍🎓 emoji hello 👨‍👩‍👦‍👦 how are 😊 you today🙅🏽🙅🏽""
#test = ""They are going to start a direct flight soon😠""

print(test)

print(split_emoji(test))
```"
866,https://github.com/nltk/nltk/issues/2833,2833,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 1641876486, 'node_id': 'MDU6TGFiZWwxNjQxODc2NDg2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/critical', 'name': 'critical', 'color': 'ff2050', 'default': False, 'description': ''}]",closed,2021-10-01 06:57:33+00:00,,7,[3.6.4] AttributeError: module 'regex' has no attribute 'Pattern',"Since I upgraded `nltk` from `3.6.2` to `3.6.4`, the following code :

```python
from nltk.tokenize.casual import TweetTokenizer, casual_tokenize
```

raises the following error :

> Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/user/miniconda3/envs/test/lib/python3.7/site-packages/nltk/__init__.py"", line 137, in <module>
    from nltk.text import *
  File ""/home/user/miniconda3/envs/test/lib/python3.7/site-packages/nltk/text.py"", line 29, in <module>
    from nltk.tokenize import sent_tokenize
  File ""/home/user/miniconda3/envs/test/lib/python3.7/site-packages/nltk/tokenize/__init__.py"", line 65, in <module>
    from nltk.tokenize.casual import TweetTokenizer, casual_tokenize
  File ""/home/user/miniconda3/envs/test/lib/python3.7/site-packages/nltk/tokenize/casual.py"", line 272, in <module>
    class TweetTokenizer:
  File ""/home/user/miniconda3/envs/test/lib/python3.7/site-packages/nltk/tokenize/casual.py"", line 357, in TweetTokenizer
    def WORD_RE(self) -> regex.Pattern:
**AttributeError: module 'regex' has no attribute 'Pattern'**"
867,https://github.com/nltk/nltk/issues/2836,2836,[],open,2021-10-01 21:17:26+00:00,,0,Managing dependencies,"Consider managing dependencies more rigorously, cf https://github.com/nltk/nltk/pull/2835
"
868,https://github.com/nltk/nltk/issues/2837,2837,[],closed,2021-10-02 08:18:05+00:00,,3,"word_tokenize splits Pokémon into ""pok"" and 'émon""","This is without specifying a language, so i assume english by default.

Is there a workaround to this?"
869,https://github.com/nltk/nltk/issues/2838,2838,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 975531894, 'node_id': 'MDU6TGFiZWw5NzU1MzE4OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/metrics', 'name': 'metrics', 'color': 'f442c8', 'default': False, 'description': ''}]",closed,2021-10-03 14:49:14+00:00,,2,ZeroDivisionError when using SmoothingFunction's method4 with a single word as hypothesis,"When using the SmoothingFunction's [method4](https://www.kite.com/python/docs/nltk.bleu_score.SmoothingFunction.method4) with a a single word as a hypothesis  I get the following error. Which is of course because math.log(1) is 0. 
I ended up using just method2 or method1 which doesn't use the function that throws the error, but I guess it would be useful to report it, as some other poor soul may stumble with it. 

```
from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.bleu_score import SmoothingFunction

refs = [['short', 'skirts'], ['too', 'expensive'], ['girls', 'wearing'], ['being', 'laughed', 'at']]
hyp = ['too']

chencherry = SmoothingFunction()
sentence_bleu(refs, hyp, weights=(1, 0, 0, 0), smoothing_function=chencherry.method4)

```

```
ZeroDivisionError                         Traceback (most recent call last)
<ipython-input-130-00301cf17f56> in <module>()
      6 
      7 chencherry = SmoothingFunction()
----> 8 sentence_bleu(refs, hyp, weights=(1, 0, 0, 0), smoothing_function=chencherry.method4)

2 frames
/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py in sentence_bleu(references, hypothesis, weights, smoothing_function, auto_reweigh, emulate_multibleu)
     87     return corpus_bleu([references], [hypothesis],
     88                         weights, smoothing_function, auto_reweigh,
---> 89                         emulate_multibleu)
     90 
     91 

/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py in corpus_bleu(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh, emulate_multibleu)
    197     #       smoothing method allows.
    198     p_n = smoothing_function(p_n, references=references, hypothesis=hypothesis,
--> 199                              hyp_len=hyp_len, emulate_multibleu=emulate_multibleu)
    200     s = (w * math.log(p_i) for i, (w, p_i) in enumerate(zip(weights, p_n)))
    201     s =  bp * math.exp(math.fsum(s))

/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py in method4(self, p_n, references, hypothesis, hyp_len, *args, **kwargs)
    542         for i, p_i in enumerate(p_n):
    543             if p_i.numerator == 0 and hyp_len != 0:
--> 544                 incvnt = i+1 * self.k / math.log(hyp_len) # Note that this K is different from the K from NIST.
    545                 p_n[i] = 1 / incvnt
    546         return p_n

ZeroDivisionError: float division by zero
```"
870,https://github.com/nltk/nltk/issues/2844,2844,[],closed,2021-10-06 15:26:00+00:00,,1,NotImplementedError in lambda DRT simplification.,"The following piece of code gives NotImplementedError on line 264 of drt.py (3.6.4)

    import nltk
    expr=""\P.\Q.\R.(([X], []) + R(X) + ((P(\Y.([],[ (X=Y) ]))) + (Q(\Z.([], [ (X=Z) ])))))(\X.(\P.(([X],[named(X, karla)]) + P(X) )(\X1.(X(X1)))))(\Y.(\P.\Q. (([X],[ant(X)]) + P(X) + Q(X) )(\Y1.(\X.([],[owner(X) ] )(Y1)))(\Z1.(Y(Z1)))))(\Z.(\P.\X.\F.(P(\E.(([],[nsubj(E,X)]) + F(E))))(\X2.(\F.(([E][arrive(E)]) + F(E) )(\Z2.(X2(Z2)))))(Z)(\Y2.(\F.( [],[] )(Y2)))))""
    dexpr = nltk.sem.drt.DrtExpression.fromstring(r""{}"".format(expr))
    dexpr.simplify()
    print(str(dexpr))

When I track the cause, I find that it is happening when  get_refs is called in DrtLambdaExpression. Since it does not exist, the  get_refs function of the abstract class is called (line 264) giving a not implemented error."
871,https://github.com/nltk/nltk/issues/2848,2848,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 975531894, 'node_id': 'MDU6TGFiZWw5NzU1MzE4OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/metrics', 'name': 'metrics', 'color': 'f442c8', 'default': False, 'description': ''}]",closed,2021-10-07 11:32:41+00:00,,0,Invalid levenstein distance for duplicated letters,"When a letter is duplicated,
When transpositions is allowed,
When a duplicated letter is present on the right argument,

Then the duplicated letter does not contribute to the distance which is wrong - should be 1 (deletion).

```python3
from nltk.distance import edit_distance

edit_distance(""duuplicated"", ""duplicated"", transpositions=False)
edit_distance(""duplicated"", ""duuplicated"", transpositions=False)
edit_distance(""duuplicated"", ""duplicated"", transpositions=True)
# all return 1 - correct

edit_distance(""duplicated"", ""duuplicated"", transpositions=True)
# returns 0 - incorrect
```

(tested on version 3.6.4)


I believe this is a bug introduced by https://github.com/nltk/nltk/pull/2736/files.

"
872,https://github.com/nltk/nltk/issues/2850,2850,"[{'id': 739448068, 'node_id': 'MDU6TGFiZWw3Mzk0NDgwNjg=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/resolved', 'name': 'resolved', 'color': '2375af', 'default': False, 'description': None}]",closed,2021-10-07 13:35:46+00:00,,3,"nltk 3.6.4 is ""yanked"" on PyPI","nltk 3.6.4 is currently yanked on PyPI. I don't see any issues except discussing why it was yanked. I see #2833 is tagged as critical but it does not have any references to the release being yanked from PyPI.

It may be helpful to pin something explaining the issue until a new release is pushed.

![image](https://user-images.githubusercontent.com/81247426/136394357-349d67dd-24d5-40d0-b20c-ccf64e382cdc.png)
"
873,https://github.com/nltk/nltk/issues/2853,2853,[],closed,2021-10-12 14:30:13+00:00,,0,TypeError: _pretty() takes 1 positional argument but 2 were given in sem/drt.py,"The following code

    import nltk
    expr=""\ P.\Q.([ ],[ ((([X],[]) + P(X)) => (Q(X))) ])(\Y1.(\X.([],[dog(X) ] )(Y1)))(\Z1.(\P.\X.\F.(P(\E.(([],[nsubj(E,X)]) + F(E))))(\X2.(\P.\Q.(([X],[]) + P(X) + Q(X) )(\X.(\X.([],[cow(X) ] )(X)))(\Y.(\P.\X.\F.(P(\E.(([],[obj(E,X)]) + F(E))))(\Z.(\F.(([E][help(E)]) + F(E) )(\Z2.(Z(Z2)))))(Y)(\X1.(X2(X1)))))))(Z1)(\Y2.(\F.( [],[] )(Y2)))))""
    dexpr = nltk.sem.drt.DrtExpression.fromstring(r""{}"".format(expr))
    dexpr.simplify()
    dexpr.pretty_print()

gives the following error:

    File ""nltk/sem/drt.py"", line 901, in _pretty
    drs, DrtTokens.IMP, self._pretty(self.consequent)
    TypeError: _pretty() takes 1 positional argument but 2 were given

When I track the cause, I see that the _pretty function of self.consequent is called incorrectly.
"
874,https://github.com/nltk/nltk/issues/2857,2857,[],open,2021-10-15 16:24:46+00:00,,6,regex package version  2021.10.8 not working,"While importing nltk it gives error on macbook M1 , python version 3.9.7

```
dlopen(/venv/lib/python3.9/site-packages/regex/_regex.cpython-39-darwin.so, 2): no suitable image found.  Did find:
	t/venv/lib/python3.9/site-packages/regex/_regex.cpython-39-darwin.so: code signature in (/venv/lib/python3.9/site-packages/regex/_regex.cpython-39-darwin.so) not valid for use in process using Library Validation: Trying to load an unsigned lib
```
	
	
After degrading regex package version to 2021.8.3 it works fine"
875,https://github.com/nltk/nltk/issues/2858,2858,"[{'id': 14019200, 'node_id': 'MDU6TGFiZWwxNDAxOTIwMA==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/good%20first%20issue', 'name': 'good first issue', 'color': '02e10c', 'default': True, 'description': ''}, {'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}]",open,2021-10-18 10:02:18+00:00,,0,Manually wrap output lines in HOWTO files,cf https://github.com/nltk/nltk/pull/2856#issuecomment-945193387
876,https://github.com/nltk/nltk/issues/2865,2865,"[{'id': 112364689, 'node_id': 'MDU6TGFiZWwxMTIzNjQ2ODk=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/enhancement', 'name': 'enhancement', 'color': '006b75', 'default': True, 'description': None}, {'id': 975531894, 'node_id': 'MDU6TGFiZWw5NzU1MzE4OTQ=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/metrics', 'name': 'metrics', 'color': 'f442c8', 'default': False, 'description': ''}, {'id': 3375727332, 'node_id': 'LA_kwDOAASTVs7JNYLk', 'url': 'https://api.github.com/repos/nltk/nltk/labels/discussion', 'name': 'discussion', 'color': '71909E', 'default': False, 'description': ''}]",open,2021-10-23 15:51:05+00:00,,3,Incorrect krippendorffs alpha result with missing value or missing data,"```python
from nltk import agreement
rater1 = [1,1,2]
rater2 = [1,1,None]
rater3 = [None,1,2]

taskdata=[[0,str(i),str(rater1[i])] for i in range(0,len(rater1))]+[[1,str(i),str(rater2[i])] for i in range(0,len(rater2))]+[[2,str(i),str(rater3[i])] for i in range(0,len(rater3))]
print(taskdata) # (annotator_id, sample_id, label_id)
ratingtask = agreement.AnnotationTask(data=taskdata)

print(""alpha "" +str(ratingtask.alpha())) # krippendorffs alpha
```

```python
alpha 0.33333333333333337
```

I am not sure if this result is correct. In my example, I assume I have 3 examples needed to annotate. Except the missing values (not every annotator annotate each example), all the annotators provided the same labels. I thought the alpha should be 1. However, I got 0.33.

Does anyone understand is 0.33 correct or 1.0 is correct for krippendorffs alpha?

Thank you!"
877,https://github.com/nltk/nltk/issues/2866,2866,"[{'id': 145489207, 'node_id': 'MDU6TGFiZWwxNDU0ODkyMDc=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/bug', 'name': 'bug', 'color': 'e11d21', 'default': True, 'description': None}, {'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}, {'id': 1641876486, 'node_id': 'MDU6TGFiZWwxNjQxODc2NDg2', 'url': 'https://api.github.com/repos/nltk/nltk/labels/critical', 'name': 'critical', 'color': 'ff2050', 'default': False, 'description': ''}]",open,2021-10-26 21:56:15+00:00,,4,word_tokenize/EN hangs on incorrect strings,"Hi NLTK team,

I have a string that I pass to `word_tokenize` (which uses `PunktSentenceTokenizer` under the hood), and the call hangs. The string in question is taken from Wikipedia and is the result of some vandalism. It can be generated by this

```
text = 'swirley thing w' + 'e' * (884779 - len('swirley thing w'))
```

The call seems to hang, I did not go deep too much but after running this for a couple of hours I just stopped the process. 
What would be an ok solution to process this in a robust fashion? I have a pipeline that has correct sentences as well as, from time to time, this kind of sentences."
878,https://github.com/nltk/nltk/issues/2867,2867,"[{'id': 718744080, 'node_id': 'MDU6TGFiZWw3MTg3NDQwODA=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/wordnet', 'name': 'wordnet', 'color': '3861d1', 'default': False, 'description': None}]",open,2021-10-27 05:46:22+00:00,,1,Rewind Wordnet data files?,"Before the recent merge of #2860, _wordnet.get_version()_ would fail whenever a read had already occurred from the _data.adj_ file:

> import nltk
> from nltk.corpus import wordnet as wn
> print(wn.get_version())

3.0

> print(wn.synset_from_pos_and_offset('a',1740))

Synset('able.a.01')

> print(wn.get_version())

None

The cause of the problem is that _synset_from_pos_and_offset_ goes to a given offset in a data file, reads a line, and stays there. This function is called all the time by other functions in the wordnet module. For ex., in wordnet.doctest, _synset_from_pos_and_offset_ is called indirectly over 10,000 times. 

But now that _get_version_ ensures that reading the data.adj starts at byte offset 0, it probably doesn't matter anywhere else, because there may not be another function that needs to read from the beginning of the data file.

It might be safer to add _data_file.seek(0)_ at the end of _synset_from_pos_and_offset_, but I am not sure that it is worth the additional processing. On the other hand, seek(0) is so cheap an operation that the time penalty would hardly be noticeable.
"
879,https://github.com/nltk/nltk/issues/2870,2870,[],open,2021-10-27 12:23:46+00:00,,1,ModuleNotFoundError: No module named 'regex._regex',"i use this code in order to build a pipeline 
---------------
import re
import nltk 
from textblob import TextBlob 
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from textblob import Word 
from nltk.util import ngrams
import re
from wordcloud import WorldCloud , STOPWORDS 
from nltk.tokenize import word_tokenize 
-----
the the error  ModuleNotFoundError: No module named 'regex._regex' come up 
any suggestions on the reason ???"
880,https://github.com/nltk/nltk/issues/2871,2871,"[{'id': 718735032, 'node_id': 'MDU6TGFiZWw3MTg3MzUwMzI=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/tokenizer', 'name': 'tokenizer', 'color': '4be5a0', 'default': False, 'description': None}]",closed,2021-10-27 17:52:00+00:00,,2,Punkt Tokenizer not achieving expected results,"Hello. 

I've been trying to train Punkt tokenizer to split sentences in a particular way, but it doesn't seem to be working. I have many PDF files that contain a specific kind of inquiry. I want the Punkt tokenizer to divide said inquiries. For example, when using the German pickle file to do the split, this happens:

> Telefax
An: 
 Telefon: 
Datum Fax-Nr.: 
Unsere Anfrage-Nr.: 
 Blatt : 1 von
 Sehr geehrte Damen u. Herren ,
 Bitte erstellen Sie uns ein Angebot, mit gültiger
 Preis- und Terminstellung frei Haus inklusive Fracht und
 Verpackung für folgende Positionen.
--
 Menge Bezeichnung Ø Maß mm Länge Material Bemerkung
1 Rohr 406,4 x 5 2000 1.4539
1 Rohr 88,9 x 3,2 1000 1.4539
2 Rohr 33,4 x 2,6 1000 1.4539
4 V-Flansch DN25 PN40 1.4539
2 V-Flansch DN200 PN 10 1.4539
4 V-Flansch DN80 PN40 1.4539
2 V-Flansch DN50 PN40 1.4539
Ausführung gemäß: AD2000
 Lieferzeit :
 Kosten für Zeugnisse, sachliche und pers.
--
Abnahmekosten bitte angeben
 Für Ihre freundliche Mühe bedanken wir uns im voraus.
--
Mit freundlichem 

This is an inquiry: 

> 4 V-Flansch DN25 PN40 1.4539

This is the code I use to achieve that result:
```
def segmentate_sentences(text):
  sent_detector = nltk.data.load('tokenizers/punkt/german.pickle')
  print('\n-----\n'.join(sent_detector.tokenize(text.strip())))
```
But what I want is to split the specific inquiries, each one in as a sentence, like this:
 
> Telefax
An: 
 Telefon: 
Datum Fax-Nr.: 
Unsere Anfrage-Nr.: 
 Blatt : 1 von
 Sehr geehrte Damen u. Herren ,
 Bitte erstellen Sie uns ein Angebot, mit gültiger
 Preis- und Terminstellung frei Haus inklusive Fracht und
 Verpackung für folgende Positionen.
--
 Menge Bezeichnung Ø Maß mm Länge Material Bemerkung
--
1 Rohr 406,4 x 5 2000 1.4539
--
1 Rohr 88,9 x 3,2 1000 1.4539
--
2 Rohr 33,4 x 2,6 1000 1.4539
--
4 V-Flansch DN25 PN40 1.4539
--
2 V-Flansch DN200 PN 10 1.4539
--
4 V-Flansch DN80 PN40 1.4539
--
2 V-Flansch DN50 PN40 1.4539
--
Ausführung gemäß: AD2000
 Lieferzeit :
 Kosten für Zeugnisse, sachliche und pers.
--
Abnahmekosten bitte angeben
 Für Ihre freundliche Mühe bedanken wir uns im voraus.
--
Mit freundlichem 

I don't really mind how the rest of the text is segmented. To achieve this, I tried training the Punkt Tokenizer. I've read a couple of issues and SO posts about training and the data format for training (Which, for my understanding, is just plain text). For the training and execution, I used the following code snippet:

```
def train_tokenizer(input_text):
  train_text = state_union.raw(""training_data.txt"")

  custom_sent_tokenizer = PunktSentenceTokenizer(train_text) 

  tokenized = custom_sent_tokenizer.tokenize(input_text)

  print('\n-----\n'.join(tokenized))
```

For training_data.txt I just wrote the entire text and separated the sentences according to the result I wanted to achieve. Regardless, the result is precisely the same as while using the pre-trained german model. I've made many modifications to the training file, like adding line breaks between sentences, trying different ways to segment, but the result doesn't seem to change.

I read in one SO post that the training required a lot of data, not specifying exactly how much. I have a document with more than 10,000 inquiries in different formats. I used it for training, but the model kept performing precisely the same. I would really appreciate some guidance :) 

(I know that could use Regex in this particular case, but that only applies to this text. Usually, my inquiries come in many different formats and forms. That's why I want to train my own sentence tokenizer) "
881,https://github.com/nltk/nltk/issues/2874,2874,"[{'id': 23593492, 'node_id': 'MDU6TGFiZWwyMzU5MzQ5Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/documentation', 'name': 'documentation', 'color': '02d7e1', 'default': True, 'description': None}, {'id': 29356472, 'node_id': 'MDU6TGFiZWwyOTM1NjQ3Mg==', 'url': 'https://api.github.com/repos/nltk/nltk/labels/admin', 'name': 'admin', 'color': '444444', 'default': False, 'description': None}, {'id': 416522805, 'node_id': 'MDU6TGFiZWw0MTY1MjI4MDU=', 'url': 'https://api.github.com/repos/nltk/nltk/labels/nice%20idea', 'name': 'nice idea', 'color': 'fef2c0', 'default': False, 'description': None}]",open,2021-10-28 08:14:42+00:00,,1,Add `CITATION.cff` file to repository ,"Github has a built-in support for citation. Can you please consider adding  CITATTION.cff file to your repository so we can easy cite your work?


Info:
https://citation-file-format.github.io/
https://twitter.com/natfriedman/status/1420122675813441540?lang=en"
