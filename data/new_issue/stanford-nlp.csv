,html_url,number,labels,state,created_at,pull_request,title,body
0,https://github.com/stanfordnlp/CoreNLP/issues/1,1,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",closed,2013-09-13 02:18:47+00:00,,Could the project switch to using log4j for logs?,"I see a lot of logs printed to System.out or System.err.
Would it be possible to use a library like log4j http://logging.apache.org/log4j/2.x/ and use log.error, log.warning, log.info, log.debug instead?
That would make it easier for users of the StanfordCoreNLP to manage which logs should be printed by choosing the log level of the project.
"
1,https://github.com/stanfordnlp/CoreNLP/issues/2,2,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2013-10-22 23:40:57+00:00,,"NullPointerException on ""hello world"" input","I get an NPE on many (but not all) test sentences I've tried, including the text ""hello world"". Stack trace:

```
java.lang.NullPointerException
  at org.ejml.simple.SimpleMatrix.<init>(SimpleMatrix.java:158)
  at edu.stanford.nlp.rnn.RNNUtils.elementwiseApplyTanh(RNNUtils.java:175)
  at edu.stanford.nlp.sentiment.SentimentCostAndGradient.forwardPropagateTree(SentimentCostAndGradient.java:328)
  at edu.stanford.nlp.sentiment.SentimentCostAndGradient.forwardPropagateTree(SentimentCostAndGradient.java:333)
  at edu.stanford.nlp.sentiment.SentimentCostAndGradient.forwardPropagateTree(SentimentCostAndGradient.java:332)
  at edu.stanford.nlp.sentiment.SentimentCostAndGradient.forwardPropagateTree(SentimentCostAndGradient.java:333)
  at edu.stanford.nlp.sentiment.SentimentCostAndGradient.forwardPropagateTree(SentimentCostAndGradient.java:333)
  at edu.stanford.nlp.sentiment.SentimentCostAndGradient.forwardPropagateTree(SentimentCostAndGradient.java:332)
  at edu.stanford.nlp.sentiment.SentimentCostAndGradient.forwardPropagateTree(SentimentCostAndGradient.java:333)
  at edu.stanford.nlp.sentiment.SentimentCostAndGradient.forwardPropagateTree(SentimentCostAndGradient.java:333)
  at edu.stanford.nlp.sentiment.SentimentCostAndGradient.forwardPropagateTree(SentimentCostAndGradient.java:332)
  at edu.stanford.nlp.sentiment.SentimentCostAndGradient.forwardPropagateTree(SentimentCostAndGradient.java:333)
  at edu.stanford.nlp.pipeline.SentimentAnnotator.annotate(SentimentAnnotator.java:46)
  at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:67)
  at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:876)
```
"
2,https://github.com/stanfordnlp/CoreNLP/issues/4,4,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",closed,2013-11-06 22:39:07+00:00,,Inconsistencies in the interfaces for constituent spans,"I wrote the following up on 2013-09-16, when I was trying to understand/modify the coreference system, which was using different access methods below in different places.

There seem to be at least three types of span/index information for words and parsetree constituents, in Stanford CoreNLP, and they are all inconsistent with one another.

CoreAnnotations.IndexAnnotation
Only applies to leaves, I think.
Initialize with: Tree.indexLeaves()
1-indexed
reliable

CoreAnnotations.BeginIndexAnnotation and CoreAnnotations.EndIndexAnnotation
Applies to both nonterminals and leaves.
Initialize with: Tree.indexSpans()
0-indexed inclusive-exclusive: [start,end)
NOT RELIABLE - sometimes are null.

CoreAnnotations.SpanAnnotation [with wrapper Tree.getSpan()]
Initialize with: Tree.setSpans()
0-indexed inclusive-inclusive: [start,end]
NOT RELIABLE - sometimes is null.

I made an example of this with Stanford CoreNLP 3.2.0.
It reads in a parse tree, then prints out the above annotations at every node in the tree.
Code and output here: https://gist.github.com/brendano/7345495
"
3,https://github.com/stanfordnlp/CoreNLP/issues/5,5,"[{'id': 45387509, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOQ==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/wontfix', 'name': 'wontfix', 'color': 'eeeeee', 'default': True, 'description': None}]",closed,2013-11-14 09:30:26+00:00,,is there any way to remove stop words from text document and stemming words not lemmatization,"i want to know how to remove stop words and stemming of words?
"
4,https://github.com/stanfordnlp/CoreNLP/issues/6,6,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",open,2013-11-17 21:20:59+00:00,,Custom features in NER package should use dependency injection,"To add a custom feature extractor to the NER package, the path of least resistance is to modify NERFeatureFactory.java, AnnotationLookup.java, CoreAnnotations.java and SeqClassifierFlags.java. A more generic approach would use dependency injection, so third-party developers wouldn't have to touch code inside CoreNLP. If it's not already on your development roadmap, I'm happy to take on that change.

Dave
"
5,https://github.com/stanfordnlp/CoreNLP/issues/7,7,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2013-11-26 10:29:11+00:00,,Inconsistencies with sentiment analysis output,"I did a quick test code to try out the new sentiment model and noticed that there is something weird going on when using RNNCoreAnnotations.getPredictedClass().

I don't know if the sentiment analysis model included to 3.3.0 is different than on the live demo site (http://nlp.stanford.edu:8080/sentiment/rntnDemo.html), but in any case the short test code is:

```
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.rnn.RNNCoreAnnotations;
import edu.stanford.nlp.sentiment.SentimentCoreAnnotations;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.util.CoreMap;
import java.util.Properties;

public class SentimentTestAppStanfordNLP {

    private StanfordCoreNLP pipeline;

    public SentimentTestAppStanfordNLP() {
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment"");
        pipeline = new StanfordCoreNLP(props);    
    }

    private void checkSentiment(String text) {        
        Annotation annotation = pipeline.process(text);
        for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {
            Tree tree = sentence.get(SentimentCoreAnnotations.AnnotatedTree.class);
            int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
            System.out.println(""Sentiment: "" + sentiment + "" String: "" + sentence.toString());
        }        
    }

    private void doMain() throws Exception {
        checkSentiment(""Radek is a really good football player"");
        checkSentiment(""Radek is a good football player"");
        checkSentiment(""Radek is an OK football player"");
        checkSentiment(""Radek is a bad football player"");
        checkSentiment(""Radek is a really bad football player"");        
        System.out.println(""-----------------------------"");
        checkSentiment(""Mark is a really good football player"");
        checkSentiment(""Mark is a good football player"");
        checkSentiment(""Mark is an OK football player"");
        checkSentiment(""Mark is a bad football player"");
        checkSentiment(""Mark is a really bad football player"");        
    }

    public static void main(String[] args) {
        try {
            SentimentTestAppStanfordNLP main = new SentimentTestAppStanfordNLP();
            main.doMain();
        } catch (Exception ex) {
            ex.printStackTrace();
        }
    }
}
```

The output baffled me; in the cases of ""Radek"", the RNNCoreAnnotations seemed to give almost random output, whereas on ""Mark"" cases the outputs were pretty much as expected (see below). When I test these same sentences on the live demo site, the ""Radek"" cases are correct, not like what the CoreNLP outputs here.

```
Sentiment: 0 String: Radek is a really good football player
Sentiment: 1 String: Radek is a good football player
Sentiment: 2 String: Radek is an OK football player
Sentiment: 2 String: Radek is a bad football player
Sentiment: 2 String: Radek is a really bad football player
-----------------------------
Sentiment: 3 String: Mark is a really good football player
Sentiment: 3 String: Mark is a good football player
Sentiment: 2 String: Mark is an OK football player
Sentiment: 1 String: Mark is a bad football player
Sentiment: 1 String: Mark is a really bad football player
```
"
6,https://github.com/stanfordnlp/CoreNLP/issues/8,8,"[{'id': 45387507, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNw==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/cantreproduce', 'name': 'cantreproduce', 'color': 'dddddd', 'default': False, 'description': None}]",closed,2013-12-29 17:34:06+00:00,,Writing sentiment analysis results to XML,"I'm having trouble figuring out how to get the sentiment analysis tool to output as an XML file when run from the command line.  When I run the command provided on http://www-nlp.stanford.edu/sentiment/code.html it works fine but only outputs plain text:

```
Adding annotator tokenize

Adding annotator ssplit

Adding annotator parse

Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [1.3 sec].

Adding annotator sentiment

This is so great.

  Very positive

It was okay I guess.

  Neutral
```

However if I try to run the full CoreNLP tool with the sentiment annotator, like such:

```
java -cp stanford-corenlp-full-2013-11-12/stanford-corenlp-3.3.0.jar:stanford-corenlp-full-2013-11-12/stanford-corenlp-3.3.0-models.jar:stanford-corenlp-full-2013-11-12/xom.jar:stanford-corenlp-full-2013-11-12/joda-time.jar:stanford-corenlp-full-2013-11-12/jollyday.jar -Xmx3g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,parse,sentiment -file  ./tweets/tweet1.txt
```

I get the following error:

```
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/ejml/simple/SimpleBase

    at edu.stanford.nlp.pipeline.SentimentAnnotator.<init>(SentimentAnnotator.java:45)

    at edu.stanford.nlp.pipeline.StanfordCoreNLP$14.create(StanfordCoreNLP.java:845)

    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:81)

    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:260)

    at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:127)

    at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:123)

    at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1430)

Caused by: java.lang.ClassNotFoundException: org.ejml.simple.SimpleBase

    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)

    at java.security.AccessController.doPrivileged(Native Method)

    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)

    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)

    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)

    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)

    ... 7 more
```

If I run the command without the sentiment annotator, it works fine but of course I can't get any sentiment results.

I should also mention that I am running everything wrapped inside a Python subprocess.Popen() call, since the rest of our project is written in Python.
"
7,https://github.com/stanfordnlp/CoreNLP/issues/9,9,"[{'id': 45387507, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNw==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/cantreproduce', 'name': 'cantreproduce', 'color': 'dddddd', 'default': False, 'description': None}]",closed,2014-01-03 09:11:16+00:00,,how to build this repo,"Hi ! i am using `ant` to build the javas,
but error followed:

```
Buildfile: /home/drill/Downloads/CoreNLP-master/build.xml

classpath:
     [echo] core

compile:
     [echo] core
    [javac] Compiling 1 source file to /home/drill/Downloads/CoreNLP-master/classes
    [javac] /home/drill/Downloads/CoreNLP-master/test/src/edu/stanford/nlp/util/IntervalTreeTest.java:70: error: cannot find symbol
    [javac]     tree.check();
    [javac]         ^
    [javac]   symbol:   method check()
    [javac]   location: variable tree of type IntervalTree<Integer,Interval<Integer>>
    [javac] /home/drill/Downloads/CoreNLP-master/test/src/edu/stanford/nlp/util/IntervalTreeTest.java:71: error: cannot find symbol
    [javac]     tree.balance();
    [javac]         ^
    [javac]   symbol:   method balance()
    [javac]   location: variable tree of type IntervalTree<Integer,Interval<Integer>>
    [javac] /home/drill/Downloads/CoreNLP-master/test/src/edu/stanford/nlp/util/IntervalTreeTest.java:72: error: cannot find symbol
    [javac]     int height = tree.height();
    [javac]                      ^
    [javac]   symbol:   method height()
    [javac]   location: variable tree of type IntervalTree<Integer,Interval<Integer>>
    [javac] /home/drill/Downloads/CoreNLP-master/test/src/edu/stanford/nlp/util/IntervalTreeTest.java:74: error: cannot find symbol
    [javac]     tree.check();
    [javac]         ^
    [javac]   symbol:   method check()
    [javac]   location: variable tree of type IntervalTree<Integer,Interval<Integer>>
    [javac] /home/drill/Downloads/CoreNLP-master/test/src/edu/stanford/nlp/util/IntervalTreeTest.java:84: error: cannot find symbol
    [javac]     tree.clear();
    [javac]         ^
    [javac]   symbol:   method clear()
    [javac]   location: variable tree of type IntervalTree<Integer,Interval<Integer>>
    [javac] /home/drill/Downloads/CoreNLP-master/test/src/edu/stanford/nlp/util/IntervalTreeTest.java:130: error: cannot find symbol
    [javac]     Iterator<Interval<Integer>> iterator = tree.iterator();
    [javac]                                                ^
    [javac]   symbol:   method iterator()
    [javac]   location: variable tree of type IntervalTree<Integer,Interval<Integer>>
    [javac] /home/drill/Downloads/CoreNLP-master/test/src/edu/stanford/nlp/util/IntervalTreeTest.java:156: error: cannot find symbol
    [javac]     Iterator<Interval<Integer>> iterator = tree.iterator();
    [javac]                                                ^
    [javac]   symbol:   method iterator()
    [javac]   location: variable tree of type IntervalTree<Integer,Interval<Integer>>
    [javac] 7 errors

BUILD FAILED
/home/drill/Downloads/CoreNLP-master/build.xml:99: Compile failed; see the compiler error output for details.
```

i have googled many times , found none. does it support the command line way ? hope you could give a guide , thx 
"
8,https://github.com/stanfordnlp/CoreNLP/issues/11,11,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2014-01-20 19:25:10+00:00,,"Some tests do not work on Locales using "","" as decimal separator.","```
[junit] Testcase: testToSortedString(edu.stanford.nlp.stats.CountersTest):  FAILED
[junit] null expected:<{c1[.0:a0.5:b0.]3}> but was:<{c1[,0:a0,5:b0,]3}>
[junit] junit.framework.ComparisonFailure: null expected:<{c1[.0:a0.5:b0.]3}> but was:<{c1[,0:a0,5:b0,]3}>
[junit]     at edu.stanford.nlp.stats.CountersTest.testToSortedString(CountersTest.java:250)
```

---

```
[junit] Testcase: testBasic(edu.stanford.nlp.util.ConfusionMatrixTest): FAILED
[junit] null expected:<...    prec=1, recall=0[.66667, spec=1, f1=0.8
[junit]               C2 = b        prec=0, recall=n/a, spec=0.]75, f1=n/a
[junit]          ...> but was:<...    prec=1, recall=0[,66667, spec=1, f1=0,8
[junit]               C2 = b        prec=0, recall=n/a, spec=0,]75, f1=n/a
[junit]          ...>
[junit] junit.framework.ComparisonFailure: null expected:<...    prec=1, recall=0[.66667, spec=1, f1=0.8
[junit]               C2 = b        prec=0, recall=n/a, spec=0.]75, f1=n/a
[junit]          ...> but was:<...    prec=1, recall=0[,66667, spec=1, f1=0,8
[junit]               C2 = b        prec=0, recall=n/a, spec=0,]75, f1=n/a
[junit]          ...>
[junit]     at edu.stanford.nlp.util.ConfusionMatrixTest.testBasic(ConfusionMatrixTest.java:41)
```
"
9,https://github.com/stanfordnlp/CoreNLP/issues/13,13,[],closed,2014-01-20 19:47:35+00:00,,"Consider removing ""classes"" directory from repository?","I wonder, is there a reason that the ""classes"" are part of the git repository? Normally, generated output. I'm asking, because I constantly see that directory dirty in my git client.
"
10,https://github.com/stanfordnlp/CoreNLP/issues/17,17,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2014-01-21 17:51:06+00:00,,testTrieFindClosest failing under Java 8,"When using Java 8 (early access), this test case fails.

java version ""1.8.0-ea""
Java(TM) SE Runtime Environment (build 1.8.0-ea-b121)
Java HotSpot(TM) 64-Bit Server VM (build 25.0-b63, mixed mode)

Switching to Java 7, it works fine. This just FYI. I didn't investigate what exactly causes this issue and if it may be due to any remaining issue in the Java 8 preview.

---

```
    [junit] Testcase: testTrieFindClosest(edu.stanford.nlp.ling.tokensregex.matcher.TrieMapTest):   FAILED
    [junit] Expecting [([a - black - cat] -> true at (0,2),2.0), ([a - black - hat] -> true at (0,2),2.0), ([a - white - hat] -> true at (0,2),3.0), ([a - white - cat] -> true at (0,2),3.0), ([a - colored - hat] -> true at (0,2),3.0)], got [([a - black - hat] -> true at (0,2),2.0), ([a - black - cat] -> true at (0,2),2.0), ([a - colored - hat] -> true at (0,2),3.0), ([a - white - cat] -> true at (0,2),3.0), ([a - white - hat] -> true at (0,2),3.0)] 
expected:
<[([a - black - cat] -> true at (0,2),2.0), ([a - black - hat] -> true at (0,2),2.0), ([a - white - hat] -> true at (0,2),3.0), ([a - white - cat] -> true at (0,2),3.0), ([a - colored - hat] -> true at (0,2),3.0)]> 
but was:
<[([a - black - hat] -> true at (0,2),2.0), ([a - black - cat] -> true at (0,2),2.0), ([a - colored - hat] -> true at (0,2),3.0), ([a - white - cat] -> true at (0,2),3.0), ([a - white - hat] -> true at (0,2),3.0)]>
```
"
11,https://github.com/stanfordnlp/CoreNLP/issues/19,19,[],closed,2014-03-12 17:54:31+00:00,,Program freezes,"I have installed the entire Stanford library on Eclipse.  When I run the following source code, the program just crashes, with the last message being ""Adding annotator sentiment"". It remains like without any tree or any output stating positive/negative/neutral. Kindly can anyone help me out?

SOURCE CODE:

```
import edu.stanford.nlp.dcoref.CorefChain;
import edu.stanford.nlp.dcoref.CorefCoreAnnotations.CorefChainAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.semgraph.SemanticGraph;
import edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.trees.TreeCoreAnnotations.TreeAnnotation;
import edu.stanford.nlp.util.CoreMap;
import java.io.IOException;
import java.util.List;
import java.util.Map;
import java.util.Properties;


public class TagText
{
    public static void main(String[] args) throws IOException, ClassNotFoundException
    {
        // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution 
        Properties props = new Properties();
        props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, sentiment"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        // read some text in the text variable
        String text = ""European Stocks Drop as Maersk, Valeo Fall on Stake Sales"";

        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);

        // these are all the sentences in this document
        // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
        List<CoreMap> sentences = document.get(SentencesAnnotation.class);

        for(CoreMap sentence: sentences) {
          // traversing the words in the current sentence
          // a CoreLabel is a CoreMap with additional token-specific methods
          for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
            // this is the text of the token
            String word = token.get(TextAnnotation.class);
            // this is the POS tag of the token
            String pos = token.get(PartOfSpeechAnnotation.class);
            // this is the NER label of the token
            String ne = token.get(NamedEntityTagAnnotation.class);       
          }

          // this is the parse tree of the current sentence
          Tree tree = sentence.get(TreeAnnotation.class);

          // this is the Stanford dependency graph of the current sentence
          SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
        }

        // This is the coreference link graph
        // Each chain stores a set of mentions that link to each other,
        // along with a method for getting the most representative mention
        // Both sentence and token offsets start at 1!
        Map<Integer, CorefChain> graph = 
          document.get(CorefChainAnnotation.class);
   }
}
```

THE FOLLOWING MESSAGES APPEAR ON THE ECLIPSE CONSOLE with no tree or any output stating positive/negative/neutral:

```
Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [4.0 sec].
Adding annotator lemma
Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [13.5 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [11.2 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [9.4 sec].
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/defs.sutime.txt
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.sutime.txt
Mar 12, 2014 6:33:22 PM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules
INFO: Ignoring inactive rule: null
Mar 12, 2014 6:33:22 PM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules
INFO: Ignoring inactive rule: temporal-composite-8:ranges
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
Initializing JollyDayHoliday for sutime with classpath:edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/defs.sutime.txt
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.sutime.txt
Mar 12, 2014 6:33:23 PM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules
INFO: Ignoring inactive rule: null
Mar 12, 2014 6:33:23 PM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules
INFO: Ignoring inactive rule: temporal-composite-8:ranges
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [2.8 sec].
Adding annotator sentiment
```
"
12,https://github.com/stanfordnlp/CoreNLP/issues/20,20,"[{'id': 103161713, 'node_id': 'MDU6TGFiZWwxMDMxNjE3MTM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/discussion', 'name': 'discussion', 'color': 'd4c5f9', 'default': False, 'description': None}]",closed,2014-03-19 11:05:15+00:00,,List<Foo> or List<? extends Foo> (discussion),"I need to annotate my text with some `ParserConstraint`. But I need those constraints to keep track of the token sequence they are constraining.  I could re-build those sequence later using the `.start` and `.end` fields, no big deal, but as I iterate over the sentence token sequence to build constraints, that would imply kind of _double_ iteration which I would really like to avoid.  

I was thinking about extending the `ParserConstraint` class, but as far as I understand the Java Generics, it's not possible as the `ParserAnnotations.ConstraintAnnotation` class' `.getType()` method returns `java.lang.Class<java.util.List<ParserConstraint>>`.  

I'm thinking that it would be great to have a covariant list, like `java.util.List<? extends ParserConstraint>`. What do you think about? Is it feasible? Thanks.

P.S. of course, I'm available for monkey coding. :-)
"
13,https://github.com/stanfordnlp/CoreNLP/issues/23,23,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",closed,2014-03-31 08:06:37+00:00,,Maven support,"Add a maven building support would be nice.
"
14,https://github.com/stanfordnlp/CoreNLP/issues/24,24,[],closed,2014-04-14 11:39:25+00:00,,NER annotation doesn't allow for setting SUTime rule path,"When the NER annotator is used in a pipeline it doesn't seem to support passing the _`sutime.rules`_ property on to the time extractors created by `NumberSequenceClassifier`, which leads to the `Options` object always being filled out with the default SUTime rules' paths. 

In Java this isn't really a problem due to the class pathing, however, I'm using the .Net bindings via IKVM and have run into a few issues with this (as such I also don't have Java installed and thus cannot create a patch).
"
15,https://github.com/stanfordnlp/CoreNLP/issues/25,25,"[{'id': 45387507, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNw==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/cantreproduce', 'name': 'cantreproduce', 'color': 'dddddd', 'default': False, 'description': None}]",closed,2014-04-17 18:32:00+00:00,,Caseless Parsers Broken,"Since v3.3.1, caseless parsers are no longer supported. Is there a new model that is supported with the updated version?

Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.caseless.ser.gz ...
java.util.zip.ZipException: Not in GZIP format
    at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:164)
    at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:78)
    at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:90)
    at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:446)
    at edu.stanford.nlp.io.IOUtils.readStreamFromString(IOUtils.java:368)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromSerializedFile(LexicalizedParser.java:606)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromFile(LexicalizedParser.java:401)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:158)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:144)
    at edu.stanford.nlp.pipeline.ParserAnnotator.loadModel(ParserAnnotator.java:187)
    at edu.stanford.nlp.pipeline.ParserAnnotator.<init>(ParserAnnotator.java:113)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$10.create(StanfordCoreNLP.java:732)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:81)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:262)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:129)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:125)
    at edu.stanford.nlp.sentiment.SentimentPipeline.main(SentimentPipeline.java:297)
Loading parser from text file edu/stanford/nlp/models/lexparser/englishPCFG.caseless.ser.gz java.util.zip.ZipException: Not in GZIP format
    at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:164)
    at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:78)
    at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:90)
    at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:446)
    at edu.stanford.nlp.io.IOUtils.readerFromString(IOUtils.java:513)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromTextFile(LexicalizedParser.java:540)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromFile(LexicalizedParser.java:403)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:158)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:144)
    at edu.stanford.nlp.pipeline.ParserAnnotator.loadModel(ParserAnnotator.java:187)
    at edu.stanford.nlp.pipeline.ParserAnnotator.<init>(ParserAnnotator.java:113)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$10.create(StanfordCoreNLP.java:732)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:81)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:262)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:129)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:125)
    at edu.stanford.nlp.sentiment.SentimentPipeline.main(SentimentPipeline.java:297)
Exception in thread ""main"" java.lang.NullPointerException
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:160)
    at edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:144)
    at edu.stanford.nlp.pipeline.ParserAnnotator.loadModel(ParserAnnotator.java:187)
    at edu.stanford.nlp.pipeline.ParserAnnotator.<init>(ParserAnnotator.java:113)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$10.create(StanfordCoreNLP.java:732)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:81)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:262)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:129)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:125)
    at edu.stanford.nlp.sentiment.SentimentPipeline.main(SentimentPipeline.java:297)
"
16,https://github.com/stanfordnlp/CoreNLP/issues/26,26,[],closed,2014-04-29 08:17:27+00:00,,Instructions to run the system locally,"1. I cloned the git repository. Updated the java/javac versions to 1.7
2. I can compile fine (meaning when I run ant compile it says build successful)
3. I cannot build fine meaning when I run ant build I get following error:

BUILD FAILED
Target ""build"" does not exist in the project ""core"".
1. I tried importing this project in eclipse (using import existing projects but seems it does not contain any existing project) but failed.

So although I can build I am not able to make any headway.

Can you please write some instructions on readme on how to run the app locally.
"
17,https://github.com/stanfordnlp/CoreNLP/issues/27,27,"[{'id': 103162424, 'node_id': 'MDU6TGFiZWwxMDMxNjI0MjQ=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/request', 'name': 'request', 'color': '94c5e9', 'default': False, 'description': None}]",closed,2014-05-21 21:47:30+00:00,,Release new version with commit 39f68fc,"Please consider releasing a new version to Maven central with commit 39f68fc800475305c8725d2e4be7df371562186c.
"
18,https://github.com/stanfordnlp/CoreNLP/issues/28,28,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",closed,2014-06-08 07:40:31+00:00,,Add Enum's for Part Of Speech,"Right now PoS returns (IMO) a cryptic String that doesn't really benefit the developer. If this could return an Enumed type that represented the PoS it would allow for more developer friendly code. 
"
19,https://github.com/stanfordnlp/CoreNLP/issues/30,30,[],closed,2014-06-24 17:52:51+00:00,,3.4 Release missing classes for SR Parser,"When setting `parse.model=edu/stanford/nlp/models/srparser/englishSR.ser.gz` and using the SR models from the site, there is a `java.lang.ClassNotFoundException` being thrown for `edu.stanford.nlp.parser.shiftreduce.BasicFeatureFactory`. 

Upon inspection it looks like the class files for `BasicFeatureFactory` & `DistsimFeatureFactory` were not included in the build/jar; this renders the SR parser unusable from 3.4 (which is a bit of a pain as we use the .Net bindings).
"
20,https://github.com/stanfordnlp/CoreNLP/issues/31,31,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2014-07-22 19:05:02+00:00,,NullRef in DeterministicCorefSieve.sortMentionsForPronoun,"When using the caseless pos-tagger, it is possible to trigger a null-reference exception in `edu.stanford.nlp.dcoref.sievepasses.DeterministicCorefSieve.sortMentionsForPronoun` when there is a dangling pronoun. 

A simple repro-case using a simplified tweet that can trigger the exception:
 `rt @bob: I really hate fifa 2015. ya`

which yields this trace:

```
Exception in thread ""main"" java.lang.RuntimeException: Error annotating C:\Users\***\Desktop\stanford-corenlp-full-2014-06-16\input.txt
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$15.run(StanfordCoreNLP.java:1288)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1348)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1390)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1460)
Caused by: java.lang.NullPointerException
        at edu.stanford.nlp.dcoref.sievepasses.DeterministicCorefSieve.sortMentionsForPronoun(DeterministicCorefSieve.java:482)
        at edu.stanford.nlp.dcoref.sievepasses.DeterministicCorefSieve.getOrderedAntecedents(DeterministicCorefSieve.java:464)
        at edu.stanford.nlp.dcoref.SieveCoreferenceSystem.coreference(SieveCoreferenceSystem.java:898)
        at edu.stanford.nlp.dcoref.SieveCoreferenceSystem.coref(SieveCoreferenceSystem.java:845)
        at edu.stanford.nlp.pipeline.DeterministicCorefAnnotator.annotate(DeterministicCorefAnnotator.java:121)
        at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:67)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:848)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP$15.run(StanfordCoreNLP.java:1276)
        ... 3 more
```

Admittedly this is not correct English in anyway, however it would be nice to see a little more robustness in the system :)
"
21,https://github.com/stanfordnlp/CoreNLP/issues/32,32,"[{'id': 77134078, 'node_id': 'MDU6TGFiZWw3NzEzNDA3OA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/documentation', 'name': 'documentation', 'color': '5319e7', 'default': True, 'description': None}]",closed,2014-09-20 12:22:40+00:00,,Train sentiment analyzer for a specific domain,"Hello, 

I am not sure if this is relevant to this forum but I want to train the sentiment analyzer model for specific domains, right now it is pretty generic. e.g. following sentences

The room was spacious  or The restaurants were short walk from the hotel

get a sentiment of 1/5 whereas they talk positively. 

Any instructions how I can achieve this extension will be appreciated.
"
22,https://github.com/stanfordnlp/CoreNLP/issues/33,33,"[{'id': 45387508, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}]",closed,2014-10-09 04:35:56+00:00,,mapreduce jobs for corenlp,"Hello,

My query is not an issue but more like how to achieve X using corenlp.

I need to process large amount of data and I was looking at corenlp as one of the options.

Since processing a review was taking about 4 secs (on a 2 year old macbook pro) I wanted to use map-reduce to run it over larger amount of data.

The way map reduce jobs are run, typically each map routine gets one line of the file to process. If I call corenlp for each line (or each review at the max) then it it a lot of overhead because corenlp has setup time and it is not efficient to setup for each line.

So I wanted to know if the authors have any thought on whether running of corenlp can be optimized for map reduce paradigm? And if there is any relevant implementation of corenlp in this paradigm which I can look at.

Thanks.
"
23,https://github.com/stanfordnlp/CoreNLP/issues/35,35,"[{'id': 103162424, 'node_id': 'MDU6TGFiZWwxMDMxNjI0MjQ=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/request', 'name': 'request', 'color': '94c5e9', 'default': False, 'description': None}]",closed,2014-10-29 10:40:27+00:00,,Provide version 3.5.0 at Maven Central,"According to http://nlp.stanford.edu/software/corenlp.shtml#Download the most current version 3.5.0 is available, but when having a look at Maven Central, only version 3.4.1 is currently obtainable from there. Please provide version 3.5.0 there as well. By the way: thank you very much for providing Stanford CoreNLP! Great tool :)
"
24,https://github.com/stanfordnlp/CoreNLP/issues/36,36,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",closed,2014-11-22 03:47:21+00:00,,Gradle Build Support,"In response to Issue #23: I think Gradle would be better. I'd be happy to contribute that.  A minimal build.gradle with ant.importBuild(""build.xml"") would enable the use as a subproject.
"
25,https://github.com/stanfordnlp/CoreNLP/issues/37,37,[],closed,2014-11-22 03:48:01+00:00,,Build Failure,"Fresh checkout.

``` bash
CoreNLP$ ant test
Buildfile: /Users/RCFischer/wkdir/JavaNLP/CoreNLP/build.xml

classpath:
     [echo] core

compile:
     [echo] core
    [javac] Compiling 1324 source files to /Users/RCFischer/wkdir/JavaNLP/CoreNLP/classes
    [javac] /Users/RCFischer/wkdir/JavaNLP/CoreNLP/src/edu/stanford/nlp/ling/tokensregex/SequenceMatchRules.java:352: error: cannot access SequencePattern
    [javac]   static public AnnotationExtractRule createTokenPatternRule(Env env, SequencePattern.PatternExpr expr, Expression result)
    [javac]                                                                       ^
    [javac]   bad class file: /Users/RCFischer/wkdir/JavaNLP/CoreNLP/classes/edu/stanford/nlp/ling/tokensregex/SequencePattern.class
    [javac]     class file contains wrong class: edu.stanford.nlp.stats.IntCounter
    [javac]     Please remove or make sure it appears in the correct subdirectory of the classpath.

BUILD FAILED
/Users/RCFischer/wkdir/JavaNLP/CoreNLP/build.xml:99: Compile failed; see the compiler error output for details.

Total time: 3 seconds
```
"
26,https://github.com/stanfordnlp/CoreNLP/issues/39,39,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2014-12-01 22:29:32+00:00,,SUTime NIGHT constant range end time is before start time,"The NIGHT constant in SUTime.java currently has the range between hour (19:00 - 5:00) giving it a duration of -14 hours

REPRODUCE:
http://nlp.stanford.edu:8080/sutime/process
Input: ""tomorrow night""
Output: <TIMEX3 range=""(2014-12-02T19:00:00.000,2014-12-02T05,PT-14H)"" tid=""t3"" type=""TIME"" value=""2014-12-02TNI"">tomorrow night</TIMEX3>

![image](https://cloud.githubusercontent.com/assets/8366770/5254570/317870d2-7977-11e4-893e-5b5145f096ab.png)

BUG LOCATION:
edu.stanford.nlp.time.SUTime.java:724
  public static final Time NIGHT = createTemporal(StandardTemporalType.TIME_OF_DAY, ""NI"", new InexactTime(new Range(new InexactTime(new Partial(DateTimeFieldType.hourOfDay(), 19)), new InexactTime(new Partial(DateTimeFieldType
      .hourOfDay(), 5)))));

FIX SHOULD BE SOMETING LIKE:
  public static final Time NIGHT = createTemporal(StandardTemporalType.TIME_OF_DAY, ""NI"", new InexactTime(new Range(new InexactTime(new Partial(DateTimeFieldType.hourOfDay(), 19)), new InexactTime(new Partial(DateTimeFieldType
      .hourOfDay(), 24)))));
"
27,https://github.com/stanfordnlp/CoreNLP/issues/40,40,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2014-12-03 07:20:20+00:00,,SUTime sample does not work,"Example of `SUTime` usage on [the site](http://nlp.stanford.edu/software/sutime.shtml) has following line

``` java
pipeline.addAnnotator(new PTBTokenizerAnnotator(false));
```

but I cannot find class `PTBTokenizerAnnotator` in source code of version `3.5.0`

Could you please provide correct example?
"
28,https://github.com/stanfordnlp/CoreNLP/issues/41,41,[],closed,2014-12-03 23:03:05+00:00,,Access to tagset in ShiftReduceParser,"it would be nice if the `ShiftReduceParser` exposed a `tagSet()` method which would basically do a 

```
return model.knownStates
```

Currently, I need to use reflection to access `ShiftReduceParser.model` and `BaseModel.knownStates` to extract the tag set.
"
29,https://github.com/stanfordnlp/CoreNLP/issues/42,42,"[{'id': 45387507, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNw==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/cantreproduce', 'name': 'cantreproduce', 'color': 'dddddd', 'default': False, 'description': None}]",closed,2014-12-07 16:40:36+00:00,,ShiftReduceParserQuery Throwing NPE in Pipeline,"I'm very excited about the new SR parser, and I'm trying to drop it into a StanfordCoreNLP pipeline, but it's throwing an NPE when it gets to ShiftReduceParserQuery.  The same code works with the PCFG parser, and I'm using version 3.5.0.  The only properties I'm applying are:

```
props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
props.put(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
pipeline = new StanfordCoreNLP(props);
```

The stack trace leads to line 72 in the ShiftReduceParserQuery class:

```
Collection<ScoredObject<Integer>> predictedTransitions = parser.model.findHighestScoringTransitions(state, true, maxBeamSize, constraints);
```

I confirmed that parser.model is null here, even though the output says the model loads successfully.  The relevant part of the output is below.

```
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/srparser/englishSR.ser.gz ...done [11.8 sec].
Adding annotator dcoref
java.lang.NullPointerException
    at edu.stanford.nlp.parser.shiftreduce.ShiftReduceParserQuery.parseInternal(ShiftReduceParserQuery.java:72)
    at edu.stanford.nlp.parser.shiftreduce.ShiftReduceParserQuery.parse(ShiftReduceParserQuery.java:47)
    at edu.stanford.nlp.pipeline.ParserAnnotator.doOneSentence(ParserAnnotator.java:263)
    at edu.stanford.nlp.pipeline.ParserAnnotator.doOneSentence(ParserAnnotator.java:215)
    at edu.stanford.nlp.pipeline.SentenceAnnotator.annotate(SentenceAnnotator.java:95)
    at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:68)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:408)
```
"
30,https://github.com/stanfordnlp/CoreNLP/issues/43,43,[],closed,2014-12-24 07:03:18+00:00,,Stanford CoreNLP v.s. Stanford Parser,"In my project, I use Stanford CoreNLP to perform some basic operations. Meanwhile I need use a caseless model for parsing, so I chose ""englichPCFG.caseless.ser.gz"" in the model of Stanford Parser. However, CoreNLP cannot read this model, so I added Stanford Parser into my project, along with CoreNLP.

But here comes the question: there are java files with the same path (same package and same name) in both Stanford CoreNLP and Stanford Parser, once there are slight differences between these two java files, things got complicated, because I don't know which function am I going to call in my project. Actually after I added Stanford Parser in my project, the original lemmatize module couldn't work, error was occurred when loading the model.

Is there anyone who tried to add both Stanford Parser and Stanford CoreNLP in one project, and could you give me some advice to avoid conflicts? Thanks. :-)
"
31,https://github.com/stanfordnlp/CoreNLP/issues/44,44,[],closed,2015-01-06 16:46:53+00:00,,Can we construct Trees from input String?,"I know StanfordNLP produces parentheses based output through print() method, but does it provide any function to read back outputted string and construct a tree?
"
32,https://github.com/stanfordnlp/CoreNLP/issues/45,45,[],closed,2015-01-12 01:35:02+00:00,,Non-string property values don't get passed to annotators,"Ref: https://github.com/stanfordnlp/CoreNLP/commit/c01f31e190c322bffe508f2776c6d8af3c052392#diff-817eb462723073c02c8fc4fd34993d18R22

Edit: This doesn't seem to come up right - the file and line in question are AnnotatorFactory.java, line 22:

``` diff
-    for(Object key: properties.keySet()) {
-      this.properties.setProperty((String) key, properties.getProperty((String) key));
+    for (String key : properties.stringPropertyNames()) {
+      this.properties.setProperty(key, properties.getProperty(key));
     }
   }
```

Using `stringPropertyNames()` causes properties with non-string values to be excluded from the copied properties set, since they are excluded from the iterator. For example:

``` java
Properties props = new Properties();
props.put(""ner.useSUTime"", false);
props.put(""customAnnotatorClass.stopword"", ""intoxicant.analytics.coreNlp.StopwordAnnotator"");
```

from this props object, only `customAnnotatorClass.stopword` will be copied into the annotator, since the value of the ""ner.useSUTime"" prop is a non-string. The documentation for stringPropertyNames says:

> [This] method returns a set of keys in this property list where the key and its corresponding value are strings, including distinct keys in the default property list if a key of the same name has not already been found from the main properties list. _Properties whose key or value is not of type String are omitted._

This results in CoreNLP silently creating annotators without the properties that have been set on the Properties object.
"
33,https://github.com/stanfordnlp/CoreNLP/issues/46,46,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2015-01-19 03:42:14+00:00,,in in /StanfordCoreNLP/src/edu/stanford/nlp/sentiment filelist option is not processed correctly ,"the fout and pout are created and closed per sentence instead of per file.
Resolution:
Change it to the following:
      for (Annotation annotation : annotations) {
          pipeline.annotate(annotation);

```
      //AR: bug move it to before the second for
      //FileOutputStream fout = new FileOutputStream(file + "".out"");
      //PrintStream pout = new PrintStream(fout);
```
"
34,https://github.com/stanfordnlp/CoreNLP/issues/47,47,[],closed,2015-01-19 06:07:38+00:00,,how to build the CoreNLP Project?,"I want to change the SUTime to use heidelTime,  but I don't how to build this project ÔºÅ
"
35,https://github.com/stanfordnlp/CoreNLP/issues/48,48,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2015-01-22 14:34:42+00:00,,"TokenMgrError: Lexical error at line 1, column 104.  Encountered: ""E"" (69), after : ""\\""","I have tried several input files, but I keep on getting the following error. Any help will be appreciated.

Exception in thread ""main"" edu.stanford.nlp.ling.tokensregex.parser.TokenMgrError: Lexical error at line 1, column 104.  Encountered: ""E"" (69), after : ""\""
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParserTokenManager.getNextToken(TokenSequenceParserTokenManager.java:1029)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.jj_ntk(TokenSequenceParser.java:3353)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.CoreMapNode(TokenSequenceParser.java:1386)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.NodeBasic(TokenSequenceParser.java:1360)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.NodeGroup(TokenSequenceParser.java:1327)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.NodeDisjConj(TokenSequenceParser.java:1266)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.BracketedNode(TokenSequenceParser.java:1127)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegexBasic(TokenSequenceParser.java:833)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegexDisjConj(TokenSequenceParser.java:1020)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegex(TokenSequenceParser.java:790)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.SeqRegexWithAction(TokenSequenceParser.java:1643)
        at edu.stanford.nlp.ling.tokensregex.parser.TokenSequenceParser.parseSequenceWithAction(TokenSequenceParser.java:37)
        at edu.stanford.nlp.ling.tokensregex.TokenSequencePattern.compile(TokenSequencePattern.java:186)
        at edu.stanford.nlp.patterns.surface.ScorePhrases.runParallelApplyPats(ScorePhrases.java:215)
        at edu.stanford.nlp.patterns.surface.ScorePhrases.applyPats(ScorePhrases.java:326)
        at edu.stanford.nlp.patterns.surface.ScorePhrases.learnNewPhrasesPrivate(ScorePhrases.java:397)
        at edu.stanford.nlp.patterns.surface.ScorePhrases.learnNewPhrases(ScorePhrases.java:177)
        at edu.stanford.nlp.patterns.surface.GetPatternsFromDataMultiClass.iterateExtractApply4Label(GetPatternsFromDataMultiClass.java:1716)
        at edu.stanford.nlp.patterns.surface.GetPatternsFromDataMultiClass.iterateExtractApply(GetPatternsFromDataMultiClass.java:1591)
        at edu.stanford.nlp.patterns.surface.GetPatternsFromDataMultiClass.main(GetPatternsFromDataMultiClass.java:2485)
"
36,https://github.com/stanfordnlp/CoreNLP/issues/49,49,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",open,2015-01-22 15:52:05+00:00,,Problems with IndexedWord word() and value(),"Hi, in the context of dkpro's StanfordCoreferenceResolver, I found the following problem:
Stanford CoreNLP (v3.4.1) seems to plan to make changes at IndexedWord: word() and value() both exist but according to a comment, should be unified at some time.

Details:

StanfordCoreferenceResolver creates the collapsed dependencies this way:
ParserAnnotatorUtils.fillInParseAnnotations(false, true, gsf, sentence, treeCopy);

Dcoref's Document.java makes use of the function getNodeByWordPattern of SemanticGraph, which in turn uses w.word(). This does not seem to be set by fillInParseAnnotations.

value() is set, however, so I preliminarily fixed the problem by adding the following right after fillInParseAnnotations in StanfordCoreferenceResolver.

SemanticGraph deps = sentence.get(SemanticGraphCoreAnnotations.CollapsedDependenciesAnnotation.class);
for (IndexedWord vertex : deps.vertexSet()) {
     vertex.setWord(vertex.value());
}

The problem should be fixed in StanfordCoreNLP, however.
"
37,https://github.com/stanfordnlp/CoreNLP/issues/50,50,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2015-01-26 23:14:40+00:00,,RuleBasedCorefMentionFinder NullPointerException with SR parser only,"same crash in 3.5 release and current build

```
$ echo 'This waste, when mixed into the soil, can be very helpful to growing plants' > tmp.txt
$ java -mx3g -cp ""./*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -parse.model edu/stanford/nlp/models/srparser/englishSR.ser.gz -file tmp.txt

Ready to process: 1 files, skipped 0, total 1
Processing file /Users/kevinh/Stanford/stanford-corenlp-full-2014-10-31/tmp.txt ... writing to /Users/kevinh/Stanford/stanford-corenlp-full-2014-10-31/tmp.txt.xml {
  Annotating file /Users/kevinh/Stanford/stanford-corenlp-full-2014-10-31/tmp.txt {
    RuleBasedCorefMentionFinder: Failed to find head token:
    Tree is: (ROOT (S (NP (NP (DT This) (NN waste)) (, ,) (SBAR (WHADVP (WRB when)) (S (VP (VBN mixed) (PP (IN into) (NP (DT the) (NN soil)))))) (, ,)) (VP (MD can) (VP (VB be) (ADJP (RB very) (JJ helpful) (PP (TO to) (NP (VBG growing) (NNS plants))))))))
    token = |waste|1|, approx=0
  } [0.476 seconds]
Exception in thread ""main"" java.lang.RuntimeException: Error annotating /Users/kevinh/Stanford/stanford-corenlp-full-2014-10-31/tmp.txt
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$processFiles$15(StanfordCoreNLP.java:877)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$17/1526062841.run(Unknown Source)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:948)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:990)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1060)
Caused by: java.lang.NullPointerException
    at edu.stanford.nlp.dcoref.RuleBasedCorefMentionFinder.findHead(RuleBasedCorefMentionFinder.java:276)
    at edu.stanford.nlp.dcoref.RuleBasedCorefMentionFinder.extractPredictedMentions(RuleBasedCorefMentionFinder.java:101)
    at edu.stanford.nlp.pipeline.DeterministicCorefAnnotator.annotate(DeterministicCorefAnnotator.java:107)
    at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:68)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:410)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$processFiles$15(StanfordCoreNLP.java:865)
    ... 4 more
```
"
38,https://github.com/stanfordnlp/CoreNLP/issues/53,53,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",closed,2015-01-28 19:21:12+00:00,,Enable Sourcegraph,"I want to use [Sourcegraph code search and code review](https://sourcegraph.com) with CoreNLP. A project maintainer needs to enable it to set up a webhook so the code is up-to-date there.

Could you please enable CoreNLP on @Sourcegraph by going to https://sourcegraph.com/github.com/stanfordnlp/CoreNLP and clicking on Settings? (It should only take 15 seconds.)

Thank you!
"
39,https://github.com/stanfordnlp/CoreNLP/issues/54,54,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",closed,2015-01-30 21:14:52+00:00,,Enable Sourcegraph,"I want to use [Sourcegraph code search and code review](https://sourcegraph.com) with CoreNLP. A project maintainer needs to enable it to set up a webhook so the code is up-to-date there.

Could you please enable CoreNLP on @Sourcegraph by going to https://sourcegraph.com/github.com/stanfordnlp/CoreNLP and clicking on Settings? (It should only take 15 seconds.)

Thank you!
"
40,https://github.com/stanfordnlp/CoreNLP/issues/55,55,[],closed,2015-01-30 22:49:58+00:00,,"CoreNLP crashes with a ""No roots in graph"" RuntimeException","I have a sentence that gives a RuntimeException based on what I'm presuming is a bad or unexpected parse when trying to do dependency conversion with the `getFirstRoot()` call.  This sounds like an NLP problem and not a system engineering problem, so ideally, it would return null, or maybe a checked exception?  Using the shift-reduce parser and version 3.5.1 I get the message

```
java.lang.RuntimeException: No roots in graph:
dep                 reln                gov                 
---                 ----                ---                 

    at edu.stanford.nlp.semgraph.SemanticGraph.getFirstRoot(SemanticGraph.java:773)
```

The text I'm parsing is the following.  This is in JSON encoding.  Sorry I don't know which sentence is causing it.

```
""days has elapsed after the \nreport is received. As used in this subsection--\n            ``(1) the term `legislative day means any calendar day on         which the House of Representatives is in session; and            ``(2) the terms `rule and `regulation mean a provision or         series of interrelated provisions stating a single, separable         rule of law..    (b) Report on Using Voter Communication Vouchers for Primary Elections.--The Commission shall submit to the House of Representa""
```

in plaintext,

```
days has elapsed after the 
report is received. As used in this subsection--
            ``(1) the term `legislative day means any calendar day on         which the House of Representatives is in session; and            ``(2) the terms `rule and `regulation mean a provision or         series of interrelated provisions stating a single, separable         rule of law..    (b) Report on Using Voter Communication Vouchers for Primary Elections.--The Commission shall submit to the House of Representa
```
"
41,https://github.com/stanfordnlp/CoreNLP/issues/56,56,[],closed,2015-01-31 06:40:48+00:00,,chinese_map_utils.jar ,"to execute the functionality described [here](https://github.com/stanfordnlp/CoreNLP/tree/master/src/edu/stanford/nlp/trees/international/pennchinese) do I have to make that chinese_map_utils.jar myself?
"
42,https://github.com/stanfordnlp/CoreNLP/issues/57,57,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2015-02-03 03:01:57+00:00,,error message when trying to parse nonsensical datetime,"I see this a lot when parsing gigaword.

```
java.lang.NumberFormatException: For input string: ""1438143814381434""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Integer.parseInt(Integer.java:583)
        at java.lang.Integer.valueOf(Integer.java:766)
        at edu.stanford.nlp.ie.pascal.ISODateInstance.extractDay(ISODateInstance.java:1107)
        at edu.stanford.nlp.ie.pascal.ISODateInstance.extractFields(ISODateInstance.java:398)
        at edu.stanford.nlp.ie.pascal.ISODateInstance.<init>(ISODateInstance.java:82)
        at edu.stanford.nlp.ie.QuantifiableEntityNormalizer.normalizedDateString(QuantifiableEntityNormalizer.java:363)
        at edu.stanford.nlp.ie.QuantifiableEntityNormalizer.normalizedDateString(QuantifiableEntityNormalizer.java:338)
        at edu.stanford.nlp.ie.QuantifiableEntityNormalizer.processEntity(QuantifiableEntityNormalizer.java:1025)
        at edu.stanford.nlp.ie.QuantifiableEntityNormalizer.addNormalizedQuantitiesToEntities(QuantifiableEntityNormalizer.java:1374)
        at edu.stanford.nlp.ie.NERClassifierCombiner.classifyWithGlobalInformation(NERClassifierCombiner.java:133)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifySentenceWithGlobalInformation(AbstractSequenceClassifier.java:327)
        at edu.stanford.nlp.pipeline.NERCombinerAnnotator.doOneSentence(NERCombinerAnnotator.java:148)
        at edu.stanford.nlp.pipeline.SentenceAnnotator.annotate(SentenceAnnotator.java:95)
        at edu.stanford.nlp.pipeline.NERCombinerAnnotator.annotate(NERCombinerAnnotator.java:137)
        at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:68)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:411)
```
"
43,https://github.com/stanfordnlp/CoreNLP/issues/58,58,[],closed,2015-02-03 19:05:16+00:00,,"In  /StanfordCoreNLP/src/edu/stanford/nlp/sentiment/SentimentPipeline.java Remove ""-file"" option the ""-fileList"" option can handle a list of 1.","In  /StanfordCoreNLP/src/edu/stanford/nlp/sentiment/SentimentPipeline.java
Remove ""-file"" option the ""-fileList"" option can handle a list of 1.

both -file and fileList options are provided which is redundant and error prone. 
Right now file handling in the code for file and filelist are not in sync. 

  } else if (args[argIndex].equalsIgnoreCase(""-file"")) {
        filename = args[argIndex + 1];
        argIndex += 2;
      } else if (args[argIndex].equalsIgnoreCase(""-fileList"")) {
"
44,https://github.com/stanfordnlp/CoreNLP/issues/59,59,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2015-02-20 11:24:04+00:00,,Truecaser not running because of missing Java class,"Hello,

I am trying to use Stanford Core NLP for an EAMT funded project which is being implemented in Java, and we would love to use the tool's truecasing functionality. However, It seems that the whenever we try to run the truecasing annotator, we get the following error:

Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.sequences.TrueCasingForNISTDocumentReaderAndWriter

It seems like this is a very old problem which had been solved in the past but is now back in the newest versions. Is there any way it can be fixed?

Thanks in advance.
"
45,https://github.com/stanfordnlp/CoreNLP/issues/60,60,[],open,2015-02-25 00:48:13+00:00,,corenlp.war: XOMReader warnings break visualize output,"java -version
java version ""1.8.0_31""
Java(TM) SE Runtime Environment (build 1.8.0_31-b13)
## Java HotSpot(TM) 64-Bit Server VM (build 25.31-b07, mixed mode)

jetty-runner corenlp.war
2015-02-24 16:37:55.366:INFO::main: Logging initialized @108ms
2015-02-24 16:37:55.372:INFO:oejr.Runner:main: Runner
2015-02-24 16:37:55.457:INFO:oejs.Server:main: jetty-9.2.2.v20140723
2015-02-24 16:38:01.093:WARN:oeja.AnnotationConfiguration:main: ServletContainerInitializers: detected. Class hierarchy: empty
2015-02-24 16:38:01.331:INFO:oejsh.ContextHandler:main: Started o.e.j.w.WebAppContext@606d8acf{/,file:/private/var/folders/qt/7v9m4kd572b0zw56pc3hxy5r0000gn/T/jetty-0.0.0.0-8080-corenlp.war-_-any-6993542127094007379.dir/webapp/,AVAILABLE}{file:/Users/spiliero/CoreNLP/corenlp.war}
2015-02-24 16:38:01.332:WARN:oejsh.RequestLogHandler:main: !RequestLog
2015-02-24 16:38:01.360:INFO:oejs.ServerConnector:main: Started ServerConnector@1d057a39{HTTP/1.1}{0.0.0.0:8080}
2015-02-24 16:38:01.361:INFO:oejs.Server:main: Started @6125ms
Searching for resource: StanfordCoreNLP.properties
Adding annotator tokenize
TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
Adding annotator ssplit
Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.0 sec].
Adding annotator lemma
Adding annotator ner
annotators=tokenize, ssplit, pos, lemma, ner, parse, dcoref
Unknown property: |annotators|
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [5.1 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [2.2 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [3.6 sec].
sutime.binder.1.
Initializing JollyDayHoliday for sutime with classpath:edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/defs.sutime.txt
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.sutime.txt
Feb 24, 2015 4:38:16 PM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules
INFO: Ignoring inactive rule: null
Feb 24, 2015 4:38:16 PM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules
INFO: Ignoring inactive rule: temporal-composite-8:ranges
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ...done [0.5 sec].
Adding annotator dcoref
**Warning:  nu.xom.xslt.XOMReader: XOMReader doesn't support http://javax.xml.XMLConstants/property/accessExternalDTD
Warning:  nu.xom.xslt.XOMReader: XOMReader doesn't support http://www.oracle.com/xml/jaxp/properties/entityExpansionLimit**
"
46,https://github.com/stanfordnlp/CoreNLP/issues/61,61,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2015-02-25 10:59:41+00:00,,Invalid JSON output format,"Hello,

When I try POS tagging with stanford-corenlp-3.5.1, I got following part of the output by StanfordCoreNLP's jsonPrint method.

{
    ""index"": ""5"",
    ""word"": ""\'s"",
    ""lemma"": ""\'s"",
    ""characterOffsetBegin"": ""17"",
    ""characterOffsetEnd"": ""19"",
    ""pos"": ""POS""
}

Sample sentence: ""I was the teacher's student.""

It looks like the ""word"" and ""lemma"" contain invalid JSON format and json validations fail. Single quote characters do not need to be escaped according to http://json.org/.
You can check it in http://jsonlint.com/

I glanced at the code, and maybe this part is relevant to it. https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/JSONOutputter.java#L178

I hope it is not my misunderstanding and I can commit for it.
Regards,
"
47,https://github.com/stanfordnlp/CoreNLP/issues/62,62,[],closed,2015-03-05 13:30:53+00:00,,CoNLLMentionExtractor always uses ``auto_conll`` files.,"Hello,

it seems that `CoNLLMentionExtractor` always uses `auto_conll` to extract entity mentions, even for gold mentions. However, gold mentions are contained in `gold_conll` files. This results in the metric scores always being equal to zero.
In particular, on line 75:

```
if (Constants.USE_CONLL_AUTO) options.setFilter("".*_auto_conll$"");
```

Is this the correct behavior or am I missing something?
"
48,https://github.com/stanfordnlp/CoreNLP/issues/63,63,"[{'id': 45387508, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}]",closed,2015-03-30 16:47:58+00:00,,Pure c# port,"I recently had finished a port/reimplementation of OpenNLP library in C#, and would like to do the same with StanfordNLP! :yum: 

There is any impediment (regarding the dual license) to make this port?
"
49,https://github.com/stanfordnlp/CoreNLP/issues/64,64,[],closed,2015-04-11 22:12:08+00:00,,EnglishFactored model is gone,"Hi! I have been using English Factored model for a long time (it's slower but seems to be more accurate than PCFG one). I have upgrade my StanfordNLP library to 3.5.1, and it seems like `edu/stanford/nlp/models/lexparser/englishFactored.ser.gz` has gone missing and the only two models there are `RNN` and `PCFG`. Can I still find it somewhere? 
"
50,https://github.com/stanfordnlp/CoreNLP/issues/66,66,[],closed,2015-04-14 13:21:12+00:00,,How to know Tree library's end of sentence,"I'm using the Tree class, and doing DFS traversal to collect different parts of the sentence. Since it is a collection process, I need to be able to add the last part and one of the common condition would be ""If you are at the end of a sentence, add the collected parts to the collection"". And normally, `treeNode == null` would suffice, but Stanford tree does not return null value. So how do I know if I have already reached the end of sentence, in a DFS??
"
51,https://github.com/stanfordnlp/CoreNLP/issues/67,67,[],closed,2015-04-14 14:05:08+00:00,,Add prominent link to mailing list in README.md,"It might be good to add a prominent link to the users mailing list to the README.md file to avoid questions being posted to the issue tracker.
"
52,https://github.com/stanfordnlp/CoreNLP/issues/68,68,[],closed,2015-04-26 13:13:44+00:00,,NullPointerException on sentence.,"I'm getting a NullPointerException when trying to parse ""People are known to be at or near places"" through the shell.

``` bash
stanford-corenlp-full-2015-04-20>  ./corenlp.sh -threads 4 -annotators tokenize,ssplit,pos,lemma,ner,parse
java -mx3g -cp ""./*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -threads 4 -annotators tokenize,ssplit,pos,lemma,ner,parse
Adding annotator tokenize
TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
Adding annotator ssplit
Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.6 sec].
Adding annotator lemma
Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [4.7 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [2.9 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [2.3 sec].
Initializing JollyDayHoliday for SUTime from classpath: edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/defs.sutime.txt
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.sutime.txt
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.7 sec].

Entering interactive shell. Type q RETURN or EOF to quit.
NLP> People are known to be at or near places
Exception in thread ""main"" java.lang.NullPointerException
    at edu.stanford.nlp.semgraph.semgrex.GraphRelation$GOVERNER$1.advance(GraphRelation.java:271)
    at edu.stanford.nlp.semgraph.semgrex.GraphRelation$SearchNodeIterator.initialize(GraphRelation.java:929)
    at edu.stanford.nlp.semgraph.semgrex.GraphRelation$SearchNodeIterator.<init>(GraphRelation.java:910)
    at edu.stanford.nlp.semgraph.semgrex.GraphRelation$GOVERNER$1.<init>(GraphRelation.java:257)
    at edu.stanford.nlp.semgraph.semgrex.GraphRelation$GOVERNER.searchNodeIterator(GraphRelation.java:257)
    at edu.stanford.nlp.semgraph.semgrex.NodePattern$NodeMatcher.resetChildIter(NodePattern.java:273)
    at edu.stanford.nlp.semgraph.semgrex.SemgrexMatcher.resetChildIter(SemgrexMatcher.java:76)
    at edu.stanford.nlp.semgraph.semgrex.CoordinationPattern$CoordinationMatcher.resetChildIter(CoordinationPattern.java:170)
    at edu.stanford.nlp.semgraph.semgrex.NodePattern$NodeMatcher.resetChild(NodePattern.java:297)
    at edu.stanford.nlp.semgraph.semgrex.NodePattern$NodeMatcher.goToNextNodeMatch(NodePattern.java:394)
    at edu.stanford.nlp.semgraph.semgrex.NodePattern$NodeMatcher.matches(NodePattern.java:511)
    at edu.stanford.nlp.semgraph.semgrex.SemgrexMatcher.find(SemgrexMatcher.java:155)
    at edu.stanford.nlp.trees.UniversalEnglishGrammaticalStructure.addCaseMarkerInformation(UniversalEnglishGrammaticalStructure.java:225)
    at edu.stanford.nlp.trees.UniversalEnglishGrammaticalStructure.collapseDependencies(UniversalEnglishGrammaticalStructure.java:817)
    at edu.stanford.nlp.trees.GrammaticalStructure.typedDependenciesCollapsed(GrammaticalStructure.java:877)
    at edu.stanford.nlp.semgraph.SemanticGraphFactory.makeFromTree(SemanticGraphFactory.java:188)
    at edu.stanford.nlp.semgraph.SemanticGraphFactory.generateCollapsedDependencies(SemanticGraphFactory.java:90)
    at edu.stanford.nlp.pipeline.ParserAnnotatorUtils.fillInParseAnnotations(ParserAnnotatorUtils.java:51)
    at edu.stanford.nlp.pipeline.ParserAnnotator.finishSentence(ParserAnnotator.java:266)
    at edu.stanford.nlp.pipeline.ParserAnnotator.doOneSentence(ParserAnnotator.java:245)
    at edu.stanford.nlp.pipeline.SentenceAnnotator.annotate(SentenceAnnotator.java:96)
    at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:68)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:412)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.process(StanfordCoreNLP.java:441)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.shell(StanfordCoreNLP.java:678)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1016)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1062)
```
"
53,https://github.com/stanfordnlp/CoreNLP/issues/69,69,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2015-04-27 17:35:42+00:00,,ArrayIndexOutOfBoundsException in SpanishVerbStripper,"This line is still reached if words.length < 3
https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/international/spanish/SpanishVerbStripper.java#L86
"
54,https://github.com/stanfordnlp/CoreNLP/issues/70,70,"[{'id': 45387508, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}]",closed,2015-04-30 00:28:50+00:00,,"3.5.2 released, but not on Maven?","Is there an ETA for when I can get 3.5.2 off of Maven?
"
55,https://github.com/stanfordnlp/CoreNLP/issues/71,71,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2015-05-02 08:30:18+00:00,,CoreNLP potential bug,"Hello,

I am running the following pipeline with Stanford CoreNLP 3.5.2
annotators = tokenize,ssplit,pos,lemma,ner,depparse

with output to json
-outputFormat json

The problem is CoreNLP doesn't split tokens to sentences, every sentence contains all tokens in the input text.

If text contains 100 tokens, then
length(sentence_1) = 100 tokens
....
length(sentence_n) = 100 tokens

Bug appears only in json format.
"
56,https://github.com/stanfordnlp/CoreNLP/issues/72,72,[],closed,2015-05-11 13:18:11+00:00,,TreeGraph missing in version 3.5 ?,"Hi , I have used TreeGraph in version 3.4 
But it seems it disappear in the new version . 
so how can I implement below code ? 

TreeGraph treegraph =  new TreeGraph(tree) ;
TreeGraphNode  root = treegraph.root();
"
57,https://github.com/stanfordnlp/CoreNLP/issues/73,73,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 706055902, 'node_id': 'MDU6TGFiZWw3MDYwNTU5MDI=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/tokenize', 'name': 'tokenize', 'color': 'c5def5', 'default': False, 'description': None}, {'id': 706056248, 'node_id': 'MDU6TGFiZWw3MDYwNTYyNDg=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/ssplit', 'name': 'ssplit', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2015-05-12 02:25:08+00:00,,chinese segmenter doesn't tokenize newline,"I read that ""A side-effect of setting ssplit.newlineIsSentenceBreak to ""two"" or ""always"" is that tokenizer will tokenize newlines."" but when I set ssplit.newlineIsSentenceBreak to always,  chinese segmenter does not keep newline tokens, which lead to ssplit's failure to split sentences by newline.
"
58,https://github.com/stanfordnlp/CoreNLP/issues/74,74,[],closed,2015-05-25 21:21:09+00:00,,Tagger loading issue.,"Hi, I use the .NET version of CoreNLP and while trying to instantiate StanfordCoreNLP, I ran into this exception: Unrecoverable error while loading a tagger model.
I heard that it is specific to the version 3.5.2. Did anyone face the same problem and did you have to downgrade the version you used to make it work?

The code I tried is taken straight from the getting started example:

```
 // Path to the folder with models extracted from ""stanford-parser-3.5.2-models""
            var jarRoot = @""C:\Users\Masuzu\Downloads\Compressed\stanford-parser-full-2015-04-20\stanford-parser-3.5.2-models"";

            // Text for processing
            var text = ""Kosgi Santosh sent an email to Stanford University. He didn't get a reply."";

            // Annotation pipeline configuration
            var props = new Properties();
            props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
            props.setProperty(""sutime.binders"", ""0"");

            // We should change current directory, so StanfordCoreNLP could find all the model files automatically
            var curDir = Environment.CurrentDirectory;
            Directory.SetCurrentDirectory(jarRoot);
            var pipeline = new StanfordCoreNLP(props);
            Directory.SetCurrentDirectory(curDir);

            // Annotation
            var annotation = new Annotation(text);
            pipeline.annotate(annotation);

            // Result - Pretty Print
            using (var stream = new ByteArrayOutputStream())
            {
                pipeline.prettyPrint(annotation, new PrintWriter(stream));
                System.Console.WriteLine(stream.toString());
                stream.close();
            }
```
"
59,https://github.com/stanfordnlp/CoreNLP/issues/75,75,"[{'id': 45387508, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}]",closed,2015-05-25 23:13:03+00:00,,regexner.mapping locations.txt missing?,"I'm looking for locations.txt, I found your sample, but what about the sample used in the visualizer. 
"
60,https://github.com/stanfordnlp/CoreNLP/issues/76,76,[],closed,2015-05-27 08:57:12+00:00,,potential bug with collapsed dependencies,"Hello,

I am using the recent version of CoreNLP for collapsed dependency parsing with output to json.

with configuration

annotators = tokenize,ssplit,pos,lemma,depparse

It looks like preposition dependencies are not got collapsed in the json output format

for example,  the sentence

""Conduct your  business in the most professional and efficient way.""

Doesn't have collapsed dependency.
"
61,https://github.com/stanfordnlp/CoreNLP/issues/77,77,[],closed,2015-05-27 20:32:18+00:00,,Spark partition issue with Stanford NLP,"I am trying to process millions of data with spark/scala integrated with stanford NLP (3.4.1). Since I am using social media data I have to use NLP for the themes generation (pos tagging) and Sentiment calulation.

I have to deal with Twitter data and NON Twitter data separately.So I have two classes that deal with Twitter/Non Twitter

I am using lasy val initialization from each class for loading the stanfordNLP

features: Seq[String] = Seq(""tokenize"",""ssplit"",""pos"",""parse"",""sentiment"")
  val props = new Properties()
  props.put(""annotators"", features.mkString("", ""))
  props.put(""pos.model"", ""tagger/gate-EN-twitter.model"")
  props.put(""parse.model"", ""tagger/englishSR.ser.gz"");
  val pipeline = new StanfordCoreNLP(props)
Note: For above Twitter I am using different pos model and shift reduce parse model for parsing. The reason I use shift reduce parser is for some of the junk data at rum time the default PCFG model takes lot of time for processing and getting some Exception.Shift reduce parser will take around 15 seconds at load time and its faster at run time while processing the data.

NonTwitter class

 features: Seq[String] = Seq(""tokenize"",""ssplit"",""pos"",""parse"",""sentiment"")
    val props = new Properties()
    props.put(""annotators"", features.mkString("", ""))
    props.put(""parse.model"", ""tagger/englishSR.ser.gz"");
Here I am using the default pos model and shift reduce parser

Problem:

Currently we am running with 8 Nodes with 6 cores and I can run with 48 partition. For to process millions of data with the above configuratin with lesser partition it works fine for me.

8 Nodes and 6 cores we have almost 48 partition and if I ran with 42 Number of partition it takes around 1 hr to finish the processing.

with the current configuration I need to scale it to 200 partition

8 Nodes and 6 cores we have almost 48 partition and if we ran with 200 Number of partition it takes around 2 hr and finally throwing some exception saying the one node is lost or java.lang.IllegalArgumentException: annotator ""sentiment"" requires annotator ""binarized_trees"" etc etc.

The problem is only if we scale up the number of partition to 200 with 8 Nodes and 6 cores which we have only 48 cores.

I have the suspect that its cuz of loading the shift reduce parser loading at each partition.i thought of loading this class at one time and then do the Broadcast but standforndNLP class is not searializable so i cannot broadcast.any thought suggestion

The reason we need to scale to 200 partition is it will run quickly with lesser time to process this data. Any thoughts suggestion is relly helpful
"
62,https://github.com/stanfordnlp/CoreNLP/issues/78,78,[],closed,2015-06-08 18:33:41+00:00,,Annotator ‚Äúsentiment‚Äù requires annotator ‚Äúbinarized_trees‚Äù‚Äè,"could you please help me with issue. I don't have any clue when this error happen

Any explanation or possible scenarios when this error can happen.

Any help is really appreciated.

i am getting this error.Any idea when this error can happen
props.setProperty(""annotators"",
                            ""tokenize, ssplit, pos, parse,sentiment"");

5/06/07 08:18:06 WARN TaskSetManager: Lost task 95.0 in stage 18.0 (TID 137, tlbd1d03.tms.xxxx.com): java.lang.IllegalArgumentException: annotator ""sentiment"" requires annotator ""binarized_trees""
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:300)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:129)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:125)

http://stackoverflow.com/questions/30468010/error-annotator-sentiment-requires-annotator-binarized-trees

thanks
vinoj
"
63,https://github.com/stanfordnlp/CoreNLP/issues/80,80,[],closed,2015-07-04 04:38:29+00:00,,uas in nndep,"I train nndep with train and dev files.
nndep represents 85% for UAS evaluation In training time.
But in testing time with dev file, the evaluation for dev file was 18%.
Why?
"
64,https://github.com/stanfordnlp/CoreNLP/issues/81,81,[],closed,2015-07-16 14:07:14+00:00,,Self-loops appearing in collapsed-ccprocessed-dependencies,"Some (many) sentences seem to produce dependency graphs with self-loops. From my understanding of the documentation, this should never occur. I found this issue in the 3.5.2 release, and the web demo seems to demonstrate it as well. I have NOT tried compiling and testing on HEAD.

This example sentence from UkWaC reproduces the error. I found countless other examples, but this was one of the simplest. Most of the faulty sentences seemed to have the issue with `conj:and` and `conj:or` relations, but I also found it with `case`. Later in my pipeline, it seems to appear with `nsubj` and other relations, but I cannot find a sentence that produces this at the moment.

Example Input:
`We have course participants from Britain , Europe and from the rest of the world - usually in roughly equal proportions .`

Relevant example output:

```
    <dep type=""conj:and"">
    <governor idx=""4"">participants</governor>
    <dependent idx=""4"" copy=""1"">participants</dependent>
    </dep>
```

Full example output:

```
<sentence id=""327"" line=""327"">
<tokens>
    <token id=""1"">
    <word>We</word>
    <lemma>we</lemma>
    <CharacterOffsetBegin>48047</CharacterOffsetBegin>
    <CharacterOffsetEnd>48049</CharacterOffsetEnd>
    <POS>PRP</POS>
    <NER>O</NER>
    </token>
    <token id=""2"">
    <word>have</word>
    <lemma>have</lemma>
    <CharacterOffsetBegin>48050</CharacterOffsetBegin>
    <CharacterOffsetEnd>48054</CharacterOffsetEnd>
    <POS>VBP</POS>
    <NER>O</NER>
    </token>
    <token id=""3"">
    <word>course</word>
    <lemma>course</lemma>
    <CharacterOffsetBegin>48055</CharacterOffsetBegin>
    <CharacterOffsetEnd>48061</CharacterOffsetEnd>
    <POS>NN</POS>
    <NER>O</NER>
    </token>
    <token id=""4"">
    <word>participants</word>
    <lemma>participant</lemma>
    <CharacterOffsetBegin>48062</CharacterOffsetBegin>
    <CharacterOffsetEnd>48074</CharacterOffsetEnd>
    <POS>NNS</POS>
    <NER>O</NER>
    </token>
    <token id=""5"">
    <word>from</word>
    <lemma>from</lemma>
    <CharacterOffsetBegin>48075</CharacterOffsetBegin>
    <CharacterOffsetEnd>48079</CharacterOffsetEnd>
    <POS>IN</POS>
    <NER>O</NER>
    </token>
    <token id=""6"">
    <word>Britain</word>
    <lemma>Britain</lemma>
    <CharacterOffsetBegin>48080</CharacterOffsetBegin>
    <CharacterOffsetEnd>48087</CharacterOffsetEnd>
    <POS>NNP</POS>
    <NER>LOCATION</NER>
    </token>
    <token id=""7"">
    <word>,</word>
    <lemma>,</lemma>
    <CharacterOffsetBegin>48088</CharacterOffsetBegin>
    <CharacterOffsetEnd>48089</CharacterOffsetEnd>
    <POS>,</POS>
    <NER>O</NER>
    </token>
    <token id=""8"">
    <word>Europe</word>
    <lemma>Europe</lemma>
    <CharacterOffsetBegin>48090</CharacterOffsetBegin>
    <CharacterOffsetEnd>48096</CharacterOffsetEnd>
    <POS>NNP</POS>
    <NER>LOCATION</NER>
    </token>
    <token id=""9"">
    <word>and</word>
    <lemma>and</lemma>
    <CharacterOffsetBegin>48097</CharacterOffsetBegin>
    <CharacterOffsetEnd>48100</CharacterOffsetEnd>
    <POS>CC</POS>
    <NER>O</NER>
    </token>
    <token id=""10"">
    <word>from</word>
    <lemma>from</lemma>
    <CharacterOffsetBegin>48101</CharacterOffsetBegin>
    <CharacterOffsetEnd>48105</CharacterOffsetEnd>
    <POS>IN</POS>
    <NER>O</NER>
    </token>
    <token id=""11"">
    <word>the</word>
    <lemma>the</lemma>
    <CharacterOffsetBegin>48106</CharacterOffsetBegin>
    <CharacterOffsetEnd>48109</CharacterOffsetEnd>
    <POS>DT</POS>
    <NER>O</NER>
    </token>
    <token id=""12"">
    <word>rest</word>
    <lemma>rest</lemma>
    <CharacterOffsetBegin>48110</CharacterOffsetBegin>
    <CharacterOffsetEnd>48114</CharacterOffsetEnd>
    <POS>NN</POS>
    <NER>O</NER>
    </token>
    <token id=""13"">
    <word>of</word>
    <lemma>of</lemma>
    <CharacterOffsetBegin>48115</CharacterOffsetBegin>
    <CharacterOffsetEnd>48117</CharacterOffsetEnd>
    <POS>IN</POS>
    <NER>O</NER>
    </token>
    <token id=""14"">
    <word>the</word>
    <lemma>the</lemma>
    <CharacterOffsetBegin>48118</CharacterOffsetBegin>
    <CharacterOffsetEnd>48121</CharacterOffsetEnd>
    <POS>DT</POS>
    <NER>O</NER>
    </token>
    <token id=""15"">
    <word>world</word>
    <lemma>world</lemma>
    <CharacterOffsetBegin>48122</CharacterOffsetBegin>
    <CharacterOffsetEnd>48127</CharacterOffsetEnd>
    <POS>NN</POS>
    <NER>O</NER>
    </token>
    <token id=""16"">
    <word>-</word>
    <lemma>-</lemma>
    <CharacterOffsetBegin>48128</CharacterOffsetBegin>
    <CharacterOffsetEnd>48129</CharacterOffsetEnd>
    <POS>:</POS>
    <NER>O</NER>
    </token>
    <token id=""17"">
    <word>usually</word>
    <lemma>usually</lemma>
    <CharacterOffsetBegin>48130</CharacterOffsetBegin>
    <CharacterOffsetEnd>48137</CharacterOffsetEnd>
    <POS>RB</POS>
    <NER>O</NER>
    </token>
    <token id=""18"">
    <word>in</word>
    <lemma>in</lemma>
    <CharacterOffsetBegin>48138</CharacterOffsetBegin>
    <CharacterOffsetEnd>48140</CharacterOffsetEnd>
    <POS>IN</POS>
    <NER>O</NER>
    </token>
    <token id=""19"">
    <word>roughly</word>
    <lemma>roughly</lemma>
    <CharacterOffsetBegin>48141</CharacterOffsetBegin>
    <CharacterOffsetEnd>48148</CharacterOffsetEnd>
    <POS>RB</POS>
    <NER>O</NER>
    </token>
    <token id=""20"">
    <word>equal</word>
    <lemma>equal</lemma>
    <CharacterOffsetBegin>48149</CharacterOffsetBegin>
    <CharacterOffsetEnd>48154</CharacterOffsetEnd>
    <POS>JJ</POS>
    <NER>O</NER>
    </token>
    <token id=""21"">
    <word>proportions</word>
    <lemma>proportion</lemma>
    <CharacterOffsetBegin>48155</CharacterOffsetBegin>
    <CharacterOffsetEnd>48166</CharacterOffsetEnd>
    <POS>NNS</POS>
    <NER>O</NER>
    </token>
    <token id=""22"">
    <word>.</word>
    <lemma>.</lemma>
    <CharacterOffsetBegin>48167</CharacterOffsetBegin>
    <CharacterOffsetEnd>48168</CharacterOffsetEnd>
    <POS>.</POS>
    <NER>O</NER>
    </token>
</tokens>
<parse>(ROOT (S (NP (PRP We)) (VP (VBP have) (NP (NP (NN course) (NNS participants)) (PP (PP (PP (IN from) (NP (NNP Britain) (, ,) (NNP Europe))) (CC and) (PP (IN from) (NP (NP (DT the) (NN rest)) (PP (IN of) (NP (DT the) (NN world)))))) (: -) (RB usually) (PP (IN in) (NP (ADJP (RB roughly) (JJ equal)) (NNS proportions)))))) (. .))) </parse>
<dependencies type=""basic-dependencies"">
    <dep type=""root"">
    <governor idx=""0"">ROOT</governor>
    <dependent idx=""2"">have</dependent>
    </dep>
    <dep type=""nsubj"">
    <governor idx=""2"">have</governor>
    <dependent idx=""1"">We</dependent>
    </dep>
    <dep type=""compound"">
    <governor idx=""4"">participants</governor>
    <dependent idx=""3"">course</dependent>
    </dep>
    <dep type=""dobj"">
    <governor idx=""2"">have</governor>
    <dependent idx=""4"">participants</dependent>
    </dep>
    <dep type=""case"">
    <governor idx=""8"">Europe</governor>
    <dependent idx=""5"">from</dependent>
    </dep>
    <dep type=""compound"">
    <governor idx=""8"">Europe</governor>
    <dependent idx=""6"">Britain</dependent>
    </dep>
    <dep type=""acl"">
    <governor idx=""4"">participants</governor>
    <dependent idx=""8"">Europe</dependent>
    </dep>
    <dep type=""cc"">
    <governor idx=""8"">Europe</governor>
    <dependent idx=""9"">and</dependent>
    </dep>
    <dep type=""case"">
    <governor idx=""12"">rest</governor>
    <dependent idx=""10"">from</dependent>
    </dep>
    <dep type=""det"">
    <governor idx=""12"">rest</governor>
    <dependent idx=""11"">the</dependent>
    </dep>
    <dep type=""conj"">
    <governor idx=""8"">Europe</governor>
    <dependent idx=""12"">rest</dependent>
    </dep>
    <dep type=""case"">
    <governor idx=""15"">world</governor>
    <dependent idx=""13"">of</dependent>
    </dep>
    <dep type=""det"">
    <governor idx=""15"">world</governor>
    <dependent idx=""14"">the</dependent>
    </dep>
    <dep type=""nmod"">
    <governor idx=""12"">rest</governor>
    <dependent idx=""15"">world</dependent>
    </dep>
    <dep type=""dep"">
    <governor idx=""8"">Europe</governor>
    <dependent idx=""17"">usually</dependent>
    </dep>
    <dep type=""case"">
    <governor idx=""21"">proportions</governor>
    <dependent idx=""18"">in</dependent>
    </dep>
    <dep type=""advmod"">
    <governor idx=""20"">equal</governor>
    <dependent idx=""19"">roughly</dependent>
    </dep>
    <dep type=""amod"">
    <governor idx=""21"">proportions</governor>
    <dependent idx=""20"">equal</dependent>
    </dep>
    <dep type=""nmod"">
    <governor idx=""8"">Europe</governor>
    <dependent idx=""21"">proportions</dependent>
    </dep>
</dependencies>
<dependencies type=""collapsed-dependencies"">
    <dep type=""root"">
    <governor idx=""0"">ROOT</governor>
    <dependent idx=""2"">have</dependent>
    </dep>
    <dep type=""nsubj"">
    <governor idx=""2"">have</governor>
    <dependent idx=""1"">We</dependent>
    </dep>
    <dep type=""compound"">
    <governor idx=""4"">participants</governor>
    <dependent idx=""3"">course</dependent>
    </dep>
    <dep type=""dobj"">
    <governor idx=""2"">have</governor>
    <dependent idx=""4"">participants</dependent>
    </dep>
    <dep type=""conj:and"">
    <governor idx=""4"">participants</governor>
    <dependent idx=""4"" copy=""1"">participants</dependent>
    </dep>
    <dep type=""case"">
    <governor idx=""8"">Europe</governor>
    <dependent idx=""5"">from</dependent>
    </dep>
    <dep type=""compound"">
    <governor idx=""8"">Europe</governor>
    <dependent idx=""6"">Britain</dependent>
    </dep>
    <dep type=""acl:from"">
    <governor idx=""4"">participants</governor>
    <dependent idx=""8"">Europe</dependent>
    </dep>
    <dep type=""cc"">
    <governor idx=""4"">participants</governor>
    <dependent idx=""9"">and</dependent>
    </dep>
    <dep type=""case"">
    <governor idx=""12"">rest</governor>
    <dependent idx=""10"">from</dependent>
    </dep>
    <dep type=""det"">
    <governor idx=""12"">rest</governor>
    <dependent idx=""11"">the</dependent>
    </dep>
    <dep type=""acl:from"">
    <governor idx=""4"" copy=""1"">participants</governor>
    <dependent idx=""12"">rest</dependent>
    </dep>
    <dep type=""case"">
    <governor idx=""15"">world</governor>
    <dependent idx=""13"">of</dependent>
    </dep>
    <dep type=""det"">
    <governor idx=""15"">world</governor>
    <dependent idx=""14"">the</dependent>
    </dep>
    <dep type=""nmod:of"">
    <governor idx=""12"">rest</governor>
    <dependent idx=""15"">world</dependent>
    </dep>
    <dep type=""dep"">
    <governor idx=""8"">Europe</governor>
    <dependent idx=""17"">usually</dependent>
    </dep>
    <dep type=""case"">
    <governor idx=""21"">proportions</governor>
    <dependent idx=""18"">in</dependent>
    </dep>
    <dep type=""advmod"">
    <governor idx=""20"">equal</governor>
    <dependent idx=""19"">roughly</dependent>
    </dep>
    <dep type=""amod"">
    <governor idx=""21"">proportions</governor>
    <dependent idx=""20"">equal</dependent>
    </dep>
    <dep type=""nmod:in"">
    <governor idx=""8"">Europe</governor>
    <dependent idx=""21"">proportions</dependent>
    </dep>
</dependencies>
<dependencies type=""collapsed-ccprocessed-dependencies"">
    <dep type=""root"">
    <governor idx=""0"">ROOT</governor>
    <dependent idx=""2"">have</dependent>
    </dep>
    <dep type=""nsubj"">
    <governor idx=""2"">have</governor>
    <dependent idx=""1"">We</dependent>
    </dep>
    <dep type=""compound"">
    <governor idx=""4"">participants</governor>
    <dependent idx=""3"">course</dependent>
    </dep>
    <dep type=""dobj"">
    <governor idx=""2"">have</governor>
    <dependent idx=""4"">participants</dependent>
    </dep>
    <dep type=""dobj"" extra=""true"">
    <governor idx=""2"">have</governor>
    <dependent idx=""4"" copy=""1"">participants</dependent>
    </dep>
    <dep type=""conj:and"">
    <governor idx=""4"">participants</governor>
    <dependent idx=""4"" copy=""1"">participants</dependent>
    </dep>
    <dep type=""case"">
    <governor idx=""8"">Europe</governor>
    <dependent idx=""5"">from</dependent>
    </dep>
    <dep type=""compound"">
    <governor idx=""8"">Europe</governor>
    <dependent idx=""6"">Britain</dependent>
    </dep>
    <dep type=""acl:from"">
    <governor idx=""4"">participants</governor>
    <dependent idx=""8"">Europe</dependent>
    </dep>
    <dep type=""cc"">
    <governor idx=""4"">participants</governor>
    <dependent idx=""9"">and</dependent>
    </dep>
    <dep type=""case"">
    <governor idx=""12"">rest</governor>
    <dependent idx=""10"">from</dependent>
    </dep>
    <dep type=""det"">
    <governor idx=""12"">rest</governor>
    <dependent idx=""11"">the</dependent>
    </dep>
    <dep type=""acl:from"">
    <governor idx=""4"" copy=""1"">participants</governor>
    <dependent idx=""12"">rest</dependent>
    </dep>
    <dep type=""case"">
    <governor idx=""15"">world</governor>
    <dependent idx=""13"">of</dependent>
    </dep>
    <dep type=""det"">
    <governor idx=""15"">world</governor>
    <dependent idx=""14"">the</dependent>
    </dep>
    <dep type=""nmod:of"">
    <governor idx=""12"">rest</governor>
    <dependent idx=""15"">world</dependent>
    </dep>
    <dep type=""dep"">
    <governor idx=""8"">Europe</governor>
    <dependent idx=""17"">usually</dependent>
    </dep>
    <dep type=""case"">
    <governor idx=""21"">proportions</governor>
    <dependent idx=""18"">in</dependent>
    </dep>
    <dep type=""advmod"">
    <governor idx=""20"">equal</governor>
    <dependent idx=""19"">roughly</dependent>
    </dep>
    <dep type=""amod"">
    <governor idx=""21"">proportions</governor>
    <dependent idx=""20"">equal</dependent>
    </dep>
    <dep type=""nmod:in"">
    <governor idx=""8"">Europe</governor>
    <dependent idx=""21"">proportions</dependent>
    </dep>
</dependencies>
</sentence>
```
"
65,https://github.com/stanfordnlp/CoreNLP/issues/82,82,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2015-07-17 05:20:56+00:00,,Output encoding issue for the Stanford Parser,"It seems that the `-encoding` option in the Stanford Parser does not set output file encoding correctly. This causes problem on some Windows systems, where the parse tree are written using the system's default encoding Windows-1252, even if `-encoding UTF-8` is set.

On systems that the default file encoding is UTF-8, the problem can be reproduced in the following way. Suppose we want to parse a file (encoded in UTF-8) containing a single word ""caf√©"".

```
$ cat sample-utf8.txt | hexdump -C
00000000  63 61 66 c3 a9 0a                                 |caf...|
00000006
```

We run the parser after setting the `file.encoding` property in Java to the Windows default.

```
$ java -mx150m -Dfile.encoding=Cp1252 -cp stanford-parser.jar edu.stanford.nlp.parser.lexparser.LexicalizedParser -encoding UTF-8 -outputFormat penn edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz sample-utf8.txt 2> /dev/null | hexdump -C
00000000  28 52 4f 4f 54 0a 20 20  28 4e 50 20 28 4e 4e 50  |(ROOT.  (NP (NNP|
00000010  20 63 61 66 e9 29 29 29  0a 0a                    | caf.)))..|
0000001a
```

The content of the output is the same regardless whether `-encoding UTF-8` is passed or not. Note that the byte representation for the character ""√©"" is wrong, which should be `c3 a9` in UTF-8 rather than `e9`. The problem exists either when the result is written to stdout, or to files via `-writeOutputFiles`.

I guess the problem is related to [Line 83](https://github.com/stanfordnlp/CoreNLP/blob/c922558c890c08aaabf5ec29aa873b7fb0a1a59d/src/edu/stanford/nlp/parser/lexparser/ParseFiles.java#L83), [Line 151](https://github.com/stanfordnlp/CoreNLP/blob/c922558c890c08aaabf5ec29aa873b7fb0a1a59d/src/edu/stanford/nlp/parser/lexparser/ParseFiles.java#L151) and [Line 173](https://github.com/stanfordnlp/CoreNLP/blob/c922558c890c08aaabf5ec29aa873b7fb0a1a59d/src/edu/stanford/nlp/parser/lexparser/ParseFiles.java#L173) in  ParseFiles.java, where `op.tlpParams.getOutputEncoding()` is not used to determine the encoding for output streams.
"
66,https://github.com/stanfordnlp/CoreNLP/issues/83,83,"[{'id': 103161713, 'node_id': 'MDU6TGFiZWwxMDMxNjE3MTM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/discussion', 'name': 'discussion', 'color': 'd4c5f9', 'default': False, 'description': None}]",closed,2015-07-22 16:37:34+00:00,,Less NE's per run,"There is a bug in the Stanford NLP Library which results in fewer named-entities while processing the same text.

see: https://github.com/WillemJan/Stanford_ner_bugreport
"
67,https://github.com/stanfordnlp/CoreNLP/issues/85,85,[],closed,2015-07-24 23:01:38+00:00,,Spanish tokenizer much slower than English tokenizer,"Hey,
I am using the Spanish tokenizer to run large jobs on common crawl using hadoop. 
While the english tokenizer works really fast, the Spanish tokenizer is painfully slow.. 

I started profiling it, and i'd like to share with you some initial results, perhaps we could find the bottleneck and fix it.

The first image shows profiling made to a large document (433,321 bytes) using the english tokenizer, which takes about ~35msecs on my computer:
![tokenize-english](https://cloud.githubusercontent.com/assets/379063/8886273/6534d4fc-321c-11e5-838a-34d9b7a0b23f.png)

The second image shows profiling made to the same exact document, using the spanish tokenizer, which takes about ~300msecs on the same computer:
![tokenize-spanish](https://cloud.githubusercontent.com/assets/379063/8886295/b0b7d12c-321c-11e5-8eff-01780c15b3d4.png)

it seems that most of the time is spent in asciiQuotes.. any ideas of why it behaves this way?

Thanks,
Shlomi
"
68,https://github.com/stanfordnlp/CoreNLP/issues/86,86,[],closed,2015-07-29 15:38:55+00:00,,edu.stanford.nlp.patterns.LuceneSentenceIndex not in v3.5.2 corenlp jar,"I downloaded the v3.5.2 corenlp package and tried to use the SPIED functionality w/lucene indexing. However the class edu.stanford.nlp.patterns.LuceneSentenceIndex is not in the jar.
"
69,https://github.com/stanfordnlp/CoreNLP/issues/87,87,"[{'id': 45387508, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}]",closed,2015-07-31 12:01:34+00:00,,edu.stanford.nlp.trees.Tree Calculating character edges of child node does not count spaces,"The methods [`leftCharEdge(Tree node)`](https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/trees/Tree.java#L2655) and [`rightCharEdge(Tree node)`](https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/trees/Tree.java#L2694) do not count spaces while calculating the edge.

Is this by design?

What methods should I be using to calculate the position of a substring represented by a child node within the original sentece? You can see an example [here](http://stackoverflow.com/questions/31743689/how-to-find-the-index-of-named-node-in-a-tregex-pattern-using-stanford-corenlp).
"
70,https://github.com/stanfordnlp/CoreNLP/issues/92,92,"[{'id': 45387508, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}]",closed,2015-09-10 14:16:55+00:00,,Train classifier to label sentences.,"Hi all,

I don't know if this is the right spot to post this question. I am new to StanfordNLP. This is more of a technical question.

I have a large number of sentences with each one tagged with 5 different classes. I wanted to ask how I could use StanfordNLP to train a classifier (e.g. Bayesian, NN, etc.) to label new incoming sentences into these 5 classes. 

Any help to point me to the right direction would be much appreciated. Thanks!
"
71,https://github.com/stanfordnlp/CoreNLP/issues/93,93,"[{'id': 45387508, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}]",closed,2015-10-02 03:42:36+00:00,,TokensRegex: AND operators,"I'm sorry if the question is not relevant.

TokensRegex supports & (AND) operators. As I understand, you can use pattern 'X & Y' to match any sequences containing both X and Y. But when I used the operator in real code, it didn't work as the way I expected. Here is my Java code:

``` java
String content = ""data is here and everywhere"";
String pattern = ""data & here"";

TokenizerFactory tf = PTBTokenizer.factory(new CoreLabelTokenFactory(), """");
List<CoreLabel> tokens = tf.getTokenizer(new StringReader(content)).tokenize();
TokenSequencePattern seqPattern = TokenSequencePattern.compile(pattern);
TokenSequenceMatcher matcher = seqPattern.getMatcher(tokens);

if(matcher.find()){
      System.out.println(""Matched""); // <- I expected to have this printed out
} else {
      System.out.println(""Unmatched""); // <- But I've got this instead :(
}
```

Would you please tell me what's wrong with my code or my understanding? Thank you in advance.
"
72,https://github.com/stanfordnlp/CoreNLP/issues/94,94,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2015-10-18 14:41:58+00:00,,"Fallback code in IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(String) fails handling "".gz"" files","The fallback code in CoreNLP 3.5.2 for loading models from other kinds of URLs in IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(String) fails to handle the "".gz"" extension as other options. It does not transparently expand gzipped files.

This was still working in CoreNLP 3.5.1

```
      try {
        in = findStreamInClasspathOrFileSystem(textFileOrUrl);
      } catch (FileNotFoundException e) {
        try {
          // Maybe this happens to be some other format of URL?
          URL u = new URL(textFileOrUrl);
          URLConnection uc = u.openConnection();
          in = uc.getInputStream();
>>> GZIP handling missing here <<<
        } catch (IOException e2) {
          // Don't make the original exception a cause, since it is almost certainly bogus
          throw new IOException(""Unable to resolve \"""" +
                  textFileOrUrl + ""\"" as either "" +
                  ""class path, filename or URL""); // , e2);
        }
      }
```
"
73,https://github.com/stanfordnlp/CoreNLP/issues/96,96,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2015-10-23 15:01:57+00:00,,Issues when concurrently deserializing Annotation objects,"Hello,

I try to deserialize previously stored Annotation objects (the annotation includes tokenization, ssplit, POS tag, pcfg parsing, and NER tag) concurrently in different threads as follows:

Annotation annotation = (Annotation) objin.readObject();

and it caused the following exception:

java.util.ConcurrentModificationException
    at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901)
    at java.util.ArrayList$Itr.next(ArrayList.java:851)
    at edu.stanford.nlp.trees.GrammaticalRelation.valueOf(GrammaticalRelation.java:165)
    at edu.stanford.nlp.trees.UniversalEnglishGrammaticalRelations.valueOf(UniversalEnglishGrammaticalRelations.java:1674)
    at edu.stanford.nlp.trees.GrammaticalRelation.readResolve(GrammaticalRelation.java:566)
"
74,https://github.com/stanfordnlp/CoreNLP/issues/98,98,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",closed,2015-10-27 22:02:13+00:00,,TimeAnnotator should have a 3-argument parameter,"Hey, Im working on a project using CoreNLP, and SUTime, specifically. The TimeAnnotator class has three constructors, the Default, the constructor which initializes with properties, and a constructor to silence log output. There is currently no way to both initialize properties _and_ silence log output. I propose there to be a 4th, 3-argument constructor `TimeAnnotator(String name, Properties props, boolean quiet)` which solves this issue.

I would send a PR, but wanted feedback first, and am quite busy, so if anyone else wants to do this, please go on ahead.
"
75,https://github.com/stanfordnlp/CoreNLP/issues/101,101,[],closed,2015-11-05 13:29:18+00:00,,pos-tagger cannot load models from stanford-corenlp-3.5.2-models.jar,"I use Stanford Core NLP in Java as a Maven dependency. I want to use the MaxentTagger with a model supplied in the stanford-corenlp-3.5.2-models package. The problem is that I cannot access this model through classpath. 

My code is 

```
tagger = new MaxentTagger(""/edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"");
```

The file ""/edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"" exists in the jar and should be loaded through classpath, but the following exception is thrown

```
Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: Unrecoverable error while loading a tagger model
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:770)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.<init>(MaxentTagger.java:298)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.<init>(MaxentTagger.java:263)
    at cz.zcu.kiv.nlp.semeval.cwi.features.POSFeature.<init>(POSFeature.java:24)
    at cz.zcu.kiv.nlp.semeval.cwi.CWIModel.train(CWIModel.java:60)
    at cz.zcu.kiv.nlp.semeval.cwi.TrainingCrossValidation.main(TrainingCrossValidation.java:51)
Caused by: java.io.IOException: Unable to resolve ""/edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"" as either class path, filename or URL
    at  edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:481)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:765)
    ... 5 more
```

If I copy the model out of the jar and use e.g.

```
tagger = new MaxentTagger(""./english-left3words-distsim.tagger"");
```

then everything works perfectly.

The problem is probably in the class IOUtils, method findStreamInClasspathOrFileSystem(String name).

In the line 

```
InputStream is = IOUtils.class.getClassLoader().getResourceAsStream(name);
```

the returned classloader is probably the JarClassLoader which loaded the library (stanford-corenlp-3.5.2.jar) and it does not have access to other libraries.

This theory is supported by the following code

```
InputStream stream = POSFeature.class.getResourceAsStream(""/edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"");
System.out.println(""Stream == null: "" + (stream == null));

tagger = new MaxentTagger(""/edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"");
```

which outputs

```
Stream == null: false
Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: Unrecoverable error while loading a tagger model
...
Caused by: java.io.IOException: Unable to resolve ""/edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"" as either class path, filename or URL
```
"
76,https://github.com/stanfordnlp/CoreNLP/issues/103,103,"[{'id': 45387508, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}]",closed,2015-11-15 19:42:34+00:00,,Unicode characters coreNLP can't handle,"I just tried to use nndep on some social media documents and got errors like 

``` java
Nov 15, 2015 5:15:38 PM edu.stanford.nlp.process.PTBLexer next
WARNING: Untokenizable: ? (U+D83D, decimal: 55357)
```

I know that those characters are automatically removed, but I wanted get rid of all those error messages and therefore preprocess the file and remove those characters. Just removing all invalid characters with 

``` java
document.replaceAll(""[^\\u0009\\u000a\\u000d\\u0020-\\uD7FF\\uE000-\\uFFFD]"", """");
```

wasn't enough, the error still occured on some characters, which I then removed with

``` java
document.replaceAll(""[\\uD83D\\uFFFD\\uFE0F\\u203C\\u3010\\u3011\\u300A\\u166D\\u200C\\u202A\\u202C\\u2049\\u20E3\\u300B\\u300C\\u3030\\u065F\\u0099\\u0F3A\\u0F3B\\uF610\\uFFFC
```

My question is: Is there a complete list of unsupported characters? The ones I removed are just based on my corpus and are not necessarily applicable on another one.
"
77,https://github.com/stanfordnlp/CoreNLP/issues/104,104,[],closed,2015-11-16 15:53:37+00:00,,Annotator openie: Unable to load clause splitter model,"When using the openie annotator I get three exceptions: 

```
Loading clause searcher from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz...java.io.IOException: unexpected exception type
    at java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1538)
    at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1110)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1810)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
    at edu.stanford.nlp.io.IOUtils.readObjectFromURLOrClasspathOrFileSystem(IOUtils.java:318)
    at edu.stanford.nlp.naturalli.ClauseSplitter.load(ClauseSplitter.java:283)
    at edu.stanford.nlp.naturalli.OpenIE.<init>(OpenIE.java:191)
    at edu.stanford.nlp.pipeline.AnnotatorImplementations.openie(AnnotatorImplementations.java:268)
    at edu.stanford.nlp.pipeline.AnnotatorFactories$20.create(AnnotatorFactories.java:644)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:85)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:357)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:130)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:126)
    at pipeline.TestClass.main(TestClass.java:22)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:483)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
Caused by: java.lang.NoSuchMethodException: edu.stanford.nlp.naturalli.ClauseSplitterSearchProblem.$deserializeLambda$(java.lang.invoke.SerializedLambda)
    at java.lang.Class.getDeclaredMethod(Class.java:2117)
    at java.lang.invoke.SerializedLambda$1.run(SerializedLambda.java:224)
    at java.lang.invoke.SerializedLambda$1.run(SerializedLambda.java:221)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:221)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:483)
    at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1104)
    ... 22 more
Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: Could not load clause splitter model at edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz: class java.io.IOException: unexpected exception type
    at edu.stanford.nlp.naturalli.OpenIE.<init>(OpenIE.java:196)
    at edu.stanford.nlp.pipeline.AnnotatorImplementations.openie(AnnotatorImplementations.java:268)
    at edu.stanford.nlp.pipeline.AnnotatorFactories$20.create(AnnotatorFactories.java:644)
    at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:85)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:357)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:130)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:126)
    at pipeline.TestClass.main(TestClass.java:22)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:483)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
```

it seems that somehow when loading the clauseSearchModel.ser.gz java cannot find the method to deserialize.

Unfortunately I am not familiar with Lambdas and days of painful googling didn't get me anywhere. Any help is appreciated.
"
78,https://github.com/stanfordnlp/CoreNLP/issues/105,105,"[{'id': 45387505, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNQ==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/duplicate', 'name': 'duplicate', 'color': 'bbbbbb', 'default': True, 'description': None}]",closed,2015-11-19 20:49:53+00:00,,The sentiment output result from -stdin differs from the when input is read from a file,"I am not sure whether this issue is related to how the sentiment tool works or it is just a weird thing that stdin does. Regardless, I thought I should bring this up as a point of awareness for researchers analyzing the output results. 

Here is a toy snapshot of a comment from reddit analyzed via -stdin vs. being read from a .txt file. Please find the differences: 

<img width=""1338"" alt=""screen shot 2015-11-19 at 12 49 22 pm"" src=""https://cloud.githubusercontent.com/assets/7051103/11283941/02125028-8ebc-11e5-9cc7-ac3bd9fa8fa5.png"">
"
79,https://github.com/stanfordnlp/CoreNLP/issues/106,106,[],closed,2015-11-19 20:56:20+00:00,,The sentiment output result from -stdin differs from the when input is read from a file,"I am not sure whether this issue is related to how the sentiment tool works or it is just a weird thing that stdin does. Regardless, I thought I should bring this up as a point of awareness for researchers analyzing the output results. 

Here is a toy snapshot of a comment from reddit analyzed via -stdin vs. being read from a .txt file. Please find the differences: 

<img width=""1338"" alt=""screen shot 2015-11-19 at 12 49 22 pm"" src=""https://cloud.githubusercontent.com/assets/7051103/11283941/02125028-8ebc-11e5-9cc7-ac3bd9fa8fa5.png"">

As you many notice, on the left side we have a sequence of  
[Negative, Neutral, Negative, Very negative, Negative, Neutral, Negative, Negative, Negative]
but on the right side we have the following sequence:
[Negative, Neutral, Neutral, Negative, Negative, Negative, Negative,  Negative, Negative, Neutral, Negative]

If you would like to try out yourself, here is the sample text: 
""States don't own.They just control.\n\n&gt;The Federal Reserve System (also known as the Federal Reserve  and informally as the Fed) is the central banking system of the United States. It was created on December 23  1913  with the enactment of the Federal Reserve Act  largely in response to a series of financial panics  particularly a severe panic in 1907.234567 Over time  the roles and responsibilities of the Federal Reserve System have expanded  and its structure has evolved.
38 Events such as the Great Depression in the 1930s were major factors leading to changes in the system.9The U.S. Congress established three key objectives for monetary policy in the Federal Reserve Act: Maximum employment  stable prices  and moderate long-term interest rates.
10 The first two objectives are sometimes referred to as the Federal Reserve's dual mandate.11 Its duties have expanded over the years  and today  according to official Federal Reserve documentation  include conducting the nation's monetary policy  suvising and regulating banking institutions  maintaining the stability of the financial system and providing financial services to depository institutions  the U.S. government  and foreign official institutions.12 The Fed also conducts research into the economy and releases numerous publications  such as the Beige Book.\n\nIt is a private bank  created by an act of the state  in order to achieve the states mandated goals. It is the state. At any moment congress decides  it can take money printing away from the fed  or eliminate the fed all together.The fact that the state CHOSE to use a private bank is mostly irrelevant. The federal reserve bank is a branch of the government that they have found a legal way to hide from public audit through private ownership.""
"
80,https://github.com/stanfordnlp/CoreNLP/issues/108,108,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2015-12-01 06:52:54+00:00,,PTB Tokenizer deletes last token,"(apologies for accidentally submitting before finishing typing)

The MaxentTagger is deleting the last token in the following input:

```
List<List<HasWord>> sentences = MaxentTagger
            .tokenizeText(new StringReader(""1. Narakattaramukku""));
```

The resulting `sentences` variable contains only `[[1, .]]`.
"
81,https://github.com/stanfordnlp/CoreNLP/issues/109,109,"[{'id': 45387508, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}]",closed,2015-12-03 03:52:11+00:00,,Downsizing this library,"This may not be the right place to add this, but any advice on downsizing this library? Trying to fit into Heroku's slug size limit (it's 300MB) and due to the size of this library, it's over 400MB (408MB to be precise).
"
82,https://github.com/stanfordnlp/CoreNLP/issues/110,110,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2015-12-09 15:46:56+00:00,,What is the correct version of master branch?,"In the http://nlp.stanford.edu/software/corenlp.shtml site the latest version is `3.5.2` but the version number in `build.gradle` is `3.4.1`.
"
83,https://github.com/stanfordnlp/CoreNLP/issues/111,111,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2015-12-09 21:01:53+00:00,,Spanish tokenizer dies with an NPE on some input,"Hey,
I am using CoreNLP to tokenize crawled text, and I get the following error:

The current spanish tokenizer dies on the input ""salos  ) ( 1 de"", with the following stack trace:

```
Unhandled java.lang.NullPointerException
Matcher.java: 1283  java.util.regex.Matcher/getTextLength
Matcher.java:  309  java.util.regex.Matcher/reset
Matcher.java:  229  java.util.regex.Matcher/<init>
Pattern.java: 1093  java.util.regex.Pattern/matcher
```

Tracing that in the code I get the following:
In DocumentPreProcessor.java, line 331 (`if ( ! (wsPattern.matcher(token.word()).matches() ||`), `token.word()` returns null. I added a debug log, here is what I got:

```
token.word(): sal
token.word(): os
token.word(): null
```

By the way, if this was upper case (`SALOS  ) ( 1 DE`), it works just fine.

WDYT?
"
84,https://github.com/stanfordnlp/CoreNLP/issues/113,113,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 706055902, 'node_id': 'MDU6TGFiZWw3MDYwNTU5MDI=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/tokenize', 'name': 'tokenize', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2015-12-10 17:26:47+00:00,,ArabicLexer does not store BeforeAnnotation and AfterAnnotation whitespace.,"First, thank you all for providing this resource. I would like to use your Arabic processing tools but am running into a problem: the tokenizer doesn't record the amount of whitespace before and after a token.

I see that in getNext() in ArabicLexer, the syntax that would set values for CoreAnnotations.BeforeAnnotation and CoreAnnotations.AfterAnnotation to be the appropriate amount of whitespace before or after a token is commented out. I tried uncommenting and added private CoreLabel prevWord and private StringBuilder prevWordAfter to ArabicLexer, and made sure invertible=true. I'm not sure why, but the BeforeAnnotation and AfterAnnotation values always end up as empty strings. prevWordAfter.StringBuilder.toString() is always returning an empty string. 

Could anyone point me in the right direction as to how to fix this problem?

Thank you.
-Taylor
"
85,https://github.com/stanfordnlp/CoreNLP/issues/116,116,[],closed,2015-12-19 03:43:14+00:00,,NullPointerException when parser timeouts,"I tried to set a timeout parameter for the Stanford parser and followed the steps described at http://stackoverflow.com/questions/33297369/how-to-set-a-timeout-on-stanford-nlp-parsing

I replicated the errors described in the page. Has this bug been fixed yet? I am using the latest 3.6.0 release.

I assume parse.maxtime is in millisecond.

Thanks!

P.S., It looks [this line](https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/SentenceAnnotator.java#L64) uses results returned by [this function](https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/util/concurrent/InterruptibleMulticoreWrapper.java#L50), which may return null. I am not familiar with Java and the logic in the codes. Could someone fix this? Thanks!
"
86,https://github.com/stanfordnlp/CoreNLP/issues/117,117,"[{'id': 45387508, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}]",closed,2015-12-21 04:31:51+00:00,,Publish 3.6.0 to maven central,"Can you please indicate when 3.6.0 will be published to maven central? I'm relying on online documentation while having to use 3.5.2 (via pom.xml dep) which makes coding difficult. I can't use a private maven repo or manual downloads either.
"
87,https://github.com/stanfordnlp/CoreNLP/issues/120,120,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",closed,2016-01-03 13:45:42+00:00,,NERClassifierCombiner cannot load models from URL,"Unlike many other annotators, the NERClassifierCombiner cannot load models from URLs. To fix this, `AbstractSequenceClassifier.loadStreamFromClasspath(String)` (and probably some other methods calling it) should use `IOUtils.getInputStreamFromURLOrClasspathOrFileSystem()`. There is even a related comment in `AbstractSequenceClassifier`:

```
todo [cdm 2015]: Replace this method with use of the method in IOUtils.
```
"
88,https://github.com/stanfordnlp/CoreNLP/issues/121,121,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2016-01-03 14:21:49+00:00,,"ParserAnnotator exception if ""annotators"" property is not set","ParserAnnotator throws an exception if ""annotators"" property is not set:

```
Caused by: java.lang.RuntimeException: Missing property: ""annotators""
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.getRequiredProperty(StanfordCoreNLP.java:179)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.usesBinaryTrees(StanfordCoreNLP.java:426)
    at edu.stanford.nlp.pipeline.ParserAnnotator.<init>(ParserAnnotator.java:140)
```

`StanfordCoreNLP.usesBinaryTrees(Properties)` tries to detect the presence of the sentiment annotator and uses this to set the default value of `usesBinary`.

I am instantiating the `ParserAnnotator` directly, so I have no need for the `annotators` property. 

I would expect that `StanfordCoreNLP.usesBinaryTrees(Properties)` simply assumes the sentiment annotator not to be present if the `annotators` property is missing.

As a workaround, I am setting `annotators` to an empty string now when creating the `ParserAnnotator`.
"
89,https://github.com/stanfordnlp/CoreNLP/issues/122,122,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 706059615, 'node_id': 'MDU6TGFiZWw3MDYwNTk2MTU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/ner', 'name': 'ner', 'color': 'c5def5', 'default': False, 'description': None}, {'id': 706083198, 'node_id': 'MDU6TGFiZWw3MDYwODMxOTg=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/sutime', 'name': 'sutime', 'color': 'c5def5', 'default': False, 'description': None}]",open,2016-01-06 20:26:37+00:00,,TimeExpressionExtractorFactory.isDefaultExtractorPresent() checks for class but not for resources,"`TimeExpressionExtractorFactory.isDefaultExtractorPresent()` check if the class for the default time expression extractor is present (i.e. `edu.stanford.nlp.time.TimeExpressionExtractorImpl` which is in the CoreNLP jar and thus likely always present). It doesn't check though if the resources required by this extractor are present (in the ""models"" jar which may actually not be present).

When not having the models jar on the classpath and using alternative means of providing the NERClassifierCombiner with models then causes the auto-detection to fail with this exception:

```
Caused by: edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl
    at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:40)
    at edu.stanford.nlp.time.TimeExpressionExtractorFactory.create(TimeExpressionExtractorFactory.java:57)
    at edu.stanford.nlp.time.TimeExpressionExtractorFactory.createExtractor(TimeExpressionExtractorFactory.java:38)
    at edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.<init>(NumberSequenceClassifier.java:81)
    at edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.<init>(NumberSequenceClassifier.java:73)
    at edu.stanford.nlp.ie.NERClassifierCombiner.<init>(NERClassifierCombiner.java:103)
       <snip>
    ... 32 more
Caused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: MetaClass couldn't create public edu.stanford.nlp.time.TimeExpressionExtractorImpl(java.lang.String,java.util.Properties) with args [sutime, {}]
    at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:235)
    at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:380)
    at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:38)
    ... 46 more
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:231)
    ... 48 more
Caused by: java.lang.RuntimeException: Error initializing binder 1
    at edu.stanford.nlp.time.Options.<init>(Options.java:92)
    at edu.stanford.nlp.time.TimeExpressionExtractorImpl.init(TimeExpressionExtractorImpl.java:45)
    at edu.stanford.nlp.time.TimeExpressionExtractorImpl.<init>(TimeExpressionExtractorImpl.java:39)
    ... 53 more
Caused by: java.lang.NullPointerException: Missing URL.
    at de.jollyday.HolidayManager.getInstance(HolidayManager.java:190)
    at edu.stanford.nlp.time.JollyDayHolidays.init(JollyDayHolidays.java:51)
    at edu.stanford.nlp.time.Options.<init>(Options.java:90)
    ... 55 more
```
"
90,https://github.com/stanfordnlp/CoreNLP/issues/123,123,[],closed,2016-01-15 11:26:38+00:00,,Possible bug in bind new annotation in Env (i.e. using custom Env),"Hello,
Since some time, I have been using Semgrex. It is no working as I expect. At the bottom of my comment, you can find working/failing test(modification of SemgrexPatternText.testEnv).

When I use custom Env, sometimes semgrex works correctly, sometimes not. After little testing, I found that it only works when custom annotation is only set for first word({}). For any other words in pattern(custom annotation) it fails. 

~~I suppose that problem may be in Env class in method lookupAnnotationKey (there is commented code and empty catch block).~~
I suppose that problem may be in NodePattern - node which should be matched, doesn't have provided annotation.

``` java
public void testEnv() throws IOException {
    SemanticGraph h = SemanticGraph.valueOf(""[married/VBN nsubjpass>Hughes/NNP auxpass>was/VBD nmod:to>Gracia/NNP]"");
    h.getFirstRoot().set(PatternsAnnotations.PatternLabel1.class,""YES"");
    // here I set PatternLabel1 to ""NO"" for word ""Hughes""
    h.getChildList(h.getFirstRoot()).get(0).set(PatternsAnnotations.PatternLabel1.class,""NO"");
    //SemanticGraph t = SemanticGraph
    //  .valueOf(""[loved/VBD\nnsubj:Hughes/NNP\ndobj:[wife/NN poss:his/PRP$ appos:Gracia/NNP]\nconj_and:[obsessed/JJ\ncop:was/VBD\nadvmod:absolutely/RB\nprep_with:[Elicia/NN poss:his/PRP$ amod:little/JJ nn:daughter/NN]]]"");
    String macro = ""macro WORD = married"";
    Env env = new Env();
    env.bind(""pattern1"",PatternsAnnotations.PatternLabel1.class);
    // when I set pattern1 for word Hughes to ""NO"", test will FAIL
    String pattern = ""({pattern1:YES}=parent >>nsubjpass {word:Hughes; pos:NNP; pattern1:NO}=node)"";
    List<SemgrexPattern> pats = SemgrexBatchParser.compileStream(new ByteArrayInputStream((macro + ""\n"" + pattern).getBytes(StandardCharsets.UTF_8)), env);
    SemgrexPattern pat3 = pats.get(0);
    boolean ignoreCase = true;
    SemgrexMatcher mat3 = pat3.matcher(h, ignoreCase);
    if (mat3.find()) {
      String parent = mat3.getNode(""parent"").word();
      String node = mat3.getNode(""node"").word();
      System.out.println(""Result: parent is "" + parent + "" and node is "" + node);
      Assert.assertEquals(parent, ""married"");
      Assert.assertEquals(node, ""Hughes"");
    } else
      throw new RuntimeException(""failed!"");
  }
```
"
91,https://github.com/stanfordnlp/CoreNLP/issues/124,124,[],closed,2016-01-15 13:03:38+00:00,,coref annotanor crashes in cli application,"I'm downloaded Stanford corenlp 3.6.0 and trying a command line application, but sometimes it crashes.

Using Mac OS X 10.11.2

‚ûú  stanford-corenlp-full-2015-12-09  java -version
java version ""1.8.0_45""
Java(TM) SE Runtime Environment (build 1.8.0_45-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)

‚ûú  stanford-corenlp-full-2015-12-09  ./corenlp.sh
java -mx5g -cp ""./*"" edu.stanford.nlp.pipeline.StanfordCoreNLP
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Searching for resource: StanfordCoreNLP.properties
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Searching for resource: edu/stanford/nlp/pipeline/StanfordCoreNLP.properties
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.7 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [2.4 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.3 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.6 sec].
[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/defs.sutime.txt
Jan 15, 2016 1:38:21 PM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules
INFO: Read 83 rules
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.sutime.txt
Jan 15, 2016 1:38:21 PM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules
INFO: Read 267 rules
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
Jan 15, 2016 1:38:21 PM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules
INFO: Read 25 rules
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.5 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator mention
Using mention detector type: rule
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref

Entering interactive shell. Type q RETURN or EOF to quit.
**NLP> the dog**
Exception in thread ""main"" java.lang.RuntimeException: Error annotating document with coref
    at edu.stanford.nlp.scoref.StatisticalCorefSystem.annotate(StatisticalCorefSystem.java:86)
    at edu.stanford.nlp.scoref.StatisticalCorefSystem.annotate(StatisticalCorefSystem.java:63)
    at edu.stanford.nlp.pipeline.CorefAnnotator.annotate(CorefAnnotator.java:100)
    at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:71)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:491)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.process(StanfordCoreNLP.java:543)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.shell(StanfordCoreNLP.java:780)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1168)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1214)
Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
    at java.util.ArrayList$SubList.rangeCheck(ArrayList.java:1217)
    at java.util.ArrayList$SubList.get(ArrayList.java:1034)
    at edu.stanford.nlp.scoref.Clusterer$State.setClusters(Clusterer.java:349)
    at edu.stanford.nlp.scoref.Clusterer$State.<init>(Clusterer.java:322)
    at edu.stanford.nlp.scoref.Clusterer.getClusterMerges(Clusterer.java:58)
    at edu.stanford.nlp.scoref.ClusteringCorefSystem.runCoref(ClusteringCorefSystem.java:67)
    at edu.stanford.nlp.scoref.StatisticalCorefSystem.annotate(StatisticalCorefSystem.java:72)
    ... 8 more
‚ûú  stanford-corenlp-full-2015-12-09  
"
92,https://github.com/stanfordnlp/CoreNLP/issues/125,125,[],closed,2016-01-16 02:56:07+00:00,,CoreNLP server inteprets input as Latin-1,"I'm using the CoreNLP server with a POS tagger I trained on Portuguese data. The tagger was trained on UTF-8 files and its properties explicitly tells it to use UTF-8. 

Running the tagger via command line works fine with UTF-8 input (calling either the tagger class or the StanfordCoreNLP class), but the server apparently always interprets input data as Latin-1. I noticed it because the tagger makes blatant mistakes on accented words when I encode the data as UTF-8, but not when it's encoded as Latin-1. 

I also tried including `encoding=utf-8` in the URL parameters, but it had no effect.
"
93,https://github.com/stanfordnlp/CoreNLP/issues/126,126,"[{'id': 284605708, 'node_id': 'MDU6TGFiZWwyODQ2MDU3MDg=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/cleanup', 'name': 'cleanup', 'color': 'fbca04', 'default': False, 'description': None}]",closed,2016-01-16 07:00:11+00:00,,Timer logs normal messages to System.err,"Thank you for this very useful library. Here is something minor I noticed:

Timer logs normal messages to System.err which results in mixed Logger and System.err lines in normal startup log.

Would be nice to log timing messages with debug level to Logger.

08:45:56 190  INFO   Adding annotator tokenize
08:45:56 193  INFO   TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
08:45:56 200  INFO   Adding annotator ssplit
08:45:56 201  INFO   Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... 08:45:57 210  INFO   Adding annotator lemma
done [1.0 sec].
08:45:57 210  INFO   Adding annotator parse
08:45:57 215  INFO   Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
done [0.3 sec].

```
  /** Print the start of timing message to stderr and start the timer. */
  public void doing(String str) {
    System.err.print(str);
    System.err.print("" ... "");
    System.err.flush();
    start();
  }

  /** Finish the line from startDoing with the end of the timing done message
   *  and elapsed time in x.y seconds.
   */
  public void done() {
    System.err.println(""done ["" + toSecondsString() + "" sec]."");
  }
```
"
94,https://github.com/stanfordnlp/CoreNLP/issues/127,127,[],closed,2016-01-16 07:14:13+00:00,,Parse sentence to tree structure like Stanford Sentiment Treebank,"As shown in the Stanford Sentiment Treebank demo on [Sentiment Analysis](http://nlp.stanford.edu/sentiment/treebank.html), it shows that the structure of the tree is definitely the binary tree,i learned that The Stanford Parser was used to parse all data sentences, and i tried it.

However, for English,The Stanford Parser has the following 4 model to parse english
- englishFactored.ser.gz 
- englishPCFG.caseless.ser.gz
- englishPCFG.ser.gz 
- englishRNN.ser.gz 

none of  them generates the same tree structure when i use the same dataset, i got three or four children on one node. I wonder which model should i choose to generate the binary structure tree like Stanford Sentiment Treebank or how can i generate such structure?
"
95,https://github.com/stanfordnlp/CoreNLP/issues/128,128,[],closed,2016-01-20 05:52:28+00:00,,hybrid coref missing model issue,"Not sure if the following code is working for random forest for 'hybrid' coref:

 public static String getPathModel(Properties props, String sievename) {
    return props.getProperty(PATH_SERIALIZED_PROP) + File.separator +
        props.getProperty(PATH_MODEL_PROP.replace(""SIEVENAME"", sievename), ""MISSING_MODEL_FOR_""+sievename);
  }
in src\edu\stanford\nlp\hcoref\CorefProperties.java
"
96,https://github.com/stanfordnlp/CoreNLP/issues/129,129,[],closed,2016-01-23 09:43:51+00:00,,PTBTokenizer / PTBLexer hanging in Apache Spark,"Hello!

I'm using CoreNLP (currently 3.6.0, but it was also appearing with 3.5.2) for extracting sentences from a paragraph and extracting dates from the sentences via SUTime. I usually have multiple cores per executor running and create a pipeline in a Scala object for the date extraction, but the text to sentences part is in it's own method. The problem is that quite often a handful of threads in an executor will just keep on running (for hours after which I kill the job) and the spark thread dumper shows the following:

```
edu.stanford.nlp.process.PTBLexer.next(PTBLexer.java:27904)
edu.stanford.nlp.process.PTBTokenizer.getNext(PTBTokenizer.java:279)
edu.stanford.nlp.process.PTBTokenizer.getNext(PTBTokenizer.java:166)
edu.stanford.nlp.process.AbstractTokenizer.hasNext(AbstractTokenizer.java:55)
edu.stanford.nlp.process.DocumentPreprocessor$PlainTextIterator.primeNext(DocumentPreprocessor.java:357)
edu.stanford.nlp.process.DocumentPreprocessor$PlainTextIterator.hasNext(DocumentPreprocessor.java:371)
scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)
scala.collection.Iterator$class.foreach(Iterator.scala:742)
scala.collection.AbstractIterator.foreach(Iterator.scala:1194)
scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
scala.collection.AbstractIterable.foreach(Iterable.scala:54)
scala.collection.TraversableLike$class.filterImpl(TraversableLike.scala:258)
scala.collection.TraversableLike$class.filter(TraversableLike.scala:270)
scala.collection.AbstractTraversable.filter(Traversable.scala:104)
Util.ParserUtils$.extractSentences(ParserUtils.scala:100)
Parser$.extractFromParagraph(Parser.scala:25)
```

My code for that part:

```
  def extractSentences(paragraph: String): List[String] = {
    val reader = new StringReader(paragraph)
    val dP = new DocumentPreprocessor(reader)
    import scala.collection.JavaConversions.asScalaIterator
    val sentences: List[String] = dP.iterator().filter(_.size() < 50).map(s => Sentence.listToString(s)).toList
    sentences
  }
```

Sometimes there is also another thread running with the `TimeExpressionExtractorImpl`, but it's not always the case, so I do not know if it's related. Not much to go on, but maybe someone who knows the code better then I do has an idea.

(By now I know that I could also change my code to have CoreNLP work on a complete paragraph and not extract dates per sentence later on :-) )
"
97,https://github.com/stanfordnlp/CoreNLP/issues/130,130,[],closed,2016-01-27 10:31:58+00:00,,edu.stanford.nlp.parser.nndep.DependencyParser not recogonized,"This folder exist but it is not import in the code .show error . please solve this problem
"
98,https://github.com/stanfordnlp/CoreNLP/issues/131,131,[],closed,2016-01-27 23:46:14+00:00,,Why Naive Bayes Classifier is not available in the latest (3.6.0) jar ?,"Why Naive Bayes Classifier is not available in the latest (3.6.0) jar ? What should I use ?

Thanks!
"
99,https://github.com/stanfordnlp/CoreNLP/issues/132,132,"[{'id': 45387507, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNw==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/cantreproduce', 'name': 'cantreproduce', 'color': 'dddddd', 'default': False, 'description': None}]",closed,2016-01-29 02:23:40+00:00,,Core NLP 3.4 with SR parser ‚Äì Null pointer exception issue,"Hi,
I am working with coreNLP 3.4 models with python wrappers specifically to mimic the project `https://github.com/openeventdata/stanford_pipeline` which is based on 3.4 version and  with SR parser 

I am using following jar files with JAVA 1.7 in place :

```
~/stanford_pipeline$ java -version
java version ""1.7.0_80""
Java(TM) SE Runtime Environment (build 1.7.0_80-b15)
Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)
```

JAR files:

```
stanford-corenlp-3.4.jar
stanford-corenlp-3.4-models.jar
Stanford-srparser-2014-10-23-models.jar 
```

When parsing the sentences, it is throwing Null pointer exception at shiftreduceparser step. I tried with SRparser and beam-SRparser and played with file paths and configurations for a fair amount of time..

Please suggest me possible workaround for this issue.

```
Setting up StanfordNLP. The program isn't dead. Promise.
/home/i57167/stanford-corenlp-jars
/home/i57167
('core nlp jar paths:', ['/home/i57167/stanford-corenlp-jars/stanford-corenlp-3.4.jar', '/home/i57167/stanford-corenlp-jars/stanford-corenlp-3.4-models.jar', '/home/i57167/stanford-corenlp-jars/stanford-srparser-2014-10-23-models.jar'])
('*****************Test*************:', '/home/i57167/stanford_pipeline/src/stanford-corenlp-pywrapper-master/stanford_corenlp_pywrapper/lib')

INFO:StanfordSocketWrap:Starting pipe subprocess, and waiting for signal it's ready, with command:  exec java -Xmx4g -cp /home/i57167/stanford_pipeline/src/stanford-corenlp-pywrapper-master/stanford_corenlp_pywrapper/lib/piperunner.jar:/home/i57167/stanford_pipeline/src/stanford-corenlp-pywrapper-master/stanford_corenlp_pywrapper/lib/guava-13.0.1.jar:/home/i57167/stanford_pipeline/src/stanford-corenlp-pywrapper-master/stanford_corenlp_pywrapper/lib/jackson-all-1.9.11.jar:/home/i57167/stanford-corenlp-jars/stanford-corenlp-3.4.jar:/home/i57167/stanford-corenlp-jars/stanford-corenlp-3.4-models.jar:/home/i57167/stanford-corenlp-jars/stanford-srparser-2014-10-23-models.jar     corenlp.PipeCommandRunner --server 12340  --configfile stanford_config.ini
[Server] Using CoreNLP configuration file: stanford_config.ini
Adding annotator tokenize
INFO:StanfordSocketWrap:Waiting for startup: ping got exception: <class 'socket.error'> [Errno 111] Connection refused
Adding annotator ssplit

Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... INFO:StanfordSocketWrap:Waiting for startup: ping got exception: <class 'socket.error'> [Errno 111] Connection refused
done [3.6 sec].
Adding annotator lemma
Adding annotator parse
INFO:StanfordSocketWrap:Waiting for startup: ping got exception: <class 'socket.error'> [Errno 111] Connection refused
Loading parser from serialized file edu/stanford/nlp/models/srparser/englishSR.beam.ser.gz INFO:StanfordSocketWrap:Waiting for startup: ping got exception: <class 'socket.error'> [Errno 111] Connection refused
INFO:StanfordSocketWrap:Waiting for startup: ping got exception: <class 'socket.error'> [Errno 111] Connection refused
done [32.8 sec].
[Server] Started socket server on port 12340
INFO:StanfordSocketWrap:Successful ping. The server has started.
INFO:StanfordSocketWrap:Subprocess is ready.
Stanford setup complete. Starting parse of 2042 stories...
INFO:stanford:Finished CoreNLP setup.
Processing story 56a964c7becf200b7dd9d5ab. 2016-01-28 18:03:01.842613
INFO:stanford:    Processing story 56a964c7becf200b7dd9d5ab
('Sample text:', 'This is Example Sentence. To Test Stanford NLP')
java.lang.NullPointerException
    at edu.stanford.nlp.parser.shiftreduce.ShiftReduceParserQuery.parseInternal(ShiftReduceParserQuery.java:73)
    at edu.stanford.nlp.parser.shiftreduce.ShiftReduceParserQuery.parse(ShiftReduceParserQuery.java:48)
    at edu.stanford.nlp.pipeline.ParserAnnotator.doOneSentence(ParserAnnotator.java:283)
    at edu.stanford.nlp.pipeline.ParserAnnotator.doOneSentence(ParserAnnotator.java:250)
    at edu.stanford.nlp.pipeline.ParserAnnotator.annotate(ParserAnnotator.java:232)
    at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:67)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:871)
    at corenlp.Parse.processTextDocument(Parse.java:240)
    at corenlp.PipeCommandRunner.runCommand(PipeCommandRunner.java:128)
    at corenlp.PipeCommandRunner.socketServerLoop(PipeCommandRunner.java:172)
    at corenlp.PipeCommandRunner.main(PipeCommandRunner.java:96)
```
"
100,https://github.com/stanfordnlp/CoreNLP/issues/133,133,[],closed,2016-01-31 12:12:19+00:00,,Verb tagged as NNP in parse,"> Jack accompanied Gill.

Results in the following parse:

```
(ROOT\n  (NP (NNP Jack) (NNP accompanied) (NNP Jill.)))
```

_accompanied_ is incorrectly tagged as a proper noun.

However in the dependancy parse it is correctly annotated:
<img width=""213"" alt=""screen shot 2016-01-31 at 12 06 40"" src=""https://cloud.githubusercontent.com/assets/1774239/12702037/aabfb342-c813-11e5-8951-0ed9853296f6.png"">
"
101,https://github.com/stanfordnlp/CoreNLP/issues/134,134,[],closed,2016-01-31 19:41:54+00:00,,Cycles in constituent to dependency conversion for English when-clauses,"Hi, I'm using the constituent to dependency conversion using original, non-collapsed dependencies like this:

```
java -mx2056m -cp ""*;"" edu.stanford.nlp.trees.EnglishGrammaticalStructure -treeFile FILE -originalDependencies -nonCollapsed -conllx
```

and I'm consistently seeing cycles in the output when a construction with tmod+when clause occurs. It looks like for some reason the object of a preposition before 'when' gets misinterpreted and a relative clause is produced instead of advcl. Here's an example from WSJ data - note how 14 has the head 17 and vice versa:

```
1   Last    _   ADJ JJ  _   2   amod    _   _
2   summer  _   NOUN    NN  _   6   tmod    _   _
3   ,   _   PUNCT   ,   _   6   punct   _   _
4   Mr. _   PROPN   NNP _   5   nn  _   _
5   Bush    _   PROPN   NNP _   6   nsubj   _   _
6   moved   _   VERB    VBD _   0   root    _   _
7   the _   DET DT  _   8   det _   _
8   administration  _   NOUN    NN  _   6   dobj    _   _
9   in  _   ADP IN  _   6   prep    _   _
10  the _   DET DT  _   11  det _   _
11  direction   _   NOUN    NN  _   9   pobj    _   _
12  of  _   ADP IN  _   11  prep    _   _
13  gradual _   ADJ JJ  _   14  amod    _   _
14  liberalization  _   NOUN    NN  _   17  tmod    _   _
15  when    _   ADV WRB _   17  advmod  _   _
16  he  _   PRON    PRP _   17  nsubj   _   _
17  told    _   VERB    VBD _   14  rcmod   _   _
18  a   _   DET DT  _   22  det _   _
19  North   _   PROPN   NNP _   22  nn  _   _
20  Atlantic    _   PROPN   NNP _   22  nn  _   _
21  Treaty  _   PROPN   NNP _   22  nn  _   _
22  Organization    _   PROPN   NNP _   23  nsubj   _   _
23  meeting _   NOUN    NN  _   17  xcomp   _   _
24  that    _   SCONJ   IN  _   27  mark    _   _
25  he  _   PRON    PRP _   27  nsubj   _   _
26  would   _   AUX MD  _   27  aux _   _
27  allow   _   VERB    VB  _   17  ccomp   _   _
28  some    _   DET DT  _   29  det _   _
29  exceptions  _   NOUN    NNS _   27  dobj    _   _
30  to  _   ADP TO  _   6   prep    _   _
31  the _   DET DT  _   33  det _   _
32  Cocom   _   PROPN   NNP _   33  nn  _   _
33  embargo _   NOUN    NN  _   30  pobj    _   _
34  of  _   ADP IN  _   33  prep    _   _
35  strategic   _   ADJ JJ  _   36  amod    _   _
36  goods   _   NOUN    NNS _   34  pobj    _   _
37  .   _   PUNCT   .   _   6   punct   _   _
```

Thanks for your help!
Amir
"
102,https://github.com/stanfordnlp/CoreNLP/issues/135,135,[],closed,2016-02-03 22:20:36+00:00,,IndexedWord index not 0-based,"The `index()` method for `IndexedWord` is not 0-based (i.e. the first word in a sentence has index 1, not 0). While this is not really a bug, it makes it very error-prone to work with since accessing words from a `List` will require decrementing the index by one each time.
"
103,https://github.com/stanfordnlp/CoreNLP/issues/136,136,"[{'id': 45387508, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}]",closed,2016-02-04 11:03:10+00:00,,Replicating conll2011 - coreference,"Hi all,
I am trying to replicate the Conll 2011 shared task results for the DEV set.
I am using the most recent version of the conll scorer (8.01).
The scores I get for MUC and CEAF approximately match the ones on the web site and in the paper, but the scores for BCUBED and BLANC are well below the numbers on the web site. (BCUBED = 52.1, BLANC=53.1). The official Conll shared task web site says that the implementation of these scores has been updated since, and you state on the Stanford CoreNLP web site that you used version 4.
Using the new scorer, are my replicated numbers correct?
Any help appreciated!
Many thanks -- Annemarie
"
104,https://github.com/stanfordnlp/CoreNLP/issues/137,137,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",closed,2016-02-06 16:22:06+00:00,,Lemmatization in spanish,"I'm trying to get the lemmas of each word from the provided text, but I couldn't find any docs about it. I'm wondering if there are any particular parameters like with the other annotators, since I'm defining the language for the tokenizer and spanish models for the pos,parse and ner. 

What I'm getting now looks like it didn't find anything and is returning the same word. 

> ""caminando"" => ""caminando"" (should be ""camina"")
> ""arboles"" => ""arboles"" (should be ""arbol"")
> ""corriendo"" => ""corriendo"" (should be ""corre"")
"
105,https://github.com/stanfordnlp/CoreNLP/issues/138,138,[],closed,2016-02-08 04:10:37+00:00,,How to install,"Hi,

I am trying to get the cmd-line stanford tokenizer to run. I haven't used or installed anything java related in about 10 years. How can I get this working?

So far I have tried : 

``` bash
$ git clone thisrepo
$ cd CoreNLP
$ CLASSPATH=stanford-parser.jar java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt
```

I am guessing I somehow need to install something.

Thank you
"
106,https://github.com/stanfordnlp/CoreNLP/issues/139,139,"[{'id': 325778431, 'node_id': 'MDU6TGFiZWwzMjU3Nzg0MzE=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/website', 'name': 'website', 'color': '0052cc', 'default': False, 'description': None}]",closed,2016-02-09 08:45:26+00:00,,SUTime demo code needs updating,"Not sure if this is the right place to report this, but the demo code for using SUTime on the [website](http://nlp.stanford.edu/software/sutime.html) does not compile (`Type mismatch: cannot convert from element type Object to CoreMap`, etc.), but the problems are quickly fixable using generics. Lines 31-33 should be as below:

```
  List<CoreMap> timexAnnsAll = annotation.get(TimeAnnotations.TimexAnnotations.class);
  for (CoreMap cm : timexAnnsAll) {
    List<CoreLabel> tokens = cm.get(CoreAnnotations.TokensAnnotation.class);
```
"
107,https://github.com/stanfordnlp/CoreNLP/issues/140,140,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2016-02-09 08:58:16+00:00,,cryptic NullPointerException with SUTime,"I'm using the demo code from the website (edited as in #139). When you comment out the line adding `WordsToSentencesAnnotator` and `POSTaggerAnnotator`, time annotation works fine. But then when you comment out the line setting the current date, it fails with the following trace:

```
No document date specified
Exception in thread ""main"" java.lang.NullPointerException
    at edu.stanford.nlp.time.TimeAnnotator.annotateSingleSentence(TimeAnnotator.java:241)
    at edu.stanford.nlp.time.TimeAnnotator.annotate(TimeAnnotator.java:230)
    at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:71)
    at SUTimeDemo.main(SUTimeDemo.java:32)
```

Only commenting out the line adding `POSTaggerAnnotator` does not cause the crash. The string I am testing with is ""Tuesday"".

The crash may be caused for some good (unknown) reason, but the error message is misleading, since it makes the user think that SUTime cannot function without the date being set.
"
108,https://github.com/stanfordnlp/CoreNLP/issues/141,141,[],closed,2016-02-10 18:02:34+00:00,,Leap Year issue in calculating relative weeks,"The expression ""next week"" with today (""2016-02-10"") as a reference date, SUtime returns a date range for last year, and not even the correct week. 

I get 

```
<TIMEX3 range=""(2015-02-09,2015-02-15,P1W)"" tid=""t2"" type=""DATE"" value=""2016-W07"">next week</TIMEX3>
```

when using either the sutime online demo or the latest corenlp. 

Perhaps this issue is due to the fact that there is an extra week in February this year?
"
109,https://github.com/stanfordnlp/CoreNLP/issues/142,142,[],closed,2016-02-11 14:01:18+00:00,,Is there any way to lemmatize superlatives with corenlp ?,"Using the MorphaAnnotator or directly Morphology class, ""highest"" doesn't get lemmatized to ""high"" as is done by other lemmatizers (for example treetagger).
Any reason for this behavior ? Any class that can provide the behavior I want in corenlp ?

(nltk can lemmatize it correctly too for example http://textanalysisonline.com/nltk-wordnet-lemmatizer )
"
110,https://github.com/stanfordnlp/CoreNLP/issues/143,143,[],closed,2016-02-11 16:35:19+00:00,,Publish model artifacts under own artifactId instead of using classifier,"Having multiple dependencies with the same groupId/artifactId but different classifiers appears to be a problem for some build systems, e.g. Sbt does not resolve dependencies with classifiers, e.g. Ivy-based ones (e.g. https://github.com/sbt/sbt/issues/285, but also hitting that problem with the Ivy used by Groovy Grapes now.)

It would be great if the model JARs for CoreNLP could be published under separate artifactIDs instead of using the classifier coordinate. That would also facilitate giving them their own version cycles, as many models do not change between CoreNLP releases.
"
111,https://github.com/stanfordnlp/CoreNLP/issues/145,145,"[{'id': 706064425, 'node_id': 'MDU6TGFiZWw3MDYwNjQ0MjU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/algorithm-error', 'name': 'algorithm-error', 'color': 'f9d0c4', 'default': False, 'description': None}, {'id': 706083198, 'node_id': 'MDU6TGFiZWw3MDYwODMxOTg=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/sutime', 'name': 'sutime', 'color': 'c5def5', 'default': False, 'description': None}]",open,2016-02-17 07:44:03+00:00,,SUTime extracts dashed numbers as timex duration,"Using the demo code from the [website](http://nlp.stanford.edu/software/sutime.html) (except I don't specify the document date), and the input ""Call 090-1234-5678"", I get this output:

```
1234-5678 [from char offset 5 to 18] --> (1234-XX-XX,5678-XX-XX,PT38955312H)
```

The Timex attributes are:

```
{tid=t3, value=PT38955312H, type=DURATION, beginPoint=t1, endPoint=t2}
```

This happens to be a Japanese phone number, so I understand that it doesn't work right out-of-the-box. However, for extracting `\d\d\d\d-\d\d\d\d` I would expect the resulting timex to be a range, from year 1234 to year 5678, not a duration consisting of the number of hours in that length of time. The referenced [Timex](http://www.timeml.org/tempeval2/tempeval2-trial/guidelines/timex3guidelines-072009.pdf) spec calls this an ""anchored duration"", and shows it being broken into multiple annotations (page 13), which would be easier to handle.

Even if this does turn out to be incorrectable or unfixable, can you help me find the source of it so I can delete it? I see plenty of duration rules in english.sutime, but they also seem to require other words to match ( like ""the"" and ""year"").
"
112,https://github.com/stanfordnlp/CoreNLP/issues/146,146,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",closed,2016-02-18 07:11:07+00:00,,"SUTime.RelativeTime API does not give access to base, tempOp or tempArg","I am unable to find any programmatic way to access the individual parts of an SUTime.RelativeTime object. Currently I am parsing the contents of `getTimexAttributes().get(""alt_value"")` to get this information.
"
113,https://github.com/stanfordnlp/CoreNLP/issues/147,147,[],closed,2016-02-18 08:42:55+00:00,,Getting TimeoutException for longer sentences,"For longer sentences e.g.

i was thrilled to discover the existence of the destiny farmstay as i felt it would serve the dual purpose of giving the group of adults we were travelling with a chance to unwind while the children would get their first exposure to life on a farm

and annotators: 'tokenize,ssplit,parse,pos,lemma'

I get 

java.util.concurrent.TimeoutException
    at java.util.concurrent.FutureTask.get(FutureTask.java:205)
    at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:389)
    at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
    at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
    at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
    at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
    at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
    at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

Corenlp version 3.6. Running it as a server by running

java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer
"
114,https://github.com/stanfordnlp/CoreNLP/issues/148,148,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2016-02-18 09:08:00+00:00,,edu.stanford.nlp.naturalli.Util.cleanTree(...) function cuts off large chunk of sentence due to self-edge issue,"I encounter this abnormal behavior when trying to perform OpenIE relation extraction on the following sentence:

> Julian served on the Carson City Council and in the California State Assembly before running for Congress in December 1995 in a special election to replace Walter Tucker III (D).

After the Util.cleanTree() method is applied onto the collapsed parse tree, only the following proportion of the sentence is retained:

> Julian served on the Carson City Council and

I realized that this error comes from the fact that the collapsed parse tree introduced a self-edge

> served -[conj:and]-> served'

After this edge is deleted, the copy node ""served'"" and the rest of the dependency structure attached to it got deleted. 

I printed out the sequence of nodes and edges deleted and found out that:
Since ""served"" is the root of the sentence, once the self-edge is removed, the copy node ""served'"" becomes a dangling node. Once ""served'"" is deleted, its dependencies become dangling node, so on and so forth, which causes a large chunk of words got deleted. 

I'm not sure if this is a known bug and you have decided to sacrifice some of these (I didn't check how often this happens) cases to get clean trees. A hacky solution is given below: when the self-edge deletion happens, add the copy node to the set of roots as well, but I'm not sure if that damages other things..

```
    // Clean edges
    Iterator<SemanticGraphEdge> iter = tree.edgeIterable().iterator();
    while (iter.hasNext()) {
      SemanticGraphEdge edge = iter.next();
      if (edge.getDependent().index() == edge.getGovernor().index()) {
        if (tree.getRoots().contains(edge.getGovernor())) {
            tree.addRoot(edge.getDependent());
        }
        // Clean self-edges
        iter.remove();
```
"
115,https://github.com/stanfordnlp/CoreNLP/issues/149,149,[],closed,2016-02-22 23:51:16+00:00,,Dedicated server ignoring annotator list,"  I'm running the CoreNLP dedicated server on AWS and trying to make a request from ruby. The server seems to be receiving the request correctly but the issue is the server seems to ignore the input annotators list and always default to all annotators. My Ruby code to make the request looks like so:

```
uri = URI.parse(URI.encode('http://ec2-************.compute.amazonaws.com//?properties={""tokenize.whitespace"": ""true"", ""annotators"": ""tokenize,ssplit,pos"", ""outputFormat"": ""json""}'))

http = Net::HTTP.new(uri.host, uri.port)
request = Net::HTTP::Post.new(""/v1.1/auth"")
request.add_field('Content-Type', 'application/json')
request.body = text
response = http.request(request)
json = JSON.parse(response.body)
```

  In the nohup.out logs on the server I see the following:

  [/38.122.182.107:53507] API call w/annotators tokenize,ssplit,pos,depparse,lemma,ner,mention,coref,natlog,openie
  ....
  INPUT TEXT BLOCK  HERE
  ....
  [pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [2.0 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse
Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... 
PreComputed 100000, Elapsed Time: 2.259 (s)
Initializing dependency parser done [5.1 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [2.6 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.2 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [7.2 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/defs.sutime.txt
Feb 22, 2016 11:37:20 PM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules
INFO: Read 83 rules
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.sutime.txt
Feb 22, 2016 11:37:20 PM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules
INFO: Read 267 rules
Reading TokensRegex rules from edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
Feb 22, 2016 11:37:20 PM edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor appendRules
INFO: Read 25 rules
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator mention
Using mention detector type: dependency
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref

etc etc. 

Any help as to why this is happening would be appreicated thanks!
"
116,https://github.com/stanfordnlp/CoreNLP/issues/150,150,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2016-02-24 12:30:40+00:00,,Issue when tokenizing tweets,"Hello,

I'm currently using the tokenizer provided by Stanford CoreNLP to tokenize tweets. And there is a strange behavior with hashtags that mix numbers and letters. For example:

```
#Trump #Trump2016 Muhammad Ali Criticises Donald Trump's Call to Ban Muslims From Entering  @FollowNewsNow
```

If you use the online version you can see that the hashtag `#Trump2016` is divided in two tokens `#Trump` and `2016`.

Is it a bug or a wanted behavior for the tokenizer?

Thanks.
"
117,https://github.com/stanfordnlp/CoreNLP/issues/151,151,[],closed,2016-02-24 17:53:59+00:00,,High memory consumption when running openie in simple.Document because of internal cache,"Class `edu.stanford.nlp.simple.Document` seems to be keeping sort of an internal cache in the attributes `customAnnotators`, `annotationPoolKeys` and `annotationPool`.

Because `openie` method consumes a significant ammount of memory, memory is filled up to 4GB with just 10 documents processed.

I think would be nice if we can disable the cache, limit it or provide a method to clear it. Currently I am clearing the cache using reflection after each document is processed with the following code and everything still working:

```
FieldUtils::readStaticField(self.getClass(), ""customAnnotators"", true).clear()
FieldUtils::readStaticField(self.getClass(), ""annotationPoolKeys"", true).clear()
FieldUtils::readStaticField(self.getClass(), ""annotationPool"", true).clear()
```
"
118,https://github.com/stanfordnlp/CoreNLP/issues/152,152,"[{'id': 45387509, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOQ==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/wontfix', 'name': 'wontfix', 'color': 'eeeeee', 'default': True, 'description': None}]",closed,2016-02-28 23:50:48+00:00,,Tokenization inconsistency,"There is an inconsistency in processing dots in the tokenizer:
The following code

```
    val ptbt = new PTBTokenizer(
      new StringReader(""Tokenization is performed.Parameters can be specified.""),
      new CoreLabelTokenFactory(), """")
    while (ptbt.hasNext()) {
      val label = ptbt.next()
      val w = label.originalText()
      println(w)
    }
```

gives ""performed.Parameters"" as an word. If I put a space after 'performed' it will work, but still, natural text can be messy and omitting spaces after punctuation is very common.
"
119,https://github.com/stanfordnlp/CoreNLP/issues/153,153,[],closed,2016-02-29 11:22:06+00:00,,NullPointerException when calling the server,"So, the server fails with NullPointer exception, the console output looks as below.
## How the server was constructed

I had troubles getting the server working by building the git version, so this particular server version was patched like so (which may be the cause):
- Downloaded version 3.6.0, extract.
- Github (mainly to get sentiment working):
  - Downloaded master.
  - `ant build`
  - Copy `javanlp-core.jar` to the extracted downloaded folder.
- Extra models (to get kbp working):
  - Downloaded `http://nlp.stanford.edu/software/stanford-english-corenlp-models-current.jar`
  - Copy to the extracted downloaded folder.

corenlp.run times out on this, so I  can't confirm whether it works there.
## Console output

`
[pool-1-thread-4] INFO CoreNLP - [/127.0.0.1:64231] API call w/annotators tokenize,ssplit,pos,depparse,lemma,ner,parse,entitymentions,mention,coref,sentiment,kbp
A Conservative Welsh assembly election candidate has been criticised for ""offensive"" remarks made on Facebook. Cardiff Central candidate Matt Smith has been reprimanded by his party after comparing the left-wing Respect party to paedophiles. Respect said the comments - made last year when he was a council candidate in Tower Hamlets, London - were ""insulting and offensive"".  The Labour Anglesey candidate has also faced criticism for Facebook comments. Mr Smith, a personal injury lawyer, is also a special adviser on education policy to Nick Bourne, leader of the Conservatives in the assembly. Responding to Mr Smith's comments, Clive Searle, national secretary of Respect, said: ""Matt Smith's comments are both insulting and offensive in that they trivialise paedophilia.  ""Politics should be about debating the real issues affecting our society, not throwing around puerile insults.   ""Respect is very proud of our record in Tower Hamlets - especially of engaging young people in the political process.  ""When so few young people see any point in voting, the Respect Party is proud to have vibrant young members making their voices and concerns heard."" A Conservative spokesman said Mr Smith's comments had been noted, which were made before his selection as an assembly candidate. He added: ""The comments were completely unacceptable and Matt has made it clear he apologises unreservedly for any offence he may have caused. 'Inexperienced candidate' ""We have issued Mr Smith with a warning as to his future conduct, which he has accepted."" The Liberal Democrats, who are defending the Cardiff Central seat, said said: ""The comments speak for themselves and we would not wish to comment further other than to say that the Conservatives' choice of an inexperienced candidate prone to misjudged and immature comments will remind Conservative supporters how completely out of the race the Conservative Party are locally."" A Plaid Cymru spokeswoman said Mr Smith's comments were ""thoroughly tasteless and immoral"". She added: ""However, it seems that the three London parties are intent on turning this election into a mud-slinging contest in order to avoid talking about the issues that are important to the people of Wales."" Welsh Labour declined to comment. The latest news comes after Joe Lock, who is Labour's Anglesey candidate, apologised on Thursday for postings he made last year about the former Conservative Prime Minister Margaret Thatcher.
[pool-2-thread-4] INFO debug-preprocessor - Cannot find node in dependency for word defending
[pool-2-thread-4] INFO debug-preprocessor - Cannot find node in dependency for word said
[pool-2-thread-4] INFO debug-preprocessor - Cannot find node in dependency for word said
[pool-2-thread-4] INFO debug-preprocessor - Cannot find node in dependency for word defending
[pool-2-thread-4] INFO debug-preprocessor - Cannot find node in dependency for word said
[pool-2-thread-4] INFO debug-preprocessor - Cannot find node in dependency for word said
java.util.concurrent.ExecutionException: java.lang.NullPointerException
    at java.util.concurrent.FutureTask.report(FutureTask.java:122)
    at java.util.concurrent.FutureTask.get(FutureTask.java:206)
    at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:400)
    at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
    at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
    at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
    at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
    at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
    at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProto(ProtobufAnnotationSerializer.java:570)
    at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProtoBuilder(ProtobufAnnotationSerializer.java:402)
    at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProto(ProtobufAnnotationSerializer.java:347)
    at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProtoBuilder(ProtobufAnnotationSerializer.java:502)
    at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProto(ProtobufAnnotationSerializer.java:464)
    at edu.stanford.nlp.pipeline.KBPAnnotator.annotate(KBPAnnotator.java:249)
    at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:75)
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:541)
    at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.lambda$handle$299(StanfordCoreNLPServer.java:382)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    ... 3 more
`
"
120,https://github.com/stanfordnlp/CoreNLP/issues/154,154,[],closed,2016-03-01 13:54:35+00:00,,dcoref always returns null,"With `dcoref` annotator, I can not get the CorefChain in the document. I even try running the sample code `StanfordCoreNlpDemo.java` provided in the `stanford-corenlp-full-2015-12-09` but it also does not work. When checking further, I see that the `Annotation` indeed contains the coreference result but with the hash key: `hcoref.CorefCoreAnnotations.CorefChainAnnotation`, not `dcoref.CorefCoreAnnotations.CorefChainAnnotation`. Therefore, the following `corefChains` variable will always return null

```
Map<Integer, CorefChain> corefChains =
                    annotation.get(CorefCoreAnnotations.CorefChainAnnotation.class);
```

Please check and fix this issue.
"
121,https://github.com/stanfordnlp/CoreNLP/issues/155,155,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",closed,2016-03-04 05:56:12+00:00,,The examples files missed in demo,"Hi, In `edu.stanford.nlp.classify.demo.ClassifierDemo.java`, it uses ""examples/cheese2007.prop"", but I can't find files in whole project.

the same with other examples, can't find necessary files.
"
122,https://github.com/stanfordnlp/CoreNLP/issues/156,156,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 735987943, 'node_id': 'MDU6TGFiZWw3MzU5ODc5NDM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/depparse', 'name': 'depparse', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2016-03-09 04:30:45+00:00,,Store language in NN dependency parser model,"The NN dependency parser models currently don't store the language and therefore unless a user specifies the `depparse.language` property it always assumes that the language is `UniversalEnglish`, which in return causes problems in the generation of _collapsed_ and _CCprocessed_ Stanford Dependencies.
"
123,https://github.com/stanfordnlp/CoreNLP/issues/157,157,[],closed,2016-03-09 14:05:28+00:00,,Sentiment prediction demo isn't working,"The demo on http://nlp.stanford.edu:8080/sentiment/rntnDemo.html isn't working due to a permission error:

java.io.IOException: Permission denied
    java.io.UnixFileSystem.createFileExclusively(Native Method)
    java.io.File.createTempFile(File.java:2024)
    com.cs224u.rt.web.batchController.handleRequestInternal(batchController.java:114)

Moreover, there is a code example to find the same sentiment like the Sentiment Treebank?
Because, the sentiment analysis found for CoreNLP was only based on the longest sentence..
Thank

Best regards,
David
"
124,https://github.com/stanfordnlp/CoreNLP/issues/158,158,[],open,2016-03-13 09:40:33+00:00,,Bug in pattern generation?,"I'm reading through the SPIED code, and I think I've found a bug:

Shouldn't the `0` in this line https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/patterns/surface/SurfacePatternFactory.java#L354
be `minWindow4Pattern`, like it is in 
https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/patterns/surface/SurfacePatternFactory.java#L302 ?
"
125,https://github.com/stanfordnlp/CoreNLP/issues/159,159,[],closed,2016-03-13 14:22:42+00:00,,nullpointer expression while running coref in the simple API,"I am getting a nullpointer expression for co-references for a certain complex sentence, though this sentence is parsed fine and co-references work fine for simpler sentences. This is in a scala REPL (ammonite REPL)

``` scala
new Sentence(""""""In mathematics a divisor of an integer n, also called a factor of n, is an integer that can be multiplied by some other integer to produce n."""""").coref 
java.lang.NullPointerException
  edu.stanford.nlp.hcoref.md.CorefMentionFinder.parse(CorefMentionFinder.java:600)
  edu.stanford.nlp.hcoref.md.CorefMentionFinder.findSyntacticHead(CorefMentionFinder.java:491)
  edu.stanford.nlp.hcoref.md.CorefMentionFinder.findHead(CorefMentionFinder.java:411)
  edu.stanford.nlp.hcoref.md.RuleBasedCorefMentionFinder.findMentions(RuleBasedCorefMentionFinder.java:95)
  edu.stanford.nlp.pipeline.MentionAnnotator.annotate(MentionAnnotator.java:79)
  edu.stanford.nlp.simple.Document.coref(Document.java:534)
  edu.stanford.nlp.simple.Document.coref(Document.java:553)
  edu.stanford.nlp.simple.Sentence.coref(Sentence.java:858)
  cmd28$.<init>(Main.scala:251)
  cmd28$.<clinit>(Main.scala:-1)
```

For contrast, here is a command that works (on the same REPL)

``` scala
new Sentence(""John likes his car"").coref 
res29: java.util.Map[Integer, edu.stanford.nlp.hcoref.data.CorefChain] = {2=CHAIN2-[""John"" in sentence 1, ""his"" in sentence 1]}
```
"
126,https://github.com/stanfordnlp/CoreNLP/issues/161,161,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2016-03-19 07:38:32+00:00,,regexner annotator does not work properly with ner annotator,"I'm trying to use `regexner` annotator so that it overwrites some of the entities recognised as `O` or `MISC` from the `ner` annotator. I'm trying to query the corenlp server like the following:

```
/?properties={""annotators"": ""tokenize, ssplit, pos, lemma, ner, regexner"", ""outputFormat"": ""json""}
```

But it does not work as expected. If I only use the `regexner` then it works, but when I wanted to use it along with the `ner` annotator, so that the `regexner` overwrites the recognised entities, it only shows the entities recognised from `ner` as if the `regexner` did nothing. Looking at the corenlp server logs, I found that the annotators are actually reordered while executing like the following: 

```
API call w/annotators tokenize,ssplit,regexner,pos,lemma,ner
```

So I guess the `regexner` is being called before the `ner` violating the order of annotators mentioned by me. Could you guys have a look into this?
"
127,https://github.com/stanfordnlp/CoreNLP/issues/162,162,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2016-03-21 18:10:32+00:00,,Problems with multiple-byte unicodes.,"io.IOutils.slurpInputStream
line nearby 1347.
amountRead is not equal to the length of chunk.
Will append extra 0 to buff.
"
128,https://github.com/stanfordnlp/CoreNLP/issues/163,163,[],closed,2016-03-24 10:20:13+00:00,,Simple coreference not recognized,"Hello,

I'm currently testing the coref annotator with few different sentences (some are easy others are pretty tricky) to see its limitations. Although, I'm impressed of its general accuracy, I'm also surprised that some very easy sentences are not handled. As example, for the sentence: `Barack Obama is an american politician. He was Born in Hawa√Ø.`, no coreference are recognized. I used the default behavior of the `coref` annotator and also with the following options: `coref.md.type=rule coref.mode=statistical coref.doClustering=true`.

Is it a bug coming from the annotator, or there is a property I have missed or not well set?

Thanks!
"
129,https://github.com/stanfordnlp/CoreNLP/issues/164,164,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 735987943, 'node_id': 'MDU6TGFiZWw3MzU5ODc5NDM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/depparse', 'name': 'depparse', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2016-03-25 17:42:53+00:00,,embeddingSize is not a command line option in nn depenency parser,"I am referring to the line [1184 in DependencyParser.java](https://github.com/stanfordnlp/CoreNLP/blob/172fd0a13beef7e885960571379e9f4a8aa8d08a/src/edu/stanford/nlp/parser/nndep/DependencyParser.java#L1184) 

  `*     <code>java edu.stanford.nlp.parser.nndep.DependencyParser -trainFile trainPath -devFile devPath -embedFile wordEmbeddingFile -embeddingSize wordEmbeddingDimensionality -model modelOutputFile.txt.gz</code>`

embeddingSize is not a commandLine option here (its a config option). Instead it should be specified in the props file.

If you specify an embeddingSize other than 50 in the command line, you will get this error 

`The dimension of embedding file does not match`

Though this is a minor issue, a lot of time can be saved for others. 

Thanks a lot for this great resource!
"
130,https://github.com/stanfordnlp/CoreNLP/issues/165,165,[],closed,2016-03-29 10:35:16+00:00,,How to start a CoreNLP Server to segment Chinese text?,"I started a CoreNLP Server with:

```
java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -props StanfordCoreNLP-chinese.properties -port 9000
```

and

```
wget --post-data 'Áõ¥ËßÇÂú∞Ë°®ÊòéËøôÁßçÊï∞ÊçÆÁªìÊûÑÂ¶Ç‰ΩïÁÆÄÂåñÂ∫îÁî®Á®ãÂ∫èÁöÑÂ§ÑÁêÜÊó∂Èó¥ÂíåÂ§çÊùÇÊÄß„ÄÇ' 'localhost:9000/?properties={""annotators"": ""tokenize,ssplit,pos"",""outputFormat"": ""json""}' -O -
```

the response is:

```
{""sentences"":[{""index"":0,""parse"":""SENTENCE_SKIPPED_OR_UNPARSABLE"",""basic-dependencies"":[{""dep"":""ROOT"",""governor"":0,""governorGloss"":""ROOT"",""dependent"":2,""dependentGloss"":""„ÄÇ""},{""dep"":""compound"",""governor"":2,""governorGloss"":""„ÄÇ"",""dependent"":1,""dependentGloss"":""Áõ¥ËßÇÂú∞Ë°®ÊòéËøôÁßçÊï∞ÊçÆÁªìÊûÑÂ¶Ç‰ΩïÁÆÄÂåñÂ∫îÁî®Á®ãÂ∫èÁöÑÂ§ÑÁêÜÊó∂Èó¥ÂíåÂ§çÊùÇÊÄß""}],""collapsed-dependencies"":[{""dep"":""ROOT"",""governor"":0,""governorGloss"":""ROOT"",""dependent"":2,""dependentGloss"":""„ÄÇ""},{""dep"":""compound"",""governor"":2,""governorGloss"":""„ÄÇ"",""dependent"":1,""dependentGloss"":""Áõ¥ËßÇÂú∞Ë°®ÊòéËøôÁßçÊï∞ÊçÆÁªìÊûÑÂ¶Ç‰ΩïÁÆÄÂåñÂ∫îÁî®Á®ãÂ∫èÁöÑÂ§ÑÁêÜÊó∂Èó¥ÂíåÂ§çÊùÇÊÄß""}],""collapsed-ccprocessed-dependencies"":[{""dep"":""ROOT"",""governor"":0,""governorGloss"":""ROOT"",""dependent"":2,""dependentGloss"":""„ÄÇ""},{""dep"":""compound"",""governor"":2,""governorGloss"":""„ÄÇ"",""dependent"":1,""dependentGloss"":""Áõ¥ËßÇÂú∞Ë°®ÊòéËøôÁßçÊï∞ÊçÆÁªìÊûÑÂ¶Ç‰ΩïÁÆÄÂåñÂ∫îÁî®Á®ãÂ∫èÁöÑÂ§ÑÁêÜÊó∂Èó¥ÂíåÂ§çÊùÇÊÄß""}],""openie"":[],""tokens"":[{""index"":1,""word"":""Áõ¥ËßÇÂú∞Ë°®ÊòéËøôÁßçÊï∞ÊçÆÁªìÊûÑÂ¶Ç‰ΩïÁÆÄÂåñÂ∫îÁî®Á®ãÂ∫èÁöÑÂ§ÑÁêÜÊó∂Èó¥ÂíåÂ§çÊùÇÊÄß"",""originalText"":""Áõ¥ËßÇÂú∞Ë°®ÊòéËøôÁßçÊï∞ÊçÆÁªìÊûÑÂ¶Ç‰ΩïÁÆÄÂåñÂ∫îÁî®Á®ãÂ∫èÁöÑÂ§ÑÁêÜÊó∂Èó¥ÂíåÂ§çÊùÇÊÄß"",""lemma"":""Áõ¥ËßÇÂú∞Ë°®ÊòéËøôÁßçÊï∞ÊçÆÁªìÊûÑÂ¶Ç‰ΩïÁÆÄÂåñÂ∫îÁî®Á®ãÂ∫èÁöÑÂ§ÑÁêÜÊó∂Èó¥ÂíåÂ§çÊùÇÊÄß"",""characterOffsetBegin"":0,""characterOffsetEnd"":28,""pos"":""NN"",""ner"":""O"",""speaker"":""PER0"",""before"":"""",""after"":""""},{""index"":2,""word"":""„ÄÇ"",""originalText"":""„ÄÇ"",""lemma"":""„ÄÇ"",""characterOffsetBegin"":28,""characterOffsetEnd"":29,""pos"":""SYM"",""ner"":""O"",""speaker"":""PER0"",""before"":""""}
```

it is not work.
"
131,https://github.com/stanfordnlp/CoreNLP/issues/167,167,[],closed,2016-04-01 08:49:59+00:00,,[OpenIE] - Doesn't work for all sentences,"Steps to Reproduce: 
1. Open http://corenlp.run/ demo
2. Set defaults parameters (parts-of-speech, named entities, dependency parse, openie)
3. Put a sentence, like ""This movie was my best film of my life""
4. Submit

Actual results
No OpenIE results

Expected results
(movie, be, best film) 
"
132,https://github.com/stanfordnlp/CoreNLP/issues/168,168,[],closed,2016-04-02 16:23:06+00:00,,Logging control,"It appears that the only way to remove the debug output from CoreNLP is possible by replacing the error output stream:

``` scala
val err = System.err;

System.setErr(new PrintStream( new OutputStream() {
  override def write(b: Int): Unit = {}
}))

// Do stuff ..

System.setErr(err)
```

It would be awesome if we had more control about that e.g. with log4j
"
133,https://github.com/stanfordnlp/CoreNLP/issues/170,170,[],closed,2016-04-11 23:57:04+00:00,,Sentiment annotator not working with CoreNLP server,"If I try to use the sentiment annotator against the CoreNLP server, for example with the following command,

`wget --post-data 'The quick brown fox jumped over the lazy dog.' 'localhost:9000/?properties={""annotators"":""sentiment"",""outputFormat"":""json""}' -O -`

 I get a `java.lang.IllegalArgumentException: Unknown annotator: sentiment`

Any idea why this would be the case? Thanks.
"
134,https://github.com/stanfordnlp/CoreNLP/issues/171,171,[],closed,2016-04-15 15:42:33+00:00,,Maven dependencies don't include protobuf-java,"The current pom.xml file appearing on Maven Central does not have a dependency to the protobuf-java artifact. As a result, if you want to use a maven dependency, you have to install it yourself.
"
135,https://github.com/stanfordnlp/CoreNLP/issues/172,172,[],closed,2016-04-15 21:40:48+00:00,,tokenize.options are ignored in pipeline.StanfordCoreNLP?,"I cannot make `ptb3Escaping` working as described in the [API](http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/PTBTokenizer.html).

```

java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP \
     -annotators tokenize,ssplit,pos,parse,sentiment \
     -outputFormat json \
     -tokenize.options ptb3Escaping=true,normalizeFractions=true 
```

Then paste ""None-sense with 1/2 colour."". I expect 1/2 and colour to be substituted. Instead I am getting:

```
     ""tokens"": [
        {
          ""index"": 1,
          ""word"": ""None-sense"",
          ""originalText"": ""None-sense"",
          ""characterOffsetBegin"": 0,
          ""characterOffsetEnd"": 10,
          ""pos"": ""JJ""
        },
        {
          ""index"": 2,
          ""word"": ""with"",
          ""originalText"": ""with"",
          ""characterOffsetBegin"": 11,
          ""characterOffsetEnd"": 15,
          ""pos"": ""IN""
        },
        {
          ""index"": 3,
          ""word"": ""1/2"",
          ""originalText"": ""1/2"",
          ""characterOffsetBegin"": 16,
          ""characterOffsetEnd"": 19,
          ""pos"": ""CD""
        },
        {
          ""index"": 4,
          ""word"": ""colour"",
          ""originalText"": ""colour"",
          ""characterOffsetBegin"": 20,
          ""characterOffsetEnd"": 26,
          ""pos"": ""NN""
        },
        {
          ""index"": 5,
          ""word"": ""."",
          ""originalText"": ""."",
          ""characterOffsetBegin"": 26,
          ""characterOffsetEnd"": 27,
          ""pos"": "".""
        }
      ]
```

I guess I am doing something wrong or misinterpreting the API page. 

Thanks.
"
136,https://github.com/stanfordnlp/CoreNLP/issues/173,173,"[{'id': 45387508, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}]",closed,2016-04-18 14:54:39+00:00,,NER requirements,"Hello, i have question about NER. In ""Stanford NER CRF FAQ"" (http://nlp.stanford.edu/software/crf-faq.shtml#pos  question 17) is written that NER don't use POS tags by default. But in `edu/stanford/nlp/pipeline/Annotator.java` there are requirements about it.
"
137,https://github.com/stanfordnlp/CoreNLP/issues/174,174,[],closed,2016-04-21 19:46:28+00:00,,StanfordCoreNLPServer's documentation doesn't mention the timeout parameter ,"The [StanfordCoreNLPServer's documentation](http://stanfordnlp.github.io/CoreNLP/corenlp-server.html) doesn't mention the `timeout` parameter.

It should be:

```
java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer [port] [timeout]
```
"
138,https://github.com/stanfordnlp/CoreNLP/issues/176,176,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 706055902, 'node_id': 'MDU6TGFiZWw3MDYwNTU5MDI=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/tokenize', 'name': 'tokenize', 'color': 'c5def5', 'default': False, 'description': None}, {'id': 706059615, 'node_id': 'MDU6TGFiZWw3MDYwNTk2MTU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/ner', 'name': 'ner', 'color': 'c5def5', 'default': False, 'description': None}]",open,2016-04-25 01:20:45+00:00,,QuantifiableEntityExtractor does not extract certain types when numeric quantity and type are space separated,"With base units supplied here
https://github.com/stanfordnlp/CoreNLP/blob/f569983c8ad4e7890139b77775865cce1b82d4dc/src/edu/stanford/nlp/ie/qe/rules/units.txt

meter, kilogram, liter get extracted regardless if numeric quantity and type are collapsed or separated by space. 10m, 10 m, 3kg 3 kg. Pretty much the rest of the unit types get extracted only
when the quantity and type are collapsed 10lb, 12acre... etc

` AnnotationPipeline pipeline = new AnnotationPipeline();

```
    pipeline.addAnnotator(new TokenizerAnnotator(false,
                                                 ""en""));
    pipeline.addAnnotator(new WordsToSentencesAnnotator(false));
    pipeline.addAnnotator(new POSTaggerAnnotator(DefaultPaths.DEFAULT_POS_MODEL,
                                                 false));
   // pipeline.addAnnotator(new NERCombinerAnnotator(false));
    //pipeline.addAnnotator(new QuantifiableEntityNormalizingAnnotator(false));
    QuantifiableEntityExtractor qex = new QuantifiableEntityExtractor();
    qex.init(new Options());
    List<String> sampleQueries = ImmutableList.of(
                                                  ""50lb cement works"",
                                                  ""50 lb cement does not extract"",
                                                  ""23 KG fish extracts"",
                                                  ""23kg fish extracts"");
 for (String sampleQuery : sampleQueries) {
        Annotation annotations = createDocument(pipeline,
                                                sampleQuery);

        List<MatchedExpression> extract = qex.extract(annotations);
        NumberNormalizer.findAndAnnotateNumericExpressions(annotations);
        for (MatchedExpression matchedExpression : extract) {
            System.out.println(sampleQuery +
                               "": Got expression "" +
                               matchedExpression.getText() +
                               "" with value "" +
                               matchedExpression.getValue());

        }
    }`
```
"
139,https://github.com/stanfordnlp/CoreNLP/issues/178,178,"[{'id': 706064425, 'node_id': 'MDU6TGFiZWw3MDYwNjQ0MjU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/algorithm-error', 'name': 'algorithm-error', 'color': 'f9d0c4', 'default': False, 'description': None}, {'id': 706082923, 'node_id': 'MDU6TGFiZWw3MDYwODI5MjM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/pos', 'name': 'pos', 'color': 'c5def5', 'default': False, 'description': None}]",open,2016-05-03 17:14:24+00:00,,Annotation errors for extraordinary names,"Hi, 

take a look at the following examples. The POS-tagger and the Coref annotator (mention detection) often fail for extraordinary names. Sentences were processed with [1]. Especially the POS-tagger often fails in recognizing names as nouns - even if the sentence starts with a name. I wonder, if this is related to the universal postags. It's maybe harder to assign the correct label for a smaller tagset. I haven't seen such pos-tagging errors with more fine grained tagsets in the past. 

> Sue was nervous about taking the driver's test. She likes apples.

`Sue` is classified as `VB` and not recognized as mention. Therefore `Sue` and `She` are not mapped to the same coreference chain. 

> Lira was so excited to meet her favorite rapper. She had backstage passes for after the concert.

`Lira` is correctly classified as `NN`, but not identified as mention. 

> Coy needed new sneakers. She went to the store and examined their selection.

`Coy` is classified as `JJ` and therefore also not identified as mention. 

> Sunny went with her family to a village. She likes apples. 

`Sunny` is classified as `JJ` again and not identified as mention. 

Best Uli

[1] corenlp.run
"
140,https://github.com/stanfordnlp/CoreNLP/issues/179,179,[],closed,2016-05-04 02:29:30+00:00,,Document.json() Javadoc Clarification,"The code example in the Javadoc for the `json` method in the `Document` class is not correct:
`String json = new Document(""Lucy in the sky with diamonds"").json(Document::parse, Document::ner);`

The `Document` class does not contain methods `parse` or `ner` so this code will not compile. It seems to me that `Sentence::parse` and `Sentence::nerTags` are the correct functions to put here.

Additionally, it seems that the functions supplied to the `json` method are only applied to the first sentence in the document:
`for (Function<Sentence, Object> f : functions) {
      f.apply(this.sentence(0));
    }`

Shouldn't the functions be applied to each sentence in the document since the method is supposed to return ""the JSON string for this document""?
"
141,https://github.com/stanfordnlp/CoreNLP/issues/181,181,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 738203548, 'node_id': 'MDU6TGFiZWw3MzgyMDM1NDg=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/coref', 'name': 'coref', 'color': 'c5def5', 'default': False, 'description': None}]",open,2016-05-04 19:00:26+00:00,,"NER Tag Disambiguation Between ""PERSON""/""PERCENT""","In Mention.java, within the `edu.stanford.nlp.hcoref.data` package, I'm noticing some inconsistencies with how the `ner` types of ""PERSON"" and ""PERCENT"" are handled. As far as I could tell, [this page](http://nlp.stanford.edu/software/CRF-NER.shtml) details the various types possible for NER. 

See below for examples of the mentioned inconsistencies within said Mention.java file:
- Line 290: not sure where `getGender` is called from but ""PERCENT"" would be lumped into this case
- Line 525: I don't believe ""PER"" by itself is a type (as attempted by the second condition)
- Line 621/629: the case for ""PERCENT"" is never reached since it starts with ""PER""
- Line 798: ""PERCENT"" lumped in with ""PERSON"" in this case
- Line 1294/1295: ""PERCENT"" lumped in with ""PERSON"" in this case
"
142,https://github.com/stanfordnlp/CoreNLP/issues/182,182,[],closed,2016-05-05 18:04:57+00:00,,Possibly incorrect LICENSE in repo,"Per http://nlp.stanford.edu/software/ CoreNLP is supposed to be under GPLv3, however the LICENSE.txt file in the repo has GPLv2's text in it.
"
143,https://github.com/stanfordnlp/CoreNLP/issues/183,183,[],closed,2016-05-07 16:44:09+00:00,,German CoreNLP NNDEP parser model is actually a French model,"Looks like the German dependency parser model was mis-packaged.

The following two files...

```
  http://nlp.stanford.edu/software/stanford-french-corenlp-2016-01-14-models.jar
  http://nlp.stanford.edu/software/stanford-german-2016-01-19-models.jar
```

contain the model files

```
  edu/stanford/nlp/models/parser/nndep/UD_French.gz
  edu/stanford/nlp/models/parser/nndep/UD_German.gz
```

which appear to both have the same size and CRC:

```
  UD_German.gz  19.01.2016 04:29:02  24.636.092 bytes  CRC 25830829
  UD_French.gz  14.01.2016 13:36:58  24.636.092 bytes  CRC 25830829
```

... and the same MD5 checksum: c2cd7c55750e2d4dcdd8a16963430c40

Looks like the UD_German.gz model is actually the French model as it contains words such as ""ph√©nom√®ne"".
"
144,https://github.com/stanfordnlp/CoreNLP/issues/185,185,"[{'id': 706064425, 'node_id': 'MDU6TGFiZWw3MDYwNjQ0MjU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/algorithm-error', 'name': 'algorithm-error', 'color': 'f9d0c4', 'default': False, 'description': None}, {'id': 706082923, 'node_id': 'MDU6TGFiZWw3MDYwODI5MjM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/pos', 'name': 'pos', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2016-05-09 16:12:20+00:00,,Unicode General Punctation U+2022 (‚Ä¢) not recognized as punctuation by POS tagger,"Hello Stanford CoreNLP team,

given sentences containing Unicode General Punctation chars U+2022 (‚Ä¢), those char sequences are  tagged wrongly. 

Examples: 

> ‚Ä¢ I couldn't be more proud and excited to team up with an amazing group of people to give the world something really special.

The POS-tagger marked the initial bullet as NN

> I couldn't be more proud and excited to team up with an amazing group of people to give the world something really special. ‚Ä¢ ‚Ä¢ ‚Ä¢

The POS-tagger marked the sequence ‚Ä¢ ‚Ä¢ ‚Ä¢ as NN CD NN

Best,
Antonino
"
145,https://github.com/stanfordnlp/CoreNLP/issues/186,186,[],closed,2016-05-09 21:12:30+00:00,,StanfordCoreNLPServer ignores parse.maxlen and pos.maxlen properties,"It seems like there's an extra negation in the following lines: https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLPServer.java#L572

It should be ""-1"".equals(props.getProperty(""parse.maxlen"", ""60"")), not !""-1"".equals(props.getProperty(""parse.maxlen"", ""60""))

@gangeli @muzny 
"
146,https://github.com/stanfordnlp/CoreNLP/issues/187,187,[],closed,2016-05-12 04:10:30+00:00,,OpenIE fails for some sentences,"Hi,

I use Stanford OpenIE (http://stanfordnlp.github.io/CoreNLP/openie.html) to extract triples from Gigaword corpus. I call ""edu.stanford.nlp.naturalli.OpenIE"" module from Stanford CoreNLP jar files as follows:

```
$ echo ""John was born in the US."" | java -mx1g -cp stanford-corenlp-3.6.0.jar:stanford-corenlp-3.6.0-models.jar:CoreNLP-to-HTML.xsl:slf4j-api.jar:slf4j-simple.jar edu.stanford.nlp.naturalli.OpenIE
```

However, some sentences from Gigaword corpus crash Stanford OpenIE as follows:

```
$ echo ""In the meantime the only road in and out of the city crosses a Bosnian Serb checkpoint."" | java -mx1g -cp stanford-corenlp-3.6.0.jar:stanford-corenlp-3.6.0-models.jar:CoreNLP-to-HTML.xsl:slf4j-api.jar:slf4j-simple.jar edu.stanford.nlp.naturalli.OpenIE
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.8 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse
Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ...
PreComputed 100000, Elapsed Time: 1.606 (s)
Initializing dependency parser done [4.6 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie
Loading clause searcher from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz...done [0.90 seconds]
Processing from stdin. Enter one sentence per line.
Exception in thread ""main"" java.util.NoSuchElementException: No value present
       at java.util.Optional.get(Optional.java:135)
       at edu.stanford.nlp.naturalli.RelationTripleSegmenter.extract(RelationTripleSegmenter.java:282)
       at edu.stanford.nlp.naturalli.OpenIE.annotateSentence(OpenIE.java:485)
       at edu.stanford.nlp.naturalli.OpenIE.lambda$annotate$3(OpenIE.java:554)
       at edu.stanford.nlp.naturalli.OpenIE$$Lambda$24/1197365356.accept(Unknown Source)
       at java.util.ArrayList.forEach(ArrayList.java:1249)
       at edu.stanford.nlp.naturalli.OpenIE.annotate(OpenIE.java:554)
       at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:71)
       at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:499)
       at edu.stanford.nlp.naturalli.OpenIE.processDocument(OpenIE.java:630)
       at edu.stanford.nlp.naturalli.OpenIE.main(OpenIE.java:736)
```

So far, I couldn't find any regularity of sentences that can cause this Java exception. For reference, I also pasted other 9 sentences that can cause the Java exception.
- The official result differed slightly from figures given Sunday, which were 52.2 percent in favour, 46.9 percent against and 0.9 percent blank ballots.
- In the meantime the only road in and out of the city crosses a Bosnian Serb checkpoint.
- ""It seems to be the kind of rehearsed introduction of a Government cave in and another tactical ceasefire by the IRA. Unionists will not be gulled a second time. Unless there is a complete, permanent and universal ceasefire, Unionists will not be taken in by it,"" he added.
- US Ford Motor's three-litre Taurus topped the March sales list, followed by Chrysler Corp.'s Stratus of and Sweden's Volvo, he said.
- A miskick by Rufus Brevett set Marcelo Marcelino clean through but the Brazilian's low strike was forced round the post by Maik Taylor at full stretch.
- It has been described as a law firm with but a single client: the Bill of Rights, or first 10 amendments to the US Constitution guaranteeing individual rights and freedoms.
- The second Audi driven by Japan's Seiji Ara, Denmark's Jan Magnussen and Germany's Marco Werner had been in contention early on but suffered mechanical problems overnight leaving them seven laps behind the leaders.
- Her on-stage antics with religious symbols may have riled believers the world over but Madonna's ticket sales in Roman Catholic Italy appear not to have suffered.
- Cox had undergone a heart lesion operation here three weeks ago but according to the team's website his condition deteriorated on Monday and he was rushed to hospital where he underwent an operation on a bleeding artery.
- The North has proposed holding one-day military talks on easing restrictions on travel in and out of the Seoul-funded estate just north of the border and two- day economic talks on reviving tours by South Koreans to the Mount Kumgang resort in the North from Tuesday.

Of course, it would be happy if the error is fixed. However, the happier solution that I personally think is to let Stanford OpenIE have ""-ignore-errors"" option, which is implemented in Ollie, University of Washington's OpenIE system (https://knowitall.github.io/ollie/). The ""-ignore-errors"" option makes the software more error-tolerant, allowiing us to skip a sentence that causes an error, and just move on to the next sentence. This should be extremely useful for parsing a large file.
"
147,https://github.com/stanfordnlp/CoreNLP/issues/189,189,[],closed,2016-05-15 12:09:53+00:00,,percentageOfTrain parameter in edu.stanford.nlp.ie.machinereading.MachineReading is never used,"Hi ,

   We wanted to fed the training set incrementally , to see the learning graph . But the parameter for that percentageOfTrain  is hashed .

// partitionTrain = keepPercentage(partitionTrain, percentageOfTrain);  

Is there any reason to hash it out ? 
"
148,https://github.com/stanfordnlp/CoreNLP/issues/193,193,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 738202921, 'node_id': 'MDU6TGFiZWw3MzgyMDI5MjE=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/tokensregex', 'name': 'tokensregex', 'color': 'c5def5', 'default': False, 'description': None}]",open,2016-05-24 22:11:39+00:00,,Regex Matching works improperly,"I try next simple code

``` java
        String template = ""[]|[]{2}"";
        TokenSequencePattern pattern = TokenSequencePattern.compile(template);
        Annotation document = new Annotation(""word1 word2"");
        new TokenizerAnnotator(false, ""en"").annotate(document);
        List<CoreLabel> tokens = document.get(CoreAnnotations.TokensAnnotation.class);

        if (pattern.getMatcher(tokens).matches())
            System.out.println(""OK!"");
        else
            System.out.println(""BAD :("");
```

and it prints ""BAD :("" instead of expected ""OK""
"
149,https://github.com/stanfordnlp/CoreNLP/issues/194,194,[],closed,2016-05-26 13:50:20+00:00,,"please, stop write log messages to sterr","You write log messages to stderr, that is why in all libs that use standford NLP people are confused by a lot of ERROR-looking messages, like 

``` bash
root[ERROR] Adding annotator tokenize
root[ERROR] Adding annotator ssplit
root[ERROR] Adding annotator pos
root[ERROR] Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1,2 sec].
root[ERROR] Adding annotator lemma
```

and then they spend time on searching what is wrong while everything is ok =(
"
150,https://github.com/stanfordnlp/CoreNLP/issues/195,195,[],closed,2016-05-26 15:56:31+00:00,,Factored methods do not work in CoreNLP 3.6.0,"Factored methods do not work in CoreNLP 3.6.0. 

The difference appears to come from the way that 3.6.0 and 3.5.2 invoke the parser inside the ParserAnnotator.

3.5.2: 

```
  tree = pq.getBestParse(); 
```

3.6.0:

```
  // todo [cdm 2015]: This should just use bestParse method if only getting 1 best parse.
  List<ScoredObject<Tree>> scoredObjects = pq.getKBestPCFGParses(this.kBest);
```

I checked and pq.getKBestPCFGParses() actually returns the same results for 3.6.0 and 3.5.2.

But apparently getBestParse() does not produce the same result as getting the getKBestPCFGParse() with the highest score.

Full details can be found in this thread on the mailing list: 

https://mailman.stanford.edu/pipermail/java-nlp-user/2016-January/007451.html
"
151,https://github.com/stanfordnlp/CoreNLP/issues/196,196,[],closed,2016-05-27 12:23:30+00:00,,Round brackets are not working in regexner?,"I have a text like ""ABC (XYZ WR) is a the best product launched by ABC company.""

I have added the ""products.txt"" file as a regex NER file and marked ""ABC (XYZ WR)"" as a PRODUCT as below:

ABC (XYZ WR)         PRODUCT

In the output it is not extracting the ABC (XYZ WR) as a PRODUCT.

I tried to escape the ""("" by using ""\"" escape character, but did not get any success.
ABC (XYZ WR)         PRODUCT          (not working)
"
152,https://github.com/stanfordnlp/CoreNLP/issues/197,197,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 383703168, 'node_id': 'MDU6TGFiZWwzODM3MDMxNjg=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/tokenizer', 'name': 'tokenizer', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2016-05-27 21:12:42+00:00,,Tokenization bug on censored profanities," I don't trust myself to poke around the tokenizer, so I'm filing a bug report instead :)

Tokenization fails on censored profanities (e.g., those matching the regexps /f\*+c?k?/ and /sh?\*+i?t?/), tokenizing instead as, e.g., `f *** k` (3 tokens).
"
153,https://github.com/stanfordnlp/CoreNLP/issues/198,198,[],open,2016-05-31 07:28:18+00:00,,Unstable output from parser with RNN model,"As discussed on the [mailing list](https://mailman.stanford.edu/pipermail/java-nlp-user/2016-May/007631.html) and reported in this [issue](https://github.com/dkpro/dkpro-core/issues/852), the parser seems to produce unstable results with the RNN model.

For a test sentence (`This is a test.`), both of the following parses can be output (with an apparent preference to the former one):
1.  `(ROOT (S (NP (DT This)) (VP (VBZ is) (NP (DT a) (NN test))) (. .)))`
2.  `(ROOT (S (NP (DT This)) (VP (VBZ is) (NP-TMP (DT a) (NN test))) (. .)))`

I've created a [test case](https://gist.github.com/carschno/3dd307c5dba4d4f0b74c6ee4597ee9c2) to reproduce the issue. In my experiments, the test fails in approximately 2-4% of all runs.
I'd like to emphasize that this issue is not about the specific parse tree (nor about the `-TMP` suffix); correct or not, I think the parse tree should always be the same for the same input sentence.
"
154,https://github.com/stanfordnlp/CoreNLP/issues/199,199,[],closed,2016-06-01 08:37:37+00:00,,Can numbers be annotated?,"I found in the online documentation the mention of a NumberAnnotator in the pipeline package. I found the source on here on GitHub, and see that the class has been in the code base since September 2014. However, the 3.6.0 code released in January of this year doesn't seem to contain it. Is there currently any way to do number annotation (I'm trying to extract numeric values from spelled words), or is alpha/unreleased?
"
155,https://github.com/stanfordnlp/CoreNLP/issues/200,200,[],open,2016-06-01 16:04:37+00:00,,Unexpected result from TokensRegex,"Hi all,
I use the TokensRegex to match sentences with several pattern, but the priority of the matched result is not consistent with what the document describe. I will take one of the sentence to describe the issue.

_Sentence:_
`History of prior malignancy within the past 5 years except for curatively treated basal cell carcinoma of the skin.`
_pattern file (I set the stage id of ""HISTORY_WITHIN_EF"" to be '5' and ""HISTORY"" to be 6, to let the ""HISTORY_WITHIN_EF"" pattern be matched first):_

```
# Case insensitive pattern matching (see java.util.regex.Pattern flags)
ENV.defaultStringPatternFlags = 2

# Map variable names to annotation keys
ner = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation"" }
normalized = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NormalizedNamedEntityTagAnnotation"" }
tokens = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$TokensAnnotation"" }

# Define ruleType to be over tokens
ENV.defaults[""ruleType""] = ""tokens""

{
  ruleType: ""tokens"",
  pattern: ( ""history"" ""of"" ( []{1,20} ) ),
  action: (Annotate($1, ner, ""CUI_DISEASE_1"")),
  priority:0,
  stage:6,
  matchWithResults: TRUE,
  result: ""HISTORY""
}
{
  ruleType: ""tokens"",
  pattern: ( ""history"" ""of"" ( []{1,20} ) ""within"" ( []{0,5} [ { ner:DURATION } ]+ []{0,5}) ""except"" ""for"" ( []{1,20}) ),
  action: (Annotate($1, ner, ""CUI_DISEASE_3""),Annotate($3, ner, ""CUI_DISEASE_EF"")),
  priority:0,
  stage:5,              
  matchWithResults: TRUE,
  result: ""HISTORY_WITHIN_EF""
}
```

_I slightly change the code as the example shown in the online document (I run the following code after using pipeline: ""annotators"", ""tokenize, ssplit, pos, lemma, ner,parse,depparse"", because I want to match the ner ""DURATION"".):_

```
 List<CoreMap> sentences = ...;
 CoreMapExpressionExtractor extractor = CoreMapExpressionExtractor.createExtractorFromFiles(TokenSequencePattern.getNewEnv(), file1, file2,...);
 for (CoreMap sentence:sentences) {
   List<MatchedExpression> matched = extractor.extractExpressions(sentence);
   ...
 }
```

_Result:_

```
History-1:O 
of-2:O  
prior-3:CUI_DISEASE_1   
malignancy-4:CUI_DISEASE_1  
within-5:CUI_DISEASE_1  
the-6:CUI_DISEASE_1 
past-7:CUI_DISEASE_1    
5-8:CUI_DISEASE_1   
years-9:CUI_DISEASE_1   
except-10:CUI_DISEASE_1 
for-11:CUI_DISEASE_1    
curatively-12:CUI_DISEASE_1 
treated-13:CUI_DISEASE_1    
basal-14:CUI_DISEASE_1  
cell-15:CUI_DISEASE_1   
carcinoma-16:CUI_DISEASE_1  
of-17:CUI_DISEASE_1 
the-18:CUI_DISEASE_1    
skin-19:CUI_DISEASE_1   
.-20:CUI_DISEASE_1  
```

It turns out that it match the pattern of ""HISTORY"", not ""HISTORY_WITHIN_EF"". I also try the attribute of ""priority"" for the pattern, but I still get this result. 

If I remove the pattern of ""HISTORY"", I will get the expected result:

```
History-1:O
of-2:O
prior-3:CUI_DISEASE_3
malignancy-4:CUI_DISEASE_3
within-5:O
the-6:DURATION
past-7:DURATION
5-8:DURATION
years-9:DURATION
except-10:O
for-11:O
curatively-12:CUI_DISEASE_EF
treated-13:CUI_DISEASE_EF
basal-14:CUI_DISEASE_EF
cell-15:CUI_DISEASE_EF
carcinoma-16:CUI_DISEASE_EF
of-17:CUI_DISEASE_EF
the-18:CUI_DISEASE_EF
skin-19:CUI_DISEASE_EF
.-20:CUI_DISEASE_EF
```

Thanks.
"
156,https://github.com/stanfordnlp/CoreNLP/issues/201,201,[],closed,2016-06-04 20:42:57+00:00,,SUTime gives wrong result (2015 rather than 2016),"When I type ""What is on my calendar next week?"" and check ""Include range,"" on the demo web page http://nlp.stanford.edu:8080/sutime/process , the result is ""next week  2016-W23    <TIMEX3 range=""(2015-06-01,2015-06-07,P1W)"" tid=""t1"" type=""DATE"" value=""2016-W23"">next week</TIMEX3>"" which is incorrect. 

It should be 2016, not 2015. The same issue for ""previous week.""

It works for ""What is on my calendar next 2 weeks,"" so maybe I miss something. Thank you very much!

![screenshot from 2016-06-04 16 42 18](https://cloud.githubusercontent.com/assets/8391016/15801975/52d7616c-2a73-11e6-88dd-cbd575616228.png)
"
157,https://github.com/stanfordnlp/CoreNLP/issues/202,202,"[{'id': 706064425, 'node_id': 'MDU6TGFiZWw3MDYwNjQ0MjU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/algorithm-error', 'name': 'algorithm-error', 'color': 'f9d0c4', 'default': False, 'description': None}, {'id': 735987943, 'node_id': 'MDU6TGFiZWw3MzU5ODc5NDM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/depparse', 'name': 'depparse', 'color': 'c5def5', 'default': False, 'description': None}]",open,2016-06-07 10:20:11+00:00,,Dependency parser often fails on multi-word verbs,"Hi,

with transitive phrasal verbs the direct object can appear between the verb and the particle. The dependency parser often fails in these cases and marks the relation as `advmod` instead of `compound:prt`. Here's a list of examples that fail. 
- She put them away.
- Can I try it on?
- Do this homework over.
- They turned the light on.
- They shut the station down.
- She spelled everything out.
- I threw the headphones away
- ...

I tried the examples with the demo at ""http://corenlp.run/"". Not sure, if you are interested in such error classes in your bug tracker. However, I wanted to let you know. 

Best Uli
"
158,https://github.com/stanfordnlp/CoreNLP/issues/203,203,[],closed,2016-06-09 17:13:46+00:00,,Simple Sentence.sentiment() doesn't work without parse().,"``` java
import edu.stanford.nlp.simple.Document;

import java.io.IOException;

public class IssueRepro {
    public static void main(String[] args) throws IOException {
        Document doc = new Document(""foo"");
        doc.sentence(0).sentiment();
    }
}
```

throws 

```
Exception in thread ""main"" java.lang.IllegalStateException: No binarized parse tree (perhaps it's not supported in this language?)
    at edu.stanford.nlp.simple.Document.runSentiment(Document.java:943)
    at edu.stanford.nlp.simple.Sentence.sentiment(Sentence.java:982)
    at edu.stanford.nlp.simple.Sentence.sentiment(Sentence.java:970)
    at com.andrejserafim.IssueRepro.main(IssueRepro.java:10)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
```

Adding parse() helps like so:

``` java
import edu.stanford.nlp.simple.Document;
import edu.stanford.nlp.simple.Sentence;

import java.io.IOException;

public class IssueRepro {
    public static void main(String[] args) throws IOException {
        Document doc = new Document(""foo"");
        Sentence sentence = doc.sentence(0);
        sentence.parse();
        sentence.sentiment();
    }
}
```

This is possibly related to #78 
"
159,https://github.com/stanfordnlp/CoreNLP/issues/205,205,[],closed,2016-06-10 23:49:40+00:00,,Strange behavior of NER options,"I'm running CoreNLP 3.6.0, and getting some strange behavior when I try to configure NER. These CoreNLP options:

Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, entitymentions"");
props.setProperty(""ner.applyNumericClassifiers"", ""false"")
props.setProperty(""ner.useSUTime"", ""false"")
props.setProperty(""ner.markTimeRanges"", ""false"")
props.setProperty(""ner.includeRange"", ""false"")
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

run on this sentence:

""I will be in San Francisco from 6 PM to 9 PM.""

tag ""San Francisco"" as a LOCATION, and that's the only tag, as expected. However, when these options are run on this sentence:

""I will be in San Francisco from January 22nd to January 27th.""

they tag ""San Francisco"" as a LOCATION and January 22nd/January 27th as DATE, even though SUTime is supposed to be switched off. With this set of options:

Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, entitymentions"");
props.setProperty(""ner.applyNumericClassifiers"", ""true"")
props.setProperty(""ner.useSUTime"", ""true"")
props.setProperty(""ner.markTimeRanges"", ""true"")
props.setProperty(""ner.includeRange"", ""true"")
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

the sentence ""I will be in San Francisco from 6 PM to 9 PM."" now includes the times, but it and the sentence ""I will be in San Francisco from January 22nd to January 27th."" have two DATE tags instead of one DURATION tag, even though ""markTimeRanges"" is set to true. ""markTimeRanges"" is supposed to be an SUTime option, but when I tried this set of options:

Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, entitymentions"");
props.setProperty(""ner.applyNumericClassifiers"", ""true"")
props.setProperty(""ner.useSUTime"", ""true"")
props.setProperty(""sutime.markTimeRanges"", ""true"")
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

I got this error:

sutime.markTimeRanges=true
Unknown property: |sutime.markTimeRanges|

while the pipeline was being loaded (before any text had been parsed). 
"
160,https://github.com/stanfordnlp/CoreNLP/issues/206,206,[],closed,2016-06-12 21:47:07+00:00,,Percentage unparsable,"Hi,

the following sentence `Waiter earned his standard 15%.` yields

> {""sentences"":[{""index"":0,""line"":1,""parse"":""SENTENCE_SKIPPED_OR_UNPARSABLE"" [...]

for the web-interface [1] and also for the latest git version (tested commit hash e25f46f76af78cdcc06fdf20b6df3f57c4f1b032). 

Command:

> wget --tries=2  --post-data  ""Waiter earned his standard 15%."" 'localhost:1337/?properties={""ssplit.eolonly"": ""true"", ""annotators"": ""tokenize,ssplit,pos,lemma,ner,depparse"",""outputFormat"": ""json"",'timeout': 9000000}' -O res.json

Best Uli

[1] http://corenlp.run/
"
161,https://github.com/stanfordnlp/CoreNLP/issues/207,207,[],closed,2016-06-15 10:01:22+00:00,,How to deal with Chinese words with Stanford NLP,"how to deal with Chinese words with Stanford NLP that like English pipeline ?
"
162,https://github.com/stanfordnlp/CoreNLP/issues/208,208,[],closed,2016-06-17 01:06:35+00:00,,CoreNLP 3.6.0 not compatible with recent versions of Jollyday,"When I downloaded Jollyday off Maven, I got this error and stack trace from CoreNLP:

https://gist.github.com/rationalism/300d00802906a6332c41a1006c756c3d

The exception seems to be caused by having downloaded the latest version (0.5.1). Downgrading to 0.4.7 fixed the problem, but 0.4.7 is four years old now. 
"
163,https://github.com/stanfordnlp/CoreNLP/issues/210,210,[],closed,2016-06-21 15:08:58+00:00,,Using languages other than English,"Hi, 

I've followed the [instructions](http://stanfordnlp.github.io/CoreNLP/cmdline.html#languages-other-than-english) using different languages than English, but still having trouble to set it up properly. 

I've:
- Edited the pom file
- Downloaded the extra model
- Executed `java -mx3g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -props StanfordCoreNLP-german.properties -file german.txt -outputFormat text` at the root dir with the model file in place

I'm sure that I've missed something. It complains about not finding the german model file: Unable to open ""StanfordCoreNLP-german.properties"" as class path, filename or URL

Any help is greatly appreciated. 
"
164,https://github.com/stanfordnlp/CoreNLP/issues/212,212,[],closed,2016-06-27 04:50:20+00:00,,StanfordCoreNLP initialize takes really a long time,"`nlp = new StanfordCoreNLP(StanfordCoreNLP-chinese.properties);`
i use the StanfordCoreNLP to write a simple [chatbot](https://github.com/xsank/Shour), but it takes really a long time to new CoreNLP, always nearly 90 seconds.
will you consider about this? thank you.
"
165,https://github.com/stanfordnlp/CoreNLP/issues/213,213,[],closed,2016-06-29 08:23:07+00:00,,Problem with non ASCII characters,"Hi there,

I am using CoreNLP Server and start it with 
`java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -prettyPrint false 2&>1 >/dev/null`

Whenever I send text containing a non ascii character to the server, the server does not return a valid JSON Object.
When I call the server from Python using pycorenlp (https://github.com/smilli/py-corenlp) and using a German NER model for German text with non ASCII characters, a string is returned. If I try to convert the string to an JSON Object using json.loads, I get the following error:
ValueError: Invalid control character at: line 1 column 3223 (char 3222) and the character is escaped 
`""hei√Ée""` instead of `hei√üe`.

Then I tested the server on http://localhost:9000/ (english sentence with non ASCII character) here the result:
![bildschirmfoto 2016-06-29 um 10 04 14](https://cloud.githubusercontent.com/assets/12842873/16444953/e7bc5546-3de0-11e6-8256-85805e80d60c.png)

When I tested the same sentence on http://corenlp.run/ it works!
![bildschirmfoto 2016-06-29 um 10 05 50](https://cloud.githubusercontent.com/assets/12842873/16445029/1abed5fe-3de1-11e6-80b7-dfc406dad701.png)

So what am I missing?

I also tried the -strict flag when starting the server, then the named entities containing a non ASCII character are not recognized.

Thanks for your help,
Johanna
"
166,https://github.com/stanfordnlp/CoreNLP/issues/214,214,[],closed,2016-06-29 13:44:05+00:00,,Similar tool with liberal license?,"I can't use this library because it's GPLv3. Does anyone know of a similar open source library with, uh, more liberal license terms?
"
167,https://github.com/stanfordnlp/CoreNLP/issues/215,215,[],closed,2016-07-06 00:52:10+00:00,,"NER consistently tags the word ""score"" as 20.0","To reproduce, run Stanford CoreNLP from the command line on the following input:

```
The student got a pretty good score.
The current score is 2 to 1.
an IQ score of 161
```

When the following command is run:

```
java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner -file input.txt
```

All occurrences of ""score"" are tagged as NUMBER with value 20.0.
"
168,https://github.com/stanfordnlp/CoreNLP/issues/216,216,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 735987943, 'node_id': 'MDU6TGFiZWw3MzU5ODc5NDM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/depparse', 'name': 'depparse', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2016-07-06 11:46:33+00:00,,-tlp properties ignored in DependencyParser constructor,"I was trying to train a dependency parser for Spanish with universal dependencies.
Following instruction in this page http://nlp.stanford.edu/software/nndep.shtml i just implemented a new language pack and passed its path as tlp parameters.
I'm using java api, so I build a Properties object with "" tlp"" keys and the full qualified path of my language pack.
At the end of the training (even if it seams to perform well on dev set) it perform very bad on the test set, so i started to debug and I found a couple of strange things in DependencyParser.java and Config.java file:
1)      this.language = config.language; (DependencyParser.java:line 123) 
here a language is assigned from the configuration (which decide to return the default one if no other one have been set). This is not a big issue but would be better if you give a mention of this parameter in the instruction page. 
2)language = props.containsKey(""language"")? getLanguage(props.getProperty(""language"")) : language;
    tlp = language.params.treebankLanguagePack(); (Config.java: 237-240)
Here the tlp object is assigned from the set language. This rise 2 problems:
**first**: given that no mention of the language parameter was given in the instructions then the standard language is chose and so also the associated language pack is returned in the last line.
**second**: the parameter ""tlp"" set in the properties object is never used, so is totally useless pass the class path as value of the key ""tlp"". This mean that you can only choose an already implemented LanguagePack (the ones listed in Language.java in which a TregexPoweredTreebankParserParams object is assigned to each supported language). 

Please let me know if i'm getting something wrong or if i missed somthing! 

Thanks a lot

Tommaso
"
169,https://github.com/stanfordnlp/CoreNLP/issues/217,217,[],open,2016-07-07 23:37:56+00:00,,Publish checksum (md5/sha1/sha256) for releases?,"I'm wondering if the checksums of the release .zip files could also be published as they seem to get updated from time to time.  I'm trying to automate the installation from [a CoreNLP wrapper in DeepDive](https://github.com/HazyResearch/deepdive/blob/b77a8bc97b37ccd98affd405835ff38ba2f9628e/util/nlp/deepdive-corenlp-install#L10) and seeking a stable way to check the integrity of the download.

The practice seems to vary across projects, but one very simple way may be to just publish a few extra files with `.sha1` and `.md5` suffixes.  Graphviz seems to be following that:
- http://graphviz.org/pub/graphviz/stable/SOURCES/graphviz-2.38.0.tar.gz
- http://graphviz.org/pub/graphviz/stable/SOURCES/graphviz-2.38.0.tar.gz.md5

On the other hand, MariaDB publishes output of `sha1sum` (and `md5sum`) commands in a `sha1sums.txt` (and `md5sums.txt`, respectively) and sign those with GnuPG in `.asc`:
- http://ftp.kaist.ac.kr/mariadb/mariadb-10.2.1/source/
- http://ftp.kaist.ac.kr/mariadb/mariadb-10.2.1/source/sha1sums.txt
- http://ftp.kaist.ac.kr/mariadb/mariadb-10.2.1/source/sha1sums.txt.asc

Some others only publish GnuPG signatures in `.sig` files, most notably all GNU projects.  I think providing checksums will be more useful as `gpg` isn't universally available, but `sha1sum` or `shasum` is available on Mac as well as Linux:
- http://ftp.gnu.org/gnu/coreutils/
- http://ftp.gnu.org/gnu/coreutils/coreutils-8.25.tar.xz
- http://ftp.gnu.org/gnu/coreutils/coreutils-8.25.tar.xz.sig
"
170,https://github.com/stanfordnlp/CoreNLP/issues/218,218,[],closed,2016-07-10 02:33:00+00:00,,ProtobufAnnotationSerializer does not support MentionsAnnotation,"I encountered this issue when calling the CoreNLP server with the `entitylink` annotator and using 

```
'outputFormat': 'serialized',
'serializer': 'edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer'
```

The error:

```
edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer$LossySerializationException: Keys are not being serialized: class edu.stanford.nlp.ling.CoreAnnotations$MentionsAnnotation
        at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProto(ProtobufAnnotationSerializer.java:350)
        at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProtoBuilder(ProtobufAnnotationSerializer.java:504)
        at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProto(ProtobufAnnotationSerializer.java:466)
        at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.write(ProtobufAnnotationSerializer.java:177)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$createOutputter$78(StanfordCoreNLP.java:926)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:488)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
        at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
        at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
        at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```

I think `WikidictAnnotator` uses `edu.stanford.nlp.ling.CoreAnnotations$MentionsAnnotation`.
"
171,https://github.com/stanfordnlp/CoreNLP/issues/219,219,[],closed,2016-07-12 20:39:21+00:00,,"UD parse has ""compound"" instead of ""name""","@ida-szubert has discovered that CoreNLP, including the [online demo](http://corenlp.run/), systematically produces `compound` dependencies which should be [`name`](http://universaldependencies.org/en/dep/name.html) relations.

E.g.: For ""I met Carl XVI Gustaf."", even though the NER correctly recognizes ""Carl XVI Gustaf"" as a PERSON, the basic and enhanced++ UD parses have `compound(Gustaf, Carl)` and `compound(Gustaf, XVI)`.

Is this fixable?
"
172,https://github.com/stanfordnlp/CoreNLP/issues/220,220,[],closed,2016-07-13 11:48:18+00:00,,Outdated LICENSE.txt file?,"The README.md file in this repo states that

> The Stanford CoreNLP code is written in Java and licensed under the GNU General Public License (v3 or later).

However, the LICENSE.txt file still contains the GPL v2 license text.

It is unclear to the user which statement is correct.
"
173,https://github.com/stanfordnlp/CoreNLP/issues/221,221,[],closed,2016-07-13 13:37:00+00:00,,CoreNLP Server does not support Chinese?,"I downloaded the CoreNLP 3.6.0. I am able to have a right result using this command:

```
java -Xmx6g edu.stanford.nlp.pipeline.StanfordCoreNLP -props StanfordCoreNLP-chinese.properties -file chinese-example-2.txt -outputFormat text
```

But when I start it as a server, like

```
java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -props StanfordCoreNLP-chinese.properties
```

and use wget:

```
wget   --post-data '‰Ω†ÊòØË∞ÅÔºü' 'localhost:9000/?properties={""annotators"":""segment,pos"",""outputFormat"":""json""}' -O -
```

It will go wrong saying ""unknown annotators: segment"".

I have searched all the results on the Internet and I found [an article](http://hujiaweibujidao.github.io/blog/2016/04/01/Head-First-Standford-NLP-4/) pointing out that the source code has problems. The part of server code does not receive the props parameter correctly and it just loads the default props.
"
174,https://github.com/stanfordnlp/CoreNLP/issues/223,223,"[{'id': 706056248, 'node_id': 'MDU6TGFiZWw3MDYwNTYyNDg=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/ssplit', 'name': 'ssplit', 'color': 'c5def5', 'default': False, 'description': None}, {'id': 706064425, 'node_id': 'MDU6TGFiZWw3MDYwNjQ0MjU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/algorithm-error', 'name': 'algorithm-error', 'color': 'f9d0c4', 'default': False, 'description': None}]",open,2016-07-16 14:07:02+00:00,,Non-uniform tokenization of sentences having dialogue,"A sentence which has quoted as well as non-quoted words in it is not parsed uniformly.

Given sentences such as-
""Where were you?"" asked Mary angrily.

It will parse roughly half the sentences as one sentence -
1. ""Where were you?"" asked Mary angrily.

and the other half as -
1. ""Where were you?""
2. asked Mary angrily.

This occurs when the following code is executed (in the most recent version)- 

```
             Properties props = new Properties();
             props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, depparse"");

             pipeline = new StanfordCoreNLP(props);

         Annotation document = new Annotation(doc);
             pipeline.annotate(document);

             List<CoreMap> sentences = document.get(SentencesAnnotation.class);

```
"
175,https://github.com/stanfordnlp/CoreNLP/issues/224,224,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",closed,2016-07-18 01:57:09+00:00,,Lemmatization of 'dunno',"It seems that the Stanford Tokenizer now lemmatizes 'dunno' as 'du nno', but it would be nice if the lemmatizer also did something reasonable with this. Perhaps tokenizing as 'du n no' and lemmatizing as 'do not know'? Or 'du nno' as 'not know'? Are these sorts of contextual lemmatizations possible in our Morpha class?

Happy to poke around the Morpha file if anyone knows of a similar case I can use as reference.
"
176,https://github.com/stanfordnlp/CoreNLP/issues/226,226,"[{'id': 706064425, 'node_id': 'MDU6TGFiZWw3MDYwNjQ0MjU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/algorithm-error', 'name': 'algorithm-error', 'color': 'f9d0c4', 'default': False, 'description': None}, {'id': 706094173, 'node_id': 'MDU6TGFiZWw3MDYwOTQxNzM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/lemma', 'name': 'lemma', 'color': 'c5def5', 'default': False, 'description': None}]",open,2016-07-19 10:41:25+00:00,,JJR and JJS tokens are not lemmatized properly,"When using the `tokenize,ssplit,pos,lemma` annotators, all tokens are lemmatized (e.g. `likes` becomes `like VBZ`), but comparative and superlative adjectives are not: e.g. `heavier` becomes `heavier JJR` and `best` is analyzed as `best JJS` (instead of `heavy JJR` and `good JJS`, respectively).

I was using the caseless POS model as my input was in lower case, but did not specify any other options.
"
177,https://github.com/stanfordnlp/CoreNLP/issues/227,227,[],closed,2016-07-20 18:29:09+00:00,,Universal dependencies giving incorrect root,"The sentence 

> The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24‚Äì10 to earn their third Super Bowl

 has the root being 'champion' when it should be 'defeated'

I'm using the latest maven release (3.6.0)
"
178,https://github.com/stanfordnlp/CoreNLP/issues/228,228,[],closed,2016-07-22 12:16:06+00:00,,the parser online demo issue,"the online demo is not working, please fix it. 
I want try something urgently.
"
179,https://github.com/stanfordnlp/CoreNLP/issues/231,231,[],closed,2016-07-22 19:04:47+00:00,,Getting coreference head with json output,"When running CoreNLP from the command line with `coref` annotator, each `mention` in the coreference output in the XML points to a `head`. If outputting to `json`, however, we don't seem to get that:

```
...
 u'50': [{u'animacy': u'UNKNOWN',
   u'endIndex': 10,
   u'gender': u'UNKNOWN',
   u'id': 50,
   u'isRepresentativeMention': True,
   u'number': u'SINGULAR',
   u'position': [8, 5],
   u'sentNum': 8,
   u'startIndex': 8,
   u'text': u'the Dish-licker',
   u'type': u'PROPER'}]
...
```

I'm aware that `json` output may not be lossless, but this is a very useful bit of data. Is there any way to get `head` in there?
"
180,https://github.com/stanfordnlp/CoreNLP/issues/232,232,"[{'id': 325778431, 'node_id': 'MDU6TGFiZWwzMjU3Nzg0MzE=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/website', 'name': 'website', 'color': '0052cc', 'default': False, 'description': None}]",closed,2016-07-25 11:44:16+00:00,,Website doesn't work over HTTPS,"Using duckduckgo.com takes me to the https version of https://stanfordnlp.github.io/CoreNLP/ and this does not load stylesheets and such because they have hardcoded addresses beginning with http:// 

You should replace 
`http://stanfordnlp.github.io/CoreNLP/css/syntax.css`
With:
`//stanfordnlp.github.io/CoreNLP/css/syntax.css`

this way the browser will use the appropriate protocol and load the files.

I would write a PR but I couldn't find the website on GitHub
"
181,https://github.com/stanfordnlp/CoreNLP/issues/233,233,[],closed,2016-07-25 15:49:40+00:00,,Duplicate nodes in dependecy graph,"Running this:

``` java
    public static void main(final String[] args) {
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        String text = ""Which birds are protected under the National Parks and Wildlife Act?"";

        Annotation document = new Annotation(text);

        pipeline.annotate(document);

        List<CoreMap> sentences = document.get(SentencesAnnotation.class);
        CoreMap sentence = sentences.get(0);

        SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);

        System.out.println(dependencies.toString());
    }
```

creates output:

```
-> protected/VBN (root)
  -> birds/NNS (nsubjpass)
    -> Which/WDT (det)
  -> are/VBP (auxpass)
  -> Parks/NNP (nmod:under)
    -> under/IN (case)
    -> the/DT (det)
    -> National/NNP (compound)
    -> and/CC (cc)
    -> Act/NNP (conj:and)
      -> Wildlife/NNP (compound)
  -> Act/NNP (nmod:under)
  -> ?/. (punct)
```

So it duplicates the ""Act"" node. This is not right, is it?
"
182,https://github.com/stanfordnlp/CoreNLP/issues/235,235,"[{'id': 706064425, 'node_id': 'MDU6TGFiZWw3MDYwNjQ0MjU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/algorithm-error', 'name': 'algorithm-error', 'color': 'f9d0c4', 'default': False, 'description': None}, {'id': 706083198, 'node_id': 'MDU6TGFiZWw3MDYwODMxOTg=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/sutime', 'name': 'sutime', 'color': 'c5def5', 'default': False, 'description': None}]",open,2016-07-26 13:43:43+00:00,,"getRange() of SUTime.Temporal created with ""this week"" returns date with wrong year","Looks like an off-by-one error somewhere. 

When I initialize a TimeExpression with strings containing ""week"" such as ""this week"", ""last week"", ""next week"", the TimeExpression itself is correct, but then when I try to get the beginning of the range (see code) I get the correct week, but a year earlier (i.e. now for 2016-07-26 when I say ""this week"" I get 2016-07-W30 from TimeExpression, but 2015-07-20 from TimeExpression.getRange.begin)

Please note that stuff like ""next monday"" ""today"" ""tomorrow"" ""last wednesday"" works as expected.

```
import java.time.LocalDate

import edu.stanford.nlp.ling.CoreAnnotations
import edu.stanford.nlp.pipeline.{Annotation, AnnotationPipeline, POSTaggerAnnotator, TokenizerAnnotator, WordsToSentencesAnnotator}
import edu.stanford.nlp.time.{TimeAnnotations, TimeAnnotator, TimeExpression}

import scala.collection.JavaConversions._

val annotationPipeline = new AnnotationPipeline()
annotationPipeline.addAnnotator(new TokenizerAnnotator(false))
annotationPipeline.addAnnotator(new WordsToSentencesAnnotator(false))
annotationPipeline.addAnnotator(new POSTaggerAnnotator(false))
annotationPipeline.addAnnotator(new TimeAnnotator())

val annotation = new Annotation(""this week"")
annotation.set(classOf[CoreAnnotations.DocDateAnnotation], LocalDate.now.toString)
annotationPipeline.annotate(annotation)

val timeAnnotations = annotation.get(classOf[TimeAnnotations.TimexAnnotations]).map { ann =>
  ann.get(classOf[TimeExpression.Annotation]).getTemporal
}

println(timeAnnotations) // <- correct
println(timeAnnotations.head.getRange.begin) // <- off by one year
```
"
183,https://github.com/stanfordnlp/CoreNLP/issues/236,236,[],closed,2016-07-27 18:45:47+00:00,,Some enhanced++ UD edges missing in online demo,"I've been noticing that the [online demo](http://corenlp.run/) isn't finding some of the enhanced++ dependencies described in the [LREC 2016](http://www.lrec-conf.org/proceedings/lrec2016/pdf/779_Paper.pdf) paper, such as `nsubj:xsubj` in ""Sue wants to buy a hat"", the `nsubj` in ""the boy who lived"", the second `amod` in ""the long and wide river"", and the copy node/edges in ""She flew to Bali or to Turkey"". Is this just a problem with the rendering on the demo page, or are these not being produced?
"
184,https://github.com/stanfordnlp/CoreNLP/issues/237,237,[],closed,2016-08-02 05:59:11+00:00,,is there any way to remove stop words from stopwords.txt document,"Stopwords.txt contains word ""CANNOT"". My sample text is as follows 
""THIS IS FOR TESTING CANNOT"".

In above line cannot is a single word, but NLP is dividing it as two different words like CAN and NOT. But what am expecting is CANNOT as a single word.
Is there a way to remove words which am not interest from the stopwords.txt document.

Thank you
"
185,https://github.com/stanfordnlp/CoreNLP/issues/238,238,[],closed,2016-08-02 19:57:32+00:00,,Stanford NLP Server:: Unknown annotator: sentiment,"I just downloaded and ran Standfor NLP 3.6.0 http://stanfordnlp.github.io/CoreNLP/index.html by using the following command:

`java -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer --public --port 1062`
I tested other annotators like POS, Tokenizer and they work just ok. However, when I try to run sentiment annotator[ http://nlp.stanford.edu/sentiment/ ] I get following errors:

```

java.lang.IllegalArgumentException: Unknown annotator: sentiment
    at edu.stanford.nlp.pipeline.StanfordCoreNLP.ensurePrerequisiteAnnotators(StanfordCoreNLP.java:281)
    at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.getProperties(StanfordCoreNLPServer.java:476)
    at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:350)
    at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
    at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
    at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
    at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
    at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
    at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

I uploaded all the files I got from the download page[http://stanfordnlp.github.io/CoreNLP/index.html#download] not sure what am I missing?
"
186,https://github.com/stanfordnlp/CoreNLP/issues/239,239,[],closed,2016-08-05 15:34:01+00:00,,[Sentiment] - sentiment.model property not used,"Hi!

I followed the documentation to retrain the sentiment model with a numHid=25

`java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath dev.txt -train -model model.ser.gz`

After 16 hours of training, I exported my model to my project, and I imported with this line:

`props.put(""sentiment.model"", ""src/main/resources/model.ser.gz"");`
or 
`props.setProperty(""sentiment.model"", ""model.ser.gz"");`

But the custom model is never used:

> [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
> [main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
> [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
> [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
> [main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... 
> done [1.4 sec].
> [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator sentiment
> This movie doesn't care about cleverness, wit or any other kind of intelligent humor.
> Negative
> 1
> Those who find ugly meanings in beautiful things are corrupt without being charming.
> Negative
> 1
> There are slow and repetitive parts, but it has just enough spice to keep it interesting.
> Positive
> 3

I tried with `props.setProperty(""sentiment.model"", ""test39randomName.ser.gz"");`
And no error during the run.

My code is:

```
String paragraph = The quick brown fox jumps over the lazy dog;
Annotation document = new Annotation(paragraph);
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, parse, sentiment"");
props.setProperty(""sentiment.model"", ""src/main/resources/model.ser.gz"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
pipeline.annotate(document);
for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {
            System.out.println(""---"");
            System.out.println(sentence);
            //System.out.println(sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class));
            Tree tree = sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class);
            int sentiment = RNNCoreAnnotations.getPredictedClass(tree);
            System.out.println(sentence.get(SentimentCoreAnnotations.SentimentClass.class));
            System.out.println(sentiment);
}
```
"
187,https://github.com/stanfordnlp/CoreNLP/issues/240,240,[],closed,2016-08-07 09:45:31+00:00,,Lack of documentation on how to use CoreNLP server options,"On the documentation page (http://stanfordnlp.github.io/CoreNLP/corenlp-server.html), it's not clear how to specify the timeout and the port number of the CoreNLP server.
I've tried:
`timeout 30000`, `timeout=30000`, `--timeout 30000`, `--timeout=30000` before finding it's `-timeout 30000`. I think the way options should be specified must be present in the documentation.
"
188,https://github.com/stanfordnlp/CoreNLP/issues/241,241,[],closed,2016-08-07 10:12:33+00:00,,Incorrect addition of unquoted null characters in JSON response from CoreNLP server,"I've used CoreNLP server for annotation tasks (tokenization, ssplit and pos).
However, sometimes the JSON response from the server is invalid as it contains unquoted null characters, even when there are no \x00 in the original text.
Here is the Python script that reproduces the error:

`    # coding: utf-8

```
import json
from urllib.parse import urljoin, quote
import requests

text = """"""I am overweight as well but wonder why it is just my left ankle and not my left.¬†¬†Could it be thyroid issues as I have many symptoms of that as well???""""""


def annotate_corenlp(text, annotators=""tokenize,ssplit,pos"", output_format='json', url=""http://localhost:9000""):
    payload_str = json.dumps({""annotators"": annotators, ""outputFormat"": output_format})

    full_url = urljoin(url, quote(payload_str))
    r = requests.post(full_url, data=text.encode('utf-8'))

    content = r.content.decode('utf-8')
    return json.loads(content)

annotate_corenlp(text)`
```

The JSON parser fails, because the \x00 are unquoted in the JSON response :
{""index"":15,""word"":""???"",""originalText"":""???"",""lemma"":""???"",""characterOffsetBegin"":148,""characterOffsetEnd"":151,""pos"":""IN"",""ner"":""O"",""speaker"":""PER0"",""before"":"""",""after"":""\x00\x00""}

I've checked the request body, and the text is formatted properly, without \x00\x00 at its end.
Surprisingly, the error disappears when removing no-break spaces (\xa0) in the text.
"
189,https://github.com/stanfordnlp/CoreNLP/issues/242,242,[],closed,2016-08-09 10:24:52+00:00,,Arabic dialect profiles,"Hi ,
I have created profiles for arabic dialects
included
1-syrian
2-egyprian
i have tested it and it work correctly 
is it usefull for you?
"
190,https://github.com/stanfordnlp/CoreNLP/issues/244,244,"[{'id': 706064425, 'node_id': 'MDU6TGFiZWw3MDYwNjQ0MjU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/algorithm-error', 'name': 'algorithm-error', 'color': 'f9d0c4', 'default': False, 'description': None}, {'id': 706083198, 'node_id': 'MDU6TGFiZWw3MDYwODMxOTg=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/sutime', 'name': 'sutime', 'color': 'c5def5', 'default': False, 'description': None}]",open,2016-08-10 14:21:52+00:00,,"Parsing ""Last winter"" with markTimeRanges and includeRange option turned on sets the end of the range incorrectly","![screen shot 2016-08-10 at 10 22 34 am](https://cloud.githubusercontent.com/assets/4483629/17557255/71cf64d4-5ee4-11e6-9c74-599a3f78c40c.png)
""Last winter"" parses to `<TIMEX3 range=""(2015-12-01,2015-03,P3M)"" tid=""t1"" type=""DATE"" value=""2015-WI"">last winter</TIMEX3>`. The year of the end of the range should be 2016 in this case.
"
191,https://github.com/stanfordnlp/CoreNLP/issues/245,245,[],closed,2016-08-10 14:31:52+00:00,,How to run StanfordCoreNLPServer with a different language?,"I can run the regular command line version of CoreNLP successful with a different language set:

```
java -mx4g -cp ""stanford-corenlp-full-2015-12-09/*:stanford-german-2016-01-19-models.jar"" \
edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,parse \
-props StanfordCoreNLP-german.properties -file german.txt -outputFormat text
```

Then I tried StanfordCoreNLPServer with the same jar, but it only processes English language:

```
java -mx6g -cp ""stanford-corenlp-full-2015-12-09/*:stanford-german-2016-01-19-models.jar"" \
edu.stanford.nlp.pipeline.StanfordCoreNLPServer -props StanfordCoreNLP-german.properties
```

How do I use StanfordCoreNLPServer with a different language set?
"
192,https://github.com/stanfordnlp/CoreNLP/issues/246,246,[],closed,2016-08-10 19:20:44+00:00,,How do I get the score distribution value for CoreNLP Sentiment?,"I have setup CoreNLP server on my ubuntu instance and it works ok. I more interested in Sentiment module and currently I get is

```
{
sentimentValue: ""2"",
sentiment: ""Neutral""
}
```

What I need is score distribution value, as you see here: http://nlp.stanford.edu:8080/sentiment/rntnDemo.html

`""scoreDistr"": [0.1685, 0.7187, 0.0903, 0.0157, 0.0068]`

What am I missing or How do I get such data ?

Thanks
"
193,https://github.com/stanfordnlp/CoreNLP/issues/247,247,[],closed,2016-08-17 04:31:43+00:00,,Stanford CoreNLP server seems to ignore port option,"As of commit `e3cf455f`, this is what happens when I try to specify a port:

```
$ java -mx10g -cp ""*:lib/*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer 9005
[main] INFO CoreNLP - Starting server...
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000
```
"
194,https://github.com/stanfordnlp/CoreNLP/issues/248,248,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",closed,2016-08-18 03:38:53+00:00,,CoreNLP tokenizer barfs on characters outside the basic multilingual plane,"It seems CoreNLP does not deal with surrogates properly, at least in the English tokenizer.

Test case: tokenize üí©

Expected: 1 token
Actual:
Aug 17, 2016 8:34:58 PM edu.stanford.nlp.process.PTBLexer next
WARNING: Untokenizable: ? (U+D83D, decimal: 55357)
and the token disappears
"
195,https://github.com/stanfordnlp/CoreNLP/issues/249,249,[],closed,2016-08-21 15:36:38+00:00,,Compilation instructions,"There are currently different compilation instructions in different places:
Website: http://nlp.stanford.edu/software/basic-compiling.txt (linked to from: http://stanfordnlp.github.io/CoreNLP/download.html)
Wiki: https://github.com/stanfordnlp/CoreNLP/wiki/Compilation-Instructions
README: https://github.com/stanfordnlp/CoreNLP/blob/master/README.md

Everything in basic-compiling.txt is in the wiki page. The README has slightly different instructions.

It might be helpful to consolidate all this in one place.

Some users (I'm one of them) aren't familiar with java, or things like maven, ant, etc. (e.g. https://github.com/stanfordnlp/CoreNLP/issues/138). So it's probably not helpful to have the platform-specific reference to brew install ant on the wiki page. I had no idea what brew was.

Finally, it might be worth explicitly saying (when?) you have to download the model jar at the bottom of the compilation instructions.

Thanks for sharing such powerful software.
"
196,https://github.com/stanfordnlp/CoreNLP/issues/251,251,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}, {'id': 706055902, 'node_id': 'MDU6TGFiZWw3MDYwNTU5MDI=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/tokenize', 'name': 'tokenize', 'color': 'c5def5', 'default': False, 'description': None}, {'id': 706082923, 'node_id': 'MDU6TGFiZWw3MDYwODI5MjM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/pos', 'name': 'pos', 'color': 'c5def5', 'default': False, 'description': None}]",open,2016-08-23 13:53:34+00:00,,Make TokenRegex and Semgrex usable with externally annotated data?,"I've been using CoreNLP extensively lately, and there is a feature I really miss.
I usually use multiple NLP tools for tokenization and annotation, I notably use a custom entity tagger.
However, it is difficult to use TokenRegex and Semgrex on data which were not annotated with CoreNLP, due to the strong dependencies of TokenRegex and Semgrex on CoreNLP internal classes.
Would it be possible to use these 2 tools with externally annotated data? Or more broadly, to import externally annotated data and convert them into CoreNLP internal classes?
A JSON import would be great for instance.
"
197,https://github.com/stanfordnlp/CoreNLP/issues/252,252,[],closed,2016-08-24 09:54:33+00:00,,Address parsing ,"Hello,
   I need to classify the address text into two classes, one class is LOCALITY which categorises locality, area, neighbourhood into it and another class which contains the rest of the address. I have the data of locality , but need to know how to go about it,how to train the model to categorise LOCALITY, which software to use, either use NER and train location label or use regexNER?   

Thank you!
"
198,https://github.com/stanfordnlp/CoreNLP/issues/255,255,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 706082923, 'node_id': 'MDU6TGFiZWw3MDYwODI5MjM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/pos', 'name': 'pos', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2016-09-01 18:11:06+00:00,,Parsing parenthesized args that contain `(` or `)` fails,"When loading e.g. the distsim clusters, a file path can be specified in the `arch` argument. If that path contains parentheses, the loading fails. The reason seems to be the code that parses the 
parenthesized args. It splits not only on comma but also on `()` and it has no escaping mechanism.

```
  /** This is used for argument parsing in arch variable.
   *  It can extract a comma separated argument.
   *  Assumes the input format is ""name(arg,arg,arg)"".
   *
   *  @param str arch variable component input
   *  @param num Number of argument
   *  @return The parenthesized String, or null if none.
   */
  static String getParenthesizedArg(String str, int num) {
    String[] args = str.split(""\\s*[,()]\\s*"");
    if (args.length <= num) {
      return null;
    }
    // System.err.println(""getParenthesizedArg split "" + str + "" into "" + args.length + "" pieces; returning number "" + num);
    // for (int i = 0; i < args.length; i++) {
    //   System.err.println(""  "" + args[i]);
    // }
    return args[num];
  }
```

Introducing an escaping mechanism to would be nice because a comma is a valid character in filenames/pathnames.
"
199,https://github.com/stanfordnlp/CoreNLP/issues/256,256,"[{'id': 45387507, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNw==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/cantreproduce', 'name': 'cantreproduce', 'color': 'dddddd', 'default': False, 'description': None}]",closed,2016-09-02 03:09:21+00:00,,Cannot find org/joda/time/ReadablePartial when running SUTime,"I'm trying to run SUTime, following the instruction from SUTime page.
The command I used is 

```
java -Dpos.model=edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger \
-cp stanford-corenlp-2016-01-10.jar:stanford-english-corenlp-2016-01-10-models.jar:xom-1.2.10.jar:joda-time.jar:jollyday-0.4.9.jar \
-Xmx3g edu.stanford.nlp.time.SUTimeMain \
-in.type TEXTFILE \
-date 2016-09-01 \
-i test.txt \
-o res.txt
```

However, I received this error msg:

```
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.6 sec].
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/joda/time/ReadablePartial
    at edu.stanford.nlp.time.TimeAnnotator.<init>(TimeAnnotator.java:189)
    at edu.stanford.nlp.time.TimeAnnotator.<init>(TimeAnnotator.java:185)
    at edu.stanford.nlp.time.SUTimeMain.getPipeline(SUTimeMain.java:900)
    at edu.stanford.nlp.time.SUTimeMain.main(SUTimeMain.java:1058)
Caused by: java.lang.ClassNotFoundException: org.joda.time.ReadablePartial
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 4 more
```

It seems that something in joda time is missing? But I add joda-time.jar to the -cp arg already.
Has anyone run into this problem before? Thx a lot.
"
200,https://github.com/stanfordnlp/CoreNLP/issues/258,258,[],closed,2016-09-05 12:49:38+00:00,,Failed to start corenlp.service: Unit corenlp.service not found.,"I'm probably missing something stupid easy, but I tried creating a dedicated core nlp server as instructed [here](http://stanfordnlp.github.io/CoreNLP/corenlp-server.html#dedicated-server). When I try to start it get:

```
$ sudo service corenlp start
Failed to start corenlp.service: Unit corenlp.service not found.
```

Any ideas what I may have missed?
"
201,https://github.com/stanfordnlp/CoreNLP/issues/260,260,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 706055902, 'node_id': 'MDU6TGFiZWw3MDYwNTU5MDI=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/tokenize', 'name': 'tokenize', 'color': 'c5def5', 'default': False, 'description': None}]",open,2016-09-08 19:37:07+00:00,,Original character positions incorrect in Spanish annotation,"@manning reported second-hand that character offset annotations seem to be wrong for Spanish NER output. Investigate.
"
202,https://github.com/stanfordnlp/CoreNLP/issues/261,261,[],closed,2016-09-08 19:41:15+00:00,,Augment Spanish NER data with CoNLL,"Rebuild NER models with [CoNLL data](http://www.cnts.ua.ac.be/conll2003/ner/).
(would assign this to myself if I could!)
"
203,https://github.com/stanfordnlp/CoreNLP/issues/263,263,"[{'id': 518893847, 'node_id': 'MDU6TGFiZWw1MTg4OTM4NDc=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/concurrency-bug', 'name': 'concurrency-bug', 'color': 'b60205', 'default': False, 'description': None}]",closed,2016-09-13 10:43:46+00:00,,ChineseSegmenterAnnotator / ChineseStringUtils NullPointerException in multi-threaded use,"When using `ChineseSegmenterAnnotator` in a multi-threaded environment a NullPointerException can sometimes occur.  The root cause seems to be that `ChineseStringUtils.processColons()` is not thread safe due to lazy initialisation of static members.

Current master revision can throw an NPE on line 300:
https://github.com/stanfordnlp/CoreNLP/blob/6ea6a2e9e9b58b41827ebbf581ca2239aeebb52f/src/edu/stanford/nlp/wordseg/ChineseStringUtils.java#L300

The issue is the (thread-)unsafe initialisation of `colonsPat` a few lines earlier.  The same issue appears to be present for `colonsWhitePat`.
"
204,https://github.com/stanfordnlp/CoreNLP/issues/264,264,[],closed,2016-09-13 13:46:53+00:00,,Preload models on server start?,"Would you add an option to preload models on server start with properties perhaps?

I would like to avoid loading hte models on first use.

Ref:
""The first use will be slow to respond while models are loaded ‚Äì it might take 30 seconds or so, but after that the server should run quite quickly.)"" http://stanfordnlp.github.io/CoreNLP/corenlp-server.html
"
205,https://github.com/stanfordnlp/CoreNLP/issues/265,265,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2016-09-15 01:42:32+00:00,,Truecase not working,"I'm working on a cultural history project with hundreds of hours of audio.  When the transcripts were created many years ago, they were done in ALL CAPS.  CoreNLP is working great identifying our names so we can link to their profile pages, but the true case annotator doesn't seem to be doing anything.  I have the stanford-corenlp-3.6.0-models-english.jar in my path and running both the server and command-line versions load the annotator and output as excepted:

```
/127.0.0.1:53700] API call w/annotators tokenize,ssplit,pos,lemma,new,truecase
NORVELL BROWN WAS LEAD MAN AT THE DUKE
21:34:53.950 [pool-1-thread-1] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
21:34:53.963 [pool-1-thread-1] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator split
21:34:53.966 [pool-1-thread-1] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [3.2 sec].
21:34:57.167 [pool-1-thread-1] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
21:34:57.168 [pool-1-thread-1] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator new
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [8.5 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [3.7 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [4.7 sec].
21:35:14.142 [pool-1-thread-1] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator true case
loadClassifier=edu/stanford/nlp/models/truecase/truecasing.fast.qn.ser.gz
mixedCaseMapFile=edu/stanford/nlp/models/truecase/MixDisambiguation.list
classBias=INIT_UPPER:-0.7,UPPER:-0.7,O:0
Loading classifier from edu/stanford/nlp/models/truecase/truecasing.fast.qn.ser.gz ... done [4.7 sec].
```

and a snippet of the output:

```
{""index"":5,""word"":""MAN"",""originalText"":""MAN"",""lemma"":""MAN"",""characterOffsetBegin"":23,""characterOffsetEnd"":26,""pos"":""NNP"",""ner"":""O"",""truecase"":""O"",""truecaseText"":""MAN""}
```

As you can see, the truecaseText property of MAN is MAN.

I've also tried setting `truecase.model` to `edu/stanford/nlp/models/truecase/truecasing.fast.qn.ser.gz`, but get the same results.

Anyone encountered this?
"
206,https://github.com/stanfordnlp/CoreNLP/issues/267,267,[],closed,2016-09-17 09:18:56+00:00,,Download page is broken,"This download page is not working:

http://nlp.stanford.edu/software/stanford-corenlp-full-2015-12-09.zip
"
207,https://github.com/stanfordnlp/CoreNLP/issues/268,268,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",open,2016-09-21 19:50:11+00:00,,Improve support for CoNLL-U files in the NN dependency parser,"We should store and output both POS tags, features, comments, and the MISC column if a CoNLL-U file is used to test the parser. 

We should also consider allowing the parser to use morphological features and both POS tags as features.
"
208,https://github.com/stanfordnlp/CoreNLP/issues/269,269,[],closed,2016-09-27 05:54:43+00:00,,German corenlp server defaulting to english models,"I use the following command to serve a  corenlp server for German language models which are downloaded as jar in the classpath , but it does not output german tags or parse but loads only english models:

`java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer   -props ./german.prop
`
german.prop contents:

```
  annotators = tokenize, ssplit, pos, depparse, parse

    tokenize.language = de

    pos.model = edu/stanford/nlp/models/pos-tagger/german/german-hgc.tagger

    ner.model = edu/stanford/nlp/models/ner/german.hgc_175m_600.crf.ser.gz
    ner.applyNumericClassifiers = false
    ner.useSUTime = false

    parse.model = edu/stanford/nlp/models/lexparser/germanFactored.ser.gz
    depparse.model = edu/stanford/nlp/models/parser/nndep/UD_German.gz

```

client command:

```

    wget --post-data ' Meine Mutter ist aus Wuppertal' 'localhost:9000/?properties""=""{""tokenize.whitespace"":""true"",""annotators"":""tokenize, ssplit, pos, depparse, parse"",""outputFormat"":""text"",""tokenize.language"" :""de"" ,
     ""pos.model"":"" edu/stanford/nlp/models/pos-tagger/german/german-hgc.tagger"",
    ""depparse.model"" : ""edu/stanford/nlp/models/parser/nndep/UD_German.gz"",
    ""parse.model"" : ""edu/stanford/nlp/models/lexparser/germanFactored.ser.gz""

     }' -O -

```

I get following incorrect output:
  `

```
 {""dep"":""dep"",""governor"":4,""governorGloss"":""aus"",""dependent"":5,""dependentGloss"":""Wuppertal""}],""openie"":[{""subject"":""Wuppertal"",""subjectSpan"":[4,5],""relation"":""is ist aus of"",""relationSpan"":[2,4],""object"":""Meine Mutter"",""objectSpan"":[0,2]}],""tokens"":[{""index"":1,""word"":""Meine"",""originalText"":""Meine"",""lemma"":""Meine"",""characterOffsetBegin"":1,""characterOffsetEnd"":6,""pos"":""NNP"",""ner"":""PERSON"",""speaker"":""PER0"",""before"":"" "",""after"":"" ""},{""index"":2,""word"":""Mutter"",""originalText"":""Mutter"",""lemma"":""Mutter"",""characterOffsetBegin"":7,""characterOffsetEnd"":13,""pos"":""NNP"",""ner"":""PERSON"",""speaker"":""PER0"",""before"":"" "",""after"":"" ""},{""index"":3,""word"":""ist"",""originalText"":""ist"",""lemma"":""ist"",""characterOffsetBegin"":14,""characterOffsetEnd"":17,""pos"":""NN"",""ner"":""O"",""speaker"":""PER0"",""before"":"" "",""after"":"" ""},{""index"":4,""word"":""aus"",""originalText"":""aus"",""lemma"":""aus"",""characterOffsetBegin"":18,""characterOffsetEnd"":21,""pos"":""NN"",""ner"":""O"",""speaker"":""PER0"",""before"":"" "",""after"":"" ""},{""index"":5,""word"":""Wuppertal"",""originalText"":""Wuppertal"",""lemma"":""Wuppertal"",""characterOffsetBegin"":22,""characterOffsetEnd"":31,""pos"":""NNP"",""ner"":""LOCATI100%[==========================================================================>] 2,
```

`

in the server log I see it loads english models eventhough it lists german models on startup:

```

    pos.model=edu/stanford/nlp/models/pos-tagger/ge...
    parse.model=edu/stanford/nlp/models/lexparser/ger...
    tokenize.language=de
    depparse.model=edu/stanford/nlp/models/parser/nndep/...
    annotators=tokenize, ssplit, pos, depparse, parse
    Starting server on port 9000 with timeout of 5000 milliseconds.
    StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000
    [/203.:61563] API call w/annotators tokenize,ssplit,pos,depparse
    Die Katze liegt auf der Matte.
    [pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
    [pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.
    [pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
    [pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
    Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.5 sec].
    [pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse
    Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ...
    PreComputed 100000, Elapsed Time: 1.396 (s)

```

The following question for same error in french models also points to the same problem but even after following , it does not resolve the problem for the server case, I am able to get the correct output without using the server and just using the `edu.stanford.nlp.pipeline.StanfordCoreNLP command` , it is the server command `edu.stanford.nlp.pipeline.StanfordCoreNLPServer` which defaults to english:
http://stackoverflow.com/questions/36223002/french-dependency-parsing-using-corenlp#
"
209,https://github.com/stanfordnlp/CoreNLP/issues/270,270,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2016-09-30 12:32:23+00:00,,Handling special characters with CoreNLP server.,"I want to do some Named Entity Recognition on several documents using a CoreNLP server that I launch this way:
`java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer`

Basic sentences like ""The quick brown fox jumps over the lazy dog"" works well on the test page at http://localhost:9000/ but as soon as special characters are involved, like the ""√©"" in ""I went to a caf√© yesterday"", I get the error:

`{""sentences"":[{""index"":0,""parse"":""SENTENCE_SKIPPED_OR_UNPARSABLE"",""basic-dependencies"": ...`

Note that this sentence works well on [http://corenlp.run/].

I tried `-strict` and `-encode utf-8` or adding `parse` in the annotators with no success.

I am using v 3.6.0 downloaded there: [http://stanfordnlp.github.io/CoreNLP/download.html]

Trying to call the server with corenlp_pywrap as python wrapper returns the following error:

```
UnicodeEncodeError                        Traceback (most recent call last)
<ipython-input-163-aa251fb39f9e> in <module>()
      2     text = profile['text']
      3     processed_chunks = process_chunks(text)
----> 4     print(idx, (find_name_in_text(processed_chunks, profile), profile['full_name']))

<ipython-input-162-2c34ab37440e> in find_name_in_text(processed_chunks, profile, stop)
      1 def find_name_in_text(processed_chunks, profile, stop=10):
      2     for chunk in processed_chunks[:stop]:
----> 3         token_dict = cn.arrange(chunk)
      4         detected_names = find_names_in_dict(token_dict)
      5         if detected_names:

/usr/local/lib/python3.5/site-packages/corenlp_pywrap/pywrap.py in arrange(self, data)
    151 
    152         current_url = self.url_calc()
--> 153         r = self.server_connection(current_url, data)
    154         try:
    155             r = r.json()

/usr/local/lib/python3.5/site-packages/corenlp_pywrap/pywrap.py in server_connection(current_url, data)
     50             server_out = requests.post(current_url, 
     51                                         data,
---> 52                                         headers={'Connection': 'close'})
     53         except requests.exceptions.ConnectionError:
     54             root.error('Connection Error, check you have server running')

/usr/local/lib/python3.5/site-packages/requests/api.py in post(url, data, json, **kwargs)
    108     """"""
    109 
--> 110     return request('post', url, data=data, json=json, **kwargs)
    111 
    112 

/usr/local/lib/python3.5/site-packages/requests/api.py in request(method, url, **kwargs)
     54     # cases, and look like a memory leak in others.
     55     with sessions.Session() as session:
---> 56         return session.request(method=method, url=url, **kwargs)
     57 
     58 

/usr/local/lib/python3.5/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    473         }
    474         send_kwargs.update(settings)
--> 475         resp = self.send(prep, **send_kwargs)
    476 
    477         return resp

/usr/local/lib/python3.5/site-packages/requests/sessions.py in send(self, request, **kwargs)
    594 
    595         # Send the request
--> 596         r = adapter.send(request, **kwargs)
    597 
    598         # Total elapsed time of the request (approximately)

/usr/local/lib/python3.5/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    421                     decode_content=False,
    422                     retries=self.max_retries,
--> 423                     timeout=timeout
    424                 )
    425 

/usr/local/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, **response_kw)
    593                                                   timeout=timeout_obj,
    594                                                   body=body, headers=headers,
--> 595                                                   chunked=chunked)
    596 
    597             # If we're going to release the connection in ``finally:``, then

/usr/local/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    361             conn.request_chunked(method, url, **httplib_request_kw)
    362         else:
--> 363             conn.request(method, url, **httplib_request_kw)
    364 
    365         # Reset the timeout for the recv() on the socket

/usr/local/Cellar/python3/3.5.2_1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py in request(self, method, url, body, headers)
   1104     def request(self, method, url, body=None, headers={}):
   1105         """"""Send a complete request to the server.""""""
-> 1106         self._send_request(method, url, body, headers)
   1107 
   1108     def _set_content_length(self, body, method):

/usr/local/Cellar/python3/3.5.2_1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py in _send_request(self, method, url, body, headers)
   1148             # RFC 2616 Section 3.7.1 says that text default has a
   1149             # default charset of iso-8859-1.
-> 1150             body = _encode(body, 'body')
   1151         self.endheaders(body)
   1152 

/usr/local/Cellar/python3/3.5.2_1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py in _encode(data, name)
    159             ""%s (%.20r) is not valid Latin-1. Use %s.encode('utf-8') ""
    160             ""if you want to send it encoded in UTF-8."" %
--> 161             (name.title(), data[err.start:err.end], name)) from None
    162 
    163 

UnicodeEncodeError: 'latin-1' codec can't encode character '\uf02a' in position 1: Body ('\uf02a') is not valid Latin-1. Use body.encode('utf-8') if you want to send it encoded in UTF-8.

```

It looks like the problem comes from how the request sent to the server is processed.
"
210,https://github.com/stanfordnlp/CoreNLP/issues/271,271,[],open,2016-10-04 13:00:55+00:00,,Saving word embeddings in model file,"I noticed that the trained parser model doesn't include all the embeddings supplied to its training (I suppose it only keeps the ones seen in the annotated data). Why is it so? Wouldn't it better to have more word embeddings in order to minimize OOV impact when running the trained model?
"
211,https://github.com/stanfordnlp/CoreNLP/issues/272,272,"[{'id': 103162424, 'node_id': 'MDU6TGFiZWwxMDMxNjI0MjQ=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/request', 'name': 'request', 'color': '94c5e9', 'default': False, 'description': None}]",closed,2016-10-05 14:51:58+00:00,,DEFAULT_SENTENCE_DELIMS,"Hi Group,

Thank you first for this open source contribution. It makes make my life much better.

The problem I have right now is that it seems not possible to split sentences with no punctuation in between, like two sentences glued together with the first having no period. Can the DocumentParser smartly spot where to break the sentences? 

I saw[ this line](https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/process/DocumentPreprocessor.java#L63) so I doubt if it is possible.

Thank you.
"
212,https://github.com/stanfordnlp/CoreNLP/issues/273,273,[],closed,2016-10-05 19:14:06+00:00,,corenlp.run is down :(,"> # You have reached a domain that is pending ICANN verification.
> 
> As of January 1, 2014 the Internet Corporation for Assigned Names and Numbers (ICANN) will mandate that all ICANN accredited registrars begin verifying the Registrant WHOIS contact information for all new domain registrations and Registrant contact modifications.
> ## Why this domain has been suspended
> 
> **Email address has not been verified.**
> This is a new domain registration and the Registrant email address has not been verified.
> &mdash;or&mdash;
> **The Registrant contact data for this domain was modified but still requires verification.**
> Specifically the First Name, Last Name and/or email address have been changed and never verified.
"
213,https://github.com/stanfordnlp/CoreNLP/issues/274,274,[],closed,2016-10-08 08:50:17+00:00,,How to use a user dictionary,"For example, I know ""‰ΩïÂâë"" is a personal name.
How to add these new words?
"
214,https://github.com/stanfordnlp/CoreNLP/issues/275,275,[],closed,2016-10-13 10:00:27+00:00,,How to pick a specific in memory loaded NER model?,"Hello,

I wanted to know if it was possible to select which NER model we would like to use even when the pipeline is already loaded in memory. For example, let's assume I create a pipeline by using the NERCombiner and loading the 3 provided models:

```
edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz
edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz
edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz
```

With the following piece of code:

```
Properties props = new Properties();
props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, mention, coref"");
props.put(""coref.md.type"", ""rule"");
props.put(""coref.mode"", ""statistical"");
props.put(""coref.doClustering"", ""true"");
props.put(""ner.model"", ""edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz,edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz,edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
```

Now every time I want to annotate a text, the 3 models will be applied. Now what I would like to do is to use only the `3class` model but without recreating the pipeline and reloading everything in memory. As in my case I use the statistical coref, it takes a moment all the time to load all its stuff in memory, and I would like to avoid this all the time. Is there any way to do that?

Thanks in advance for any help!
"
215,https://github.com/stanfordnlp/CoreNLP/issues/276,276,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 737384331, 'node_id': 'MDU6TGFiZWw3MzczODQzMzE=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/parse', 'name': 'parse', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2016-10-14 05:54:37+00:00,,Null Pointer exception during parsing,"Faced following error while processing large dataset:

```
java.lang.NullPointerException
        at edu.stanford.nlp.trees.UniversalEnglishGrammaticalStructure.correctWHAttachment(UniversalEnglishGrammaticalStructure.java:690)
        at edu.stanford.nlp.trees.UniversalEnglishGrammaticalStructure.postProcessDependencies(UniversalEnglishGrammaticalStructure.java:173)
        at edu.stanford.nlp.trees.GrammaticalStructure.getDeps(GrammaticalStructure.java:560)
        at edu.stanford.nlp.trees.GrammaticalStructure.<init>(GrammaticalStructure.java:215)
        at edu.stanford.nlp.trees.UniversalEnglishGrammaticalStructure.<init>(UniversalEnglishGrammaticalStructure.java:92)
        at edu.stanford.nlp.trees.UniversalEnglishGrammaticalStructure.<init>(UniversalEnglishGrammaticalStructure.java:71)
        at edu.stanford.nlp.trees.UniversalEnglishGrammaticalStructureFactory.newGrammaticalStructure(UniversalEnglishGrammaticalStructureFactory.java:29)
        at edu.stanford.nlp.trees.UniversalEnglishGrammaticalStructureFactory.newGrammaticalStructure(UniversalEnglishGrammaticalStructureFactory.java:5)
        at edu.stanford.nlp.pipeline.ParserAnnotatorUtils.fillInParseAnnotations(ParserAnnotatorUtils.java:59)
        at edu.stanford.nlp.pipeline.ParserAnnotator.finishSentence(ParserAnnotator.java:290)
        at edu.stanford.nlp.pipeline.ParserAnnotator.doOneSentence(ParserAnnotator.java:260)
        at edu.stanford.nlp.pipeline.SentenceAnnotator.annotate(SentenceAnnotator.java:98)
        at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:71)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:499)
```

Unfortunately can not reproduce this error nor say which data cause this exception 
"
216,https://github.com/stanfordnlp/CoreNLP/issues/278,278,[],closed,2016-10-17 08:19:45+00:00,,"annotator ""coref"" requires annotation ""TreeAnnotation"".","I am trying to run the latest all-models-jar and code on my windows machine. Some annotators like ner, pos, parse,relation work fine. But when i put coref, i get an error. This work with stanford-corenlp-full-2015-12-09 but not with latest code. Am I doing anything wrong?
[/0:0:0:0:0:0:0:1:49275] API call w/annotators tokenize,ssplit,pos,depparse,lemm
a,natlog,ner,mention,coref,openie
The quick brown fox jumped over the lazy dog.
Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Adding annotator depparse
Adding annotator lemma
Adding annotator natlog
Adding annotator ner
Adding annotator mention
Using mention detector type: dependency
Adding annotator coref
Loading coref model edu/stanford/nlp/models/coref/statistical/clustering_model.s
er.gz ... done [0.0 sec].
Loading coref model edu/stanford/nlp/models/coref/statistical/classification_mod
el.ser.gz ... done [1.0 sec].
Loading coref model edu/stanford/nlp/models/coref/statistical/ranking_model.ser.
gz ... done [1.3 sec].
Loading coref model edu/stanford/nlp/models/coref/statistical/anaphoricity_model
.ser.gz ... done [0.2 sec].
java.lang.IllegalArgumentException: annotator ""coref"" requires annotation ""TreeA
nnotation"". The usual requirements for this annotator are: tokenize,ssplit,pos,l
emma,ner,mention
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.j
ava:460)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java
:154)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java
:145)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.mkStanfordCoreNLP(Sta
nfordCoreNLPServer.java:244)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.access$500(StanfordCo
reNLPServer.java:47)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle
(StanfordCoreNLPServer.java:551)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Unknown Source)
        at sun.net.httpserver.AuthFilter.doFilter(Unknown Source)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Unknown Source)
        at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(Unknown Sou
rce)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Unknown Source)
        at sun.net.httpserver.ServerImpl$Exchange.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
"
217,https://github.com/stanfordnlp/CoreNLP/issues/281,281,[],open,2016-10-22 19:03:53+00:00,,NegativeArraySizeException in AbstractStochasticCachingDiffFunction.initial,"We're building a linear classifier with a fair number of classes/labels.  50k or so... 

It's not much data.  About 200MB of text.

However, you're creating a large array for all these features and getting a negative array size to due integer overflow.  

Can this array be sparse?  

One solution is that if domainDimension returns negative at least throw an exception.

Guava Preconditions could help out greatly here... 

```
  @Override
  public int domainDimension() {
    return numFeatures * numClasses;
  }
```
"
218,https://github.com/stanfordnlp/CoreNLP/issues/282,282,[],closed,2016-10-23 15:15:55+00:00,,"OpenIEDemo run error.  Tagger file not found ""english-left3words-distsim.tagger""","Unable to open ""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"" as class path, filename or URL
"
219,https://github.com/stanfordnlp/CoreNLP/issues/285,285,[],open,2016-10-24 21:38:26+00:00,,Use tfidf as a better scoring function for trimming features (vs applyFeatureCountThreshold),"There are to features in GeneralDataset for using the dimensionality of the feature vector.

`applyFeatureCountThreshold` and `applyFeatureMaxCountThreshold`

However, these might be somewhat naive.  

Wouldn't it be better to compute the tfidf score of all the features and then just trim to remove <= 80th percentile. (give or take).

This would be much better than computing a rough 'count' as you would need to know what reasonable counts look like before.

Further, tfidf would prefer terms that are generally very valuable for the classifier to use to predict these classes.  

Once could compute the tfidf per document or per label and this is super fast to compute in RAM.. probably only adding say 1% of the classification time.  
"
220,https://github.com/stanfordnlp/CoreNLP/issues/286,286,[],closed,2016-10-24 21:42:47+00:00,,Excessive Arrays.fill in Java not necessary.,"There is a lot of code in CoreNLP like the following:

```
    float[] initial = new float[domainDimension()];
    Arrays.fill(initial, 0.0f);
```

I counted 10-20 in just 15 seconds by looking at the code by grepping for `Arrays.fill`

This isn't needed as Java defaults to 0.0 for floats and doubles so this is just wasting time.

The only argument FOR it is if you're trying to force the RAM into a cache line but I don't see any commentary on this.

Otherwise it might be just wasting time.  

If you're trying to force it into a cache line calling something like Cache.forceIntoCacheline (a new method) would be a better way to document this.   
"
221,https://github.com/stanfordnlp/CoreNLP/issues/287,287,[],closed,2016-10-28 07:38:11+00:00,,"When new same sentence and lemma or posTag more than one , the speed will slow and memory leak  ","```
    public static void main(String[] args) {
        for (int i = 0 ; i < 1000; i++) {
            Sentence sen = new Sentence(""Snowden, which had its world premiere on Friday at the Toronto International Film Festival and hits theatres on September 16, sees Gordon-Levitt, 35, play the 33-year-old Snowden through a decade of his life. When veteran filmmaker Oliver Stone was casting an actor to play former NSA contractor-turned-whistleblower Edward Snowden in a film, he said he went to only one person - Joseph Gordon-Levitt. ‚ÄúI don‚Äôt know why, he just looked like, and felt like, and acted like he was one of that generation, very much the same age and computer knowledgeable,‚Äù Stone told Reuters last month in Los Angeles.    Snowden, which had its world premiere on Friday at the Toronto International Film Festival and hits theatres on September 16, sees Gordon-Levitt, 35, play the 33-year-old Snowden through a decade of his life. Gordon-Levitt, who achieved fame as a child actor in television series 3rd Rock from the Sun, said that by playing Snowden, he hoped to understand his motivations. ‚ÄúI was kind of trying to figure out why he did what he did, what was going on in his head,‚Äù he said. ‚ÄúAnd one of the questions everyone asks is, ‚ÄòWhy didn‚Äôt he just, you know, voice his concerns through proper channels?‚Äô‚Äù The film leads up to the events of 2013, when Snowden fled the United States after exposing the government‚Äôs mass surveillance programs to journalist Glenn Greenwald and documentary filmmaker Laura Poitras.    The US government filed espionage charges against Snowden and he was granted asylum in Russia, where he has lived since, with his girlfriend Lindsay Mills. Actor Shailene Woodley plays Mills in the film. Gordon-Levitt said he related to Snowden‚Äôs disillusionment with the US government after watching US National Intelligence director James Clapper deny, before a congressional committee, that the NSA was collecting records on millions of Americans.   ‚ÄúIf the director of National Intelligence is being asked by a senator under oath, ‚ÄòHey, is this happening?‚Äô and he‚Äôs telling a lie, well, then, what is some guy that works at the NSA going to accomplish by complaining through proper channels?‚Äù the actor said. Tech-savvy Gordon-Levitt, who said he donated his fee for the film to the American Civil Liberties Union, is the founder of HitRecord, an online collaborative creative hub to brings together artists from around the world. He said he tended to be optimistic about new technology but the movie made him more aware of its negative aspects. ‚ÄúIt‚Äôs worth being optimistic about all those things, but it‚Äôs also probably worth paying attention and considering what might the downsides be of this new technology that we‚Äôre inheriting,‚Äù he said."");
            sen.lemmas();
        }
    }
```
"
222,https://github.com/stanfordnlp/CoreNLP/issues/288,288,[],closed,2016-10-28 18:56:39+00:00,,Refactor build system towards Maven,"As an outsider I'm somewhat confused as to the build system.

Admittedly I'm not a big proponent of Gradle as all the larger project I work on use Maven.

That said, I think moving to EITHER Gradle or Maven, with included depenencies, would be better than what we have now.

Storing .jars inline in the repo is generally not a good idea.    It also has the problem of not being able to compute full transitive dependencies which both gradle and maven can do just fine. 

I think the current strategy is to have a README in the lib directory with all the depenencies

Couldn't this be replaced with a full maven/gradle build with the dependencies included?

This almost works with pom-full.xml but needs to be improved a bit to add the remaining dependencies.  

Sees straight forward.

I think the problem now, from an outsiders perspective, is that to get a custom CoreNLP build (we will need to run a fork for a while until you accept our PRs) is difficult without maven.

Anyway. I think having this as a SIDE build system, along with the current one, shouldn't be too hard.  

Thoughts?

Thanks!
"
223,https://github.com/stanfordnlp/CoreNLP/issues/290,290,[],closed,2016-10-31 14:46:46+00:00,,Server doesn't start because of missing .js file (CoreNLP 3.7.0 (beta)),"```
~/Projects/corenlp $ java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer
[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - setting default constituency parser
[main] INFO CoreNLP - using SR parser: edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP -     Threads: 8
[main] INFO CoreNLP - Starting server...
java.io.IOException: Unable to open ""edu/stanford/nlp/pipeline/demo/corenlp-parseviewer.js"" as class path, filename or URL
	at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:478)
	at edu.stanford.nlp.io.IOUtils.readerFromString(IOUtils.java:615)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$FileHandler.<init>(StanfordCoreNLPServer.java:448)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.run(StanfordCoreNLPServer.java:1198)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.main(StanfordCoreNLPServer.java:1274)
[Thread-0] INFO CoreNLP - CoreNLP Server is shutting down.
```

I'm using macOS 10.12 and have just downloaded and unzipped the files from your website."
224,https://github.com/stanfordnlp/CoreNLP/issues/293,293,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 706088581, 'node_id': 'MDU6TGFiZWw3MDYwODg1ODE=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/kbp', 'name': 'kbp', 'color': 'c5def5', 'default': False, 'description': None}, {'id': 706089022, 'node_id': 'MDU6TGFiZWw3MDYwODkwMjI=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/windows', 'name': 'windows', 'color': '1d76db', 'default': False, 'description': None}]",closed,2016-11-01 09:31:12+00:00,,CoreNLP 3.7-beta kpb models,"New kpb models uses `:` (colon)  in files names that is not supported by Win file system
![image](https://cloud.githubusercontent.com/assets/1197905/19885140/25503c9c-a02e-11e6-9e74-1a9e723488c0.png)
Is possible to rename those files?

It slightly affects [Stanford.NLP.NET](https://github.com/sergey-tihon/Stanford.NLP.NET)
`stanford-corenlp-3.7.0-models.jar` cannot be unzipped using standard .NET API
![image](https://cloud.githubusercontent.com/assets/1197905/19885263/b5f4b57a-a02e-11e6-9510-4cce773c1942.png)
https://ci.appveyor.com/project/sergey-tihon/stanford-nlp-net/build/0.0.1.56#L65

`7zip` for example automatically rename all files (that also may lead to confusion)
![image](https://cloud.githubusercontent.com/assets/1197905/19885293/dfb00504-a02e-11e6-91e2-887005dbed79.png)


"
225,https://github.com/stanfordnlp/CoreNLP/issues/294,294,[],closed,2016-11-01 10:37:41+00:00,,Maven Repository Link Broken,The readme markdown thinger on this GitHub project has a broken link to a Maven thing that seems to not exist.
226,https://github.com/stanfordnlp/CoreNLP/issues/296,296,[],open,2016-11-06 03:19:19+00:00,,How to pass one's own word vectors ? Any example ?,"I can see in SentimentModel class there is a an api readWordVectors , I want to pass my own word vector to a new sentiment model ? How can I do it ? Is there an example file with the format , for the same ?
https://github.com/stanfordnlp/CoreNLP/blob/d558d95d80b36b5b45bc21882cbc0ef7452eda24/src/edu/stanford/nlp/sentiment/SentimentModel.java
"
227,https://github.com/stanfordnlp/CoreNLP/issues/297,297,[],closed,2016-11-06 14:00:01+00:00,,How to create Chinese Sentiment Annotator?,"Stanford Core NLP software has an annotator of sentiment , but it only supports for English , I want to create an sentiment annotator for Chinese . What should I do ? Can someone give me some advice on it , thank you very much!"
228,https://github.com/stanfordnlp/CoreNLP/issues/298,298,[],closed,2016-11-09 11:49:56+00:00,,what is TokensRegexNERAnnotator Annotation name?,"hi, TokensRegexNERAnnotator ? Annotation name
thanks"
229,https://github.com/stanfordnlp/CoreNLP/issues/299,299,[],closed,2016-11-10 06:45:32+00:00,,TokensRegexNERAnnotator: regexner.mapping.field.<fieldname>,"Class mapping for annotation fields other than ner.

but, 
props.put(""tokenregexner.mapping.field.priority"", ""edu.stanford.nlp.ling.CoreAnnotations$NormalizedNamedEntityTagAnnotation"")

not work, i want to record, how do 

thanks"
230,https://github.com/stanfordnlp/CoreNLP/issues/300,300,[],closed,2016-11-10 14:20:17+00:00,,Build Failure while executing tests cases,"I have installed java & ant and performed following steps to compile CoreNLP from github repo-  
#java -version
openjdk version ""1.8.0_91""
OpenJDK Runtime Environment (build 1.8.0_91-8u91-b14-3ubuntu1~16.04.1-b14)
OpenJDK 64-Bit Server VM (build 25.91-b14, mixed mode)

#ant -version
Apache Ant(TM) version 1.9.6 compiled on July 8 2015

#git clone https://github.com/stanfordnlp/CoreNLP.git
#cd CoreNLP
#ant clean
BUILD SUCCESSFUL

#ant compile
BUILD SUCCESSFUL

#ant test

[junit] Testsuite: edu.stanford.nlp.international.spanish.SpanishUnknownWordSignaturesTest
    [junit] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 sec
    [junit]
    [junit] Testsuite: edu.stanford.nlp.international.spanish.process.SpanishTokenizerTest
    [junit] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.051 sec
    [junit]
    [junit] Testcase: testIr(edu.stanford.nlp.international.spanish.process.SpanishTokenizerTest):      FAILED
    [junit] Number of tokens doesn't match reference 'tengo que irme ahora' expected:<5> but was:<4>
    [junit] junit.framework.AssertionFailedError: Number of tokens doesn't match reference 'tengo que irme ahora' expected:<5> but was:<4>
    [junit]     at edu.stanford.nlp.international.spanish.process.SpanishTokenizerTest.testOffset(SpanishTokenizerTest.java:26)
    [junit]     at edu.stanford.nlp.international.spanish.process.SpanishTokenizerTest.testIr(SpanishTokenizerTest.java:42)
    [junit]
    [junit]

BUILD FAILED
/CoreNLP/build.xml:146: Tests failed

Any suggestions how to fix this ?"
231,https://github.com/stanfordnlp/CoreNLP/issues/301,301,[],closed,2016-11-14 11:03:36+00:00,,Sentiment live demo: connection refused,"When trying to access the Sentiment Live Demo (http://nlp.stanford.edu:8080/sentiment/rntnDemo.html), the server returns a ""connection refused"" error message.

#157 "
232,https://github.com/stanfordnlp/CoreNLP/issues/302,302,[],closed,2016-11-15 14:39:03+00:00,,Fails to identify compound nouns,"corenlp fails to identify common compound nouns like ""hot dogs"". For example, ""I like hot dogs."" - in this sentence corenlp identifies relationship between hot dogs as ""amod(dogs, hot)"", whereas it should be ""compound(dogs, hot)"""
233,https://github.com/stanfordnlp/CoreNLP/issues/303,303,[],closed,2016-11-16 01:55:09+00:00,,How to correctly import in Intellij?,"Hello,

I import the jar file via dependencies windows of Intellij. 
I import them (stanford-english-corenlp-models-current & stanford-corenlp-models-current) as classes.
Problem, the package Pipeline only contain .demo...

import edu.stanford.nlp.pipeline.demo.*;

I would like to have the Annotation import.
Also nlp doesn't contain .ling or .util.

What do i have to do to get all the import needed by this example : http://stackoverflow.com/a/19909327 ?"
234,https://github.com/stanfordnlp/CoreNLP/issues/304,304,[],closed,2016-11-17 13:43:08+00:00,,openie can't work well with corenlp,"when i run openie in eclipse with jar of the model,there is a error:
local class incompatible:stream classdesc serialVersionUID=4155...,local class serialVersionUID=-7360...
@gangeli "
235,https://github.com/stanfordnlp/CoreNLP/issues/305,305,[],open,2016-11-21 22:39:33+00:00,,Clarify ColumnDataClassifier Global and Column-wise Flags,"In the ColumnDataClassifier JavaDoc, it is not clear that which flags need a column index at the start. For example, there is no information about whether the l1reg, useAdaptL1, limitFeatures are global or column wise. Do we use them such as`1.l1reg` or `l1reg`?"
236,https://github.com/stanfordnlp/CoreNLP/issues/306,306,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}, {'id': 711772827, 'node_id': 'MDU6TGFiZWw3MTE3NzI4Mjc=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/sentiment', 'name': 'sentiment', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2016-11-22 03:54:37+00:00,,Use Sentiment analysis system to test a lot of sentences will spend long time,"I think sentiment analysis system of stanfordCoreNlp with high accuracy, But I found if I test a lot of sentences, long time would be spent. I have not  find solution for this problem, I hope someone will help me! Thank you very much!"
237,https://github.com/stanfordnlp/CoreNLP/issues/307,307,[],closed,2016-11-22 04:12:31+00:00,,StanfordCoreNLPClient not connecting to StanfordCoreNLPServer when running as user,"I have the server running under my user.

java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer

Firefox connects to the server at localhost:9000 and processes the example OK.

Under my user in a separate terminal window,

java -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPClient -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt -backends localhost:9000
Processing file /mnt/Documents/downloads/CoreNLP/stanford-corenlp-full-2016-10-31/input.txt ... writing to /mnt/Documents/downloads/CoreNLP/stanford-corenlp-full-2016-10-31/input.txt.out

StanfordCoreNLPClient appears to hang and the file input.txt.out is not created. Submitting a sentence from Firefox shows the submitted sentence on the server terminal window. Nothing appears on the server terminal window for the above StanfordCoreNLPClient command.

I would rather not install the server in /opt/corenlp in that I only need it for the local computer.
"
238,https://github.com/stanfordnlp/CoreNLP/issues/308,308,[],closed,2016-11-22 23:59:12+00:00,,"Unable to open ""edu/stanford/nlp/models/kbp/wikidict.tab.gz""","On Firefox, URL http://localhost:9000/, click on the white area in the annotators box to a get a drop down. 'Select wikipedia entities'. The following error is obtained.

java.lang.RuntimeException: edu.stanford.nlp.io.RuntimeIOException: java.io.IOException: Unable to open ""edu/stanford/nlp/models/kbp/wikidict.tab.gz"" as class path, filename or URL

All the other options appear to work. There is no '\*wikidict\*' file in the distribution."
239,https://github.com/stanfordnlp/CoreNLP/issues/309,309,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2016-11-23 20:44:33+00:00,,StanfordCoreNLPClient does not terminate when run as a command line client,"Running CoreNLP 3.6.0 on Max OS X EI Capitan v 10.11.6
The same behavior is observed with CoreNLP 3.7.0 (beta)
```
$ java -version
java version ""1.8.0_66""
Java(TM) SE Runtime Environment (build 1.8.0_66-b17)
Java HotSpot(TM) 64-Bit Server VM (build 25.66-b17, mixed mode)
```

Starting server as:
```
$java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9010
-- listing properties --
port=9010
Starting server on port 9010 with timeout of 5000 milliseconds.
StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9010

```

Starting client as:
```
$java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLPClient -props StanfordCoreNLP-chinese.properties -annotators tokenize,ssplit -file ~/Tmp/input_chinese.txt -outputDirectory ~/Tmp/ -outputFormat text
Processing file /Users/zapreevis/Tmp/input_chinese.txt ... writing to /Users/zapreevis/Tmp/input_chinese.txt.out
Annotating file /Users/zapreevis/Tmp/input_chinese.txt
done.
```

After  **done.** is printed, the processed text is indeed written into */Users/zapreevis/Tmp/input_chinese.txt.out*, yet the client does not quite but keeps running blocking the console.

"
240,https://github.com/stanfordnlp/CoreNLP/issues/310,310,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2016-11-24 07:21:05+00:00,,A bug in /edu/stanford/nlp/simple/Sentence.java,"in method public SentimentClass sentiment(Properties props){},   'case ""verynegative"":' is wrong , should be changed to 'case ""very negative"":'. "
241,https://github.com/stanfordnlp/CoreNLP/issues/311,311,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2016-11-25 04:03:53+00:00,,Unable to instantiate TokensRegexAnnotator under windows,"The constructor of `TokensRegexAnnotator` relies on `PropertiesUtils.getStringArray` to extract the rules property. This in turn uses `MetaClass.cast` to cast the property string to an array. This then delegates to `StringUtils.decodeArray`, which turns the string `C:\Users\BELLCH~1\AppData\Local\Temp\bill-ie5804201486895318826regex_rules.txt` (a valid windows filepath) into `[""C:UsersBELLCH~1AppDataLocalTempbill-ie5804201486895318826regex_rules.txt""]` as it skips over the backslash characters (`chars[i] == '\\'`)."
242,https://github.com/stanfordnlp/CoreNLP/issues/312,312,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}, {'id': 547907037, 'node_id': 'MDU6TGFiZWw1NDc5MDcwMzc=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/multilingual', 'name': 'multilingual', 'color': 'fef2c0', 'default': False, 'description': None}]",closed,2016-11-27 09:37:11+00:00,,French POS tagger outputs French Treebank POS tags and dependency parsing expects Universal Dependencies tags,"The French POS tagger provided by CoreNLP outputs French Treebank POS tags and the French dependency parser have been trained with UniversalDependencies POS tags.
So, it is not possible to use CoreNLP POS tagger to run the CoreNLP dependency parsing.

I have written a hack to CoreNLP in order to make the French POS tagger output UD POS tags: https://github.com/askplatypus/CoreNLP/commit/e6215bdc5d4903bc3e2d2fb533da7e3938fa825f

See also: http://stackoverflow.com/questions/36634101/dependency-parsing-for-french-with-corenlp
and https://mailman.stanford.edu/pipermail/java-nlp-user/2016-April/007560.html"
243,https://github.com/stanfordnlp/CoreNLP/issues/313,313,[],closed,2016-11-28 05:23:05+00:00,,java.lang.OutOfMemoryError: Java heap space,"I am trying to do sentiment analysis on a text file with about 10, 000 lines and each line per sentence. I run the tool with command:

    java -cp ""*"" -Xmx10g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,parse,sentiment -file ../gp/post_content/DOTA2.txt_content.txt

But it throws an error: 

    Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3332)
	at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:137)
	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:121)
	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:421)
	at java.lang.StringBuilder.append(StringBuilder.java:136)
	at edu.stanford.nlp.util.logging.OutputHandler.handle(OutputHandler.java:393)
	at edu.stanford.nlp.util.logging.Redwood$RecordHandlerTree.process(Redwood.java:743)
	at edu.stanford.nlp.util.logging.Redwood$RecordHandlerTree.process(Redwood.java:779)
	at edu.stanford.nlp.util.logging.Redwood$RecordHandlerTree.access$000(Redwood.java:620)
	at edu.stanford.nlp.util.logging.Redwood.log(Redwood.java:306)
	at edu.stanford.nlp.util.logging.Redwood$RedwoodChannels.log(Redwood.java:1231)
	at edu.stanford.nlp.util.logging.Redwood$RedwoodChannels.info(Redwood.java:1283)
	at edu.stanford.nlp.pipeline.ParserAnnotator.doOneSentence(ParserAnnotator.java:358)
	at edu.stanford.nlp.pipeline.ParserAnnotator.doOneSentence(ParserAnnotator.java:254)
	at edu.stanford.nlp.pipeline.SentenceAnnotator.annotate(SentenceAnnotator.java:102)
	at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:75)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:605)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:615)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$37/22429093.accept(Unknown Source)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1167)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:948)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1256)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1326)

I wonder how can I solve this issue? Thanks in advance!"
244,https://github.com/stanfordnlp/CoreNLP/issues/314,314,[],closed,2016-11-28 16:05:02+00:00,,"Stanford Core NLP gets tokens lost with: '-annotators tokenize,ssplit'","When running version 3.6.0 for Chinese, I see that when doing tokenization and sentence splitting some characters get disappeared! 

Server:
`$java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9010`

Client:
`$java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLPClient -props StanfordCoreNLP-chinese.properties -annotators tokenize,ssplit -file ~/Tmp/input_chinese.txt -outputDirectory ~/Tmp/ -outputFormat text -port 9010`

Content of the Chinese inptut text file:
`$cat ~/Tmp/input_chinese.txt`
`Â∑¥ÊãâÁâπ ËØ¥ Ôºö „Äå Êàë‰ª¨ Êú™ ÂÜç Ëé∑Âæó ‰ªª‰Ωï ÁªìÊûú „ÄÇ „Äç`
`„ÄäÈáëËûçÊó∂Êä•„ÄãÂë®‰∏â`
`$`

The result is:
 `$cat ~/Tmp/input_chinese.txt`
`Sentence #1 (10 tokens):`
`Â∑¥ÊãâÁâπ ËØ¥ Ôºö „Äå Êàë‰ª¨ Êú™ ÂÜç Ëé∑Âæó ‰ªª‰Ωï ÁªìÊûú „ÄÇ`
`[Text=Â∑¥ÊãâÁâπ CharacterOffsetBegin=0 CharacterOffsetEnd=3]`
`[Text=ËØ¥ CharacterOffsetBegin=4 CharacterOffsetEnd=5]`
`[Text=Ôºö CharacterOffsetBegin=6 CharacterOffsetEnd=7]`
`[Text=Êàë‰ª¨ CharacterOffsetBegin=10 CharacterOffsetEnd=12]`
`[Text=Êú™ CharacterOffsetBegin=13 CharacterOffsetEnd=14]`
`[Text=ÂÜç CharacterOffsetBegin=15 CharacterOffsetEnd=16]`
`[Text=Ëé∑Âæó CharacterOffsetBegin=17 CharacterOffsetEnd=19]`
`[Text=‰ªª‰Ωï CharacterOffsetBegin=20 CharacterOffsetEnd=22]`
`[Text=ÁªìÊûú CharacterOffsetBegin=23 CharacterOffsetEnd=25]`
`[Text=„ÄÇ CharacterOffsetBegin=26 CharacterOffsetEnd=27]`
`Sentence #2 (2 tokens):`
`ÈáëËûçÊó∂Êä•„ÄãÂë®‰∏â`
`[Text=ÈáëËûçÊó∂Êä• CharacterOffsetBegin=31 CharacterOffsetEnd=35]`
`[Text=Âë®‰∏â CharacterOffsetBegin=36 CharacterOffsetEnd=38]`

Here there are two issues:

1. Characters „Äçand  are„Ää skipped completely, even from the complete sentences
2. The characters  „Ää, „Äã, „Äå and „Äçare not present as tokens.
"
245,https://github.com/stanfordnlp/CoreNLP/issues/315,315,[],closed,2016-11-29 07:13:08+00:00,,TokenizerAnnotator: unknown tokenize.language property zh version 3.6.0,"I read source code and it looks like 
` public enum TokenizerType {
    Unspecified(null, null, ""invertible,ptb3Escaping=true""),
    Spanish    (""es"", ""SpanishTokenizer"", ""invertible,ptb3Escaping=true,splitAll=true""),
    English    (""en"", ""PTBTokenizer"", ""invertible,ptb3Escaping=true""),
    German     (""de"", null, ""invertible,ptb3Escaping=true""),
    French     (""fr"", ""FrenchTokenizer"", """"),
    Whitespace (null, ""WhitespaceTokenizer"", """");

    private final String abbreviation;
    private final String className;
    private final String defaultOptions;

    TokenizerType(String abbreviation, String className, String defaultOptions) {
      this.abbreviation = abbreviation;
      this.className = className;
      this.defaultOptions = defaultOptions;
    }`


i can't find zh in TokenizerType
"
246,https://github.com/stanfordnlp/CoreNLP/issues/316,316,[],closed,2016-11-29 23:00:06+00:00,,"Sentence type: Imperative, Declarative, Interrogative.","There is also exclamatory, but that does not appear syntactically different from the others except for an exclamation point.

This sentence type annotation may be somewhere but is not obvious in the annotation documentation though it seems like an easy thing to do. Stanford Pattern-based Information Extraction and Diagnostics (SPIED) under the Bootstrap link looks like it may work for this.

"
247,https://github.com/stanfordnlp/CoreNLP/issues/317,317,[],open,2016-12-02 12:01:46+00:00,,Chinese Dictionary not work as proposed in Segmentation,"Hi, I'm working on a Chinese Word Segment task. According to the CoreNLP doc, I can add some additional dictionary files into segment.serDictionary to improve the word segment result.

So I make up a txt file containing 5 Chinese words as an additional segment.serDictionary:
~~~~~~~~~~~~~
Â±èÂπï‰øùÊä§Á®ãÂ∫è
È´òË¥®Èáè
ÂéªÂì™ÂÑøÁΩë
Ê∑±Â∫¶Â≠¶‰π†
Ê∑±Â∫¶Â≠¶‰π†
~~~~~~~~~~~~~
And what I expect is that every word in the txt file won't be split up by Seg Annotator.

However, when I run Word Segment on the same file, I only get ""ÂéªÂì™ÂÑøÁΩë"" as a whole word, and other words come into pieces.
Using CoreNLP, Chinese model 3.7.0 Beta.

So my question is:
1. What happened? Is it an expected behavior? (serDictionary is indicating but not a forcing rule)
2. What's the best way I should do if I want to do that (prevent some segmentation) ? I found I couldn't simply use a NERAnnotator because some word/char is falsely segmented.
3. And more, can I have a POS dictionary (even successfully seg into a whole word, it still can get a wrong POS)? or I have to implement an Annotator by my own?

I tried to read the code, but still feel confusing, hope and thanks for any kind explanation"
248,https://github.com/stanfordnlp/CoreNLP/issues/318,318,[],closed,2016-12-03 23:38:29+00:00,,NoClassDefFoundError - Message I keep getting,"Hi,

I've added all the jar files in NetBeans to the library folder, but I still keep getting the:

NoClassDefFoundError

error whenever I run the project. The class(es) that it's looking for are the Document and Sentence class. As this is the Core NLP Simple version, the import statement works and NetBeans doesn't show any problems in the editor, but when I run the project it gives me this error.

Would appreciate the help. "
249,https://github.com/stanfordnlp/CoreNLP/issues/319,319,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 737152820, 'node_id': 'MDU6TGFiZWw3MzcxNTI4MjA=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/multithread', 'name': 'multithread', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2016-12-04 00:52:34+00:00,,Timing information does not wait for last thread to finish,"The timing information given by CoreNLP at the end does not wait for last thread to finish and is thus wrong (even though the information on when all threads were running in parallel may be interesting, too).

Simple test: Use 4 threads and parse 1 file only:

    Annotation pipeline timing information:
    TokenizerAnnotator: 0.0 sec.
    CleanXmlAnnotator: 0.0 sec.
    WordsToSentencesAnnotator: 0.0 sec.
    POSTaggerAnnotator: 0.0 sec.
    TrueCaseAnnotator: 0.0 sec.
    MorphaAnnotator: 0.0 sec.
    NERCombinerAnnotator: 0.0 sec.
    ParserAnnotator: 0.0 sec.
    DependencyParseAnnotator: 0.0 sec.
    DeterministicCorefAnnotator: 0.0 sec.
    RelationExtractorAnnotator: 0.0 sec.
    NaturalLogicAnnotator: 0.0 sec.
    QuoteAnnotator: 0.0 sec.
    SentimentAnnotator: 0.0 sec.
    TOTAL: 0.0 sec. for 0 tokens at NaN tokens/sec.
    Pipeline setup: 45.5 sec.
    Total time for StanfordCoreNLP pipeline: 45.5 sec.
    Annotating file /home/pruhrig/stanford-corenlp-full-2016-10-31/test_normal.txt ... done [0.7 sec].

"
250,https://github.com/stanfordnlp/CoreNLP/issues/320,320,[],closed,2016-12-14 07:07:19+00:00,,Null pointer exception when using CoreNLP server for processing Chinese,"Hi,

When I use `wget --post-data '<insert Chinese text here>' 'localhost:9000/?properties={""pipelineLanguage"":""zh"", ""outputFormat"":""json""}' -O -` to process Chinese, as suggested in [this thread](https://github.com/stanfordnlp/CoreNLP/issues/221), I get a NullPointerException:

java.lang.NullPointerException
at java.util.Properties$LineReader.readLine(Properties.java:434)
at java.util.Properties.load0(Properties.java:353)
at java.util.Properties.load(Properties.java:341)
at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.getProperties(StanfordCoreNLPServer.java:681)
at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:540) at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)

I have already put the Chinese model file `stanford-chinese-corenlp-2016-01-19-models.jar` into the root folder of CoreNLP. To have better chance of getting things to work, I also put a copy of `StanfordCoreNLP-chinese.properties` into the root folder. However, I have no luck. The same NullPointerException keeps popping up. If I change `pipelineLanguage` to `en` it works. If I change `pipelineLanguage` to `fr` or `de` it doesn't work. (I haven't downloaded the French or German models, so that could be the reason why it fails on the last two occasions. But regardless, I have the Chinese models and I'm still getting the same error messages for setting `pipelineLanguage` to `zh`.)

I can get the non-server program to work with Chinese, using the command:
`java -mx3g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -props StanfordCoreNLP-chinese.properties -file chinese.txt -outputFormat text`

Someone please help. I only have a couple of days."
251,https://github.com/stanfordnlp/CoreNLP/issues/321,321,"[{'id': 45387507, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNw==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/cantreproduce', 'name': 'cantreproduce', 'color': 'dddddd', 'default': False, 'description': None}]",closed,2016-12-14 07:43:33+00:00,,"Exception in thread ""main"" java.lang.NoSuchMethodError: edu.stanford.nlp.process.WordToSentenceProcessor.<init>(Ljava/lang/String;Ljava/lang/String;Ljava/util/Set;Ljava/util/Set;Ledu/stanford/nlp/process/WordToSentenceProcessor$NewlineIsSentenceBreak;Ledu/stanford/nlp/ling/tokensregex/SequencePattern;Ljava/util/Set;)V","exception stack Ôºö
Exception in thread ""main"" java.lang.NoSuchMethodError: edu.stanford.nlp.process.WordToSentenceProcessor.<init>(Ljava/lang/String;Ljava/lang/String;Ljava/util/Set;Ljava/util/Set;Ledu/stanford/nlp/process/WordToSentenceProcessor$NewlineIsSentenceBreak;Ledu/stanford/nlp/ling/tokensregex/SequencePattern;Ljava/util/Set;)V
	at edu.stanford.nlp.pipeline.WordsToSentencesAnnotator.<init>(WordsToSentencesAnnotator.java:53)
	at edu.stanford.nlp.pipeline.AnnotatorImplementations.wordToSentences(AnnotatorImplementations.java:54)
	at edu.stanford.nlp.pipeline.AnnotatorFactories$3.create(AnnotatorFactories.java:213)
	at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:152)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:451)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:154)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:150)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:137)

‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äîhere is my code like thatÔºö‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
`
Properties props = 
		PropertiesUtils.asProperties(""annotators"", "" tokenize, ssplit, pos, lemma, ner, parse, mention, coref"",
				""tokenize.language"" ,""zh"",
				""segment.model"" ,""edu/stanford/nlp/models/segmenter/chinese/ctb.gz"",
				""segment.sighanCorporaDict"", ""edu/stanford/nlp/models/segmenter/chinese"",
				""segment.serDictionary"" , ""edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz"",
				""segment.sighanPostProcessing"" , ""true"",
				""ssplit.boundaryTokenRegex"" , ""[.„ÄÇ]|[!?ÔºÅÔºü]+"",
				""pos.model"" , ""edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger"",
				""ner.language"" , ""chinese"",
				""ner.model"", ""edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz"",
				""ner.applyNumericClassifiers"" ,""true"",
				""ner.useSUTime"", ""false"",
				""regexner.mapping"",""edu/stanford/nlp/models/kbp/cn_regexner_mapping.tab"",
				""regexner.validpospattern"" , ""^(NR|NN|JJ).*"",
				""regexner.ignorecase"" , ""true"",
				""regexner.noDefaultOverwriteLabels"" , ""CITY"",
				""parse.model"" , ""edu/stanford/nlp/models/srparser/chineseSR.ser.gz"",
				""depparse.model"",""edu/stanford/nlp/models/parser/nndep/UD_Chinese.gz"",
				""depparse.language"", ""chinese"",
				""coref.sieves"" ,""ChineseHeadMatch, ExactStringMatch, PreciseConstructs, StrictHeadMatch1, StrictHeadMatch2, StrictHeadMatch3, StrictHeadMatch4, PronounMatch"",
				""coref.input.type"" ,""raw"",
				""coref.postprocessing"" ,""true"",
				""coref.calculateFeatureImportance"" , ""false"",
				""coref.useConstituencyTree"" , ""true"",
				""coref.useSemantics"" , ""false"",
				""coref.algorithm"" ,""hybrid"",
				""coref.path.word2vec"" ,"""",
				""coref.language"" , ""zh"",
				""coref.defaultPronounAgreement"" , ""true"",
				""coref.zh.dict"" , ""edu/stanford/nlp/models/dcoref/zh-attributes.txt.gz"",
				""coref.print.md.log"" , ""false"",
				""coref.md.type"" , ""RULE"",
				""coref.md.liberalChineseMD"" ,""false""
				);
		StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

		// read some text in the text variable
		String text = ""ËøôÈáåÊòØ‰∏≠ÂõΩ"";
		// create an empty Annotation just with the given text
		Annotation document = new Annotation(text);
		// run all Annotators on this text
		pipeline.annotate(document);


`

"
252,https://github.com/stanfordnlp/CoreNLP/issues/322,322,"[{'id': 284605708, 'node_id': 'MDU6TGFiZWwyODQ2MDU3MDg=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/cleanup', 'name': 'cleanup', 'color': 'fbca04', 'default': False, 'description': None}]",closed,2016-12-15 01:19:45+00:00,,"Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: Error deserializing / invalid stream header:","As soon as I complete the training, I'm saving the trained LinearClassifier resulting from method [makeClassifier](http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/classify/ColumnDataClassifier.html#makeClassifier-edu.stanford.nlp.classify.GeneralDataset-) using the static method [writeClassifier](http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/classify/LinearClassifier.html#writeClassifier-edu.stanford.nlp.classify.LinearClassifier-java.lang.String-)
When I try to load the saved model using the command line tool property `loadClassifier` I get an error:

```
ColumnDataClassifier invoked on Thu Dec 15 02:10:09 CET 2016 with arguments:
   -loadClassifier ./my_model
Loading classifier from ./my_model...
Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: Error deserializing ./my_model
	at edu.stanford.nlp.classify.ColumnDataClassifier.setProperties(ColumnDataClassifier.java:1580)
	at edu.stanford.nlp.classify.ColumnDataClassifier.<init>(ColumnDataClassifier.java:1926)
	at edu.stanford.nlp.classify.ColumnDataClassifier.main(ColumnDataClassifier.java:1946)
Caused by: java.io.StreamCorruptedException: invalid stream header: 303D4241
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:808)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:301)
	at edu.stanford.nlp.io.IOUtils.readStreamFromString(IOUtils.java:398)
	at edu.stanford.nlp.classify.ColumnDataClassifier.setProperties(ColumnDataClassifier.java:1574)
	... 2 more
```

While using the command line as usual - `serializeTo` and `loadClassifier` properties to save and load automatically, it works.

If I look at the source code I can see how a `LinearClassifier` will be load from the `loadClassifier` path [here](https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/classify/ColumnDataClassifier.java#L1586) using an `IOUtils.readStreamFromString`, and the `IOUtils.writeStreamFromString` is used to serialize the classifier to the disk [here](https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/classify/ColumnDataClassifier.java#L2039), while the static method `writeClassifier` is using the `IOUtils.writeObjectToFile`, so apparently those two apis seem to do not have the same `BOM`:

```
Caused by: java.io.StreamCorruptedException: invalid stream header: 303D4241
```"
253,https://github.com/stanfordnlp/CoreNLP/issues/324,324,[],closed,2016-12-20 21:25:55+00:00,,Release version 3.7.0 to maven central,Last Maven central release is 3.6.0 from 2016 January. CoreNLP has been updated since then.
254,https://github.com/stanfordnlp/CoreNLP/issues/328,328,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 706055902, 'node_id': 'MDU6TGFiZWw3MDYwNTU5MDI=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/tokenize', 'name': 'tokenize', 'color': 'c5def5', 'default': False, 'description': None}]",open,2016-12-28 08:25:55+00:00,,Vanishing features on use of  flags.useOutDict2 in Gale2007ChineseSegmenterFeatureFactory ,"Code block of `edu.stanford.nlp.wordseg.Gale2007ChineseSegmenterFeatureFactory`  from 490 to 497 tries to match an out dict as feature. 
```
     features.add(outDict.getW(charp+charc)+""outdict"");       // -1 0
     features.add(outDict.getW(charc+charc2)+""outdict"");      // 0 1
     features.add(outDict.getW(charp2+charp)+""outdict"");      // -2 -1
     features.add(outDict.getW(charp2+charp+charc)+""outdict"");      // -2 -1 0
     features.add(outDict.getW(charp3+charp2+charp)+""outdict"");      // -3 -2 -1
     ...
```
https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/wordseg/Gale2007ChineseSegmenterFeatureFactory.java#L490

`outDict.getW(String)` returns either `0` or `1`, so the real features produced by sample code above might be `[0outdict, 0outdict, 1outdict, 0outdict, 0outdict]`.   The problem is that in `getCliqueFeatures` (https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/wordseg/Gale2007ChineseSegmenterFeatureFactory.java#L83), the container for all these features is a `HashSet`, so features with the same name will be overridden.
"
255,https://github.com/stanfordnlp/CoreNLP/issues/329,329,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2017-01-01 00:18:38+00:00,,Token offset misalignment when ssplit.newlineIsSentenceBreak is set to always,"Hello,

I'm running into some issue with the token offsets when `ssplit.newlineIsSentenceBreak` is set to `always` (or `two`). It seems that new newline tokens are introduced but the token offsets are not adjusted (for both `SentencesAnnotation` and `CorefMention` in `CorefChain` as far as I can tell). I tested both 3.6.0 and 3.7.0 versions and the issue appears in both cases. Please see the example code and the output below.

Code:
```java
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.pipeline.Annotation;

import edu.stanford.nlp.util.CoreMap;
import edu.stanford.nlp.ling.CoreAnnotations.TokenBeginAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.CorefChainAnnotation;

public class Demo {

  public static void main(String[] args) {
      Properties props = new Properties();
      props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
      props.setProperty(""ssplit.newlineIsSentenceBreak"", ""always"");
      StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
      String text = ""Stanford University is located in California.\nIt is a great university, founded in 1891."";
      Annotation document = new Annotation(text);
      pipeline.annotate(document);
      List<CoreMap> sentences = document.get(SentencesAnnotation.class);
      List<CoreLabel> tokens = document.get(TokensAnnotation.class);
      for (CoreMap sentence: sentences) {
          System.out.println(""Sentence: "" + sentence.get(TextAnnotation.class));
          System.out.println(""The first token is "" + tokens.get(sentence.get(TokenBeginAnnotation.class)));
      }
  }

}
```

Output:
```
Sentence: Stanford University is located in California.
The first token is Stanford-1
Sentence: It is a great university, founded in 1891.
The first token is *NL*
```
The first token of the second sentence should be `It`.

Thanks!"
256,https://github.com/stanfordnlp/CoreNLP/issues/332,332,[],closed,2017-01-04 16:14:05+00:00,,NER cannot work correctly with Cyprus and cyprus ,"Step:
1. search Cyprus: ""ner"":""POSITION""
2. search cyprus: ""ner"":""O""
3. search Cyprus: ""ner"":""O""

After those three steps, we cannot get correct ner with 'Cyprus'.

In 3.6.0"
257,https://github.com/stanfordnlp/CoreNLP/issues/333,333,"[{'id': 737144532, 'node_id': 'MDU6TGFiZWw3MzcxNDQ1MzI=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/custom-annotator', 'name': 'custom-annotator', 'color': 'c5def5', 'default': False, 'description': None}, {'id': 737145153, 'node_id': 'MDU6TGFiZWw3MzcxNDUxNTM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/annotator-pool', 'name': 'annotator-pool', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2017-01-04 23:26:34+00:00,,customAnnotatorClass static object ?,"I can create a customAnnotator with some parameters in a properties object and then create a brand new pipeline with 

    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

My customAnnotator works fine. Now when I create another pipeline object with the customAnnotator and some other parameters defined in a new properties file, the customAnnotator class gets not instantiated a second time. Instead, the second pipeline seems to be using the created class of the customAnnotator from the first pipeline.

It looks to me that customAnnotators in CoreNLP live somehow in static objects, and it is not intended to create a second pipeline with the same annotator and different parameters in a single JVM, but maybe I am missing something.

Clearing the pool does not help, as this only sets the pool to null.

    pipeline.clearAnnotatorPool();

Is this intended behavior or a bug? (CoreNLP 3.7.0)

How can I instantiate a new object of class StanfordCoreNLP, so that a customAnnotators constructor is also called every time and its object created every time anew?"
258,https://github.com/stanfordnlp/CoreNLP/issues/334,334,[],closed,2017-01-08 11:43:47+00:00,,Wrong slf4j.jar version?,"I have the feeling that the current `slf4j` version shipped within Stanford CoreNLP is different from the one that is actually required by it. This command will fail:

```bash
/usr/bin/java -mx1000m -cp my_path/stanford_core_nlp/stanford-postagger-full-2016-10-31/stanford-postagger-3.7.0.jar:my_path/stanford_core_nlp/stanford-postagger-full-2016-10-31/stanford-postagger.jar:my_path/stanford_core_nlp/stanford-postagger-full-2016-10-31/stanford-postagger-3.7.0-sources.jar:my_path/stanford_core_nlp/stanford-postagger-full-2016-10-31/stanford-postagger-3.7.0-javadoc.jar:my_path/stanford_core_nlp/stanford-postagger-full-2016-10-31/lib/slf4j-simple.jar:my_path/stanford_core_nlp/stanford-postagger-full-2016-10-31/lib/slf4j-api.jar edu.stanford.nlp.tagger.maxent.MaxentTagger -model my_path/stanford_core_nlp/stanford-postagger-full-2016-10-31/models/english-bidirectional-distsim.tagger -textFile /tmp/tmp1ap3auf6 -tokenize false -outputFormatOptions keepEmptySentences -encoding utf8
```
The error is the following:

```
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.<clinit>(MaxentTagger.java:224)
Caused by: java.lang.IllegalStateException: Could not find SLF4J in your classpath
	at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers.lambda$static$530(RedwoodConfiguration.java:190)
	at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers$7.buildChain(RedwoodConfiguration.java:309)
	at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers$7.apply(RedwoodConfiguration.java:318)
	at edu.stanford.nlp.util.logging.RedwoodConfiguration.lambda$handlers$535(RedwoodConfiguration.java:363)
	at edu.stanford.nlp.util.logging.RedwoodConfiguration.apply(RedwoodConfiguration.java:41)
	at edu.stanford.nlp.util.logging.Redwood.<clinit>(Redwood.java:609)
	... 1 more
Caused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: java.lang.ClassNotFoundException: edu.stanford.nlp.util.logging.SLF4JHandler
	at edu.stanford.nlp.util.MetaClass.createFactory(MetaClass.java:364)
	at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:381)
	at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers.lambda$static$530(RedwoodConfiguration.java:186)
	... 6 more
Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.util.logging.SLF4JHandler
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at edu.stanford.nlp.util.MetaClass$ClassFactory.construct(MetaClass.java:135)
	at edu.stanford.nlp.util.MetaClass$ClassFactory.<init>(MetaClass.java:202)
	at edu.stanford.nlp.util.MetaClass$ClassFactory.<init>(MetaClass.java:69)
	at edu.stanford.nlp.util.MetaClass.createFactory(MetaClass.java:360)
```

The manifest file of `lib/slf4j-api.jar` that you are currently shipping states:
```
Bundle-Description: The slf4j API
Bundle-Version: 1.7.2
Implementation-Version: 1.7.2
```

Conversely, in your [`core-nlp/pom-full.xml`][1] you state that version `1.7.12` is required:
```xml
<dependency>
  <groupId>org.slf4j</groupId>
  <artifactId>slf4j-api</artifactId>
  <version>1.7.12</version>
</dependency>
```

If I substitute your `lib/slf4j-api.jar` with the `1.7.12` version, I can correctly run the above command and get the result:
```bash
Loading default properties from tagger my_path/stanford_core_nlp/stanford-postagger-full-2016-10-31/models/english-bidirectional-distsim.tagger
Loading POS tagger from my_path/stanford_core_nlp/stanford-postagger-full-2016-10-31/models/english-bidirectional-distsim.tagger ... done [2.3 sec].
What_WP is_VBZ the_DT airspeed_NN of_IN an_DT unladen_JJ swallow_VB ?_.
Tagged 9 words at 65.22 words per second.
```

[1]: https://github.com/stanfordnlp/CoreNLP/blob/3eeaff39eee27287925ed4c625e7ac167bccfba1/doc/corenlp/pom-full.xml#L114"
259,https://github.com/stanfordnlp/CoreNLP/issues/335,335,[],closed,2017-01-09 17:51:11+00:00,,"Getting programmatically accuracy/micro-averaged F1: -0,00000","I'm getting a wrong(?) accuracy like

```
Average accuracy/micro-averaged F1: -0,00000
Average macro-averaged F1: -0,00000
```

when calling programmatically the training. 

```
1.usePrefixSuffixNGrams = true
QNsize = 15
printClassifier = HighWeight
useQN = true
displayAllAnswers = true
goldAnswerColumn = 0
1.minNGramLeng = 1
trainFile = /.../dataset_train.csv
tolerance = 0.0001
1.maxNGramLeng = 4
testFile = /.../dataset_test.csv
sigma = 4
useNGrams = true
printClassifierParam = 200
intern = true
displayedColumn = 1
1.useNGrams = true
Reading dataset from /.../dataset_train.csv ... done [11,5s, 2930 items].
numDatums: 2930
numDatumsPerLabel: {NOTEXPLICIT=1466.0, MILDLYEXPLICIT=219.0, EXPLICIT=1245.0}
numLabels: 3 [EXPLICIT, NOTEXPLICIT, MILDLYEXPLICIT]
numFeatures (Phi(X) types): 75065 [1-#-ts a, 1-#-e ki, 1-#-t , 1-#-ag w, 1-#-ts c, ...]
Reading dataset from /.../dataset_test.csv ... done [1,2s, 526 items].
Average accuracy/micro-averaged F1: -0,00000
Average macro-averaged F1: -0,00000

```

This does not happen in when using the jar with the property file:

```
Classifier from explicit/train.prop
Invoked on Mon Jan 09 17:43:27 CET 2017 with arguments: -prop explicit/train.prop
useQN = true
testFile = /.../dataset_test.csv
1.minNGramLeng = 1
displayAllAnswers = 1
intern = true
goldAnswerColumn = 0
printTo = /.../classifier_model_linear
displayedColumn = 1
printClassifierParam = 200
1.useNGrams = true
QNsize = 15
sigma = 4
serializeTo = /...classifier_model
tolerance = 0.0001
trainFile = /...dataset_train.csv
1.usePrefixSuffixNGrams = true
1.maxNGramLeng = 4
printClassifier = HighWeight
^Cmacbookproloretolocal:classifiers loretoparisi$ ./trainortest.sh explicit/train.prop 
Classifier from explicit/train.prop
Invoked on Mon Jan 09 17:43:46 CET 2017 with arguments: -prop explicit/train.prop
useQN = true
testFile = /...dataset_test.csv
1.minNGramLeng = 1
displayAllAnswers = 1
intern = true
goldAnswerColumn = 0
printTo = /...classifier_model_linear
displayedColumn = 1
printClassifierParam = 200
1.useNGrams = true
QNsize = 15
sigma = 4
serializeTo = /...classifier_model
tolerance = 0.0001
trainFile = /...dataset_train.csv
1.usePrefixSuffixNGrams = true
1.maxNGramLeng = 4
printClassifier = HighWeight
Reading dataset from /...dataset_train.csv ... done [10,5s, 2930 items].
numDatums: 2930
numDatumsPerLabel: {NOTEXPLICIT=1466.0, MILDLYEXPLICIT=219.0, EXPLICIT=1245.0}
numLabels: 3 [EXPLICIT, NOTEXPLICIT, MILDLYEXPLICIT]
numFeatures (Phi(X) types): 75065 [1-#-ts a, 1-#-e ki, 1-#-t , 1-#-ag w, 1-#-ts c, ...]
Built classifier described in file /...classifier_model_linear
Serializing classifier to /...classifier_model...
Done.
Reading dataset from /...dataset_test.csv ... done [1,1s, 526 items]
```

"
260,https://github.com/stanfordnlp/CoreNLP/issues/336,336,[],open,2017-01-10 08:03:48+00:00,,Training own true case  models,"Hi,
There seems to be many inconsistencies in the truce casing model , hence I need to retrain it on my data , how can I train the same using my own training data?
I need examples for the following training settings, i.e what is the format of noUN.input:
https://github.com/jnorthrup/stanford-corenlp/blob/master/src/main/resources/edu/stanford/nlp/models/truecase/truecasing.fast.prop
```
serializeTo=truecasing.fast.qn.ser.gz
trainFileList=/scr/nlp/data/gale/NIST09/truecaser/crf/noUN.input
testFile=/scr/nlp/data/gale/AE-MT-eval-data/mt06/cased/ref0

```"
261,https://github.com/stanfordnlp/CoreNLP/issues/338,338,[],closed,2017-01-11 14:18:47+00:00,,Handling unclosed quotations,"Currently in the quotations analyzer, it writes a warning to the console that unclosed quotations were found, but there is no way to get those notifications programatically. 

Is there a way to make those notifications visible to users as an interim to implementing the ""closeUnclosedQuotes"" feature (which is currently marked with  todo)

https://github.com/stanfordnlp/CoreNLP/search?utf8=%E2%9C%93&q=closeUnclosedQuotes

Here's the relevant code from QuoteAnnotator.java:

```
//    // TODO: determine if we want to be more strict w/ single quotes than double
//    // answer: we do want to.
//    // if we reached then end and we have an open quote, close it
//    if (closeUnclosedQuotes && start >= 0 && start < text.length() - 2) {
//      if (!quotesMap.containsKey(quote)) {
//        quotesMap.put(quote, new ArrayList<>());
//      }
//      quotesMap.get(quote).add(new Pair(start, text.length()));
//    } else
    if (start >= 0 && start < text.length() - 3) {
      String warning = text;
      if (text.length() > 150) {
        warning = text.substring(0, 150) + ""..."";
      }
      log.info(""WARNING: unmatched quote of type "" +
          quote + "" found at index "" + start + "" in text segment: "" + warning);
    }
```
"
262,https://github.com/stanfordnlp/CoreNLP/issues/339,339,[],closed,2017-01-12 06:08:49+00:00,,Chinese Word Segmentation result difference due to the period mark ,"I tried to use CoreNLP Chinese word segmenter to do the segmentation of the following two sentencesÔºå ""Ê≤°ÊúâËßÅËøá‰Ω†Âê¨ÊëáÊªö‰πê„ÄÇ"" and ""Ê≤°ÊúâËßÅËøá‰Ω†Âê¨ÊëáÊªö‰πê"" . The only difference is that the first one sentence has period.

The results are 
1. Ê≤°ÊúâËßÅËøá‰Ω†Âê¨ÊëáÊªö‰πê„ÄÇ => Ê≤°Êúâ ËßÅ Ëøá ‰Ω† Âê¨ÊëáÊªö‰πê „ÄÇ 
2. Ê≤°ÊúâËßÅËøá‰Ω†Âê¨ÊëáÊªö‰πê=> Ê≤°Êúâ ËßÅ Ëøá ‰Ω† Âê¨ ÊëáÊªö‰πê

The command options are used as following, when CoreNLP as service 
properties={""tokenize.whitespace"": ""true"", ""annotators"": ""segment,ssplit,pos"", ""outputFormat"": ""json"", ""customAnnotatorClass.segment"": ""edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator"", ""segment.model"": ""edu/stanford/nlp/models/segmenter/chinese/ctb.gz"", ""segment.sighanCorporaDict"": ""edu/stanford/nlp/models/segmenter/chinese"", ""segment.serDictionary"": ""edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz"", ""segment.sighanPostProcessing"": ""true"", ""pos.model"": ""edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger""}


I think Âê¨ÊëáÊªö‰πê should be divided to Âê¨Ôºàlisten to) ÊëáÊªö‰πê(rock music), which means the second sentence is ""correct"". But why does the difference depend on the period mark?

Any thought? Thanks. 


"
263,https://github.com/stanfordnlp/CoreNLP/issues/340,340,[],closed,2017-01-12 16:20:52+00:00,,"Unable to open ""edu/stanford/nlp/pipeline/demo/corenlp-brat.html"" as class path, filename or URL","I have cloned latest CoreNLP repo, built with `mvn package` and put the latest models root in the `CLASSPATH`:


```
CoreNLPDataset loretoparisi$ ls -l
total 3662400
-rw-r--r--@ 1 loretoparisi  staff   362133856 12 Gen 17:03 stanford-corenlp-models-current.jar
-rw-r--r--@ 1 loretoparisi  staff  1039006700 12 Gen 17:07 stanford-english-corenlp-models-current.jar
-rw-r--r--@ 1 loretoparisi  staff   474001837 12 Gen 17:03 stanford-english-kbp-corenlp-models-current.jar
```

When I try to startup the CoreNLP server like `java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer`

 I get

```
$ java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer
--- StanfordCoreNLPServer#main() called ---
setting default constituency parser
warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz
using: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz instead
to use shift reduce parser download English models jar from:
http://stanfordnlp.github.io/CoreNLP/download.html
    Threads: 8
Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: java.io.IOException: Unable to open ""edu/stanford/nlp/pipeline/demo/corenlp-brat.html"" as class path, filename or URL
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.main(StanfordCoreNLPServer.java:1263)
Caused by: java.io.IOException: Unable to open ""edu/stanford/nlp/pipeline/demo/corenlp-brat.html"" as class path, filename or URL
	at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:480)
	at edu.stanford.nlp.io.IOUtils.readerFromString(IOUtils.java:637)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$FileHandler.<init>(StanfordCoreNLPServer.java:460)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$FileHandler.<init>(StanfordCoreNLPServer.java:457)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.main(StanfordCoreNLPServer.java:1261)
CoreNLP Server is shutting down.
```

It seems it cannot found the resource of the html server index page in https://github.com/stanfordnlp/CoreNLP/blob/2e4c4dc48ab8a34f6696757a5351a48412f66d61/src/edu/stanford/nlp/pipeline/StanfordCoreNLPServer.java#L1261


So my guess was a `mvn` problem, trying with `ant` at the end it seems resources have been copied:

```
     [copy] Copying 5 files to /Users/loretoparisi/Documents/Projects/CoreNLP/classes/edu/stanford/nlp/pipeline/demo
     [copy] Copying 13 files to /Users/loretoparisi/Documents/Projects/CoreNLP/classes/edu/stanford/nlp/pipeline

```

but I get the same error starting up the server.

"
264,https://github.com/stanfordnlp/CoreNLP/issues/341,341,[],closed,2017-01-12 20:51:27+00:00,,JSONOutputter Public Classes,"**[Objective:]**
Implement Stanford CoreNLP in Spring Boot container to support a custom authentication mechanism as well as some other goodies.

**[Method:]**
Re-implement/factor `StanfordCoreNLPServer` by breaking apart the core logic from the http server and resource handlers.  (i.e. implementing a Spring `@Service ` for the `StanfordCoreNLPServer`)

**[Issue:]**
The `StanfordCoreNLPServer` uses `JSONOutputter` which has subclasses [`JSONWriter`](https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/JSONOutputter.java#L323) and [`Writer`](https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/JSONOutputter.java#L527), both of these classes are not public.

Is there any reason why these classes are not `public static` instead of `protected static`?  It seems silly to make a copy of JSONWritter just to get access to those classes."
265,https://github.com/stanfordnlp/CoreNLP/issues/342,342,[],closed,2017-01-13 11:00:37+00:00,,MaxSizeConcurrentHashSet.java is not threadsafe,"From the naming I would expect this class to be threadsafe, but when checking for maximum size here:

https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/util/MaxSizeConcurrentHashSet.java#L104 

and here:

https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/util/MaxSizeConcurrentHashSet.java#L74

we see this is not the case."
266,https://github.com/stanfordnlp/CoreNLP/issues/343,343,"[{'id': 547907037, 'node_id': 'MDU6TGFiZWw1NDc5MDcwMzc=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/multilingual', 'name': 'multilingual', 'color': 'fef2c0', 'default': False, 'description': None}]",open,2017-01-13 14:18:45+00:00,,Adding new language models,"Hi, I was interested to know how can be possible to create a new language model.
Let's say that I'm interested in adding an Italian language model, how can I start building that model?

Thanks"
267,https://github.com/stanfordnlp/CoreNLP/issues/344,344,[],closed,2017-01-14 09:38:04+00:00,,Custom Relationship,"Hi all. I'm trying to create my own relationship model and be able to extract my own custom relationships using a training file. I'm trying to use what is provided on StanfordNLP's website as a test, http://nlp.stanford.edu/software/relationExtractor.shtml.

I'm using the default roth.properties file as follows:

```
#Below are some basic options. See edu.stanford.nlp.ie.machinereading.MachineReadingProperties class for more options.

# Pipeline options
annotators = tokenize,ssplit, pos, lemma, parse
parse.maxlen = 100

# MachineReading properties. You need one class to read the dataset into correct format. See edu.stanford.nlp.ie.machinereading.domains.ace.AceReader for another example.
datasetReaderClass = edu.stanford.nlp.ie.machinereading.domains.roth.RothCONLL04Reader

#Data directory for training. The datasetReaderClass reads data from this path and makes corresponding sentences and annotations.
trainPath = kill.corp (http://l2r.cs.uiuc.edu/~cogcomp/Data/ER/kill.corp)

#Whether to crossValidate, that is evaluate, or just train.
crossValidate = false
kfold = 10

#Change this to true if you want to use CoreNLP pipeline generated NER tags. The default model generated with the relation extractor release uses the CoreNLP pipeline provided tags (option set to true).
trainUsePipelineNER= false

# where to save training sentences. uses the file if it exists, otherwise creates it.
serializedTrainingSentencesPath = roth_sentences.ser

serializedEntityExtractorPath = roth_entity_model.ser

# where to store the output of the extractor (sentence objects with relations generated by the model). This is what you will use as the model when using 'relation' annotator in the CoreNLP pipeline.
serializedRelationExtractorPath = roth_relation_model_pipeline.ser

# uncomment to load a serialized model instead of retraining
# loadModel = true

#relationResultsPrinters = edu.stanford.nlp.ie.machinereading.RelationExtractorResultsPrinter,edu.stanford.nlp.ie.machinereading.domains.roth.RothResultsByRelation. For printing output of the model.
relationResultsPrinters = edu.stanford.nlp.ie.machinereading.RelationExtractorResultsPrinter

#In this domain, this is trivial since all the entities are given (or set using CoreNLP NER tagger).
entityClassifier = edu.stanford.nlp.ie.machinereading.domains.roth.RothEntityExtractor

extractRelations = true
extractEvents = false

#We are setting the entities beforehand so the model does not learn how to extract entities etc.
extractEntities = false

#Opposite of crossValidate. 
trainOnly=true

# The set chosen by feature selection using RothCONLL04:
relationFeatures = arg_words,arg_type,dependency_path_lowlevel,dependency_path_words,surface_path_POS,entities_between_args,full_tree_path

# The above features plus the features used in Bjorne BioNLP09:
# relationFeatures = arg_words,arg_type,dependency_path_lowlevel,dependency_path_words,surface_path_POS,entities_between_args,full_tree_path,dependency_path_POS_unigrams,dependency_path_word_n_grams,dependency_path_POS_n_grams,dependency_path_edge_lowlevel_n_grams,dependency_path_edge-node-edge-grams_lowlevel,dependency_path_node-edge-node-grams_lowlevel,dependency_path_directed_bigrams,dependency_path_edge_unigrams,same_head,entity_counts
```

My Java code is as follows:

```
    Properties props = StringUtils.propFileToProperties(""roth.properties"");
    props.setProperty(""sup.relation.model"", ""roth_relation_model_pipeline.ser"");

    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    String text = ""James killed Mary. "";

    Annotation document = new Annotation(text);
    pipeline.annotate(document);

    List<CoreMap> sentences = document.get(SentencesAnnotation.class);

    for (CoreMap sentence : sentences) {
        List<RelationMention> relations = sentence.get(MachineReadingAnnotations.RelationMentionsAnnotation.class);
        for (RelationMention r : relations) {
            System.out.println(r.toString());
        }
    }

```

Serialised files should be created if they do not exists. However, no serialised files were created. I'm getting a NullExceptionPointer error.

Any help is greatly appreciated. Thank you for your time. :1st_place_medal: 
 
"
268,https://github.com/stanfordnlp/CoreNLP/issues/345,345,[],closed,2017-01-16 14:02:02+00:00,,Unique shutdown key file names,"When the Stanford CoreNLP server is started, it creates the file `/tmp/corenlp.shutdown` file, which contains the shutdown key. The problem is, if two servers are started on two different ports (e.g. to speed up analysis on a multicore system), then both instances will write their keys to this file. This, of course, makes it impossible to stop one of them via the shutdown URL.

The solution to this situation is making sure the files have unique names. The easiest way would be to append the port to the name of the file, e.g. `/tmp/corenlp.shutdown.9000`."
269,https://github.com/stanfordnlp/CoreNLP/issues/346,346,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 706055902, 'node_id': 'MDU6TGFiZWw3MDYwNTU5MDI=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/tokenize', 'name': 'tokenize', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2017-01-23 20:14:53+00:00,,PTBEscapingProcessor leaves unescaped parentheses in output,"PTBEscapingProcessor properly escapes parentheses to -LRB- and -RRB-, but seems to unescape them in the output, interfering with the tree format.  For example,

`They live in London ( Ontario ) .`

becomes

`(ROOT (S (NP (PRP They)) (VP (VBP live) (PP (IN in) (NP (NP (NNP London)) (PRN (-LRB- () (NP (NNP Ontario)) (-RRB- )))))) (. .)))
`

Notice that the NP containing `Ontario` has erroneously been made a child of the -LRB-, due to the unescaped left parenthesis.  The expected output is:

`(ROOT (S (NP (PRP They)) (VP (VBP live) (PP (IN in) (NP (NP (NNP London)) (PRN (-LRB- -LRB-) (NP (NNP Ontario)) (-RRB- -RRB-))))) (. .)))`"
270,https://github.com/stanfordnlp/CoreNLP/issues/347,347,[],closed,2017-01-25 17:35:02+00:00,,sentiment annotator: java.lang.NoClassDefFoundError: org/ejml/simple/SimpleBase,"I have added the `sentiment` annotator among the others:

```
annotators=""tokenize,ssplit,pos,lemma,ner,sentiment"";
```

I then get an exception

```
Adding annotator sentiment
{ Error: Error creating class
java.lang.NoClassDefFoundError: org/ejml/simple/SimpleBase
	at edu.stanford.nlp.pipeline.SentimentAnnotator.<init>(SentimentAnnotator.java:52)
	at edu.stanford.nlp.pipeline.AnnotatorImplementations.sentiment(AnnotatorImplementations.java:264)
	at edu.stanford.nlp.pipeline.AnnotatorFactories$16.create(AnnotatorFactories.java:446)
	at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:152)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:451)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:154)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:150)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:137)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
Caused by: java.lang.ClassNotFoundException: org.ejml.simple.SimpleBase
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 12 more

    at Error (native)
    at new StanfordCoreNLP (/Users/loretoparisi/Documents/Projects/musixmatch-intelligence-node-sdk/lib/nlp/stanford/index.js:104:28)
    at cld.detectCLDP.then.res (/Users/loretoparisi/Documents/Projects/musixmatch-intelligence-node-sdk/lib/tests/nlp/corenlp.js:65:31)
    at process._tickCallback (internal/process/next_tick.js:103:7)
    at Module.runMain (module.js:606:11)
    at run (bootstrap_node.js:394:7)
    at startup (bootstrap_node.js:149:9)
    at bootstrap_node.js:509:3 cause: nodeJava_java_lang_NoClassDefFoundError {} }
```

According to external docs, it seems it's missing the Java math library EJML, so I have downloaded from the sources [here](https://github.com/lessthanoptimal/ejml.git) and compiled from scratch using `gradle`.

I have 

```
java -version
java version ""1.8.0_121""
Java(TM) SE Runtime Environment (build 1.8.0_121-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)
```

I then copied the built jars i.e. the following ones in the `CLASSPATH`:

```
core-0.30.jar
dense64-0.30.jar
simple-0.30.jar
```

I have compiled against the JVM 8.
Now my `CLASSPATH` folder looks like

```
simple-0.30-javadoc.jar
simple-0.30-sources.jar
simple-0.30.jar
stanford-arabic-corenlp-2016-10-31-models.jar
stanford-chinese-corenlp-2016-10-31-models.jar
stanford-corenlp-3.7.0-models.jar
stanford-corenlp.jar
stanford-english-corenlp-2016-10-31-models.jar
stanford-english-kbp-corenlp-2016-10-31-models.jar
stanford-french-corenlp-2016-10-31-models.jar
stanford-german-corenlp-2016-10-31-models.jar
stanford-spanish-corenlp-2016-10-31-models.jar
```

When I try to load the annotators I then get a new error

```
{ Error: Error creating class
edu.stanford.nlp.io.RuntimeIOException: java.io.InvalidClassException: org.ejml.simple.SimpleBase; local class incompatible: stream classdesc serialVersionUID = 7560584869544985034, local class serialVersionUID = -4908174115141247692
	at edu.stanford.nlp.sentiment.SentimentModel.loadSerialized(SentimentModel.java:633)
	at edu.stanford.nlp.pipeline.SentimentAnnotator.<init>(SentimentAnnotator.java:52)
	at edu.stanford.nlp.pipeline.AnnotatorImplementations.sentiment(AnnotatorImplementations.java:264)
	at edu.stanford.nlp.pipeline.AnnotatorFactories$16.create(AnnotatorFactories.java:446)
	at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:152)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:451)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:154)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:150)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:137)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
Caused by: java.io.InvalidClassException: org.ejml.simple.SimpleBase; local class incompatible: stream classdesc serialVersionUID = 7560584869544985034, local class serialVersionUID = -4908174115141247692
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:616)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1630)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1630)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)
...
```

that seems to be a JVM compiled class file version error (JVM version). 
The others jars from CoreNLP are ok as I can see from the logs:

```
LANG ENGLISH en
Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0,9 sec].
Adding annotator lemma
Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [2,2 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1,5 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0,8 sec].
Adding annotator sentiment
```

I understand that this could be a bit out of the blue, but since those jars of¬†`EJML` are required, which version and against which JVM must be compiled?

Thanks."
271,https://github.com/stanfordnlp/CoreNLP/issues/348,348,[],closed,2017-01-28 14:29:58+00:00,,Failed to load cusom stopword annotator,"Hello,
I'm maintaining an old project which used CoreNLP 3.2.0, I've updated the dependency to 3.7.0. 
I'm trying to add a [custom annotator](https://github.com/jconwell/coreNlp/blob/master/src/main/java/intoxicant/analytics/coreNlp/StopwordAnnotator.java) that functioned with 3.2.0. Required class was still used back than to set and check for annotators requirement satisfaction.
I get an exception because Annotator$Requiered class is not found. Is there  a way to add this old style annotator? because there is no newer version of if."
272,https://github.com/stanfordnlp/CoreNLP/issues/349,349,[],closed,2017-02-03 11:28:16+00:00,,Euro symbol(‚Ç¨) converting to Dollar symbol($) ,"Hi,

We were trying to extract the euro value from the file. The **stanford** is recognizing the money however its **converting** **‚Ç¨1000** to **$1000**. 
Could you please guide us on how to overcome this issue.

Thankyou"
273,https://github.com/stanfordnlp/CoreNLP/issues/350,350,[],closed,2017-02-03 14:06:04+00:00,,SUTime Language Support,"Hi,

I want to use another language with SUTime (Turkish). While initializing TimeAnnotator, I set new property for sutime.rules. When I change months, days etc. it works perfectly. However, when I want to annotate Ninth April 2017 in Turkish (which is ""Dokuzuncu Nisan 2017""), the phrase can not be captured. It's probably because numbers and ordinals are annotated differently and they are inside the source code. Is it possible to use my definitions of numbers and ordinals without changing the source code?

Thanks."
274,https://github.com/stanfordnlp/CoreNLP/issues/351,351,[],closed,2017-02-03 18:41:12+00:00,,Sentiment depends on the name of an actor,"I use [`stanford-corenlp-full-2016-10-31`](http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip) with Python (as described [here](http://stackoverflow.com/a/40496870/850781)), and I get surprising results:

**the sentiment of a sentence depends on which word is the subject**.

Specifically, I tested the following sentences: `X loves playing footballY` where `X` is a  name from this list:
```
names = [""John"",""Jane"",""Bill"",""Billy"",""Jenny"",""Foo"",""Bar"",""Baz"",""Quux"",""Mike"",
         ""Davie"",""Dave"",""David"",""Sandy"",""House"",""Bob"",""Robert"",""Sam""]
```
and `Y` is period, exclamation point, or empty.
Names were also supplied capitalized and lower-case.

The result is: all sentences were assigned `Positive` sentiment with the following exceptions:
```
Billy [Billy]: Neutral [Billy.]: Neutral
Bar [Bar]: Neutral [Bar.]: Neutral
Dave [Dave!]: Verypositive
David [David]: Neutral [David.]: Neutral
House [House]: Neutral [House.]: Neutral
```

It is expected that period and blank are less positive than `!`.

**However why is `Dave` so much more positive than `David`?**



See also [Stanford NLP sentiment ambiguous result](http://stackoverflow.com/q/42027119/850781)."
275,https://github.com/stanfordnlp/CoreNLP/issues/352,352,[],closed,2017-02-07 14:25:16+00:00,,No roots in graph,"I constantly get the following exception for single-token sentences (at least when the single token is a ""."" or a ""...""):

```
java.lang.RuntimeException: No roots in graph:
dep                 reln                gov                 
---                 ----                ---                 

Find where this graph was created and make sure you're adding roots.
	at edu.stanford.nlp.semgraph.SemanticGraph.getFirstRoot(SemanticGraph.java:790)
	at edu.stanford.nlp.semgraph.semgrex.SemgrexPattern.matcher(SemgrexPattern.java:231)
	at edu.stanford.nlp.trees.UniversalEnglishGrammaticalStructure.demoteQuantificationalModifiers(UniversalEnglishGrammaticalStructure.java:1724)
	at edu.stanford.nlp.trees.UniversalEnglishGrammaticalStructure.addEnhancements(UniversalEnglishGrammaticalStructure.java:871)
	at edu.stanford.nlp.trees.GrammaticalStructure.typedDependenciesEnhancedPlusPlus(GrammaticalStructure.java:924)
	at edu.stanford.nlp.semgraph.SemanticGraphFactory.makeFromTree(SemanticGraphFactory.java:249)
	at edu.stanford.nlp.semgraph.SemanticGraphFactory.generateEnhancedPlusPlusDependencies(SemanticGraphFactory.java:129)
	at edu.stanford.nlp.pipeline.ParserAnnotatorUtils.fillInParseAnnotations(ParserAnnotatorUtils.java:67)
	at edu.stanford.nlp.pipeline.ParserAnnotator.finishSentence(ParserAnnotator.java:297)
	at edu.stanford.nlp.pipeline.ParserAnnotator.doOneSentence(ParserAnnotator.java:267)
	at edu.stanford.nlp.pipeline.SentenceAnnotator$AnnotatorProcessor.process(SentenceAnnotator.java:31)
	at edu.stanford.nlp.pipeline.SentenceAnnotator$AnnotatorProcessor.process(SentenceAnnotator.java:21)
	at edu.stanford.nlp.util.concurrent.MulticoreWrapper$CallableJob.call(MulticoreWrapper.java:255)
	at edu.stanford.nlp.util.concurrent.MulticoreWrapper$CallableJob.call(MulticoreWrapper.java:236)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
```
When I look into the argument given to `edu.stanford.nlp.semgraph.SemanticGraphFactory.makeFromTree` I see the following tree: `(ROOT (NP (: ...)))`, so there seems to be a root set after all. The problem only happens when `SemanticGraphFactory.generateEnhancedPlusPlusDependencies(gsf.newGrammaticalStructure(tree))` is called in `edu.stanford.nlp.pipeline.ParserAnnotatorUtils.fillInParseAnnotations`"
276,https://github.com/stanfordnlp/CoreNLP/issues/353,353,[],closed,2017-02-09 23:13:24+00:00,,Enhanced++ Dependencies arc name descriptions,"The http://localhost:9000/ web-page that communicates with the edu.stanford.nlp.pipeline.StanfordCoreNLPServer provides a nice chart under Enhanced++ Dependencies with somewhat cryptic arc names.

Is there a table of arc names vs. descriptions?"
277,https://github.com/stanfordnlp/CoreNLP/issues/354,354,[],closed,2017-02-10 12:08:09+00:00,,TokenRegex Combining Words,"Hello,
I want to match a phrase combining two words like ""twentytwo"" (without the space). Lets say that both ""twenty"" and ""two"" are annotated as NUMBER ner. Is there a way to achieve that? Thanks"
278,https://github.com/stanfordnlp/CoreNLP/issues/355,355,[],closed,2017-02-11 18:28:52+00:00,,Error in training a model for stanford Pos tagger ,"hi every body, 
i am trying to train the stanford pos tagger for arabic language, to do so i followed this tutorial [tutorial](http://www.florianboudin.org/wiki/doku.php?id=nlp_tools_related&DokuWiki=9d6b70b2ee818e600edc0359e3d7d1e8), but when i do so i am getting this error : 
`Exception in thread ""main"" edu.stanford.nlp.util.ReflectionLoading$ReflectionLoa
dingException: Error creating edu.stanford.nlp.optimization.OWLQNMinimizer
        at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLo
ading.java:40)
        at edu.stanford.nlp.maxent.CGRunner.solveL1(CGRunner.java:184)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.trainAndSaveModel(MaxentT
agger.java:1199)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.runTraining(MaxentTagger.
java:1254)
        at edu.stanford.nlp.tagger.maxent.MaxentTagger.main(MaxentTagger.java:18
87)
Caused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: java.lang.Cla
ssNotFoundException: edu.stanford.nlp.optimization.OWLQNMinimizer
        at edu.stanford.nlp.util.MetaClass.createFactory(MetaClass.java:364)
        at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:381)
        at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLo
ading.java:38)
        ... 4 more
Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.optimization.OWLQN
Minimizer
        at java.net.URLClassLoader.findClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
        at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Unknown Source)
        at edu.stanford.nlp.util.MetaClass$ClassFactory.construct(MetaClass.java
:135)
        at edu.stanford.nlp.util.MetaClass$ClassFactory.<init>(MetaClass.java:20
2)
        at edu.stanford.nlp.util.MetaClass$ClassFactory.<init>(MetaClass.java:69
)
        at edu.stanford.nlp.util.MetaClass.createFactory(MetaClass.java:360)
        ... 6 more`"
279,https://github.com/stanfordnlp/CoreNLP/issues/358,358,[],closed,2017-02-13 08:50:47+00:00,,Where is chinese.misc.distsim.crf.ser.gz ?,"Hi, I am looking a tutorial about using stanfordnlp to cope Chinese.

In that tutorial, they use 2015-12-08 versionÔºåand mention need copy `chinese.misc.distsim.crf.ser.gz` and `chinese.misc.distsim.prop`  to `stanford-ner-2015-12-09/classifiers`.

Current corenlp version is 2016-10-31, and I only find `stanford-chinese-corenlp-2016-10-31-models.jar` without `chinese.misc.distsim.crf.ser.gz` and `chinese.misc.distsim.prop` on http://nlp.stanford.edu/software/CRF-NER.shtml

Does `stanford-chinese-corenlp-2016-10-31-models.jar` already contain that? Or I have to download from other place?

"
280,https://github.com/stanfordnlp/CoreNLP/issues/359,359,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}, {'id': 706088581, 'node_id': 'MDU6TGFiZWw3MDYwODg1ODE=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/kbp', 'name': 'kbp', 'color': 'c5def5', 'default': False, 'description': None}]",open,2017-02-22 03:24:27+00:00,,Relation Extractor custom entities,"Hi All,

Thanks for the great software. I would like to ask you the following please.

When training specific relations to be extracted from custom Entity types, using the Relation Extractor, I noted that the current possible entities are ""hard-coded"" in some parts, e.g.:
- https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/ie/machinereading/domains/roth/RothEntityExtractor.java#L16
- https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/ie/machinereading/domains/roth/RothCONLL04Reader.java#L64

By modifying these 2 bits, one can re-use the Relation Extractor successfully with custom entities in case its needed, but this requires then a recompilation and an initial troubleshooting as to understand this.

Would you be interested in a pull-request that refactors these hard-coded methods in something that is obtainable from the properties file? E.g.: in the properties file one can indicate a ""entitiesPath"" option which would then point to a tab separated file with the normalised and not normalised values of these entities as its columns.

If this option is not provided potentially these default hard coded entities can then be used as to maintain the current behaviour.

This would cause potential Relation Extractor workflows with custom entities to be possible without code recompilation.

Please advise.

Again, thanks!"
281,https://github.com/stanfordnlp/CoreNLP/issues/360,360,[],closed,2017-02-22 15:12:53+00:00,,Issue with NER and adjacent names and same type.,"Supposed to have in my input text a NER output like the following


<img width=""1174"" alt=""schermata 2017-02-22 alle 15 53 29"" src=""https://cloud.githubusercontent.com/assets/163333/23216676/176d9c36-f917-11e6-9c01-d73a79bbf834.png"">

We can see how it is wrong with the consecutive keywords `Quavo Ratatouille Run` that are recognized as `TYPE=PERSON`, while that classification is for `Quavo Ratatouille` of course.

What happens is the two or more words, that are consecutive and having the same type will be grouped by that type.

 In this specific case we have a line break separator in the text, so the NER module should consider `Run` as not consecutive. 

Is there a way to achieve that?
"
282,https://github.com/stanfordnlp/CoreNLP/issues/361,361,[],closed,2017-02-22 17:44:23+00:00,,Property ner.language not recognized: ENGLISH,"Hi all,

I am trying to use the NERCombinedClassifier to classify a text. When I used the default classifier bundled with the stanford-ner.jar I get the following error: Property ner.language not recognized: ENGLISH. Looking into this I find that it seems like a merge error since this error message has nothing to do with the actual if statement surrounding it:
`this.nerLanguage = NER_LANGUAGE_DEFAULT;
    // build the nsc, note that initProps should be set by ClassifierCombiner
    this.nsc = new NumberSequenceClassifier(new Properties(), useSUTime, props);
    if (PropertiesUtils.getBool(props, NERClassifierCombiner.APPLY_GAZETTE_PROPERTY, NERClassifierCombiner.APPLY_GAZETTE_DEFAULT) ) {
      this.gazetteMapping = readRegexnerGazette(DefaultPaths.DEFAULT_NER_GAZETTE_MAPPING);
    } else {
      this.gazetteMapping = Collections.emptyMap();
      log.fatal(""Property ner.language not recognized: "" + nerLanguage);
    }`

I am getting this error by instantiating the classifier:
`Properties props = new Properties();
                props.load(new FileInputStream(classifierLocation +serializedClassifier.replace("".ncc.ser.gz"","".prop"")));
                classifier = NERClassifierCombiner.getClassifier(classifierLocation + serializedClassifier, props);`

I have not used this repo to get the jar, but did find the same code in the repo. Original download: http://nlp.stanford.edu/software/CRF-NER.shtml#Download"
283,https://github.com/stanfordnlp/CoreNLP/issues/362,362,[],closed,2017-02-22 21:02:05+00:00,,NER: Timeout in loading fonts,"Some fonts requested by brat visualizer (Visualizer.js) are not available in the cloud storage that is used by CoreNLP like

```
https://storage.googleapis.com/corenlp/static/fonts/Liberation_Sans-Regular.ttf
```

This refers to `visualizer.js` (line 3073) that tries to load default fonts:

```javascript
var webFontConfig = {
          custom: {
            families: [
              'Astloch',
              'PT Sans Caption',
              //        'Ubuntu',
              'Liberation Sans'
            ],
            /* For some cases, in particular for embedding, we need to
              allow for fonts being hosted elsewhere */
            urls: webFontURLs !== undefined ? webFontURLs : [
              'static/fonts/Astloch-Bold.ttf',
              'static/fonts/PT_Sans-Caption-Web-Regular.ttf',
              //
              'static/fonts/Liberation_Sans-Regular.ttf'
            ],
          },
          active: proceedWithFonts,
          inactive: proceedWithFonts,
          fontactive: function(fontFamily, fontDescription) {
            // Note: Enable for font debugging
            //console.log(""font active: "", fontFamily, fontDescription);
          },
          fontloading: function(fontFamily, fontDescription) {
            // Note: Enable for font debugging
            //console.log(""font loading:"", fontFamily, fontDescription);
          },
        };
        WebFont.load(webFontConfig);
```

This will result in a 404 error and a `Timeout in loading fonts` error.
Would it be possibile to add those fonts to the cloud storage to avoid it?

The fonts that are requested by brat embed as described here (http://brat.nlplab.org/embed.html) are

```javascript
var webFontURLs = [
    bratLocation + '/static/fonts/Astloch-Bold.ttf',
    bratLocation + '/static/fonts/PT_Sans-Caption-Web-Regular.ttf',
    bratLocation + '/static/fonts/Liberation_Sans-Regular.ttf'
];
```
Thank you."
284,https://github.com/stanfordnlp/CoreNLP/issues/363,363,[],closed,2017-02-25 08:30:26+00:00,,English Tokenizer does not recognize a comma after an email address,"If a sentence contains a comma immediately after an email address, with no space in between, the default tokenizer for English (PTBTokenizer I believe) will treat the comma as part of the email address token instead of punctuation.
This causes problems when going to extract email addresses from text because a comma is not valid in the domain part."
285,https://github.com/stanfordnlp/CoreNLP/issues/364,364,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 547907037, 'node_id': 'MDU6TGFiZWw1NDc5MDcwMzc=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/multilingual', 'name': 'multilingual', 'color': 'fef2c0', 'default': False, 'description': None}]",open,2017-02-25 21:49:28+00:00,,Bogus POS tags in new Spanish models,"The new Spanish models trained on LDC data contain a few bogus POS tags.
I call them ""bogus"" because they only show up a few times in the corpus, and only seem to confuse the models.

Task: ensure the list of unique POS tags in all models matches those described in our Spanish docs."
286,https://github.com/stanfordnlp/CoreNLP/issues/365,365,[],closed,2017-02-26 04:47:55+00:00,,Stanford nlp-Coreference resolution - ‚Äújava.lang.OutOfMemoryError: Java heap space‚Äù,"I tried to train the statistical coreference resolution system with conll 2012 trial data(http://conll.cemantix.org/2012/data.html). I wanted to train it for medical data. But I started with conll 2012 trial data inorder to understand the statistical coreference pipeline. I took only two "".conll"" files of size less than 2MB(eng_0012.conll,eng_0014.conll). These two files contains total of 8 training docs.

I followed below link to build the model.
http://stanfordnlp.github.io/CoreNLP/coref.html
(java -Xmx60g -cp stanford-corenlp-3.7.0.jar:stanford-english-corenlp-models-3.7.0.jar:* edu.stanford.nlp.coref.statistical.StatisticalCorefTrainer -props )

Here the heap size is mentioned as 60g. I used 60g heap size and 15g swap memory and 16 core processor.

But I got ""java.lang.OutOfMemoryError: Java heap space"" exception while building the model.

Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
    at java.util.Arrays.copyOf(Arrays.java:3181)
    at java.util.ArrayList.grow(ArrayList.java:261)
    at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:235)
    at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:227)
    at java.util.ArrayList.add(ArrayList.java:458)
    at      edu.stanford.nlp.coref.statistical.Clusterer.getFeatures(Clusterer.java:661)
    at edu.stanford.nlp.coref.statistical.Clusterer.access$700(Clusterer.java:27)
    at edu.stanford.nlp.coref.statistical.Clusterer$State.getActions(Clusterer.java:460)
    at edu.stanford.nlp.coref.statistical.Clusterer.runPolicy(Clusterer.java:225)
    at edu.stanford.nlp.coref.statistical.Clusterer.doTraining(Clusterer.java:167)
    at edu.stanford.nlp.coref.statistical.StatisticalCorefTrainer.doTraining(StatisticalCorefTrainer.java:127)
    at edu.stanford.nlp.coref.statistical.StatisticalCorefTrainer.main(StatisticalCorefTrainer.java:146)
When i reduced the training doc for from 8 to 4 in ""doTraining"" method of ""edu/stanford/nlp/coref/statistical/Clusterer.java"" class, it ran successfully.

for (ClustererDoc trainDoc : trainDocs){
    examples.add(runPolicy(trainDoc, Math.pow(EXPERT_DECAY(iteration +1))));
}
I don't understand why I am getting this out of memory exception even after giving required configuration for a very small amount of data(less than 2 MB)

Is there any way to optimize the memory usage?

When I went through the source code I found some files like demonyms.txt, gender.data.gz, inanimate.unigrams.txt, state-abbrevations.txt etc. Do I need to create any files specifying medical entities for training the medical domain to get better accuracy ?"
287,https://github.com/stanfordnlp/CoreNLP/issues/366,366,[],closed,2017-02-27 09:12:17+00:00,,coref or dcoref gives no result ,"i have a problem that for specific input no coref tags are produced (for other inputs with the same server and command, the corefs are properly output), just this strange input is not handled

````
.
000.
THROUGH THE LOOKING-GLASS .

````

with the command 
    wget --post-file=/home/frank/Scratch/converts/in.text 'localhost:9000/?properties={""annotators"":""tokenize,ssplit,pos,lemma,ner,parse,dcoref"",""outputFormat"":""xml""}' -O in.xml

and the server 

    java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000

from 

    /stanford-corenlp-full-2016-10-31

the resulting xml file does not contain a <coref></coref> entry, which i assume to be empty, but still the tags should be there (possibly even <coreference><coreference></coreference></coreference>. this anomaly makes parsing the xml file difficult, because it requires handling the case where no coreference tag is found. "
288,https://github.com/stanfordnlp/CoreNLP/issues/367,367,[],closed,2017-02-28 17:29:00+00:00,,Error: Error creating class edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl,"I get this error when having in the `CLASSPATH` the DeepLearning4J and ND4J jars. This happens only when initializing `CoreNLP` *and* `DeepLearning4J`, while the latter works alone.

```
{ Error: Error creating class
edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl
	at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:40)
	at edu.stanford.nlp.time.TimeExpressionExtractorFactory.create(TimeExpressionExtractorFactory.java:57)
	at edu.stanford.nlp.time.TimeExpressionExtractorFactory.createExtractor(TimeExpressionExtractorFactory.java:38)
	at edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.<init>(NumberSequenceClassifier.java:86)
	at edu.stanford.nlp.ie.NERClassifierCombiner.<init>(NERClassifierCombiner.java:136)
	at edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(AnnotatorImplementations.java:121)
	at edu.stanford.nlp.pipeline.AnnotatorFactories$6.create(AnnotatorFactories.java:273)
	at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:152)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:451)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:154)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:150)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:137)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
Caused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: MetaClass couldn't create public edu.stanford.nlp.time.TimeExpressionExtractorImpl(java.lang.String,java.util.Properties) with args [sutime, {}]
	at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:237)
	at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:382)
	at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:38)
	... 15 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:233)
	... 17 more
Caused by: java.lang.NoClassDefFoundError: de/jollyday/ManagerParameter
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at edu.stanford.nlp.time.Options.<init>(Options.java:87)
	at edu.stanford.nlp.time.TimeExpressionExtractorImpl.init(TimeExpressionExtractorImpl.java:44)
	at edu.stanford.nlp.time.TimeExpressionExtractorImpl.<init>(TimeExpressionExtractorImpl.java:39)
	... 22 more
Caused by: java.lang.ClassNotFoundException: de.jollyday.ManagerParameter
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 27 more

    at Error (native)
    at new StanfordCoreNLP (/Users/loretoparisi/Documents/Projects/intelligence-sdk/lib/nlp/stanford/index.js:127:28)
    at cld.detectCLDP.then.res (/Users/loretoparisi/Documents/Projects/intelligence-sdk/lib/tests/nlp/nlpandclassifier.js:118:35)
    at process._tickCallback (internal/process/next_tick.js:103:7)
    at Module.runMain (module.js:606:11)
    at run (bootstrap_node.js:394:7)
    at startup (bootstrap_node.js:149:9)
    at bootstrap_node.js:509:3
  cause: nodeJava_edu_stanford_nlp_util_ReflectionLoading_ReflectionLoadingException {} }
```"
289,https://github.com/stanfordnlp/CoreNLP/issues/368,368,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2017-02-28 17:39:01+00:00,,Exception when sending a HEAD request on /,"```
Feb 28, 2017 6:30:37 PM sun.net.httpserver.ExchangeImpl sendResponseHeaders
WARNING: sendResponseHeaders: being invoked with a content length for a HEAD request
java.io.IOException: response headers not sent yet
        at sun.net.httpserver.PlaceholderOutputStream.checkWrap(ExchangeImpl.java:428)
        at sun.net.httpserver.PlaceholderOutputStream.write(ExchangeImpl.java:438)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:623)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
        at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
        at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
        at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```"
290,https://github.com/stanfordnlp/CoreNLP/issues/369,369,[],closed,2017-03-03 08:38:12+00:00,,"Typo in error message: ""PattenrType not specified. Options are SURFACE and DEP""","edu.stanford.nlp.patterns.GetPatternsFromDataMultiClass.java

""**PatternType** not specified. Options are SURFACE and DEP"""
291,https://github.com/stanfordnlp/CoreNLP/issues/370,370,[],open,2017-03-03 11:48:25+00:00,,how to use Stanford OpenIE train chinese model,"I have studied Stanford OpenIE for whole 3 days, and I can use it to get some relation for English. Now, I want to use it for Chinese. As for Chinese sentence, the Stanford segmenter, NER, tagger, and parser are all available for Chinese, but the openie does not support Chinese. I know the file named ""clauseSearcherModel.ser.gz"", which is most important for the openie. But how to train the model for clauseSearcherModel.ser.gz with Chinese? "
292,https://github.com/stanfordnlp/CoreNLP/issues/371,371,[],closed,2017-03-03 13:58:17+00:00,,NERCombinerAnnotator unused variable,"The NER annotator loads a lot of memory when the sentence is too long, therefore it should be useful to add a limit.

Indeed, the limit is already there, in `NERCombinerAnnotator.java`, and it is called `maxSentenceLength`. Unfortunately, it is never used (sic!).

On lines 111/112 you should add:

```
        // line 111 --- List<CoreLabel> tokens = sentence.get(CoreAnnotations.TokensAnnotation.class);
        if (maxSentenceLength > 0 && tokens.size() > maxSentenceLength) {

            // For compatibility with dcoref
            for (CoreLabel token : tokens) {
                token.set(CoreAnnotations.NamedEntityTagAnnotation.class, ""O"");
                token.set(CoreAnnotations.NormalizedNamedEntityTagAnnotation.class, ""O"");
            }
            return;
        }
        // line 112 --- List<CoreLabel> output; // only used if try assignment works.
```

This code skip a sentence (and load ""O"" as NER into its tokens, for compatibility) when the sentence is longer than `maxSentenceLength` tokens."
293,https://github.com/stanfordnlp/CoreNLP/issues/372,372,[],closed,2017-03-03 16:29:03+00:00,,CoNLL file pattern problem,"Hello,
I have a problem running stanford CoreNLP on my CoNLL generated files.
My files have a names like ``cctv_0005.v9_auto_mentions_conll``, while in ``DocumentMaker.getDocumentReader`` function there is a setting ``options.setFilter("".*_auto_conll$"");``, which does not correspond to my pattern.
Is there a problem here?

The same problem appears to be in ``CoNLL2011DocumentReader``, but with gold files: ``this("".*_gold_conll$"");`` "
294,https://github.com/stanfordnlp/CoreNLP/issues/373,373,[],closed,2017-03-05 07:02:45+00:00,,Is the word 'list' not a verb here ?,"I tried the following sentence in POS tagger. and I got the following output.

![corenlp run](https://cloud.githubusercontent.com/assets/5524260/23585267/8d8f03ea-012e-11e7-9f94-92ef8cc31962.png)

Is the word ""list"" here not a verb ?"
295,https://github.com/stanfordnlp/CoreNLP/issues/374,374,[],closed,2017-03-05 07:07:48+00:00,,"Is this word ""cost"" in this sentence not a noun ?","I gave the sentence ""list the chairs that have cost lesser than three hundred USD"" to POS tagger

and got the following output

![1](https://cloud.githubusercontent.com/assets/5524260/23585286/5c0b35fe-012f-11e7-8484-68b5f1ca8c52.png)

Is this word ""cost"" in this sentence not a noun ?"
296,https://github.com/stanfordnlp/CoreNLP/issues/376,376,[],open,2017-03-07 16:14:43+00:00,,Update to Jollyday 0.5.x and Java 8 java.time instead of JodaTime,"Since Java 8 has a time API comparable to JodaTime, projects such as jollyday are switching to this newer API, and so should CoreNLP.

These APIs are quite similar, but the transition is not a simple search and replace.

See e.g. this post for how classes align:

http://blog.joda.org/2014/11/converting-from-joda-time-to-javatime.html"
297,https://github.com/stanfordnlp/CoreNLP/issues/378,378,[],open,2017-03-10 11:34:40+00:00,,Where can i learn what does each tag means? Arabic parser,"I am testing Arabic POS tagger and it shows tags as


I have found here but almost all links are broken and no useful info : http://nlp.stanford.edu/software/parser-arabic-faq.shtml

ÿßŸÑŸÑÿßÿπÿ®/DTNN
ÿπÿßŸÖŸÑ/NN
ÿßŸÉÿ™ÿ®/VBP
ÿ≥ÿ¨ŸÑ/VBD
ÿ™ÿ≥ÿ¨ŸÑ/VBP
ŸÇŸäÿØ/NN
ÿ£ÿ±ÿ≥ŸÑ/VBD ÿ®ÿßŸÑÿ®ÿ±ŸäÿØ/NNP ÿßŸÑŸÖÿ≥ÿ¨ŸÑ/DTJJ
ÿ≥ÿ¨ŸÑ/VBD ÿßŸÑÿ≥Ÿäÿßÿ±ÿ©/DTNN
ÿØŸàŸÜ/NN ŸÅŸä/IN ÿ≥ÿ¨ŸÑ/NN

Where can i learn what does each tag means? And i need the full list? 

**Moreover, is that possible to learn is word a plural. If plural what is the singular version? If comparative, what is the original adjective form etc?**

I have found this file : http://catalog.ldc.upenn.edu/docs/LDC2010T13/atb1-v4.1-taglist-conversion-to-PennPOS-forrelease.lisp

It contains tags like these and where can i learn what they mean?

DET
NOUN_NUM
NSUFF_FEM_SG
CASE_DEF_ACC DT
CD

This is the code i use

```
            var tagger = new MaxentTagger(@""D:\77 sozluk projesi\StandFordPosTagger\stanford-parser-full-2016-10-31\stanford-parser-3.7.0-models\edu\stanford\nlp\models\pos-tagger\arabic\arabic.tagger"");

            List<string> lstTryWords = new List<string> { ""ÿßŸÑÿ®ÿ¥ÿ±"", ""ÿ®ÿ¥ÿ±Ÿä"", ""ÿßŸÑŸÑÿßÿπÿ®"", ""ÿπÿßŸÖŸÑ"", ""ÿßŸÉÿ™ÿ®"", ""ÿ≥ÿ¨ŸÑ"", "" ÿ™ÿ≥ÿ¨ŸÑ"", "" ŸÇŸäÿØ"", ""ÿ£ÿ±ÿ≥ŸÑ ÿ®ÿßŸÑÿ®ÿ±ŸäÿØ ÿßŸÑŸÖÿ≥ÿ¨ŸÑ"", "" ÿ≥ÿ¨ŸÑ ÿßŸÑÿ≥Ÿäÿßÿ±ÿ©"", "" ÿØŸàŸÜ ŸÅŸä ÿ≥ÿ¨ŸÑ"" };

            foreach (var item in lstTryWords)
            {
                var sentences = MaxentTagger.tokenizeText(new java.io.StringReader(item.Trim())).toArray();
                foreach (java.util.ArrayList sentence in sentences)
                {
                    var taggedSentence = tagger.tagSentence(sentence);
                    Debug.WriteLine(SentenceUtils.listToString(taggedSentence, false));
                }
            }
```"
298,https://github.com/stanfordnlp/CoreNLP/issues/379,379,[],closed,2017-03-10 12:35:33+00:00,,"TimingTest is locale dependant, fails.","TimingTest fails on `de_DE` locale, but works fine on `C`:
```
testTiming(edu.stanford.nlp.util.TimingTest)  Time elapsed: 0.106 sec  <<< FAILURE!
junit.framework.ComparisonFailure: Wrong formatted time expected:<0[.]1> but was:<0[,]1>
	at junit.framework.Assert.assertEquals(Assert.java:100)
	at junit.framework.TestCase.assertEquals(TestCase.java:261)
	at edu.stanford.nlp.util.TimingTest.testTiming(TimingTest.java:40)
```

As far as I can tell, the tests `setUp` method executes too late. At this point, the formats used by `Timing` have already been set up to the system locale.

https://github.com/stanfordnlp/CoreNLP/blob/18bbdccdc4f0ee4575a73ebd0de6b5c6d757f52e/test/src/edu/stanford/nlp/util/TimingTest.java#L18

To fix this, maybe you want to have `Timing` always use `Locale.ROOT`?

Modify https://github.com/stanfordnlp/CoreNLP/blob/cb22557615c146bffe8a7a1268404451252f2afe/src/edu/stanford/nlp/util/Timing.java#L51
```
private static final NumberFormat nf = new DecimalFormat(""0.0"",
       DecimalFormatSymbols.getInstance(Locale.ROOT));
```
"
299,https://github.com/stanfordnlp/CoreNLP/issues/380,380,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 706055902, 'node_id': 'MDU6TGFiZWw3MDYwNTU5MDI=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/tokenize', 'name': 'tokenize', 'color': 'c5def5', 'default': False, 'description': None}, {'id': 706056248, 'node_id': 'MDU6TGFiZWw3MDYwNTYyNDg=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/ssplit', 'name': 'ssplit', 'color': 'c5def5', 'default': False, 'description': None}]",open,2017-03-10 13:03:13+00:00,,Incorrect sentence splitting in German (and some other European languages) at dots after numbers (e.g. German: `1. Bundesliga`),"German (and some [other European languages](https://en.wikipedia.org/wiki/Date_and_time_notation_in_Europe)) use a dot to denote ordinals.

I.e. instead of ""1st place"", German uses ""1. Platz"".
Instead of ""July 28th"", German uses ""28. Juli"".

Examples can be found en masse, for example:
[dewiki:Fu√üball-Bundesliga](https://de.wikipedia.org/wiki/Fu√üball-Bundesliga) (`28. Juli`, `2. Bundesliga`, `1. Liga`)
[dewiki:9/11](https://de.wikipedia.org/wiki/Terroranschl√§ge_am_11._September_2001) (`11. September`)
[dewiki:Stanford University](https://de.wikipedia.org/wiki/Stanford_University) (`Der Grund und Boden wurde am 11. November 1885 von Leland Stanford zur Gr√ºndung der Universit√§t gestiftet`)

And the Duden, the ""prescriptive source for German language spelling"" (Wikipedia) uses:
[`Duden - Die deutsche Rechtschreibung, 26. Auflage`](http://www.duden.de/Shop/Duden-Die-deutsche-Rechtschreibung-26-Auflage-f√ºr-Windows-Mac-OSX-und-Linux-0)

Unfortunately, CoreNLP will split all these sentences at the dot.

So **CoreNLP currently cannot reliably split German sentences** if they contain ordinal numbers or dates.

I am currently using the following workaround hack:
```
  private static class FilteredTokenizer implements Annotator {
    private TokenizerAnnotator inner;

    public FilteredTokenizer(TokenizerAnnotator inner) {
      this.inner = inner;
    }

    @Override
    public void annotate(Annotation annotation) {
      inner.annotate(annotation);
      List<CoreLabel> tokens = annotation.get(CoreAnnotations.TokensAnnotation.class);
      ArrayList<CoreLabel> filtered = new ArrayList<>(tokens.size());
      CoreLabel previous = null;
      for(CoreLabel t : tokens)
        if(previous == null || !updateAnnotation(previous, t))
          filtered.add(previous = t);
      annotation.set(CoreAnnotations.TokensAnnotation.class, filtered);
    }

    private boolean updateAnnotation(CoreLabel prev, CoreLabel curr) {
      int begin = curr.beginPosition(), end = curr.endPosition();
      if(begin + 1 != end || begin != prev.endPosition() || prev.beginPosition() == prev.endPosition())
        return false;
      String ct = curr.getString(CoreAnnotations.OriginalTextAnnotation.class);
      if(!""."".equals(ct))
        return false;
      String pt = prev.getString(CoreAnnotations.OriginalTextAnnotation.class);
      for(int i = 0; i < pt.length(); i++)
        if(!Character.isDigit(pt.charAt(i)))
          return false;
      // We keep TextAnnotation unmodified, to 1. gets labeled CARDINAL.
      prev.set(CoreAnnotations.OriginalTextAnnotation.class, pt + ct);
      prev.setEndPosition(end);
      return true;
    }

    @SuppressWarnings(""rawtypes"")
    @Override
    public Set<Class<? extends CoreAnnotation>> requirementsSatisfied() {
      return inner.requirementsSatisfied();
    }

    @SuppressWarnings(""rawtypes"")
    @Override
    public Set<Class<? extends CoreAnnotation>> requires() {
      return inner.requires();
    }
  }
```"
300,https://github.com/stanfordnlp/CoreNLP/issues/381,381,[],closed,2017-03-10 23:22:58+00:00,,"Attempted to fetch annotator ""parse"" before the annotator pool was created!","Running CoreNLP 3.7.0 server using Python, getting this error message for seemingly arbitrary inputs?

`[pool-2-thread-2] ERROR edu.stanford.nlp.pipeline.StanfordCoreNLP - Attempted to fetch annotator ""parse"" before the annotator pool was created!
java.util.concurrent.ExecutionException: java.lang.AssertionError: Failed to get parser - this should not be possible
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:603)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
	at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
	at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
	at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.AssertionError: Failed to get parser - this should not be possible
	at edu.stanford.nlp.dcoref.RuleBasedCorefMentionFinder.getParser(RuleBasedCorefMentionFinder.java:453)
	at edu.stanford.nlp.dcoref.RuleBasedCorefMentionFinder.parse(RuleBasedCorefMentionFinder.java:443)
	at edu.stanford.nlp.dcoref.RuleBasedCorefMentionFinder.findSyntacticHead(RuleBasedCorefMentionFinder.java:334)
	at edu.stanford.nlp.dcoref.RuleBasedCorefMentionFinder.findHead(RuleBasedCorefMentionFinder.java:272)
	at edu.stanford.nlp.dcoref.RuleBasedCorefMentionFinder.extractPredictedMentions(RuleBasedCorefMentionFinder.java:98)
	at edu.stanford.nlp.pipeline.DeterministicCorefAnnotator.annotate(DeterministicCorefAnnotator.java:117)
	at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:605)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.lambda$handle$358(StanfordCoreNLPServer.java:585)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler$$Lambda$44/1829860859.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 `"
301,https://github.com/stanfordnlp/CoreNLP/issues/383,383,[],closed,2017-03-12 09:02:46+00:00,,Stanford segmenter nltk Could not find SLF4J in your classpath,"I set up a nltk and stanford environment, nltk and stanford jars has downloaded, the program with nltk was ok, but I had a trouble with stanford segmenter. just make a simple program via stanford segmenter, I got a error is Could not find SLF4J in your classpath, although I had export all jars include slf4j-api.jar. Detail as follows

Python3.5 NLTK, 3.2.2, Standford jars 3.7

OS: Centos

Environment variable:

   ```
 export JAVA_HOME=/usr/java/jdk1.8.0_60
    export NLTK_DATA=/opt/nltk_data
    export STANFORD_SEGMENTER_PATH=/opt/stanford/stanford-segmenter-3.7
    export CLASSPATH=$CLASSPATH:$STANFORD_SEGMENTER_PATH/stanford-segmenter.jar
    export STANFORD_POSTAGGER_PATH=/opt/stanford/stanford-postagger-full-2016-10-31
    export CLASSPATH=$CLASSPATH:$STANFORD_POSTAGGER_PATH/stanford-postagger.jar
    export STANFORD_NER_PATH=/opt/stanford/stanford-ner-2016-10-31
    export CLASSPATH=$CLASSPATH:$STANFORD_NER_PATH/stanford-ner.jar
    export STANFORD_MODELS=$STANFORD_NER_PATH/classifiers:$STANFORD_POSTAGGER_PATH/models
    export STANFORD_PARSER_PATH=/opt/stanford/stanford-parser-full-2016-10-31
    export CLASSPATH=$CLASSPATH:$STANFORD_PARSER_PATH/stanford-parser.jar:$STANFORD_PARSER_PATH/stanford-parser-3.6.0-models.jar:$STANFORD_PARSER_PATH/slf4j-api.jar:$STANFORD_PARSER_PATH/ejml-0.23.jar
    export STANFORD_CORENLP_PATH=/opt/stanford/stanford-corenlp-full-2016-10-31
    export CLASSPATH=$CLASSPATH:$STANFORD_CORENLP_PATH/stanford-corenlp-3.7.0.jar:$STANFORD_CORENLP_PATH/stanford-corenlp-3.7.0-models.jar:$STANFORD_CORENLP_PATH/javax.json.jar:$STANFORD_CORENLP_PATH/joda-time.jar:$STANFORD_CORENLP_PATH/jollyday.jar:$STANFORD_CORENLP_PATH/protobuf.jar:$STANFORD_CORENLP_PATH/slf4j-simple.jar:$STANFORD_CORENLP_PATH/xom.jar
    export STANFORD_CORENLP=$STANFORD_CORENLP_PATH

```
The program as follows:

```
from nltk.tokenize import StanfordSegmenter
>>> segmenter = StanfordSegmenter(
    path_to_sihan_corpora_dict=""/opt/stanford/stanford-segmenter-3.7/data/"",
    path_to_model=""/opt/stanford/stanford-segmenter-3.7/data/pku.gz"",
    path_to_dict=""/opt/stanford/stanford-segmenter-3.7/data/dict-chris6.ser.gz""
)
>>> res = segmenter.segment(u""ËøôÊòØÊñØÂù¶Á¶è‰∏≠ÊñáÂàÜËØçÂô®ÊµãËØï"")
```
The error as follows:

```
Exception in thread ""main"" java.lang.ExceptionInInitializerError
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.<clinit>(AbstractSequenceClassifier.java:88)
Caused by: java.lang.IllegalStateException: Could not find SLF4J in your classpath
    at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers.lambda$static$530(RedwoodConfiguration.java:190)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers$7.buildChain(RedwoodConfiguration.java:309)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers$7.apply(RedwoodConfiguration.java:318)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration.lambda$handlers$535(RedwoodConfiguration.java:363)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration.apply(RedwoodConfiguration.java:41)
    at edu.stanford.nlp.util.logging.Redwood.<clinit>(Redwood.java:609)
    ... 1 more
Caused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: java.lang.ClassNotFoundException: edu.stanford.nlp.util.logging.SLF4JHandler
    at edu.stanford.nlp.util.MetaClass.createFactory(MetaClass.java:364)
    at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:381)
    at edu.stanford.nlp.util.logging.RedwoodConfiguration$Handlers.lambda$static$530(RedwoodConfiguration.java:186)
    ... 6 more
Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.util.logging.SLF4JHandler
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:264)
    at edu.stanford.nlp.util.MetaClass$ClassFactory.construct(MetaClass.java:135)
    at edu.stanford.nlp.util.MetaClass$ClassFactory.<init>(MetaClass.java:202)
    at edu.stanford.nlp.util.MetaClass$ClassFactory.<init>(MetaClass.java:69)
    at edu.stanford.nlp.util.MetaClass.createFactory(MetaClass.java:360)
    ... 8 more

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/python3/lib/python3.5/site-packages/nltk/tokenize/stanford_segmenter.py"", line 96, in segment
    return self.segment_sents([tokens])
  File ""/usr/local/python3/lib/python3.5/site-packages/nltk/tokenize/stanford_segmenter.py"", line 123, in segment_sents
    stdout = self._execute(cmd)
  File ""/usr/local/python3/lib/python3.5/site-packages/nltk/tokenize/stanford_segmenter.py"", line 143, in _execute
    cmd,classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE)
  File ""/usr/local/python3/lib/python3.5/site-packages/nltk/internals.py"", line 134, in java
    raise OSError('Java command failed : ' + str(cmd))
OSError: Java command failed : ['/usr/java/jdk1.8.0_60/bin/java', '-mx2g', '-cp', '/opt/stanford/stanford-segmenter-3.7/stanford-segmenter.jar:/opt/stanford/stanford-parser-full-2016-10-31/slf4j-api.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-sighanCorporaDict', '/opt/stanford/stanford-segmenter-3.7/data/', '-textFile', '/tmp/tmpkttpldl6', '-sighanPostProcessing', 'true', '-keepAllWhitespaces', 'false', '-loadClassifier', '/opt/stanford/stanford-segmenter-3.7/data/pku.gz', '-serDictionary', '/opt/stanford/stanford-segmenter-3.7/data/dict-chris6.ser.gz', '-inputEncoding', 'UTF-8']
```
"
302,https://github.com/stanfordnlp/CoreNLP/issues/384,384,[],closed,2017-03-15 20:51:09+00:00,,Can CoreNLP server run without internet connection ?,"When working offline (Windows version), CoreNLP server does not seem to work (API not responding), did anyone experience the same ?

CoreNLP server runs as:
java -Xmx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000

"
303,https://github.com/stanfordnlp/CoreNLP/issues/390,390,[],closed,2017-03-19 00:33:41+00:00,,Running two instances of ColumnDataClassifier overlap loaded properties printTo and serializeTo,"I have two ColumnDataClassifier instances running for two different dataset and models, let's call them X and Y.
What happens is that the options `printTo` and `serializeTo` of the instance configuration are overlapped by the last configuration loaded i.e. if I load X, then I load Y, when I train X, I get for `printTo` and `serializeTo` the values being set for the latter classifier Y.
If I avoid to load an instance of X I will get the correct configuration loaded for the latter classifier.

I can clearly see this from the logging, so when I start to build the X classifier I get

```
...
01:10:09.127 [Thread-36] INFO  e.s.n.classify.ColumnDataClassifier - Built classifier described in file /.../Y_classifier_model_linear
01:10:09.127 [Thread-36] INFO  e.s.n.classify.ColumnDataClassifier - Serializing classifier to /...Y_classifier_model...
01:10:09.249 [Thread-36] INFO  e.s.n.classify.ColumnDataClassifier - Done.
0
```

even if I can see from the init the right values for both X and Y instances, so when X starts:

```
01:21:25.508 [main] INFO  e.s.n.classify.ColumnDataClassifier - Loading classifier from /.../X_classifier_model ... done [0,1 sec].
01:21:25.513 [main] INFO  e.s.n.classify.ColumnDataClassifier - Setting ColumnDataClassifier properties
01:21:25.513 [main] INFO  e.s.n.classify.ColumnDataClassifier - useQN = true
01:21:25.513 [main] INFO  e.s.n.classify.ColumnDataClassifier - testFile = /.../X_dataset_test.csv
01:21:25.513 [main] INFO  e.s.n.classify.ColumnDataClassifier - 1.splitWordsTokenizerRegexp = M[ -_]{60} |[\p{L}][\p{L}0-9]*|(?:\$ ?)?[0-9]+(?:\.[0-9]{2})?%?|\s+|[\x80-\uFFFD]|.
01:21:25.515 [main] INFO  e.s.n.classify.ColumnDataClassifier - 1.minNGramLeng = 1
01:21:25.515 [main] INFO  e.s.n.classify.ColumnDataClassifier - displayAllAnswers = 1
01:21:25.515 [main] INFO  e.s.n.classify.ColumnDataClassifier - intern = true
01:21:25.515 [main] INFO  e.s.n.classify.ColumnDataClassifier - goldAnswerColumn = 0
01:21:25.516 [main] INFO  e.s.n.classify.ColumnDataClassifier - printTo = /.../X_classifier_model_linear
01:21:25.516 [main] INFO  e.s.n.classify.ColumnDataClassifier - displayedColumn = -1
01:21:25.516 [main] INFO  e.s.n.classify.ColumnDataClassifier - printClassifierParam = 200
01:21:25.516 [main] INFO  e.s.n.classify.ColumnDataClassifier - 1.useNGrams = true
01:21:25.516 [main] INFO  e.s.n.classify.ColumnDataClassifier - 1.splitWordsIgnoreRegexp = M[ -_]{60} |\s+
01:21:25.516 [main] INFO  e.s.n.classify.ColumnDataClassifier - QNsize = 15
01:21:25.516 [main] INFO  e.s.n.classify.ColumnDataClassifier - sigma = 4
01:21:25.516 [main] INFO  e.s.n.classify.ColumnDataClassifier - serializeTo = /.../X_classifier_model
01:21:25.516 [main] INFO  e.s.n.classify.ColumnDataClassifier - tolerance = 0.0001
01:21:25.517 [main] INFO  e.s.n.classify.ColumnDataClassifier - trainFile = /.../X_dataset_train.csv
01:21:25.517 [main] INFO  e.s.n.classify.ColumnDataClassifier - 1.usePrefixSuffixNGrams = true
01:21:25.517 [main] INFO  e.s.n.classify.ColumnDataClassifier - verboseOptimization = true
01:21:25.517 [main] INFO  e.s.n.classify.ColumnDataClassifier - loadClassifier = /.../X_classifier_model
01:21:25.517 [main] INFO  e.s.n.classify.ColumnDataClassifier - 1.maxNGramLeng = 4
01:21:25.517 [main] INFO  e.s.n.classify.ColumnDataClassifier - printClassifier = HighWeight
01:21:25.517 [main] INFO  e.s.n.classify.ColumnDataClassifier - 1.useSplitWords = true
01:21:26.047 [main] INFO  e.s.n.classify.ColumnDataClassifier - Reading dataset from /.../X_dataset_train.csv ... done [0,5s, 71 items].
01:21:26.050 [main] INFO  edu.stanford.nlp.classify.Dataset - numDatums: 71
numDatumsPerLabel: {NOTEXPLICIT=48.0, EXPLICT=23.0}
numLabels: 2 [EXPLICT, NOTEXPLICIT]
numFeatures (Phi(X) types): 28062 [1-#-ts a, 1-SW-Cadillac, 1-#-t , 1-#-ra, 1-#-' n, ...]
01:21:26.165 [main] INFO  e.s.n.classify.ColumnDataClassifier - Reading dataset from /.../X_dataset_test.csv ... done [0,1s, 19 items]
````

while when Y starts I will get

```
01:21:25.508 [main] INFO  e.s.n.classify.ColumnDataClassifier - Loading classifier from /.../Y_classifier_model ... done [0,3 sec].
01:21:25.516 [main] INFO  e.s.n.classify.ColumnDataClassifier - printTo = /.../Y_classifier_model_linear
01:21:25.516 [main] INFO  e.s.n.classify.ColumnDataClassifier - serializeTo = /.../Y_classifier_model
01:21:26.047 [main] INFO  e.s.n.classify.ColumnDataClassifier - Reading dataset from /.../Y_dataset_train.csv ... done [1,3s, 344 items].
```
etc."
304,https://github.com/stanfordnlp/CoreNLP/issues/391,391,[],closed,2017-03-19 10:06:56+00:00,,running multiple copies of corenlp - address already in use for liveness server,"i try to run on a server multiple copies of corenlp for different languages, each with a different port. it works, but i get an error message when starting the second corenlp process:

ERROR Could not start liveness server. This will probably result in very bad things happening soon. - java.net.BindException: Address already in use

what is the liveness server doing? why is it always starting on port 9000, even when the process starting is on a different port (e.g. 9001 for a german corenlp process). 

the error message is ignored and the process seems to work ok."
305,https://github.com/stanfordnlp/CoreNLP/issues/394,394,[],closed,2017-03-22 13:45:53+00:00,,Exceptions crushing,"Hi,

I often get the following exceptions (1 or 2) and then server stops responding.
is there a common solution for it?
I'm running the 3.7.0 following the dedicated server guide on Ubuntu.

> Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ""server-timer""
Exception in thread ""Thread-3"" Exception in thread ""Thread-5"" java.lang.OutOfMemoryError: Java heap space
java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space
        at java.util.concurrent.FutureTask.report(java.base@9-internal/FutureTask.java:123)
        at java.util.concurrent.FutureTask.get(java.base@9-internal/FutureTask.java:207)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:593)
        at com.sun.net.httpserver.Filter$Chain.doFilter(jdk.httpserver@9-internal/Filter.java:77)
        at sun.net.httpserver.AuthFilter.doFilter(jdk.httpserver@9-internal/AuthFilter.java:82)
        at com.sun.net.httpserver.Filter$Chain.doFilter(jdk.httpserver@9-internal/Filter.java:80)
        at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(jdk.httpserver@9-internal/ServerImpl.java:685)
        at com.sun.net.httpserver.Filter$Chain.doFilter(jdk.httpserver@9-internal/Filter.java:77)
        at sun.net.httpserver.ServerImpl$Exchange.run(jdk.httpserver@9-internal/ServerImpl.java:657)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@9-internal/ThreadPoolExecutor.java:1158)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@9-internal/ThreadPoolExecutor.java:632)
        at java.lang.Thread.run(java.base@9-internal/Thread.java:804)
Caused by: java.lang.OutOfMemoryError: Java heap space
Exception in thread ""Timer-0"" java.lang.OutOfMemoryError: Java heap space
        at java.util.IdentityHashMap$EntrySet.iterator(java.base@9-internal/IdentityHashMap.java:1186)
        at edu.stanford.nlp.pipeline.AnnotatorPool$1$1.run(AnnotatorPool.java:49)
        at java.util.TimerThread.mainLoop(java.base@9-internal/Timer.java:555)
        at java.util.TimerThread.run(java.base@9-internal/Timer.java:505)
java.lang.OutOfMemoryError: Java heap space
java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space
        at java.util.concurrent.FutureTask.report(java.base@9-internal/FutureTask.java:123)
        at java.util.concurrent.FutureTask.get(java.base@9-internal/FutureTask.java:207)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:593)
        at com.sun.net.httpserver.Filter$Chain.doFilter(jdk.httpserver@9-internal/Filter.java:77)
        at sun.net.httpserver.AuthFilter.doFilter(jdk.httpserver@9-internal/AuthFilter.java:82)
        at com.sun.net.httpserver.Filter$Chain.doFilter(jdk.httpserver@9-internal/Filter.java:80)
        at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(jdk.httpserver@9-internal/ServerImpl.java:685)
        at com.sun.net.httpserver.Filter$Chain.doFilter(jdk.httpserver@9-internal/Filter.java:77)
        at sun.net.httpserver.ServerImpl$Exchange.run(jdk.httpserver@9-internal/ServerImpl.java:657)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@9-internal/ThreadPoolExecutor.java:1158)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@9-internal/ThreadPoolExecutor.java:632)
        at java.lang.Thread.run(java.base@9-internal/Thread.java:804)
Caused by: java.lang.OutOfMemoryError: Java heap space
java.util.concurrent.TimeoutException
        at java.util.concurrent.FutureTask.get(java.base@9-internal/FutureTask.java:206)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:593)
        at com.sun.net.httpserver.Filter$Chain.doFilter(jdk.httpserver@9-internal/Filter.java:77)
        at sun.net.httpserver.AuthFilter.doFilter(jdk.httpserver@9-internal/AuthFilter.java:82)
        at com.sun.net.httpserver.Filter$Chain.doFilter(jdk.httpserver@9-internal/Filter.java:80)
        at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(jdk.httpserver@9-internal/ServerImpl.java:685)
        at com.sun.net.httpserver.Filter$Chain.doFilter(jdk.httpserver@9-internal/Filter.java:77)
        at sun.net.httpserver.ServerImpl$Exchange.run(jdk.httpserver@9-internal/ServerImpl.java:657)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@9-internal/ThreadPoolExecutor.java:1158)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@9-internal/ThreadPoolExecutor.java:632)
        at java.lang.Thread.run(java.base@9-internal/Thread.java:804)


2)

> java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space
        at java.util.concurrent.FutureTask.report(java.base@9-internal/FutureTask.java:123)
        at java.util.concurrent.FutureTask.get(java.base@9-internal/FutureTask.java:207)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:593)
        at com.sun.net.httpserver.Filter$Chain.doFilter(jdk.httpserver@9-internal/Filter.java:77)
        at sun.net.httpserver.AuthFilter.doFilter(jdk.httpserver@9-internal/AuthFilter.java:82)
        at com.sun.net.httpserver.Filter$Chain.doFilter(jdk.httpserver@9-internal/Filter.java:80)
        at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(jdk.httpserver@9-internal/ServerImpl.java:685)
        at com.sun.net.httpserver.Filter$Chain.doFilter(jdk.httpserver@9-internal/Filter.java:77)
        at sun.net.httpserver.ServerImpl$Exchange.run(jdk.httpserver@9-internal/ServerImpl.java:657)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@9-internal/ThreadPoolExecutor.java:1158)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@9-internal/ThreadPoolExecutor.java:632)
        at java.lang.Thread.run(java.base@9-internal/Thread.java:804)
Caused by: java.lang.OutOfMemoryError: Java heap space
        at edu.stanford.nlp.util.StringUtils.editDistance(StringUtils.java:1497)
        at edu.stanford.nlp.coref.statistical.FeatureExtractor.getFeatures(FeatureExtractor.java:318)
        at edu.stanford.nlp.coref.statistical.FeatureExtractor.extract(FeatureExtractor.java:128)
        at edu.stanford.nlp.coref.statistical.StatisticalCorefAlgorithm.runCoref(StatisticalCorefAlgorithm.java:96)
        at edu.stanford.nlp.coref.CorefSystem.annotate(CorefSystem.java:59)
        at edu.stanford.nlp.coref.CorefSystem.annotate(CorefSystem.java:47)
        at edu.stanford.nlp.pipeline.CorefAnnotator.annotate(CorefAnnotator.java:76)
        at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:75)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:605)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.lambda$handle$353(StanfordCoreNLPServer.java:575)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler$$Lambda$88/595261723.call(Unknown Source)
        at java.util.concurrent.FutureTask.run(java.base@9-internal/FutureTask.java:266)
        ... 3 more
java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space
        at java.util.concurrent.FutureTask.report(java.base@9-internal/FutureTask.java:123)
        at java.util.concurrent.FutureTask.get(java.base@9-internal/FutureTask.java:207)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:593)
        at com.sun.net.httpserver.Filter$Chain.doFilter(jdk.httpserver@9-internal/Filter.java:77)
        at sun.net.httpserver.AuthFilter.doFilter(jdk.httpserver@9-internal/AuthFilter.java:82)
        at com.sun.net.httpserver.Filter$Chain.doFilter(jdk.httpserver@9-internal/Filter.java:80)
        at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(jdk.httpserver@9-internal/ServerImpl.java:685)
        at com.sun.net.httpserver.Filter$Chain.doFilter(jdk.httpserver@9-internal/Filter.java:77)
        at sun.net.httpserver.ServerImpl$Exchange.run(jdk.httpserver@9-internal/ServerImpl.java:657)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(java.base@9-internal/ThreadPoolExecutor.java:1158)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(java.base@9-internal/ThreadPoolExecutor.java:632)
        at java.lang.Thread.run(java.base@9-internal/Thread.java:804)
Caused by: java.lang.OutOfMemoryError: Java heap space
        at edu.stanford.nlp.stats.ClassicCounter.incrementCount(ClassicCounter.java:232)
        at edu.stanford.nlp.coref.statistical.FeatureExtractor.getFeatures(FeatureExtractor.java:320)
        at edu.stanford.nlp.coref.statistical.FeatureExtractor.extract(FeatureExtractor.java:128)
        at edu.stanford.nlp.coref.statistical.StatisticalCorefAlgorithm.runCoref(StatisticalCorefAlgorithm.java:96)
        at edu.stanford.nlp.coref.CorefSystem.annotate(CorefSystem.java:59)
        at edu.stanford.nlp.coref.CorefSystem.annotate(CorefSystem.java:47)
        at edu.stanford.nlp.pipeline.CorefAnnotator.annotate(CorefAnnotator.java:76)
        at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:75)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:605)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.lambda$handle$353(StanfordCoreNLPServer.java:575)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler$$Lambda$88/595261723.call(Unknown Source)
        at java.util.concurrent.FutureTask.run(java.base@9-internal/FutureTask.java:266)
        ... 3 more
[pool-1-thread-3] INFO CoreNLP - [/10.0.32.92:55592] API call w/annotators tokenize,ssplit,pos,lemma,ner,depparse,mention,coref
Exception in thread ""Thread-5"" java.lang.OutOfMemoryError: Java heap space
Exception in thread ""pool-1-thread-5"" java.lang.OutOfMemoryError: Java heap space"
306,https://github.com/stanfordnlp/CoreNLP/issues/395,395,[],closed,2017-03-22 21:13:07+00:00,,numDatumsPerLabel does not return any results,"When I start my [ColumnDataClassifier](https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/classify/ColumnDataClassifier.html)

I can see how it calls `numDatumsPerLabel` method to print datum for each label:
```
22:08:19.128 [main] INFO  edu.stanford.nlp.classify.Dataset - numDatums: 8063
numDatumsPerLabel: {A=1116.0, B=221.0, C=2134.0, D=3551.0, E=1041.0}
numLabels: 5 [A, B, C, D, E]
```

But if I try to call this method from my class

```
 res = this.classifier.numDatumsPerLabel();
```

I get a {} object. In my understanding the [numDatumsPerLabel] method (https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/classify/GeneralDataset.html#numDatumsPerLabel--) should return a [ClassicCounter](https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/stats/ClassicCounter.html) object that calculates the increment for each labels.

I have my training and testing set loaded."
307,https://github.com/stanfordnlp/CoreNLP/issues/396,396,[],open,2017-03-24 03:33:49+00:00,,Find a bug of NER model ,"When I use the stanfordnlp to Chinese named entity recognition, I find a bug:

String orignString = ""ËÄ≥Êú∫ÂéöÂ∫¶‰ªÖ‰∏∫7.6 mm„ÄÇ"";
List<String> segmentedWords = segmenter.segmentString(sample); // result( ""ËÄ≥Êú∫ ÂéöÂ∫¶ ‰ªÖ ‰∏∫ 7.6 mm  „ÄÇ""); 
String str = ""ËÄ≥Êú∫ ÂéöÂ∫¶ ‰ªÖ ‰∏∫ 7.6 mm  „ÄÇ""
ner.classifyWithInlineXML(sent); // result(""ËÄ≥Êú∫ ÂéöÂ∫¶ ‰ªÖ ‰∏∫ <MISC>7.6</MISC>mm  „ÄÇ"")
ner.classifyToString(sent);// result(""ËÄ≥Êú∫/O ÂéöÂ∫¶/O ‰ªÖ/O ‰∏∫/O 7.6/MISCmm/O  „ÄÇ/O"") 
"
308,https://github.com/stanfordnlp/CoreNLP/issues/397,397,[],closed,2017-03-26 16:46:19+00:00,,lemmatization for german incorrect,"i use a coreNLP server for german (version 2016-10-31) which works ok but the lemmas included in the output are off. for examle i see `ging`as a lemma (correct would be `gehen`) etc. 

`<token id=""2""><word>ging</word><lemma>ging</lemma><CharacterOffsetBegin>23..27</CharacterOffsetEnd><POS>VVFIN< ....`


internally, the parser must have a better lemmatizatin to determine (correctly) for the parse `(VVFIN ging)`. i assume this is a bug somewhere which does not copy the correct lemmas into the output file. 

a bugfix/patch would be greatly appreciated!

other issue with the german parser: requires `latin1` encoding ...
"
309,https://github.com/stanfordnlp/CoreNLP/issues/398,398,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 706055902, 'node_id': 'MDU6TGFiZWw3MDYwNTU5MDI=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/tokenize', 'name': 'tokenize', 'color': 'c5def5', 'default': False, 'description': None}, {'id': 706056248, 'node_id': 'MDU6TGFiZWw3MDYwNTYyNDg=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/ssplit', 'name': 'ssplit', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2017-03-27 02:46:01+00:00,,Chinese CORENLP Parsing : Parse per line,"Hi, 

I use the parser of CoreNLP for Chinese, here is my commandline : 
`java -mx2100m -cp stanford-corenlp.jar:stanford-chinese-corenlp-2016-10-31-models.jar edu.stanford.nlp.pipeline.StanfordCoreNLP -props StanfordCoreNLP-chinese.properties -annotators tokenize,ssplit,pos,depparse -file /Users/xx/xx/xx/xx/xx/myfile.txt -outputFormat conll -ssplit.eolonly`

The option ssplit.eolonly should help to provide a parsing made line by line. But it gives one tree for all the file. If I don't add this option, it will parse sentence by sentence.

How can I fix that? I want that the parser splits at each ""\n"". "
310,https://github.com/stanfordnlp/CoreNLP/issues/399,399,[],closed,2017-03-27 12:58:47+00:00,,java.io.IOException: The process cannot access the file because another process has locked a portion of the file,"There are many Maven tests failing with:

java.io.IOException: The process cannot access the file because another process has locked a portion of the file

eg

  testPutLocal(edu.stanford.nlp.util.FileBackedCacheTest)
"
311,https://github.com/stanfordnlp/CoreNLP/issues/400,400,[],closed,2017-03-30 23:24:35+00:00,,Stanford Parser for Processing French Text in GATE Developer.,"Hi, 
I need to process French text in Gate Developer 8.2. I am giving frenchFactored.ser.gzas URL to parameter parseFile for the StanfordParser processing resource . Kindly suggest what shall be given for tlppClass String?
The pipeline has Stanford PTB Tokenizer , ANNIE Sentence Splitter and Stanford POS Tagger such the the tagger parameter modelFile URL is file:/home/stanford-french-corenlp-2016-10-31-models/edu/stanford/nlp/models/pos-tagger/french/french.tagger.
"
312,https://github.com/stanfordnlp/CoreNLP/issues/401,401,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 706055902, 'node_id': 'MDU6TGFiZWw3MDYwNTU5MDI=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/tokenize', 'name': 'tokenize', 'color': 'c5def5', 'default': False, 'description': None}, {'id': 706065874, 'node_id': 'MDU6TGFiZWw3MDYwNjU4NzQ=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/cleanxml', 'name': 'cleanxml', 'color': 'c5def5', 'default': False, 'description': None}]",open,2017-04-05 11:59:28+00:00,,cleanxml forgets XML tags,"Dear all,

CoreNLP 3.7.0 with the cleanxml annotator apparently fails to remove tags in certain conditions. The problems I encountered so far resulted in the following tokens:
```
=_<
~_<
-_<
x_<
```
So it looks as if instances where the tag follows an underscore cause problems.

Thus, in the following example, one &lt;/ccline&gt; tag remains and is subsequently split up.

```
java -Xmx10g -XX:+UseNUMA -cp ""/path/to/stanford-corenlp-full-2016-10-31/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -pos.model edu/stanford/nlp/models/pos-tagger/english-caseless-left3words-distsim.tagger -annotators tokenize,cleanxml,ssplit,pos,truecase,lemma,ner,depparse -parse.maxlen 100 -ssplit.eolonly true -truecase.overwriteText true

[main] INFO [ REMOVED CLUTTER]  ... done [29.4 sec].

Entering interactive shell. Type q RETURN or EOF to quit.
NLP> <sentenceboundary /><ccline start=""20060813114948.000"" end=""20060813114953.000"">FIND OUT MORE AT SMALLSTEP.GOV.</ccline> <ccline start=""20060813114953.000"" end=""20060813114956.333"">w~_</ccline> <ccline start=""20060813114956.333"" end=""20060813114959.667"">WITH CODY GONE, I&apos;LL HAVE TO</ccline> <ccline start=""20060813114959.667"" end=""20060813115003.000"">FIGURE OUT ANOTHER WAY</ccline> <ccline start=""20060813115003.000"" end=""20060813115005.000"">TO GET THAT CODE.</ccline>
Sentence #1 (28 tokens):
FIND OUT MORE AT SMALLSTEP.GOV.</ccline> <ccline start=""20060813114953.000"" end=""20060813114956.333"">w~_</ccline> <ccline start=""20060813114956.333"" end=""20060813114959.667"">WITH CODY GONE, I&apos;LL HAVE TO</ccline> <ccline start=""20060813114959.667"" end=""20060813115003.000"">FIGURE OUT ANOTHER WAY</ccline> <ccline start=""20060813115003.000"" end=""20060813115005.000"">TO GET THAT CODE.
[Text=Find CharacterOffsetBegin=80 CharacterOffsetEnd=84 PartOfSpeech=VB TrueCase=INIT_UPPER TrueCaseText=Find Lemma=find NamedEntityTag=O]
[Text=out CharacterOffsetBegin=85 CharacterOffsetEnd=88 PartOfSpeech=RP TrueCase=LOWER TrueCaseText=out Lemma=out NamedEntityTag=O]
[Text=more CharacterOffsetBegin=89 CharacterOffsetEnd=93 PartOfSpeech=JJR TrueCase=LOWER TrueCaseText=more Lemma=more NamedEntityTag=O]
[Text=at CharacterOffsetBegin=94 CharacterOffsetEnd=96 PartOfSpeech=IN TrueCase=LOWER TrueCaseText=at Lemma=at NamedEntityTag=O]
[Text=SMALLSTEP.GOV CharacterOffsetBegin=97 CharacterOffsetEnd=110 PartOfSpeech=NNP TrueCase=O TrueCaseText=SMALLSTEP.GOV Lemma=SMALLSTEP.GOV NamedEntityTag=O]
[Text=. CharacterOffsetBegin=110 CharacterOffsetEnd=111 PartOfSpeech=. TrueCase=O TrueCaseText=. Lemma=. NamedEntityTag=O]
[Text=W CharacterOffsetBegin=181 CharacterOffsetEnd=182 PartOfSpeech=NNP TrueCase=UPPER TrueCaseText=W Lemma=W NamedEntityTag=O]
[Text=~_< CharacterOffsetBegin=182 CharacterOffsetEnd=185 PartOfSpeech=NNP TrueCase=O TrueCaseText=~_< Lemma=~_< NamedEntityTag=O]
[Text=/ CharacterOffsetBegin=185 CharacterOffsetEnd=186 PartOfSpeech=: TrueCase=O TrueCaseText=/ Lemma=/ NamedEntityTag=O]
[Text=CCLINE CharacterOffsetBegin=186 CharacterOffsetEnd=192 PartOfSpeech=NN TrueCase=UPPER TrueCaseText=CCLINE Lemma=ccline NamedEntityTag=O]
[Text=> CharacterOffsetBegin=192 CharacterOffsetEnd=193 PartOfSpeech=JJR TrueCase=O TrueCaseText=> Lemma=> NamedEntityTag=O]
[Text=With CharacterOffsetBegin=254 CharacterOffsetEnd=258 PartOfSpeech=IN TrueCase=INIT_UPPER TrueCaseText=With Lemma=with NamedEntityTag=O]
[Text=Cody CharacterOffsetBegin=259 CharacterOffsetEnd=263 PartOfSpeech=NNP TrueCase=INIT_UPPER TrueCaseText=Cody Lemma=Cody NamedEntityTag=PERSON]
[Text=Gone CharacterOffsetBegin=264 CharacterOffsetEnd=268 PartOfSpeech=VBN TrueCase=INIT_UPPER TrueCaseText=Gone Lemma=go NamedEntityTag=PERSON]
[Text=, CharacterOffsetBegin=268 CharacterOffsetEnd=269 PartOfSpeech=, TrueCase=O TrueCaseText=, Lemma=, NamedEntityTag=O]
[Text=I CharacterOffsetBegin=270 CharacterOffsetEnd=271 PartOfSpeech=PRP TrueCase=UPPER TrueCaseText=I Lemma=I NamedEntityTag=O]
[Text='ll CharacterOffsetBegin=271 CharacterOffsetEnd=279 PartOfSpeech=MD TrueCase=LOWER TrueCaseText='ll Lemma=will NamedEntityTag=O]
[Text=have CharacterOffsetBegin=280 CharacterOffsetEnd=284 PartOfSpeech=VB TrueCase=LOWER TrueCaseText=have Lemma=have NamedEntityTag=O]
[Text=to CharacterOffsetBegin=285 CharacterOffsetEnd=287 PartOfSpeech=TO TrueCase=LOWER TrueCaseText=to Lemma=to NamedEntityTag=O]
[Text=figure CharacterOffsetBegin=357 CharacterOffsetEnd=363 PartOfSpeech=VB TrueCase=LOWER TrueCaseText=figure Lemma=figure NamedEntityTag=O]
[Text=out CharacterOffsetBegin=364 CharacterOffsetEnd=367 PartOfSpeech=RP TrueCase=LOWER TrueCaseText=out Lemma=out NamedEntityTag=O]
[Text=another CharacterOffsetBegin=368 CharacterOffsetEnd=375 PartOfSpeech=DT TrueCase=LOWER TrueCaseText=another Lemma=another NamedEntityTag=O]
[Text=way CharacterOffsetBegin=376 CharacterOffsetEnd=379 PartOfSpeech=NN TrueCase=LOWER TrueCaseText=way Lemma=way NamedEntityTag=O]
[Text=to CharacterOffsetBegin=449 CharacterOffsetEnd=451 PartOfSpeech=TO TrueCase=LOWER TrueCaseText=to Lemma=to NamedEntityTag=O]
[Text=get CharacterOffsetBegin=452 CharacterOffsetEnd=455 PartOfSpeech=VB TrueCase=LOWER TrueCaseText=get Lemma=get NamedEntityTag=O]
[Text=that CharacterOffsetBegin=456 CharacterOffsetEnd=460 PartOfSpeech=DT TrueCase=LOWER TrueCaseText=that Lemma=that NamedEntityTag=O]
[Text=code CharacterOffsetBegin=461 CharacterOffsetEnd=465 PartOfSpeech=NN TrueCase=LOWER TrueCaseText=code Lemma=code NamedEntityTag=O]
[Text=. CharacterOffsetBegin=465 CharacterOffsetEnd=466 PartOfSpeech=. TrueCase=O TrueCaseText=. Lemma=. NamedEntityTag=O]
root(ROOT-0, Find-1)
compound:prt(Find-1, out-2)
dobj(Find-1, more-3)
case(SMALLSTEP.GOV-5, at-4)
nmod:at(more-3, SMALLSTEP.GOV-5)
punct(Find-1, .-6)
compound(~_<-8, W-7)
dep(Find-1, ~_<-8)
punct(~_<-8, /-9)
dep(~_<-8, CCLINE-10)
dep(Gone-14, >-11)
mark(Gone-14, With-12)
nsubj(Gone-14, Cody-13)
advcl:with(have-18, Gone-14)
punct(have-18, ,-15)
nsubj(have-18, I-16)
nsubj:xsubj(figure-20, I-16)
aux(have-18, 'll-17)
acl:relcl(CCLINE-10, have-18)
mark(figure-20, to-19)
xcomp(have-18, figure-20)
compound:prt(figure-20, out-21)
det(way-23, another-22)
dobj(figure-20, way-23)
mark(get-25, to-24)
acl:to(way-23, get-25)
det(code-27, that-26)
dobj(get-25, code-27)
punct(Find-1, .-28)

NLP>
```"
313,https://github.com/stanfordnlp/CoreNLP/issues/402,402,[],closed,2017-04-05 16:58:55+00:00,,Running Init Script with the -serverProperties flag,"StanfordCore: 2016-10.13

Using the script for starting up stanfordNLP I added the -serverProperties flag such that 

nohup su ""$SERVER_USER"" -c ""/usr/local/bin/authbind --deep java -Djava.net.preferIPv4Stack=true -Djava.io.tmpdir=""$CORENLP_DIR"" -cp ""$CLASSPATH"" -mx15g edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 80 -serverProperties server.properties""

In the $CORENLP_DIR I have the server.properties file with 664 permissions.  Yet, when I kick off the service I get the following error:

Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException:  argsToProperties could not read properties file: server.properties
  at edu.standord.nlp.util.StringUtiols.argsToProperties(StringUtils.java:1014)
 ......
.......
Caused by: java.io.IOException: Unable to open ""server.properties"" as class path, filenaem or URL
  at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLorClasspathOrFileSystem(IOUtils.java:470)


"
314,https://github.com/stanfordnlp/CoreNLP/issues/403,403,[],closed,2017-04-06 15:08:00+00:00,,Server API NullPointerException when text contains a single quote,"I have downloaded **CoreNLP 3.7.0** (English) as well as English (KBP) models.
Started the server:
`java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer & disown`

Created a python **pycorenlp** script that takes a single sentence and queries CoreNLP server.
For example:
```python
from pycorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP('http://localhost:9000')
text = ""Carrie would sneak rides on her sister Dan 's bike . ""
output = nlp.annotate(text, properties={
  'annotators': 'tokenize,ssplit,pos,lemma,ner,parse,mention,coref',
  'coref.algorithm': 'neural',
  'outputFormat': 'json'
  })
print output
```

However, I am getting `NullPointerException` error.
Looks like sentences containing a **single quote** `'` crash the pipeline. 
```
[pool-1-thread-4] INFO CoreNLP - [/0:0:0:0:0:0:0:1:49410] API call w/annotators tokenize,ssplit,pos,lemma,ner,parse,mention,coref
Carrie would sneak rides on her sister Dan 's bike .
[pool-2-thread-4] ERROR edu.stanford.nlp.pipeline.StanfordCoreNLP - Attempted to fetch annotator ""parse"" before the annotator pool was created!
java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:603)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
	at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
	at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
	at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
```

I have tried the same sentence with other third-party APIs as well, the error persists.
"
315,https://github.com/stanfordnlp/CoreNLP/issues/404,404,[],closed,2017-04-06 20:59:21+00:00,,Is -ner.model ignored when starting the corenlp server?,"I want the default annotator to be `ner` and only `3class` model to be used for it. I start the server as
```bash
$ java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000 -ner.model edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz -annotators ner
```

When I curl a request 
```
$ curl 'http://localhost:9000/' --data 'President Obama is male.'
```

I see in the console all `ner` models are loaded:
```
[pool-1-thread-1] INFO CoreNLP - [/127.0.0.1:50632] API call w/annotators tokenize,ssplit,pos,lemma,ner
President Obama is male.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[pool-1-thread-1] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.7 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.6 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.2 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
```

How can I force unwanted `ner` models not to load?"
316,https://github.com/stanfordnlp/CoreNLP/issues/405,405,[],closed,2017-04-07 12:06:10+00:00,,Annotators dependency bug for englishSR.ser.gz parse model,"I start the server with default annotator `parse` and force the `englishSR.ser.gz` parse model by adding stanford-english-corenlp-2016-10-31-models.jar to the classpath. 

Next I do a single curl request
`curl 'http://localhost:9000/' --data 'President Obama is male.'`

As you can see the `pos` annotator required by the model isn't loaded automaticly.

```
stanford-corenlp-full-2016-10-31$ java -mx4g -cp ""*:../stanford-english-corenlp-2016-10-31-models.jar"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000 -annotators parse
[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - setting default constituency parser
[main] INFO CoreNLP - using SR parser: edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP -     Threads: 4
[main] INFO CoreNLP - Starting server...
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000
[pool-1-thread-1] INFO CoreNLP - [/127.0.0.1:56758] API call w/annotators tokenize,ssplit,parse
President Obama is male.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[pool-1-thread-1] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/srparser/englishSR.ser.gz ... done [9.3 sec].
java.lang.IllegalArgumentException: annotator ""parse"" requires annotation ""PartOfSpeechAnnotation"". The usual requirements for this annotator are: tokenize,ssplit
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:460)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:154)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:145)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.mkStanfordCoreNLP(StanfordCoreNLPServer.java:273)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.access$500(StanfordCoreNLPServer.java:50)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:583)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
	at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
	at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
	at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
```

As only I start the server with `-annotators pos,parse` it starts working."
317,https://github.com/stanfordnlp/CoreNLP/issues/406,406,[],closed,2017-04-09 16:58:16+00:00,,Sentiment train binaryModel fails,"I am trying to train the Stanford NLP sentiment analysis tool with the default test and train set from https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip in binary mode. I am using stanford-corenlp-full-2016-10-31.

To do so I use this command:

        java -cp ""*"" -Xmx10g edu.stanford.nlp.sentiment.SentimentTraining         
        -trainPath train.txt 
        -devPath dev.txt 
        -train 
        -model ORIG_BIN.ser.gz 
        -binaryModel

Which leads to the following error:


        [main] INFO edu.stanford.nlp.sentiment.SentimentTraining - Read in 8544 training trees
        [main] INFO edu.stanford.nlp.sentiment.SentimentTraining - Read in 1101 dev trees
        [main] INFO edu.stanford.nlp.sentiment.SentimentTraining - Sentiment model options:
        GENERAL OPTIONS
        randomSeed=-1081864962
        wordVectors=null
        unkWord=UNK
        randomWordVectors=true
        numHid=25
        numClasses=2
        lowercaseWordVectors=false
        useTensors=true
        simplifiedModel=true
        combineClassification=true
        classNames=Negative,Positive
        equivalenceClasses=0;1
        equivalenceClassNames=Negative,Positive
        TRAIN OPTIONS
        batchSize=27
        epochs=400
        debugOutputEpochs=8
        maxTrainTimeSeconds=86400
        learningRate=0.01
        scalingForInit=1.0
        classWeights=null
        regTransformMatrix=0.001
        regTransformTensor=0.001
        regClassification=1.0E-4
        regWordVector=1.0E-4
        initialAdagradWeight=0.0
        adagradResetFrequency=1
        shuffleMatrices=true
        initialMatrixLogPath=null
        nThreads=1
        TEST OPTIONS
        ngramRecordSize=0
        ngramRecordMaximumLength=0
        printLengthAccuracies=false
        [main] INFO edu.stanford.nlp.sentiment.SentimentTraining - Training on 8544 trees in 317 batches
        [main] INFO edu.stanford.nlp.sentiment.SentimentTraining - Times through each training batch: 400
        [main] INFO edu.stanford.nlp.sentiment.SentimentTraining - ======================================
        [main] INFO edu.stanford.nlp.sentiment.SentimentTraining - Starting epoch 0
        [main] INFO edu.stanford.nlp.sentiment.SentimentTraining - ======================================
        [main] INFO edu.stanford.nlp.sentiment.SentimentTraining - Epoch 0 batch 0
        Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 3
                at org.ejml.data.D1Matrix64F.set(Unknown Source)
                at org.ejml.simple.SimpleBase.set(Unknown Source)
                at edu.stanford.nlp.sentiment.SentimentCostAndGradient.backpropDerivativesAndError(SentimentCostAndGradient.java:374)
                at edu.stanford.nlp.sentiment.SentimentCostAndGradient.backpropDerivativesAndError(SentimentCostAndGradient.java:352)
                at edu.stanford.nlp.sentiment.SentimentCostAndGradient.scoreDerivatives(SentimentCostAndGradient.java:221)
                at edu.stanford.nlp.sentiment.SentimentCostAndGradient.calculate(SentimentCostAndGradient.java:247)
                at edu.stanford.nlp.optimization.AbstractCachingDiffFunction.ensure(AbstractCachingDiffFunction.java:140)
                at edu.stanford.nlp.optimization.AbstractCachingDiffFunction.derivativeAt(AbstractCachingDiffFunction.java:151)
                at edu.stanford.nlp.sentiment.SentimentTraining.executeOneTrainingBatch(SentimentTraining.java:33)
                at edu.stanford.nlp.sentiment.SentimentTraining.train(SentimentTraining.java:83)
                at edu.stanford.nlp.sentiment.SentimentTraining.main(SentimentTraining.java:230)

I also tried several other options like `-equivalenceClasses, -equivalenceClassNames -numClasses, -classNames` with more or less the same results.

What options do I need to make this work ?

----------

I am also interested into training my own corpus with three classes (Positive, Negative, Neutral) but with this task I had the same problems.

----------

I also (like suggested in https://nlp.stanford.edu/software/) made a stackoverflow question: http://stackoverflow.com/questions/43250625/stanford-nlp-sentiment-train-binarymodel-fails
"
318,https://github.com/stanfordnlp/CoreNLP/issues/407,407,"[{'id': 626016953, 'node_id': 'MDU6TGFiZWw2MjYwMTY5NTM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/analysis-bug', 'name': 'analysis-bug', 'color': 'f98685', 'default': False, 'description': None}]",closed,2017-04-09 21:16:17+00:00,,Stanford online NER tool vs stanford-ner-2016-10-31 cli tool,"I wonder why results at http://nlp.stanford.edu:8080/ner/ differ from those by the stanford-ner-2016-10-31 cli tool?

For example test string=HAHAHAHAHA

The online tool detects nothing with the 3-class classifier while the cli tool detects something
```
stanford-ner-2016-10-31$ ./ner.sh <(echo HAHAHAHAHA)
Invoked on Mon Apr 10 00:02:59 EEST 2017 with arguments: -loadClassifier ./classifiers/english.all.3class.distsim.crf.ser.gz -textFile /dev/fd/63
loadClassifier=./classifiers/english.all.3class.distsim.crf.ser.gz
textFile=/dev/fd/63
Loading classifier from ./classifiers/english.all.3class.distsim.crf.ser.gz ... done [1.5 sec].
HAHAHAHAHA/PERSON 
CRFClassifier tagged 1 words in 1 documents at 13.70 words per second.
```

Trying different samples I see the online tool performs **better** than the cli one.

Is it because the online tool's english.all.3class.distsim.crf.ser.gz model differs from the one bundled with stanford-ner-2016-10-31?"
319,https://github.com/stanfordnlp/CoreNLP/issues/408,408,[],closed,2017-04-13 22:47:00+00:00,,Bug: Using Neural Coref with ShiftReduce Parser,"Whenever I use the Neural Coref annotator with the ShiftReduce parser, I get the following error:

```
Exception in thread ""main"" java.lang.IllegalArgumentException: Input word not tagged
	at edu.stanford.nlp.parser.shiftreduce.ShiftReduceParser.initialStateFromTaggedSentence(ShiftReduceParser.java:236)
	at edu.stanford.nlp.parser.shiftreduce.ShiftReduceParserQuery.parse(ShiftReduceParserQuery.java:54)
	at edu.stanford.nlp.pipeline.ParserAnnotator.doOneSentence(ParserAnnotator.java:323)
	at edu.stanford.nlp.pipeline.ParserAnnotator.doOneSentence(ParserAnnotator.java:254)
	at edu.stanford.nlp.pipeline.SentenceAnnotator.annotate(SentenceAnnotator.java:102)
	at edu.stanford.nlp.coref.md.CorefMentionFinder.parse(CorefMentionFinder.java:645)
	at edu.stanford.nlp.coref.md.CorefMentionFinder.findSyntacticHead(CorefMentionFinder.java:536)
	at edu.stanford.nlp.coref.md.CorefMentionFinder.findHead(CorefMentionFinder.java:456)
	at edu.stanford.nlp.coref.md.RuleBasedCorefMentionFinder.findMentions(RuleBasedCorefMentionFinder.java:100)
	at edu.stanford.nlp.pipeline.MentionAnnotator.annotate(MentionAnnotator.java:102)
	at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:605)
	at annotation.DefaultPipeline.annotate(DefaultPipeline.java:25)
	at NLPeeps.getRelations(NLPeeps.java:53)
	at Ask.ask(Ask.java:13)
	at Ask.main(Ask.java:47)
```"
320,https://github.com/stanfordnlp/CoreNLP/issues/409,409,[],closed,2017-04-18 03:03:51+00:00,,Stanford Chinese CoreNLP model don't write to file without the ssplit,"Using the following `.properties` file with only the segmentation step, the output is empty:

```
annotators = segment
customAnnotatorClass.segment = edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator
segment.model = edu/stanford/nlp/models/segmenter/chinese/ctb.gz
segment.sighanCorporaDict = edu/stanford/nlp/models/segmenter/chinese
segment.serDictionary = edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz
segment.sighanPostProcessing = true
```

And the input file is 

```
ÁîüÂëäÊøÄÂäπÈõëÈ°åÈπøÈöõÂÜôÂæπËºù‰∏á„ÄÇÊúÄÂçîÈ†òÈáé‰∫¨ÈÉ®ÁøíÈ†ÇÁµåÊó•‰∫§Êó•ËÄÉÂΩìÂÇô„ÄÇ
```

The command:

```
$ java -mx10g -cp ""stanford-corenlp-3.7.0.jar:stanford-chinese-corenlp-2016-10-31-models.jar""  edu.stanford.nlp.pipeline.StanfordCoreNLP -props StanfordCoreNLP-chinese.tokenize.properties -file input.txt -outputFormat json

Registering annotator segment with class edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator
Adding annotator segment
Loading Chinese dictionaries from 1 file:
  edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz
Done. Unique words in ChineseDictionary is: 423200.
Loading classifier from edu/stanford/nlp/models/segmenter/chinese/ctb.gz ... done [12.6 sec].

Processing file /Users/liling.tan/stanford-stuff/input.txt ... writing to /Users/liling.tan/stanford-stuff/input.txt.json
Loading character dictionary file from edu/stanford/nlp/models/segmenter/chinese/dict/character_list [done].
Loading affix dictionary from edu/stanford/nlp/models/segmenter/chinese/dict/in.ctb [done].
Annotating file /Users/liling.tan/stanford-stuff/input.txt ... done [0.2 sec].

Annotation pipeline timing information:
ChineseSegmenterAnnotator: 0.2 sec.
TOTAL: 0.2 sec. for 16 tokens at 85.1 tokens/sec.
Pipeline setup: 12.8 sec.
Total time for StanfordCoreNLP pipeline: 13.0 sec.
```

And the output in `input.txt.json`:

```
{
}
```

But if the full pipeline is used as suggested on https://stanfordnlp.github.io/CoreNLP/human-languages.html#chinese with the properties:

```
annotators = segment, ssplit, pos, ner, parse
customAnnotatorClass.segment = edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator
segment.model = edu/stanford/nlp/models/segmenter/chinese/ctb.gz
segment.sighanCorporaDict = edu/stanford/nlp/models/segmenter/chinese
segment.serDictionary = edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz
segment.sighanPostProcessing = true
ssplit.boundaryTokenRegex = [.]|[!?]+|[„ÄÇ]|[ÔºÅÔºü]+
pos.model = edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger
ner.model = edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz
ner.applyNumericClassifiers = false
ner.useSUTime = false
parse.model = edu/stanford/nlp/models/lexparser/chineseFactored.ser.gz
```

With the same command the console output:

```
Registering annotator segment with class edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator
Adding annotator segment
Loading Chinese dictionaries from 1 file:
  edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz
Done. Unique words in ChineseDictionary is: 423200.
Loading classifier from edu/stanford/nlp/models/segmenter/chinese/ctb.gz ... done [13.8 sec].
Adding annotator ssplit
Adding annotator pos
Loading POS tagger from edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger ... done [1.5 sec].
Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz ... done [3.3 sec].
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/chineseFactored.ser.gz ... done [8.8 sec].

Processing file /Users/liling.tan/stanford-stuff/input.txt ... writing to /Users/liling.tan/stanford-stuff/input.txt.json
Loading character dictionary file from edu/stanford/nlp/models/segmenter/chinese/dict/character_list [done].
Loading affix dictionary from edu/stanford/nlp/models/segmenter/chinese/dict/in.ctb [done].
Annotating file /Users/liling.tan/stanford-stuff/input.txt ... done [1.2 sec].

Annotation pipeline timing information:
ChineseSegmenterAnnotator: 0.2 sec.
WordsToSentencesAnnotator: 0.0 sec.
POSTaggerAnnotator: 0.1 sec.
NERCombinerAnnotator: 0.0 sec.
ParserAnnotator: 0.9 sec.
TOTAL: 1.2 sec. for 16 tokens at 13.0 tokens/sec.
Pipeline setup: 27.8 sec.
Total time for StanfordCoreNLP pipeline: 29.2 sec.
```

It does output the json in `input.txt.json`:

```
{
  ""sentences"": [
    {
      ""index"": 0,
      ""parse"": ""(ROOT\n  (IP\n    (VP\n      (VP (VV Áîü)\n        (VP (VV Âëä)\n          (NP (NN ÊøÄÂäπ) (NN ÈõëÈ°å) (NN ÈπøÈöõ))))\n      (VP (VV ÂÜô)\n        (QP (CD ÂæπËºù‰∏á))))\n    (PU „ÄÇ)))"",
      ""basicDependencies"": [
        {
          ""dep"": ""ROOT"",
          ""governor"": 0,
          ""governorGloss"": ""ROOT"",
          ""dependent"": 2,
          ""dependentGloss"": ""Âëä""
        },
        {
          ""dep"": ""xcomp"",
          ""governor"": 2,
          ""governorGloss"": ""Âëä"",
          ""dependent"": 1,
          ""dependentGloss"": ""Áîü""
        },
        {
          ""dep"": ""compound:nn"",
          ""governor"": 5,
          ""governorGloss"": ""ÈπøÈöõ"",
          ""dependent"": 3,
          ""dependentGloss"": ""ÊøÄÂäπ""
        },
        {
          ""dep"": ""compound:nn"",
          ""governor"": 5,
          ""governorGloss"": ""ÈπøÈöõ"",
          ""dependent"": 4,
          ""dependentGloss"": ""ÈõëÈ°å""
        },
        {
          ""dep"": ""dobj"",
          ""governor"": 2,
          ""governorGloss"": ""Âëä"",
          ""dependent"": 5,
          ""dependentGloss"": ""ÈπøÈöõ""
        },
        {
          ""dep"": ""conj"",
          ""governor"": 2,
          ""governorGloss"": ""Âëä"",
          ""dependent"": 6,
          ""dependentGloss"": ""ÂÜô""
        },
        {
          ""dep"": ""nmod:range"",
          ""governor"": 6,
          ""governorGloss"": ""ÂÜô"",
          ""dependent"": 7,
          ""dependentGloss"": ""ÂæπËºù‰∏á""
        },
        {
          ""dep"": ""punct"",
          ""governor"": 2,
          ""governorGloss"": ""Âëä"",
          ""dependent"": 8,
          ""dependentGloss"": ""„ÄÇ""
        }
      ],
      ""enhancedDependencies"": [
        {
          ""dep"": ""ROOT"",
          ""governor"": 0,
          ""governorGloss"": ""ROOT"",
          ""dependent"": 2,
          ""dependentGloss"": ""Âëä""
        },
        {
          ""dep"": ""xcomp"",
          ""governor"": 2,
          ""governorGloss"": ""Âëä"",
          ""dependent"": 1,
          ""dependentGloss"": ""Áîü""
        },
        {
          ""dep"": ""compound:nn"",
          ""governor"": 5,
          ""governorGloss"": ""ÈπøÈöõ"",
          ""dependent"": 3,
          ""dependentGloss"": ""ÊøÄÂäπ""
        },
        {
          ""dep"": ""compound:nn"",
          ""governor"": 5,
          ""governorGloss"": ""ÈπøÈöõ"",
          ""dependent"": 4,
          ""dependentGloss"": ""ÈõëÈ°å""
        },
        {
          ""dep"": ""dobj"",
          ""governor"": 2,
          ""governorGloss"": ""Âëä"",
          ""dependent"": 5,
          ""dependentGloss"": ""ÈπøÈöõ""
        },
        {
          ""dep"": ""conj"",
          ""governor"": 2,
          ""governorGloss"": ""Âëä"",
          ""dependent"": 6,
          ""dependentGloss"": ""ÂÜô""
        },
        {
          ""dep"": ""nmod:range"",
          ""governor"": 6,
          ""governorGloss"": ""ÂÜô"",
          ""dependent"": 7,
          ""dependentGloss"": ""ÂæπËºù‰∏á""
        },
        {
          ""dep"": ""punct"",
          ""governor"": 2,
          ""governorGloss"": ""Âëä"",
          ""dependent"": 8,
          ""dependentGloss"": ""„ÄÇ""
        }
      ],
      ""enhancedPlusPlusDependencies"": [
        {
          ""dep"": ""ROOT"",
          ""governor"": 0,
          ""governorGloss"": ""ROOT"",
          ""dependent"": 2,
          ""dependentGloss"": ""Âëä""
        },
        {
          ""dep"": ""xcomp"",
          ""governor"": 2,
          ""governorGloss"": ""Âëä"",
          ""dependent"": 1,
          ""dependentGloss"": ""Áîü""
        },
        {
          ""dep"": ""compound:nn"",
          ""governor"": 5,
          ""governorGloss"": ""ÈπøÈöõ"",
          ""dependent"": 3,
          ""dependentGloss"": ""ÊøÄÂäπ""
        },
        {
          ""dep"": ""compound:nn"",
          ""governor"": 5,
          ""governorGloss"": ""ÈπøÈöõ"",
          ""dependent"": 4,
          ""dependentGloss"": ""ÈõëÈ°å""
        },
        {
          ""dep"": ""dobj"",
          ""governor"": 2,
          ""governorGloss"": ""Âëä"",
          ""dependent"": 5,
          ""dependentGloss"": ""ÈπøÈöõ""
        },
        {
          ""dep"": ""conj"",
          ""governor"": 2,
          ""governorGloss"": ""Âëä"",
          ""dependent"": 6,
          ""dependentGloss"": ""ÂÜô""
        },
        {
          ""dep"": ""nmod:range"",
          ""governor"": 6,
          ""governorGloss"": ""ÂÜô"",
          ""dependent"": 7,
          ""dependentGloss"": ""ÂæπËºù‰∏á""
        },
        {
          ""dep"": ""punct"",
          ""governor"": 2,
          ""governorGloss"": ""Âëä"",
          ""dependent"": 8,
          ""dependentGloss"": ""„ÄÇ""
        }
      ],
      ""tokens"": [
        {
          ""index"": 1,
          ""word"": ""Áîü"",
          ""originalText"": """",
          ""characterOffsetBegin"": 0,
          ""characterOffsetEnd"": 1,
          ""pos"": ""VV"",
          ""ner"": ""O""
        },
        {
          ""index"": 2,
          ""word"": ""Âëä"",
          ""originalText"": """",
          ""characterOffsetBegin"": 1,
          ""characterOffsetEnd"": 2,
          ""pos"": ""VV"",
          ""ner"": ""O""
        },
        {
          ""index"": 3,
          ""word"": ""ÊøÄÂäπ"",
          ""originalText"": """",
          ""characterOffsetBegin"": 2,
          ""characterOffsetEnd"": 4,
          ""pos"": ""NN"",
          ""ner"": ""O""
        },
        {
          ""index"": 4,
          ""word"": ""ÈõëÈ°å"",
          ""originalText"": """",
          ""characterOffsetBegin"": 4,
          ""characterOffsetEnd"": 6,
          ""pos"": ""NN"",
          ""ner"": ""O""
        },
        {
          ""index"": 5,
          ""word"": ""ÈπøÈöõ"",
          ""originalText"": """",
          ""characterOffsetBegin"": 6,
          ""characterOffsetEnd"": 8,
          ""pos"": ""NN"",
          ""ner"": ""O""
        },
        {
          ""index"": 6,
          ""word"": ""ÂÜô"",
          ""originalText"": """",
          ""characterOffsetBegin"": 8,
          ""characterOffsetEnd"": 9,
          ""pos"": ""VV"",
          ""ner"": ""O""
        },
        {
          ""index"": 7,
          ""word"": ""ÂæπËºù‰∏á"",
          ""originalText"": """",
          ""characterOffsetBegin"": 9,
          ""characterOffsetEnd"": 12,
          ""pos"": ""CD"",
          ""ner"": ""MISC""
        },
        {
          ""index"": 8,
          ""word"": ""„ÄÇ"",
          ""originalText"": """",
          ""characterOffsetBegin"": 12,
          ""characterOffsetEnd"": 13,
          ""pos"": ""PU"",
          ""ner"": ""O""
        }
      ]
    },
    {
      ""index"": 1,
      ""parse"": ""(ROOT\n  (IP\n    (NP\n      (NP (NR ÊúÄÂçîÈ†ò))\n      (NP (NN Èáé‰∫¨ÈÉ®)))\n    (NP (NR ÁøíÈ†ÇÁµå) (NR Êó•))\n    (VP\n      (ADVP (AD ‰∫§Êó•))\n      (VP (VV ËÄÉ)\n        (NP (NN ÂΩìÂÇô))))\n    (PU „ÄÇ)))"",
      ""basicDependencies"": [
        {
          ""dep"": ""ROOT"",
          ""governor"": 0,
          ""governorGloss"": ""ROOT"",
          ""dependent"": 6,
          ""dependentGloss"": ""ËÄÉ""
        },
        {
          ""dep"": ""nmod:assmod"",
          ""governor"": 2,
          ""governorGloss"": ""Èáé‰∫¨ÈÉ®"",
          ""dependent"": 1,
          ""dependentGloss"": ""ÊúÄÂçîÈ†ò""
        },
        {
          ""dep"": ""nmod:topic"",
          ""governor"": 6,
          ""governorGloss"": ""ËÄÉ"",
          ""dependent"": 2,
          ""dependentGloss"": ""Èáé‰∫¨ÈÉ®""
        },
        {
          ""dep"": ""name"",
          ""governor"": 4,
          ""governorGloss"": ""Êó•"",
          ""dependent"": 3,
          ""dependentGloss"": ""ÁøíÈ†ÇÁµå""
        },
        {
          ""dep"": ""nsubj"",
          ""governor"": 6,
          ""governorGloss"": ""ËÄÉ"",
          ""dependent"": 4,
          ""dependentGloss"": ""Êó•""
        },
        {
          ""dep"": ""advmod"",
          ""governor"": 6,
          ""governorGloss"": ""ËÄÉ"",
          ""dependent"": 5,
          ""dependentGloss"": ""‰∫§Êó•""
        },
        {
          ""dep"": ""dobj"",
          ""governor"": 6,
          ""governorGloss"": ""ËÄÉ"",
          ""dependent"": 7,
          ""dependentGloss"": ""ÂΩìÂÇô""
        },
        {
          ""dep"": ""punct"",
          ""governor"": 6,
          ""governorGloss"": ""ËÄÉ"",
          ""dependent"": 8,
          ""dependentGloss"": ""„ÄÇ""
        }
      ],
      ""enhancedDependencies"": [
        {
          ""dep"": ""ROOT"",
          ""governor"": 0,
          ""governorGloss"": ""ROOT"",
          ""dependent"": 6,
          ""dependentGloss"": ""ËÄÉ""
        },
        {
          ""dep"": ""nmod:assmod"",
          ""governor"": 2,
          ""governorGloss"": ""Èáé‰∫¨ÈÉ®"",
          ""dependent"": 1,
          ""dependentGloss"": ""ÊúÄÂçîÈ†ò""
        },
        {
          ""dep"": ""nmod:topic"",
          ""governor"": 6,
          ""governorGloss"": ""ËÄÉ"",
          ""dependent"": 2,
          ""dependentGloss"": ""Èáé‰∫¨ÈÉ®""
        },
        {
          ""dep"": ""name"",
          ""governor"": 4,
          ""governorGloss"": ""Êó•"",
          ""dependent"": 3,
          ""dependentGloss"": ""ÁøíÈ†ÇÁµå""
        },
        {
          ""dep"": ""nsubj"",
          ""governor"": 6,
          ""governorGloss"": ""ËÄÉ"",
          ""dependent"": 4,
          ""dependentGloss"": ""Êó•""
        },
        {
          ""dep"": ""advmod"",
          ""governor"": 6,
          ""governorGloss"": ""ËÄÉ"",
          ""dependent"": 5,
          ""dependentGloss"": ""‰∫§Êó•""
        },
        {
          ""dep"": ""dobj"",
          ""governor"": 6,
          ""governorGloss"": ""ËÄÉ"",
          ""dependent"": 7,
          ""dependentGloss"": ""ÂΩìÂÇô""
        },
        {
          ""dep"": ""punct"",
          ""governor"": 6,
          ""governorGloss"": ""ËÄÉ"",
          ""dependent"": 8,
          ""dependentGloss"": ""„ÄÇ""
        }
      ],
      ""enhancedPlusPlusDependencies"": [
        {
          ""dep"": ""ROOT"",
          ""governor"": 0,
          ""governorGloss"": ""ROOT"",
          ""dependent"": 6,
          ""dependentGloss"": ""ËÄÉ""
        },
        {
          ""dep"": ""nmod:assmod"",
          ""governor"": 2,
          ""governorGloss"": ""Èáé‰∫¨ÈÉ®"",
          ""dependent"": 1,
          ""dependentGloss"": ""ÊúÄÂçîÈ†ò""
        },
        {
          ""dep"": ""nmod:topic"",
          ""governor"": 6,
          ""governorGloss"": ""ËÄÉ"",
          ""dependent"": 2,
          ""dependentGloss"": ""Èáé‰∫¨ÈÉ®""
        },
        {
          ""dep"": ""name"",
          ""governor"": 4,
          ""governorGloss"": ""Êó•"",
          ""dependent"": 3,
          ""dependentGloss"": ""ÁøíÈ†ÇÁµå""
        },
        {
          ""dep"": ""nsubj"",
          ""governor"": 6,
          ""governorGloss"": ""ËÄÉ"",
          ""dependent"": 4,
          ""dependentGloss"": ""Êó•""
        },
        {
          ""dep"": ""advmod"",
          ""governor"": 6,
          ""governorGloss"": ""ËÄÉ"",
          ""dependent"": 5,
          ""dependentGloss"": ""‰∫§Êó•""
        },
        {
          ""dep"": ""dobj"",
          ""governor"": 6,
          ""governorGloss"": ""ËÄÉ"",
          ""dependent"": 7,
          ""dependentGloss"": ""ÂΩìÂÇô""
        },
        {
          ""dep"": ""punct"",
          ""governor"": 6,
          ""governorGloss"": ""ËÄÉ"",
          ""dependent"": 8,
          ""dependentGloss"": ""„ÄÇ""
        }
      ],
      ""tokens"": [
        {
          ""index"": 1,
          ""word"": ""ÊúÄÂçîÈ†ò"",
          ""originalText"": """",
          ""characterOffsetBegin"": 13,
          ""characterOffsetEnd"": 16,
          ""pos"": ""NR"",
          ""ner"": ""ORGANIZATION""
        },
        {
          ""index"": 2,
          ""word"": ""Èáé‰∫¨ÈÉ®"",
          ""originalText"": """",
          ""characterOffsetBegin"": 16,
          ""characterOffsetEnd"": 19,
          ""pos"": ""NN"",
          ""ner"": ""ORGANIZATION""
        },
        {
          ""index"": 3,
          ""word"": ""ÁøíÈ†ÇÁµå"",
          ""originalText"": """",
          ""characterOffsetBegin"": 19,
          ""characterOffsetEnd"": 22,
          ""pos"": ""NR"",
          ""ner"": ""ORGANIZATION""
        },
        {
          ""index"": 4,
          ""word"": ""Êó•"",
          ""originalText"": """",
          ""characterOffsetBegin"": 22,
          ""characterOffsetEnd"": 23,
          ""pos"": ""NR"",
          ""ner"": ""O""
        },
        {
          ""index"": 5,
          ""word"": ""‰∫§Êó•"",
          ""originalText"": """",
          ""characterOffsetBegin"": 23,
          ""characterOffsetEnd"": 25,
          ""pos"": ""AD"",
          ""ner"": ""O""
        },
        {
          ""index"": 6,
          ""word"": ""ËÄÉ"",
          ""originalText"": """",
          ""characterOffsetBegin"": 25,
          ""characterOffsetEnd"": 26,
          ""pos"": ""VV"",
          ""ner"": ""O""
        },
        {
          ""index"": 7,
          ""word"": ""ÂΩìÂÇô"",
          ""originalText"": """",
          ""characterOffsetBegin"": 26,
          ""characterOffsetEnd"": 28,
          ""pos"": ""NN"",
          ""ner"": ""O""
        },
        {
          ""index"": 8,
          ""word"": ""„ÄÇ"",
          ""originalText"": """",
          ""characterOffsetBegin"": 28,
          ""characterOffsetEnd"": 29,
          ""pos"": ""PU"",
          ""ner"": ""O""
        }
      ]
    }
  ]
}
```

----

But by adding `ssplit` in `.properties`, 

```
annotators = segment, ssplit
customAnnotatorClass.segment = edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator
segment.model = edu/stanford/nlp/models/segmenter/chinese/ctb.gz
segment.sighanCorporaDict = edu/stanford/nlp/models/segmenter/chinese
segment.serDictionary = edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz
segment.sighanPostProcessing = true
ssplit.boundaryTokenRegex = [.]|[!?]+|[„ÄÇ]|[ÔºÅÔºü]+
```

it does write the output to file:

```
{
  ""sentences"": [
    {
      ""index"": 0,
      ""tokens"": [
        {
          ""index"": 1,
          ""word"": ""Áîü"",
          ""originalText"": """",
          ""characterOffsetBegin"": 0,
          ""characterOffsetEnd"": 1
        },
        {
          ""index"": 2,
          ""word"": ""Âëä"",
          ""originalText"": """",
          ""characterOffsetBegin"": 1,
          ""characterOffsetEnd"": 2
        },
        {
          ""index"": 3,
          ""word"": ""ÊøÄÂäπ"",
          ""originalText"": """",
          ""characterOffsetBegin"": 2,
          ""characterOffsetEnd"": 4
        },
        {
          ""index"": 4,
          ""word"": ""ÈõëÈ°å"",
          ""originalText"": """",
          ""characterOffsetBegin"": 4,
          ""characterOffsetEnd"": 6
        },
        {
          ""index"": 5,
          ""word"": ""ÈπøÈöõ"",
          ""originalText"": """",
          ""characterOffsetBegin"": 6,
          ""characterOffsetEnd"": 8
        },
        {
          ""index"": 6,
          ""word"": ""ÂÜô"",
          ""originalText"": """",
          ""characterOffsetBegin"": 8,
          ""characterOffsetEnd"": 9
        },
        {
          ""index"": 7,
          ""word"": ""ÂæπËºù‰∏á"",
          ""originalText"": """",
          ""characterOffsetBegin"": 9,
          ""characterOffsetEnd"": 12
        },
        {
          ""index"": 8,
          ""word"": ""„ÄÇ"",
          ""originalText"": """",
          ""characterOffsetBegin"": 12,
          ""characterOffsetEnd"": 13
        }
      ]
    },
    {
      ""index"": 1,
      ""tokens"": [
        {
          ""index"": 1,
          ""word"": ""ÊúÄÂçîÈ†ò"",
          ""originalText"": """",
          ""characterOffsetBegin"": 13,
          ""characterOffsetEnd"": 16
        },
        {
          ""index"": 2,
          ""word"": ""Èáé‰∫¨ÈÉ®"",
          ""originalText"": """",
          ""characterOffsetBegin"": 16,
          ""characterOffsetEnd"": 19
        },
        {
          ""index"": 3,
          ""word"": ""ÁøíÈ†ÇÁµå"",
          ""originalText"": """",
          ""characterOffsetBegin"": 19,
          ""characterOffsetEnd"": 22
        },
        {
          ""index"": 4,
          ""word"": ""Êó•"",
          ""originalText"": """",
          ""characterOffsetBegin"": 22,
          ""characterOffsetEnd"": 23
        },
        {
          ""index"": 5,
          ""word"": ""‰∫§Êó•"",
          ""originalText"": """",
          ""characterOffsetBegin"": 23,
          ""characterOffsetEnd"": 25
        },
        {
          ""index"": 6,
          ""word"": ""ËÄÉ"",
          ""originalText"": """",
          ""characterOffsetBegin"": 25,
          ""characterOffsetEnd"": 26
        },
        {
          ""index"": 7,
          ""word"": ""ÂΩìÂÇô"",
          ""originalText"": """",
          ""characterOffsetBegin"": 26,
          ""characterOffsetEnd"": 28
        },
        {
          ""index"": 8,
          ""word"": ""„ÄÇ"",
          ""originalText"": """",
          ""characterOffsetBegin"": 28,
          ""characterOffsetEnd"": 29
        }
      ]
    }
  ]
}
```"
321,https://github.com/stanfordnlp/CoreNLP/issues/410,410,[],closed,2017-04-19 19:24:59+00:00,,CoreNLP Dedicated Server,"I am a part of a team participating in the development of an application that uses CoreNLP and I'm responsible for configuring a dedicated server but... I don't know how the software works. So, after following the instructions on their site (https://stanfordnlp.github.io/CoreNLP/corenlp-server.html#dedicated-server), I'm not confident I have completed the task with respect to what my project manager is looking for. He wants to bring the data from the Stanford CoreNLP server down to our local server to eliminate the need to leave our local network to analyze ingested data."
322,https://github.com/stanfordnlp/CoreNLP/issues/411,411,[],closed,2017-04-20 02:36:24+00:00,,Online demo result differ,How should I config to get the exact result in http://corenlp.run/ ?
323,https://github.com/stanfordnlp/CoreNLP/issues/412,412,[],closed,2017-04-21 15:31:15+00:00,,Subjectivity detection API?,"Does coreNLP comes with subjectivity detection / genre classification? It seems the sentiment analysis tool cannot do this?

Thank you for your work!"
324,https://github.com/stanfordnlp/CoreNLP/issues/413,413,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2017-04-21 19:51:39+00:00,,originalText is not reliable,"I  send ""2 1/2"" string to it (I explicitly write the space as \x20) and in output I get a no-break space (utf8=c2 a0)  between ""2"" and  ""1/2"" under `originalText`:

```bash
$ echo -e '2\x201/2' | java -cp '*' edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit -outputFormat json 2>/dev/null | grep originalText | xxd
0000000: 2020 2020 2020 2020 2020 226f 7269 6769            ""origi
0000010: 6e61 6c54 6578 7422 3a20 2232 c2a0 312f  nalText"": ""2..1/
0000020: 3222 2c0a                                2"",.
```

"
325,https://github.com/stanfordnlp/CoreNLP/issues/414,414,[],closed,2017-04-21 21:59:37+00:00,,NER classifier results depend on previous requests,"I've noticed I've got the same token in the same sentence classified sometimes as PERSON and sometimes as ORGANIZATION. I've narrowed down the ""sometimes"" case to a very simple example.

Reproducing
--
**Step 1.**
Try NER with sentence
> Now I want off the Trump train before it crashes into the red button at 1,488 miles per hour.

You'll get Trump=PERSON

**Step 2.**
Try NER with two sentences in series:
> i bet he is a trump supporter

> Now I want off the Trump train before it crashes into the red button at 1,488 miles per hour.

You'll get Trump=ORGANIZATION (for the second sentence).

Sample code in bash
--
```bash
#!/bin/bash

corenlp_dir=/usr/local/share/corenlp/stanford-corenlp-full-2016-10-31

export CLASSPATH=""${corenlp_dir}/*:${CLASSPATH}""

function corenlp(){
	/usr/bin/java \
		edu.stanford.nlp.pipeline.StanfordCoreNLP \
		-annotators tokenize,ssplit,pos,lemma,ner \
		-outputFormat json
}

sample=""i bet he is a trump supporter
Now I want off the Trump train before it crashes into the red button at 1,488 miles per hour.""

# NER for both sentences in series
result2=$(echo ""$sample"" | corenlp)
# NER for the second sentence only
result1=$(echo ""$sample"" | sed 1d | corenlp)

diff -U 5 <(echo ""$result1"") <(echo ""$result2"")
```

Diff output (**scroll down to the very bottom**)
--
```diff
--- /dev/fd/63	2017-04-22 00:55:48.002305659 +0300
+++ /dev/fd/62	2017-04-22 00:55:48.002305659 +0300
@@ -3,10 +3,103 @@
     {
       ""index"": 0,
       ""tokens"": [
         {
           ""index"": 1,
+          ""word"": ""i"",
+          ""originalText"": ""i"",
+          ""lemma"": ""i"",
+          ""characterOffsetBegin"": 0,
+          ""characterOffsetEnd"": 1,
+          ""pos"": ""LS"",
+          ""ner"": ""O"",
+          ""before"": """",
+          ""after"": "" ""
+        },
+        {
+          ""index"": 2,
+          ""word"": ""bet"",
+          ""originalText"": ""bet"",
+          ""lemma"": ""bet"",
+          ""characterOffsetBegin"": 2,
+          ""characterOffsetEnd"": 5,
+          ""pos"": ""NN"",
+          ""ner"": ""O"",
+          ""before"": "" "",
+          ""after"": "" ""
+        },
+        {
+          ""index"": 3,
+          ""word"": ""he"",
+          ""originalText"": ""he"",
+          ""lemma"": ""he"",
+          ""characterOffsetBegin"": 6,
+          ""characterOffsetEnd"": 8,
+          ""pos"": ""PRP"",
+          ""ner"": ""O"",
+          ""before"": "" "",
+          ""after"": "" ""
+        },
+        {
+          ""index"": 4,
+          ""word"": ""is"",
+          ""originalText"": ""is"",
+          ""lemma"": ""be"",
+          ""characterOffsetBegin"": 9,
+          ""characterOffsetEnd"": 11,
+          ""pos"": ""VBZ"",
+          ""ner"": ""O"",
+          ""before"": "" "",
+          ""after"": "" ""
+        },
+        {
+          ""index"": 5,
+          ""word"": ""a"",
+          ""originalText"": ""a"",
+          ""lemma"": ""a"",
+          ""characterOffsetBegin"": 12,
+          ""characterOffsetEnd"": 13,
+          ""pos"": ""DT"",
+          ""ner"": ""O"",
+          ""before"": "" "",
+          ""after"": "" ""
+        },
+        {
+          ""index"": 6,
+          ""word"": ""trump"",
+          ""originalText"": ""trump"",
+          ""lemma"": ""trump"",
+          ""characterOffsetBegin"": 14,
+          ""characterOffsetEnd"": 19,
+          ""pos"": ""NN"",
+          ""ner"": ""O"",
+          ""before"": "" "",
+          ""after"": "" ""
+        },
+        {
+          ""index"": 7,
+          ""word"": ""supporter"",
+          ""originalText"": ""supporter"",
+          ""lemma"": ""supporter"",
+          ""characterOffsetBegin"": 20,
+          ""characterOffsetEnd"": 29,
+          ""pos"": ""NN"",
+          ""ner"": ""O"",
+          ""before"": "" "",
+          ""after"": """"
+        }
+      ]
+    }
+  ]
+}
+{
+  ""sentences"": [
+    {
+      ""index"": 0,
+      ""tokens"": [
+        {
+          ""index"": 1,
           ""word"": ""Now"",
           ""originalText"": ""Now"",
           ""lemma"": ""now"",
           ""characterOffsetBegin"": 0,
           ""characterOffsetEnd"": 3,
@@ -75,11 +168,11 @@
           ""originalText"": ""Trump"",
           ""lemma"": ""Trump"",
           ""characterOffsetBegin"": 19,
           ""characterOffsetEnd"": 24,
           ""pos"": ""NNP"",
-          ""ner"": ""PERSON"",
+          ""ner"": ""ORGANIZATION"",
           ""before"": "" "",
           ""after"": "" ""
         },
         {
           ""index"": 7,
```"
326,https://github.com/stanfordnlp/CoreNLP/issues/415,415,[],closed,2017-04-22 21:03:49+00:00,,dependency parsing line by line,"when i use following command to parse a text file which contains 10 sentences.

java -Xmx2g -cp ""*"" edu.stanford.nlp.parser.nndep.DependencyParser -model edu/stanford/nlp/models/parser/nndep/CTB_CoNLL_params.txt.gz -tagger.model edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger -textFile input.txt -outFile input.txt.out

but one sentence in the text file have no period punctuation, i found if one sentence do not end with period punctuation, the parser will not treat it as one sentence. i do not know why.

the old version of parser can specify the delimiter by using -sentences newline parameter, how do i specify the delimiter in the new version of parser ? 

thank you very much !


"
327,https://github.com/stanfordnlp/CoreNLP/issues/416,416,[],closed,2017-04-24 09:52:45+00:00,,CoreNLP server no response,"i did as http://stanfordnlp.github.io/CoreNLP/corenlp-server.html
start the server
# Run the server using all jars in the current directory (e.g., the CoreNLP home directory)
java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000

and write java api to call it

// creates a StanfordCoreNLP object with POS tagging, lemmatization, NER, parsing, and coreference resolution
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
StanfordCoreNLPClient pipeline = new StanfordCoreNLPClient(props, ""localhost"", 9000, 2);
// read some text in the text variable
String text = ... // Add your text here!
// create an empty Annotation just with the given text
Annotation document = new Annotation(text);
// run all Annotators on this text
pipeline.annotate(document);
.........................................
	              // display the token-level annotations
	              String[] tokenAnnotations = {
	                      ""Text"", ""PartOfSpeech"", ""Lemma"", ""Answer"", ""NamedEntityTag"",
	                      ""CharacterOffsetBegin"", ""CharacterOffsetEnd"", ""NormalizedNamedEntityTag"",
	                      ""Timex"", ""TrueCase"", ""TrueCaseText"", ""SentimentClass"", ""WikipediaEntity"" };
	              for (CoreLabel token: tokens) {
	            	  out.print(token.word()+"",""+token.tag()+"",""+token.ner());
	            	  out.println();
	            	  out.print(token.toShorterString(tokenAnnotations));
	            	  out.println();
	              }


but no response at all,  
but when i shutdown server, it will give some output at that time


"
328,https://github.com/stanfordnlp/CoreNLP/issues/417,417,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2017-04-25 14:10:54+00:00,,Tokenizer error in spanish model,"Hi, I'm using the Stanford NLP framework in order to recognize named entities (dates, initial dates and final dates) with the spanish models.
I'm using the following configuration for the pipeline:
```
annotators = tokenize, ssplit, pos, lemma, ner, regexner, entitymentions
tokenize.language = es
pos.model = edu/stanford/nlp/models/pos-tagger/spanish/spanish-distsim.tagger
ner.model = edu/stanford/nlp/models/ner/spanish.ancora.distsim.s512.crf.ser.gz
ner.applyNumericClassifiers = false
ner.useSUTime = false
parse.model = edu/stanford/nlp/models/lexparser/spanishPCFG.ser.gz
depparse.model = edu/stanford/nlp/models/parser/nndep/UD_Spanish.gz
depparse.language = spanish
regexner.ignorecase = true
```
I include the regexner.mapping property in java code. The mapping file has, in summary, the following entries.
```
[0-9]{1,2}[-\/][0-9]{1,2}[-\/][0-9]{2,4}	DATE
[0-9]{1,2} [0-9]{1,2} [0-9]{2,4}	DATE
de el [0-9]{1,2}[-\/][0-9]{1,2}[-\/][0-9]{2,4}	INITIAL_DATE
de el [0-9]{1,2} [0-9]{1,2} [0-9]{2,4}	INITIAL_DATE
a el [0-9]{1,2}[-\/][0-9]{1,2}[-\/][0-9]{2,4}	FINAL_DATE
a el [0-9]{1,2} [0-9]{1,2} [0-9]{2,4}	FINAL_DATE
```
The words ""de el"" or ""a el"" are not correct in Spanish syntactically, but I've a normalization method that converts ""del"" to ""de el"" and ""al"" to ""a el"". The process works with the following examples:
```
12 12 2004 => DATE
12-12-2004 => DATE
12/12/2004 => DATE

del 12 12 2004 => NORMALIZATION => de el 12 12 2004 => INITIAL_DATE
del 12-12-2004 => NORMALIZATION => de el 12-12-2004 => INITIAL_DATE
del 12/12/2004 => NORMALIZATION => de el 12/12/2004 => INITIAL_DATE

al 12 12 2004 => NORMALIZATION => a el 12 12 2004 => FINAL_DATE
al 12-12-2004 => NORMALIZATION => a el 12-12-2004 => FINAL_DATE
al 12/12/2004 => NORMALIZATION => a el 12/12/2004 => FINAL_DATE
```
Even If the input is ""del 12-12-2005 al 12-12-2016"", both entities are recognized (""de el 12-12-2005"" as INITIAL_DATE and ""a el 12-12-2016"" as FINAL_DATE).

The problem appear with the input ""de el 12 12 2005 a el 12-12-2016"". In this case only 12-12-2016 is recognized as DATE. I've debugged the application and I've found a problem in the function ""annotate"" of ""TokensRegexNERAnnotator"" class. In this function, the tokens are obtained. If you see the tokens, which are a list of CoreLabel, you will see the following:
`[de-1, el-2, 12-3, 12-4, 2005 a-5, el-6, 12-12-2016-7]`
It seems that ""2005 a"" is recognized as an unique token. This provokes a bad segmentation and because of this the entities are not recognized as INITIAL_DATE and FINAL_DATE.

I'm using the Stanford CoreNLP version 3.7.0. I also have tried the example ""de el 12 12 2005 a el 12-12-2016"" in http://nlp.stanford.edu:8080/parser/index.jsp and it has the same behaviour (""2005 a"" recognize as a token). On the other hand, the input ""del 12 12 2005 al 12-12-2016"" returns the correct tokenization.

I'm wondering if this is a bug or if I have any mistake. Maybe I'd have to omit my own normalization?

Thanks in advance,
Fran."
329,https://github.com/stanfordnlp/CoreNLP/issues/418,418,[],closed,2017-04-25 20:46:50+00:00,,Coreference: the right usage? ,"I am using [your latest system (version 3.7.0)](https://github.com/CogComp/cogcomp-nlp/blob/35dea894ea9b02d1158dafec53c941c4e40b7547/external/pom.xml#L40-L50) and [here is how I am making calls to your co-reference system](https://github.com/CogComp/cogcomp-nlp/blob/35dea894ea9b02d1158dafec53c941c4e40b7547/external/src/main/java/edu/illinois/cs/cogcomp/pipeline/handlers/StanfordCorefHandler.java#L54-L59). 

In practice when I try a document in your demo here is what I see: 
<img width=""1226"" alt=""screen shot 2017-04-25 at 3 45 00 pm"" src=""https://cloud.githubusercontent.com/assets/2441454/25406969/30b69e26-29ce-11e7-91d3-2696743c8f3d.png"">


While my code generates this: 
<img width=""1142"" alt=""screen shot 2017-04-25 at 3 44 50 pm"" src=""https://cloud.githubusercontent.com/assets/2441454/25406974/3539aa88-29ce-11e7-9840-15ee0a346e93.png"">


Any thoughts where I am going wrong?  

Here is my text btw: 
```
(LOS ANGELES, May 10, 1989)-The circus has come to town! The circus members arrived last night in trucks and buses. Today, they will set up their acts and get ready to perform. Everyone can enjoy the circus. The lion tamer sticks his head inside a lion's mouth. Tigers stand on their hind legs for their masters. Elephants dance around the stage. There's plenty to see in the air, too. Men and women fly through the air on a swing called a trapeze. At one time, the circus took place in a huge tent. The show was called ""under the big top."" The best-known circus was started by a man named P.T. Barnum. His circus is about 100 years old. It is called ""the greatest show on Earth."" In 1938, not many people had money to go to the circus. So most of them closed. But the children and their parents loved Barnum's circus. They saved it from closing. 
```

FYI @snigdhac @shyamupa
"
330,https://github.com/stanfordnlp/CoreNLP/issues/419,419,[],closed,2017-04-26 03:27:01+00:00,,bug: ParseAndSetLabels,"line 262, it should be:

    if (sentencesFile != null) {  // now it is == null
      sentences = readSentences(sentencesFile);
    } else {
      sentences = new ArrayList<String>(labelMap.keySet());
    }
"
331,https://github.com/stanfordnlp/CoreNLP/issues/420,420,[],closed,2017-04-26 04:30:14+00:00,,"Speed of coreNLP very slow, any tuning advice?","for this file, CoreNLP-chinese.properties
i only used 3 annotators 
annotators = tokenize, ssplit, pos
#, lemma, ner, parse, mention, coref

but speed is (pipeline.timingInformation())
TOTAL: 11.3 sec. for 32105 tokens at 2830.6 tokens/sec.
TOTAL: 0.3 sec. for 23 tokens at 68.7 tokens/sec.
TOTAL: 17.6 sec. for 69637 tokens at 3951.5 tokens/sec.
TOTAL: 18.1 sec. for 73458 tokens at 4049.3 tokens/sec.

as some other tools like mmseg4j  aclaiming 1200 KB/s to 1900 KB/s
does corenlp have a performance bottleneck or need some other tuning work?

"
332,https://github.com/stanfordnlp/CoreNLP/issues/421,421,[],closed,2017-04-26 14:25:22+00:00,,local class incompatible with serialVersionUID,"Hi,
I am trying to run the CoreNLP DependencyParserDemo program and got the following error,

Error:

Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:917)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:815)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:789)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.<init>(MaxentTagger.java:312)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.<init>(MaxentTagger.java:265)
	at edu.stanford.nlp.parser.nndep.demo.DependencyParserDemo.main(DependencyParserDemo.java:46)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.io.InvalidClassException: edu.stanford.nlp.tagger.maxent.ExtractorDistsim; local class incompatible: stream classdesc serialVersionUID = 1, local class serialVersionUID = 2
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:616)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1829)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1919)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2231)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2155)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2013)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.readExtractors(MaxentTagger.java:620)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:866)
	... 10 more

Process finished with exit code 1

pom.xml:

<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
  <modelVersion>4.0.0</modelVersion>
  <groupId>edu.stanford.nlp</groupId>
  <artifactId>stanford-corenlp</artifactId>
  <version>3.7.0</version>
  <packaging>jar</packaging>
  <name>Stanford CoreNLP</name>
  <description>Stanford CoreNLP provides a set of natural language analysis tools which can take raw English language text input and give the base forms of words, their parts of speech, whether they are names of companies, people, etc., normalize dates, times, and numeric quantities, mark up the structure of sentences in terms of phrases and word dependencies, and indicate which noun phrases refer to the same entities. It provides the foundational building blocks for higher level text understanding applications.</description>
  <url>http://nlp.stanford.edu/software/corenlp.shtml</url>
  <licenses>
    <license>
      <name>GNU General Public License Version 3</name>
      <url>http://www.gnu.org/licenses/gpl-3.0.txt</url>
    </license>
  </licenses>
  <scm>
    <url>http://nlp.stanford.edu/software/stanford-corenlp-2016-10-31.zip</url>
    <connection>http://nlp.stanford.edu/software/stanford-corenlp-2016-10-31.zip</connection>
  </scm>
  <developers>
    <developer>
      <id>christopher.manning</id>
      <name>Christopher Manning</name>
      <email>manning@stanford.edu</email>
    </developer>
    <developer>
      <id>jason.bolton</id>
      <name>Jason Bolton</name>
      <email>jebolton@stanford.edu</email>
    </developer>
  </developers>
  <properties>
    <maven.compiler.source>1.8</maven.compiler.source>
    <maven.compiler.target>1.8</maven.compiler.target>
    <encoding>UTF-8</encoding>
  </properties>
  <dependencies>

    <dependency>
      <groupId>com.apple</groupId>
      <artifactId>AppleJavaExtensions</artifactId>
      <version>1.4</version>
    </dependency>

    <dependency>
      <groupId>de.jollyday</groupId>
      <artifactId>jollyday</artifactId>
      <version>0.4.9</version>
    </dependency>

    <dependency>
      <groupId>org.apache.commons</groupId>
      <artifactId>commons-lang3</artifactId>
      <version>3.3.1</version>
    </dependency>

    <dependency>
      <groupId>org.apache.lucene</groupId>
      <artifactId>lucene-queryparser</artifactId>
      <version>4.10.3</version>
    </dependency>

    <dependency>
      <groupId>org.apache.lucene</groupId>
      <artifactId>lucene-analyzers-common</artifactId>
      <version>4.10.3</version>
    </dependency>

    <dependency>
      <groupId>org.apache.lucene</groupId>
      <artifactId>lucene-queries</artifactId>
      <version>4.10.3</version>
    </dependency>

    <dependency>
      <groupId>org.apache.lucene</groupId>
      <artifactId>lucene-core</artifactId>
      <version>4.10.3</version>
    </dependency>

    <dependency>
      <groupId>javax.servlet</groupId>
      <artifactId>javax.servlet-api</artifactId>
      <version>3.0.1</version>
    </dependency>

    <dependency>
      <groupId>com.io7m.xom</groupId>
      <artifactId>xom</artifactId>
      <version>1.2.10</version>
    </dependency>

    <dependency>
      <groupId>joda-time</groupId>
      <artifactId>joda-time</artifactId>
      <version>2.9</version>
    </dependency>

    <dependency>
      <groupId>com.googlecode.efficient-java-matrix-library</groupId>
      <artifactId>ejml</artifactId>
      <version>0.23</version>
    </dependency>

    <dependency>
      <groupId>org.glassfish</groupId>
      <artifactId>javax.json</artifactId>
      <version>1.0.4</version>
    </dependency>

    <dependency>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-api</artifactId>
      <version>1.7.12</version>
    </dependency>

    <dependency>
      <groupId>com.google.protobuf</groupId>
      <artifactId>protobuf-java</artifactId>
      <version>2.6.1</version>
    </dependency>

    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>4.12</version>
      <scope>test</scope>
    </dependency>

    <dependency>
      <groupId>com.pholser</groupId>
      <artifactId>junit-quickcheck-core</artifactId>
      <version>0.5</version>
      <scope>test</scope>
    </dependency>
    
    <dependency>
      <groupId>com.pholser</groupId>
      <artifactId>junit-quickcheck-generators</artifactId>
      <version>0.5</version>
      <scope>test</scope>
    </dependency>
      <dependency>
          <groupId>edu.stanford.nlp</groupId>
          <artifactId>stanford-corenlp</artifactId>
          <version>3.6.0</version>
      </dependency>
      <dependency>
          <groupId>edu.stanford.nlp</groupId>
          <artifactId>stanford-corenlp</artifactId>
          <version>3.6.0</version>
          <classifier>models</classifier> <!--  will get the dependent model jars -->
      </dependency>

      <dependency>
        <groupId>edu.stanford.nlp</groupId>
        <artifactId>stanford-corenlp</artifactId>
        <version>3.1.4</version>
      </dependency>
      <dependency>
        <groupId>edu.stanford.nlp</groupId>
        <artifactId>stanford-corenlp</artifactId>
        <version>3.1.4</version>
        <classifier>models</classifier>
      </dependency>

          <dependency>
              <groupId> org.apache.cassandra</groupId>
              <artifactId>cassandra-all</artifactId>
              <version>0.8.1</version>

              <exclusions>
                  <exclusion>
                      <groupId>org.slf4j</groupId>
                      <artifactId>slf4j-log4j12</artifactId>
                  </exclusion>
                  <exclusion>
                      <groupId>log4j</groupId>
                      <artifactId>log4j</artifactId>
                  </exclusion>
              </exclusions>

          </dependency>


  </dependencies>
  <build>
    <sourceDirectory>src</sourceDirectory>
    <testSourceDirectory>test/src</testSourceDirectory>
    <plugins>
      <plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>build-helper-maven-plugin</artifactId>
        <version>1.7</version>
        <executions>
          <execution>
            <id>attach-models</id>
            <phase>package</phase>
            <goals>
              <goal>attach-artifact</goal>
            </goals>
            <configuration>
              <artifacts>
                <artifact>
                  <file>${project.basedir}/stanford-corenlp-3.7.0-models.jar</file>
                  <type>jar</type>
                  <classifier>models</classifier>
                </artifact>
              </artifacts>
            </configuration>
          </execution>
        </executions>
      </plugin>
    </plugins>
  </build>
</project>

Code:

public class DependencyParserDemo  {

  /** A logger for this class */
  private static Redwood.RedwoodChannels log = Redwood.channels(DependencyParserDemo.class);

  public static void main(String[] args) {
    String modelPath = ""D:/dl4j/CoreNLP-master/src/main/java/edu/stanford/nlp/models/parser/nndep/english_UD.gz"";
    String taggerPath = ""D:/dl4j/CoreNLP-master/src/main/java/edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"";

    for (int argIndex = 0; argIndex < args.length; ) {
      switch (args[argIndex]) {
        case ""-tagger"":
          taggerPath = args[argIndex + 1];
          argIndex += 2;
          break;
        case ""-model"":
          modelPath = args[argIndex + 1];
          argIndex += 2;
          break;
        default:
          throw new RuntimeException(""Unknown argument "" + args[argIndex]);
      }
    }

    String text = ""I can almost always tell when movies use fake dinosaurs."";

    MaxentTagger tagger = new MaxentTagger(taggerPath);
    DependencyParser parser = DependencyParser.loadFromModelFile(modelPath);

    DocumentPreprocessor tokenizer = new DocumentPreprocessor(new StringReader(text));
    for (List<HasWord> sentence : tokenizer) {
      List<TaggedWord> tagged = tagger.tagSentence(sentence);
      GrammaticalStructure gs = parser.predict(tagged);

      // Print typed dependencies
      log.info(gs);
    }
  }

}

Please let me know, how to resolve this.

Thanks in advance.!!"
333,https://github.com/stanfordnlp/CoreNLP/issues/422,422,[],closed,2017-04-28 06:32:35+00:00,,RelationExtractorAnnotator.java,"please explain the output of the RelationExtractorAnnotator.java 

what are the entities and their relations"
334,https://github.com/stanfordnlp/CoreNLP/issues/424,424,[],closed,2017-04-29 16:04:40+00:00,,java.lang.ClassNotFoundException: edu.stanford.nlp.parser.lexparser.ArabicUnknownWordModel error,"How can i fix ClassNotFoundException problem while reading arabicFactored.ser for loading model into LexicalizedParser?

please, help me.

thanks in advance."
335,https://github.com/stanfordnlp/CoreNLP/issues/425,425,"[{'id': 45387507, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNw==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/cantreproduce', 'name': 'cantreproduce', 'color': 'dddddd', 'default': False, 'description': None}]",closed,2017-04-30 15:09:01+00:00,,there are chinese label when i parsed chinese sentence by using standford dependency parser,"hello, everyone, i found a strange thing, when i parsed chinese sentence by using standford denpendency parser, there are some (not very many) dependency labels which is chinese characters and there are duplicate dependency edgesÔºåi selected some examples from the parsing result as following:

`
ÂÖ≥‰∫é(ËßÑÂàí-28, Âè∏Ê≥ïÈÉ®-14)
`
`
ÂÖ≥‰∫é(ËßÑÂàí-62, Âè∏Ê≥ïÈÉ®-48)
`

is this normal ?
i do not know why ? can some one explain this for me ? any reply will be appreciated."
336,https://github.com/stanfordnlp/CoreNLP/issues/426,426,[],closed,2017-05-03 14:17:20+00:00,,lexical parser grammar,"Hi,
I ran the parserDemo program from CoreNLP and i get the output in pennTree 
(ROOT
  (S
    (NP (DT This))
    (VP (VBZ is)
      (NP (DT an) (JJ easy) (NN sentence)))
    (. .)))
Is there a way to get in grammar format something like this 
S -> NP VP
  NP -> Det N
  PP -> P NP
  VP -> 'slept' | 'saw' NP | 'walked' PP
  Det -> 'the' | 'a'
  N -> 'man' | 'park' | 'dog'
  P -> 'in' | 'with'.

Thanks in Advance."
337,https://github.com/stanfordnlp/CoreNLP/issues/427,427,[],closed,2017-05-03 17:33:56+00:00,,/semgrex in server mode ignore properties,"Hello,
I am trying to do semgrex query in French language, Enhanced Dependency ++ works fine with french, so I guess using semgrex should work OK.
It looks like query made to /semgrex are ignoring properties set on the request.
For example, I still get 
`[pool-2-thread-1] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ...`
in console, even if I specify `pipelineLanguage=fr` in the query.
I guess the culprit is line 936 of [StanfordCoreNLPServer.java](https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLPServer.java) but have no clue how to correct it."
338,https://github.com/stanfordnlp/CoreNLP/issues/428,428,[],closed,2017-05-09 13:41:09+00:00,,How to manage a label with more than one type in regexner,"Hi, I'm using tokensregexner annotator with a mapping file and I'm wondering which is the proper way to insert a label which has several types.

For example, New York is a CITY, but it is a LOCATION too.

Until now, I was inserting the following entry (I'm overriding the default ner annotations):
`New York\tCITY,LOCATION`
By this way only one type consisting in the string ""CITY,LOCATION"" is associated to ""New York"". Then, when I'm retrieving the results of the annotator, I make a split to get the different types from the string.

Another way to do this is by adding two entries:
```
New York\tCITY
New York\tLOCATION
```
But in this way, the annotator only gives one type when New York is found in a text.
Nowadays I'm working on a custom annotator based on tokensregexner, whose entries are read from a lucene index. Internally, TokensRegexNERAnnotator class works with an internal static class called Entry:
```
private static class Entry {
    public final String tokensRegex;
    public final String[] regex; // the regex, tokenized by splitting on white space
    public final String[] types; // the associated types
    public final Set<String> overwritableTypes; // what types can be overwritten by this entry
    public final double priority;
    public final double weight;
    public final int annotateGroup;

    public Entry(String tokensRegex, String[] regex, String[] types, Set<String> overwritableTypes, double priority, double weight, int annotateGroup) {
      this.tokensRegex = tokensRegex;
      this.regex = regex;
      this.types = new String[types.length];
      for (int i = 0; i < types.length; i++) {
        // TODO: for some types, it doesn't make sense to be interning...
        this.types[i] = types[i].intern();
      }
      this.overwritableTypes = overwritableTypes;
      this.priority = priority;
      this.weight = weight;
      this.annotateGroup = annotateGroup;
    }

    public String getTypeDescription() {
      return ""["" + StringUtils.join(types, "","") + ""]"";
    }

    public String toString() {
      return ""Entry{"" + ((tokensRegex != null) ? tokensRegex: StringUtils.join(regex)) + ' '
          + StringUtils.join(types) + ' ' + overwritableTypes + ' ' + priority + '}';
    }
  }
```
This class has the property types, which is an array that stores the list of types of the entity. When I initialize the annotator filling this array with the different types of the entry, it only returns a single type.

I've seen the method annotateMatched from the TokensRegexNERAnnotator class, whose function is to associate a token with its types:
```
private void annotateMatched(List<CoreLabel> tokens) {
    List<SequenceMatchResult<CoreMap>> matched = multiPatternMatcher.findNonOverlapping(tokens);
    for (SequenceMatchResult<CoreMap> m:matched) {
      Entry entry = patternToEntry.get(m.pattern());

      // Check if we will overwrite the existing annotation with this annotation
      int g = entry.annotateGroup;
      int start = m.start(g);
      int end = m.end(g);

      String str = m.group(g);
      if (commonWords.contains(str)) {
        if (verbose) {
          log.info(""Not annotating (common word) '"" + str + ""': "" +
              StringUtils.joinFields(m.groupNodes(g), CoreAnnotations.NamedEntityTagAnnotation.class)
              + "" with "" + entry.getTypeDescription() + "", sentence is '"" + StringUtils.joinWords(tokens, "" "") + ""'"");
        }
        continue;
      }

      boolean overwriteOriginalNer = checkPosTags(tokens, start, end);
      if (overwriteOriginalNer) {
        overwriteOriginalNer = checkOrigNerTags(entry, tokens, start, end);
      }
      if (overwriteOriginalNer) {
        for (int i = start; i < end; i++) {
          CoreLabel token = tokens.get(i);
          for (int j = 0; j < annotationFields.size(); j++) {
            token.set(annotationFields.get(j), entry.types[j]);
          }
         // tokens.get(i).set(CoreAnnotations.NamedEntityTagAnnotation.class, entry.type);
        }
      } else {
        if (verbose) {
          log.info(""Not annotating  '"" + m.group(g) + ""': "" +
                  StringUtils.joinFields(m.groupNodes(g), CoreAnnotations.NamedEntityTagAnnotation.class)
                  + "" with "" + entry.getTypeDescription() + "", sentence is '"" + StringUtils.joinWords(tokens, "" "") + ""'"");
        }
      }
    }
  }
```
The only part where the types are used is in
```
if (overwriteOriginalNer) {
        for (int i = start; i < end; i++) {
          CoreLabel token = tokens.get(i);
          for (int j = 0; j < annotationFields.size(); j++) {
            token.set(annotationFields.get(j), entry.types[j]);
          }
         // tokens.get(i).set(CoreAnnotations.NamedEntityTagAnnotation.class, entry.type);
        }
      }
```
It is not iterating over all types, it is assigning the type j to the token, where j goes from 0 to annotationFields.size(). In my case, annotationFields only contains one element, relative to ""ner"" annotation. For this reason only one type is read.

So, my question is what is the way to deal with entries which have several types. How to insert them in the mapping file and how to retrieve all types of an entity when it is located in a text.

Thanks in advance."
339,https://github.com/stanfordnlp/CoreNLP/issues/429,429,[],closed,2017-05-10 07:40:12+00:00,,Sentence Generator from CFG,"Hi,

Is there any Java program which generates meaningful sentences from the CFG ??
Kindly share the link of the program.

Thanks in Advance."
340,https://github.com/stanfordnlp/CoreNLP/issues/432,432,[],closed,2017-05-14 23:47:52+00:00,,Do Stanford TokensRegex support other language like Chinese?,"As title, I can't find any docs on other language in 
https://nlp.stanford.edu/software/tokensregex.shtml
"
341,https://github.com/stanfordnlp/CoreNLP/issues/433,433,[],closed,2017-05-15 15:32:35+00:00,,Option to suppress logging,"When run, the server prints a lot of logging messages to stderr; the most pesky is logging every single API call. It would be nice having an option that turned logging off. Even more useful would be an option to specify the logging level, so that if one is interested in startup and shutdown messages, but not in the individual API calls, could have their cake and eat it.

Of course, this latter use-case is not relevant until all log messages are on the INFO level..."
342,https://github.com/stanfordnlp/CoreNLP/issues/434,434,[],closed,2017-05-17 01:26:05+00:00,,"""Transfer learning"" on CFClassifier","Is it possible to do 'transfer learning' on the pre trained CRFClassifier?. I want to show additional examples to the classifier and that it fit the parameters. There is a method called train on CRFClassifier.java, but it is not clear to me if it start again or if it uses the previous weights.
"
343,https://github.com/stanfordnlp/CoreNLP/issues/435,435,"[{'id': 77134078, 'node_id': 'MDU6TGFiZWw3NzEzNDA3OA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/documentation', 'name': 'documentation', 'color': '5319e7', 'default': True, 'description': None}, {'id': 609575435, 'node_id': 'MDU6TGFiZWw2MDk1NzU0MzU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/help%20wanted', 'name': 'help wanted', 'color': 'da8000', 'default': True, 'description': None}]",open,2017-05-17 20:59:37+00:00,,CoreNLP on Windows as a Service,"Has anyone tried setting up the CoreNLP package as a service on Windows ?

I setup a .bat file that has the follwing content:

java -mx8g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9005

Next I created a service using the sc command from within Windows.  But it does not seem to work.

_Ben"
344,https://github.com/stanfordnlp/CoreNLP/issues/437,437,[],closed,2017-05-22 21:47:10+00:00,,CRFClassifier classifySentence and classify difference,"I noticed something similar as was [posted](https://mailman.stanford.edu/pipermail/java-nlp-user/2017-May/008098.html) recently on the `java-nlp-user` CoreNLP mailinglist.

Following the [Stanford NER CRF FAQ](https://nlp.stanford.edu/software/crf-faq.html) with the same sample properties, I annotated my corpus and created my own classifier.

Using only the CLI, I noticed that the output of the named entities differs, depending on if I use the textFile or testFile arguments:

1. `java -cp stanford-ner.jar:lib/* edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier my-crf-ser.gz -textFile myFile.txt`
or
2. `java -cp stanford-ner.jar:lib/* edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier my-crf-ser.gz -testFile myFile-tokenized.tsv`

Using the `ner-gui.sh` or using the API and `List<IN> classify(List<IN> document)` also gives the entities as in 1. (using -textFile). I noticed that the quality of recognized named entities is worse here.

However, as @manning mentioned [on the mailing list](https://mailman.stanford.edu/pipermail/java-nlp-user/2017-May/008098.html), there is the method `List<IN> classifySentence(List<? extends HasWord> tokenSequence)`. I created some code that just reads my text file, tokenizes it and passes it to the classifier to `classifySentence` (even though it is a full document and not only sentence):

```
        AbstractSequenceClassifier<CoreLabel> classifier = CRFClassifier.getClassifier(""my-crf-ser.gz"");
        final String inputFile = ""myFile.txt""
        Properties props = new Properties();
        props.put(""annotators"", ""tokenize"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        Annotation document = new Annotation(IOUtils.slurpFile(inputFile));
        pipeline.annotate(document);        
        List<CoreLabel> tokens = document.get(TokensAnnotation.class);
        List<CoreLabel> classifiedTokens = classifier.classifySentence(tokens);
        for (CoreLabel token : classifiedTokens) {
            System.out.println(token.word() + '/' + token.get(CoreAnnotations.AnswerAnnotation.class) + ' ');
        }
```

Here the named entities match the ones found with `-testFile myFile-tokenized.tsv`, and are, from the look of it, of better quality than when using `classify`.

Is my code above a proper way to use a trained classifier through the API to obtain named entities from a document?"
345,https://github.com/stanfordnlp/CoreNLP/issues/438,438,[],closed,2017-05-23 10:51:17+00:00,,Fetch Original value from NormalizedNamedEntity value,"Example Sentence: **At December 31, 2015 employees in bulk hired.**
In the above sentence we are trying to extract the original date. This date is split into array[4] all of them contains the NormalizedNamedEntity value like 
[0] December
_20151231_
[1] 31
_20151231_
[2] ,
_20151231_
[3] 2015
_20151231_

But we want to extract the original date in the form **December 31, 2015**, is there at any stage **original date can be fetched a single token and not in 4 different tokens**. 
Thank you"
346,https://github.com/stanfordnlp/CoreNLP/issues/439,439,[],closed,2017-05-23 16:01:02+00:00,,Stanford relation extraction program,"for the program given in https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/RelationExtractorAnnotator.java

public Set<Class<? extends CoreAnnotation>> requires()  and
public Set<Class<? extends CoreAnnotation>> requirementsSatisfied() this 2 statements are giving same error when tried for execution
error is-
return type Set<Class<? extends CoreAnnotation>> is not compatible with Set <Requirement>"
347,https://github.com/stanfordnlp/CoreNLP/issues/442,442,[],open,2017-05-25 07:41:42+00:00,,serializeTextClassifier serialization format is not text,"I am trying to serialize the CRFClassifier to a text file and the serialization is running correctly. 

However, the serialization format is not text. Therefore, when I load the classifier it is giving me an error (Exception in loading text classifier => java.lang.RuntimeException: format error).

Thanks,
Hussein"
348,https://github.com/stanfordnlp/CoreNLP/issues/443,443,[],closed,2017-05-25 23:10:36+00:00,,IndexOutOfBoundsException while using stanford naturalli.OpenIE,"I'm using the `naturalli.OpenIE` module from coreNLP. I get an IndexOutOfBoundsException while using combined/simultaneous options `-triple.all_nominals true` and `-format reverb`. Please tell me how can I fix it. This is my line command:

    java -mx2g -cp ""*"" edu.stanford.nlp.naturalli.OpenIE -filelist list.txt -triple.all_nominals true -format reverb > triples.reverb

where the input is either a sentence-by-row file or a list of file names (a sentence by file). This does not occur if I use the mentioned options differently, e.g. `(-triple.all_nominals false, -format reverb)` or `(-triple.all_nominals true, -format ollie)`.

This is a sample input file (a sentence by row, the openIE module crashes with the 3rd row):

    The ability of IHF alone to activate fimB expression show IHF to repress ompC transcription
    Thus , NanR and NagC appear to activate fimB expression independently .
    Thus MarA is able to repress the rob promoter after the preformed open complex clears the promoter .
    However , in vitro experiments show IHF to repress ompC transcription from two of its three promoters .
    These results show that constitutive and inducible expression of the chromosomal MarA was able to upregulate nfnB expression in contrast with the lack of activation mediated by constitutive expression of the chromosomal SoxS and Rob proteins .
    The Rob homologs MarA and SoxS can activate transcription of a broad range of genes in vivo ( reviewed in references 2 and 14 ) , including sodA , fumC , micF , zwf , and inaA , suggesting broadly overlapping activities of these three regulators .
    It thus seems likely that the ability of IHF alone to activate fimB expression is enhanced in the D3 mutant background
    In conclusion , MelR and CRP bind to the melAB promoter co - operatively to adjacent sites in order to activate transcription in a co - dependent manner

This is the output, which I get multiple times in the log.

    [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
    [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
    [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
    Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.1 sec].
    [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
    [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse
    Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... 
    PreComputed 100000, Elapsed Time: 2.424 (s)
    Initializing dependency parser done [6.8 sec].
    [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog
    [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie
    Loading clause searcher from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz...done [0.210 seconds]
    Processing file: /almac/ignacio/rie_gce/karla_tests
    All files have been queued; awaiting termination...
    /almac/ignacio/rie_gce/karla_tests	0	ability	show	IHF	1	2	9	10	10	11	1.000	the ability of IHF alone to activate fimB expression show IHF to repress ompC transcription Thus , NanR and NagC appear to activate fimB expression independently .	DT NN IN NN RB TO VB NN NN VBP NN TO VB NN NN RB , NN CC NN VBP TO VB NN NN RB .	ability	show	ihf
    /almac/ignacio/rie_gce/karla_tests	0	NanR	activate	NagC	17	18	22	23	19	20	1.000	the ability of IHF alone to activate fimB expression show IHF to repress ompC transcription Thus , NanR and NagC appear to activate fimB expression independently .	DT NN IN NN RB TO VB NN NN VBP NN TO VB NN NN RB , NN CC NN VBP TO VB NN NN RB .	nanr	activate	nagc
    /almac/ignacio/rie_gce/karla_tests	0	IHF	repress	ompC transcription	10	11	12	13	13	15	0.620	the ability of IHF alone to activate fimB expression show IHF to repress ompC transcription Thus , NanR and NagC appear to activate fimB expression independently .	DT NN IN NN RB TO VB NN NN VBP NN TO VB NN NN RB , NN CC NN VBP TO VB NN NN RB .	ihf	repress	ompc transcription
    /almac/ignacio/rie_gce/karla_tests	0	NanR	activate independently	NagC	17	18	22	26	19	20	1.000	the ability of IHF alone to activate fimB expression show IHF to repress ompC transcription Thus , NanR and NagC appear to activate fimB expression independently .	DT NN IN NN RB TO VB NN NN VBP NN TO VB NN NN RB , NN CC NN VBP TO VB NN NN RB .	nanr	activate independently	nagc
    /almac/ignacio/rie_gce/karla_tests	0	NanR	activate	fimB expression	17	18	22	23	23	25	1.000	the ability of IHF alone to activate fimB expression show IHF to repress ompC transcription Thus , NanR and NagC appear to activate fimB expression independently .	DT NN IN NN RB TO VB NN NN VBP NN TO VB NN NN RB , NN CC NN VBP TO VB NN NN RB .	nanr	activate	fimb expression
    /almac/ignacio/rie_gce/karla_tests	0	NanR	activate independently	fimB expression	17	18	22	26	23	251.000	the ability of IHF alone to activate fimB expression show IHF to repress ompC transcription Thus , NanR and NagC appear to activate fimB expression independently .	DT NN IN NN RB TO VB NN NN VBP NN TO VB NN NN RB , NN CC NN VBP TO VB NN NN RB .	nanr	activate independently	fimb expression
    java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
    	at java.util.ArrayList.rangeCheck(ArrayList.java:653)
    	at java.util.ArrayList.get(ArrayList.java:429)
    	at edu.stanford.nlp.ie.util.RelationTriple.toReverbString(RelationTriple.java:373)
    	at edu.stanford.nlp.naturalli.OpenIE.tripleToString(OpenIE.java:604)
    	at edu.stanford.nlp.naturalli.OpenIE.processDocument(OpenIE.java:638)
    	at edu.stanford.nlp.naturalli.OpenIE.lambda$main$12(OpenIE.java:760)
    	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    	at java.lang.Thread.run(Thread.java:745)
    DONE processing files. 1 exceptions encountered.

I've used `stanford-corenlp-{3.6.0, 3.7}`

In the case that there is an option to generate marks for the output results that can be a surrogate of origin marks of the `reverb` format please tell me how to do it. That's why I need this format given an input `-filelist` or a `-file` containing a sentence by row. Also other configurations of the openIE (coreNLP) module that process sentences in parallel while keeping track between the input and the output would be useful alternatives.

Thank you very much."
349,https://github.com/stanfordnlp/CoreNLP/issues/444,444,[],closed,2017-05-26 08:50:33+00:00,,regresstion test suite,"I would like to the procedure to run the regression test suite. Upon submitting the fix for one of the issues, integration fails quoting test failure for edu.stanford.nlp.pipeline.QuoteAnnotatorTest...

testMultiParagraphQuoteSingle(edu.stanford.nlp.pipeline.QuoteAnnotatorTest) Time elapsed: 0.008 sec <<< FAILURE!

junit.framework.AssertionFailedError: expected:<1> but was:<2>

at junit.framework.Assert.fail(Assert.java:57)

at junit.framework.Assert.failNotEquals(Assert.java:329)

at junit.framework.Assert.assertEquals(Assert.java:78)

at junit.framework.Assert.assertEquals(Assert.java:234)

at junit.framework.Assert.assertEquals(Assert.java:241)

at junit.framework.TestCase.assertEquals(TestCase.java:409)

at edu.stanford.nlp.pipeline.QuoteAnnotatorTest.assertInnerAnnotationValues(QuoteAnnotatorTest.java:471)

at edu.stanford.nlp.pipeline.QuoteAnnotatorTest.testMultiParagraphQuoteSingle(QuoteAnnotatorTest.java:383)

at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

May I know the procedure to run this test on our custom solution?"
350,https://github.com/stanfordnlp/CoreNLP/issues/447,447,[],closed,2017-05-31 09:43:07+00:00,,R coreNLP package error (foreign language),"Hello,

I have some issue using CoreNLP on R for french language. The following code gave me an error:

```
library(coreNLP)
downloadCoreNLP(type = ""french"")
initCoreNLP(type = ""french"")

[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Searching for resource: StanfordCoreNLP-french.properties
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
Error in rJava::.jnew(""edu.stanford.nlp.pipeline.StanfordCoreNLP"", basename(path)) : 
  java.lang.RuntimeException: edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file)
```

The _StanfordCoreNLP-french.properties_ file is in extdata folder and the file _stanford-french-corenlp-2016-01-14-models_ is in _stanford-corenlp-full-2015-12-09_ folder.

Moreover, there was no problem on adding tokenize and ssplit methods, the problem is for the part of speech model.

Any idea of what is the problem?

Thanks, Laurent.



"
351,https://github.com/stanfordnlp/CoreNLP/issues/448,448,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}, {'id': 706055902, 'node_id': 'MDU6TGFiZWw3MDYwNTU5MDI=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/tokenize', 'name': 'tokenize', 'color': 'c5def5', 'default': False, 'description': None}, {'id': 706065874, 'node_id': 'MDU6TGFiZWw3MDYwNjU4NzQ=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/cleanxml', 'name': 'cleanxml', 'color': 'c5def5', 'default': False, 'description': None}]",open,2017-06-01 08:35:40+00:00,,cleanxml and -tokenize.whitespace true do not work together,"Dear all,

I get an exception when trying to annotate a very simple XML file.
I'd be very grateful to hear about ideas for workarounds since this is currently stopping me from working with CoreNLP on a pre-tokenized dataset.

Here is the command:
` java -cp ""./*:"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,cleanxml,ssplit,pos -tokenize.whitespace true -tokenize.keepeol true -ssplit.eolonly true -outputFormat json -file ~/parse_2017/orga/test_input.txt`
[tests with clean.allowflawedxml true, clean.singlesentencetags true, etc. did not work either]

Here is the output of CoreNLP:

```
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator cleanxml
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.9 sec].

Processing file /home/hpc/sles/sles000h/parse_2017/orga/test_input.txt ... writing to /home/woody/sles/sles000h/stanford-corenlp-full-2016-10-31/test_input.txt.json
Exception in thread ""main"" java.lang.IllegalArgumentException: Got a close tag s which does not match any open tag
        at edu.stanford.nlp.pipeline.CleanXmlAnnotator.process(CleanXmlAnnotator.java:624)
        at edu.stanford.nlp.pipeline.CleanXmlAnnotator.annotate(CleanXmlAnnotator.java:244)
        at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:605)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:615)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1164)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:945)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1253)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1323)
```

Here is the content of test_input.txt:

```
<corpus> 
<s id=""1""> The cat sat on the mat . She knows how to write papers . </s> 
<s id=""2""> When will she ever learn? </s> 
</corpus> 

```
[I am aware this is no sensible input. I'm just using it to test that CoreNLP really does no tokenization and sentence-splitting by itself.]

Best,
Peter

Edit: This is the current (3.7.0) release version of CoreNLP."
352,https://github.com/stanfordnlp/CoreNLP/issues/449,449,[],closed,2017-06-02 05:10:22+00:00,,[help-wanted] coref: how to get all Mentions for a document,"I'm wondering how to get all coreference `Mention`s for a document.

I'm using the following annotators: `tokenize, ssplit, pos, lemma, ner, parse, mention, coref`

Using `CorefChainAnnotation` to get `CorefChain.CorefMention`s for an entire document is simple enough, but I'd really like to access `Mention`s, which provide much more information.

Things I've tried:

- `CorefMentionsAnnotation` is great for getting `Mention`s, but only works for sentences, not entire documents. Each `Mention` has a `corefClusterID`, so maybe it's possible to easily get all coref clusters. I wasn't able to figure it out though.
- `CorefChainAnnotation` gets `CorefMention`s. However, I couldn't figure out a way to get `Mention`s from `CorefMentions`s.

Theoretical solutions (if they exist and work):

- `document.get(CorefMentionsAnnotation)`
- `document.get(CorefClusterAnnotation)`

Current solution:

- create a Map<Int, Mention> of Mention ids to Mentions: use `sentence.get(CorefMentionsAnnotation)` on all sentences and store `Mention`s in a map

Any advice would be greatly appreciated!"
353,https://github.com/stanfordnlp/CoreNLP/issues/450,450,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}, {'id': 547907037, 'node_id': 'MDU6TGFiZWw1NDc5MDcwMzc=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/multilingual', 'name': 'multilingual', 'color': 'fef2c0', 'default': False, 'description': None}, {'id': 609575435, 'node_id': 'MDU6TGFiZWw2MDk1NzU0MzU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/help%20wanted', 'name': 'help wanted', 'color': 'da8000', 'default': True, 'description': None}]",closed,2017-06-02 09:25:37+00:00,,Japanese model,"Hi, I am interested to using CoreNLP (and DeepDive) with Japanese.
Are you working on Japanese?
And how can I start building a Japanese model?
How did you build Chinese or English models?

Thanks!"
354,https://github.com/stanfordnlp/CoreNLP/issues/452,452,[],closed,2017-06-05 06:00:19+00:00,,CoreNLPServer REST API for Non-English model,"When using the `StanfordCoreNLP-chinese.properties` from https://stanfordnlp.github.io/CoreNLP/human-languages.html with 

```
java -mx4g \
-cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLP  \
-props StanfordCoreNLP-chinese.properties \
-port 9001 -timeout 15000
```

[out]:

```
~/stanford-corenlp-full-2016-10-31$ java -mx4g \
> -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLP \
> -props StanfordCoreNLP-chinese.properties \
> -port 9001 -timeout 15000

[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.wordseg.ChineseDictionary - Loading Chinese dictionaries from 1 file:
[main] INFO edu.stanford.nlp.wordseg.ChineseDictionary -   edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz
[main] INFO edu.stanford.nlp.wordseg.ChineseDictionary - Done. Unique words in ChineseDictionary is: 423200.
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/segmenter/chinese/ctb.gz ... done [12.3 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger ... done [1.1 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz ... done [7.2 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/srparser/chineseSR.ser.gz ... done [14.4 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator mention
[main] INFO edu.stanford.nlp.pipeline.MentionAnnotator - Using mention detector type: rule
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref

Entering interactive shell. Type q RETURN or EOF to quit.
NLP> 

```

The REST server didn't start but it goes into the streaming mode. 

**Is this the intended behaviour?**

**Is it possible to start the REST server for non-English model(s)?**

**Does the server mode only work for English?**"
355,https://github.com/stanfordnlp/CoreNLP/issues/453,453,[],closed,2017-06-05 07:36:07+00:00,,"Missing ""originalText"" field in Chinese model json output","Starting the Chinese model with CoreNLP server as such:

```
$ wget http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip

$ unzip stanford-corenlp-full-2016-10-31.zip && cd stanford-corenlp-full-2016-10-31

$ cd tanford-corenlp-full-2016-10-31

~/stanford-corenlp-full-2016-10-31$ wget http://nlp.stanford.edu/software/stanford-chinese-corenlp-2016-10-31-models.jar

~/stanford-corenlp-full-2016-10-31$ java -Xmx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-chinese.properties -preload tokenize,ssplit,pos,lemma,ner,parse  -port 9001 -timeout 15000
```

Posting the get request:

```
$ wget --post-data 'ÊàëÂÆ∂Ê≤°ÊúâÁîµËÑë„ÄÇ' 'localhost:9001/?properties={""annotators"": ""tokenize,ssplit"", ""outputFormat"":""json""}' -O tokenized.json
--2017-06-05 15:32:02--  http://localhost:9001/?properties=%7B%22annotators%22:%20%22tokenize,ssplit%22,%20%22outputFormat%22:%22json%22%7D
Resolving localhost... ::1, 127.0.0.1
Connecting to localhost|::1|:9001... connected.
HTTP request sent, awaiting response... 200 OK
Length: 411 [application/json]
Saving to: ‚Äòtokenized.json‚Äô

tokenized.json                100%[==============================================>]     411  --.-KB/s    in 0s      

2017-06-05 15:32:02 (39.2 MB/s) - ‚Äòtokenized.json‚Äô saved [411/411]
```

The values for the `originalText` fields are all empty:

```
$ cat tokenized.json 
{""sentences"":[{""index"":0,""tokens"":[{""index"":1,""word"":""ÊàëÂÆ∂"",""originalText"":"""",""characterOffsetBegin"":0,""characterOffsetEnd"":2},{""index"":2,""word"":""Ê≤°Êúâ"",""originalText"":"""",""characterOffsetBegin"":2,""characterOffsetEnd"":4},{""index"":3,""word"":""ÁîµËÑë"",""originalText"":"""",""characterOffsetBegin"":4,""characterOffsetEnd"":6},{""index"":4,""word"":""„ÄÇ"",""originalText"":"""",""characterOffsetBegin"":6,""characterOffsetEnd"":7}]}]}
```"
356,https://github.com/stanfordnlp/CoreNLP/issues/454,454,[],closed,2017-06-05 12:34:08+00:00,,CoreNLP online demo URL,"Hi, 

We were using the below links to check the Named entities identified by Stanford NLP.
http://nlp.stanford.edu:8080/corenlp/process
http://www.corenlp.run/

But now both the links seems not working. Is the demo link changed ? If yes could you please provide us the link. 

Thank you in advance.
"
357,https://github.com/stanfordnlp/CoreNLP/issues/455,455,"[{'id': 623871829, 'node_id': 'MDU6TGFiZWw2MjM4NzE4Mjk=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/packaging-bug', 'name': 'packaging-bug', 'color': 'e96695', 'default': False, 'description': None}]",closed,2017-06-06 09:31:36+00:00,,ptb trees to UD conversion,"Hello,

Could you please help me with converting ptb trees into CONLL-U format.
I am trying to use the following command:
java -mx1g edu.stanford.nlp.trees.ud.UniversalDependenciesConverter -treeFile treebank > treebank.conllu
from https://nlp.stanford.edu/software/stanford-dependencies.shtml
Unfortunately I get the following error:

""Could not find or load main class edu.stanford.nlp.trees.ud.UniversalDependenciesConverter""

Indeed, UniversalDependenciesConverter.java is not in stanford-parser-3.7.0-sources.jar, the corresponding .class file is missing either.
I downloaded the parser from https://nlp.stanford.edu/software/lex-parser.shtml
The parser (even the new one, nndep) works fine. 
Thank you for your helo,
Maria"
358,https://github.com/stanfordnlp/CoreNLP/issues/456,456,[],closed,2017-06-06 12:56:22+00:00,,Splitting sentence: Difference in Results from online corenlp.run server from local server ,"I have a difference while splitting a a text in online version vs my local version of corenlp server. Is it a configuration issue, or model issue? Let me know

## corenlp.run server

**Text**
Added Rs. 5000 to your Paytm wallet. Transaction ID: 7214651302. Current Balance: 5703.05. Upto Rs 150 Cashback on Movie tickets! Use code MOVIE150. http://m.p-y.tm/oo. Book Now

**Result**
![image](https://cloud.githubusercontent.com/assets/74857/26830069/3d413696-4ae5-11e7-8bce-34c5f5ae8c3c.png)

## local server (ver 3.7.0)

**Text**
Added Rs. 5000 to your Paytm wallet. Transaction ID: 7214651302. Current Balance: 5703.05. Upto Rs 150 Cashback on Movie tickets! Use code MOVIE150. http://m.p-y.tm/oo. Book Now

**Result**
![image](https://cloud.githubusercontent.com/assets/74857/26830010/0f9c1b3e-4ae5-11e7-9140-eae4d62ac315.png)
"
359,https://github.com/stanfordnlp/CoreNLP/issues/457,457,[],closed,2017-06-07 20:22:39+00:00,,Getting Coref Location in Constituency Parse,Does anyone know of a way to locate the position of a coref object in the output from the constituency parse?
360,https://github.com/stanfordnlp/CoreNLP/issues/459,459,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2017-06-08 20:26:28+00:00,,SEVERE: WARNING: failed to parse sentence. Will continue with the right-most head heuristic:,"Here's the error I receive.


> Jun 08, 2017 8:23:52 PM edu.stanford.nlp.ie.machinereading.GenericDataSetReader assignSyntacticHead
> SEVERE: WARNING: failed to parse sentence. Will continue with the right-most head heuristic:

> java.lang.IllegalArgumentException: Input word not tagged
> 	at edu.stanford.nlp.parser.shiftreduce.ShiftReduceParser.initialStateFromTaggedSentence(ShiftReduceParser.java:236)
> 	at edu.stanford.nlp.parser.shiftreduce.ShiftReduceParserQuery.parse(ShiftReduceParserQuery.java:54)
> 	at edu.stanford.nlp.pipeline.ParserAnnotator.doOneSentence(ParserAnnotator.java:323)
> 	at edu.stanford.nlp.pipeline.ParserAnnotator.doOneSentence(ParserAnnotator.java:254)
> 	at edu.stanford.nlp.pipeline.SentenceAnnotator.annotate(SentenceAnnotator.java:102)
> 	at edu.stanford.nlp.ie.machinereading.GenericDataSetReader.parse(GenericDataSetReader.java:531)
> 	at edu.stanford.nlp.ie.machinereading.GenericDataSetReader.findSyntacticHead(GenericDataSetReader.java:398)
> 	at edu.stanford.nlp.ie.machinereading.GenericDataSetReader.assignSyntacticHead(GenericDataSetReader.java:219)
> 	at edu.stanford.nlp.ie.machinereading.MachineReading.assignSyntacticHeadToEntities(MachineReading.java:655)
> 	at edu.stanford.nlp.ie.machinereading.MachineReading.annotate(MachineReading.java:596)
> 	at edu.stanford.nlp.ie.machinereading.MachineReading.annotate(MachineReading.java:567)
> 	at edu.stanford.nlp.pipeline.RelationExtractorAnnotator.annotate(RelationExtractorAnnotator.java:55)
> 	at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
> 	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:605)
> 	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.lambda$handle$0(StanfordCoreNLPServer.java:588)
> 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
> 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
> 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
> 	at java.lang.Thread.run(Thread.java:748)
> "
361,https://github.com/stanfordnlp/CoreNLP/issues/460,460,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2017-06-12 13:49:44+00:00,,Stream Closed IOException in PTBTokenizer.java,"I am trying to run the code given at https://github.com/abisee/cnn-dailymail to tokenize cnn and dailymail data using PTBTokenizer. 

While executing this line: 
$ java edu.stanford.nlp.process.PTBTokenizer -ioFileList -preserveLines mapping.txt 

by calling subprocess.command(...) in python, I get the following exception:

Exception in thread ""main"" java.io.IOException: Stream closed
        at java.io.BufferedWriter.ensureOpen(BufferedWriter.java:116)
        at java.io.BufferedWriter.write(BufferedWriter.java:221)
        at java.io.Writer.write(Writer.java:157)
        at edu.stanford.nlp.process.PTBTokenizer.tokReader(PTBTokenizer.java:505)
        at edu.stanford.nlp.process.PTBTokenizer.tok(PTBTokenizer.java:450)
        at edu.stanford.nlp.process.PTBTokenizer.main(PTBTokenizer.java:813)

I looked at the file PTBTokenizer.java, and looks like the output file is not being closed anywhere and apparently that is causing this exception.

Could you please look into this issue?

Thanks!"
362,https://github.com/stanfordnlp/CoreNLP/issues/462,462,[],open,2017-06-13 03:26:45+00:00,,Using own NER model and modify UI Brat,"Hello!
I'm very new with NLP, and have a little java knowledge. I've test this software with english article, and its has incredible accuracy. But I have a problem while implementing coreNLP with other language (Bahasa Indonesia). I have train about 300 article and its create a model, let say indonesia-1.ser.gz . In the forum I read, I have to add server.properties file that consist ner model configuration path, then I have to execute with -serverProperties flag when running coreNLP server like this :

`java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties server.properties -port 9000 -timeout 15000`

It run very smooth on my localhost:9000 and can detect NER on my bahasa article that I input first. my question are :
1. if I want segmented my model (indonesia-1.ser.gz, indonesia-2.ser.gz, ...) can I just add model path in server.properties file? Then how can I choose model during the testing?
2. In stanford CoreNLP UI, I see there are language option (English, Chinese, Spanish, etc). Can I modify that in corenlp-brat.html with my own model? and how to connect the UI and my model? So I can choose my model from the UI.

Thank you for your attention, I'm wait for any answers. And sorry for my bad english :("
363,https://github.com/stanfordnlp/CoreNLP/issues/463,463,"[{'id': 626016953, 'node_id': 'MDU6TGFiZWw2MjYwMTY5NTM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/analysis-bug', 'name': 'analysis-bug', 'color': 'f98685', 'default': False, 'description': None}]",closed,2017-06-13 14:08:44+00:00,,POS tagger incorrectly returns NN instead of VB for this utterance,"While comparing the Stanford NLP POS tagger against 2 others, I noticed that this sentence was being incorrectly parsed:
""unlock my debit card""
POS returns
unlock / NN  <<<< should be VB (or VBP)
my / PRP$
credit / NN
card / NN"
364,https://github.com/stanfordnlp/CoreNLP/issues/464,464,"[{'id': 45387508, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}]",closed,2017-06-13 21:31:18+00:00,,CoreNLP 3.8.0 questions,"README says:

```
---------------------------------
CHANGES
---------------------------------

2017-06-09    3.8.0     Web service annotator, discussion forum 
                        handling, new French and Spanish models 
```

1. What are those ""Web service annotator"" and ""discussion forum handling""?

2. Are there any significant changes/improvements to NER (or corresponding models for the NER pipeline) compared to 3.7.0?"
365,https://github.com/stanfordnlp/CoreNLP/issues/465,465,[],closed,2017-06-16 18:14:53+00:00,,Retrieve phrase based sentiment from Stanford Core NLP,"The Stanford Core NLP [online demo][1] gives a very nice visualization of phrase based sentiment. Every phrase in the parse tree has a sentiment. For their standard test example 

> This movie doesn't care about cleverness, wit or any other kind of intelligent humor.

You can see that the phrase `doesn't care about cleverness, wit or any other kind of intelligent humor` is marked as negative (77%) while the phrase ` cleverness, wit or any other kind of intelligent humor` is marked as positive (76%). You can also [get this][2] information in JSON format from the website. I haven't found out a way to get these from the API. Could I get such phrase fine grained sentiments via Stanford Core NLP?

Currently, I am using the Stanford Core NLP server with the following properties 

    props = {""annotators"": ""tokenize,ssplit,pos,parse,sentiment"", 'outputFormat': 'json'}




  [1]: http://nlp.stanford.edu:8080/sentiment/rntnDemo.html
  [2]: https://pastebin.com/yvvmikLZ"
366,https://github.com/stanfordnlp/CoreNLP/issues/466,466,"[{'id': 706064425, 'node_id': 'MDU6TGFiZWw3MDYwNjQ0MjU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/algorithm-error', 'name': 'algorithm-error', 'color': 'f9d0c4', 'default': False, 'description': None}, {'id': 735991731, 'node_id': 'MDU6TGFiZWw3MzU5OTE3MzE=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/trees', 'name': 'trees', 'color': 'c5def5', 'default': False, 'description': None}]",open,2017-06-16 20:34:10+00:00,,head finder - misplaced head for ing form,"When using headFinder with texts such as ""working years"" and """"training times last year"", Corenlp tagged ""working"" and ""training"" as head, which were incorrect (should be ""years"" and ""times""). However, if the text is ""training times"" without ""last year"" the head is tagged correctly as ""times"". Hope this gets fixed. thanks!"
367,https://github.com/stanfordnlp/CoreNLP/issues/467,467,[],closed,2017-06-20 04:41:10+00:00,,How can I get the same result as CoreNlp.run?,"sorry to bother you.
I use CoreNlp to get the parse tree ,and the maven dependency is like below:

    <dependency>
        <groupId>edu.stanford.nlp</groupId>
        <artifactId>stanford-corenlp</artifactId>
        <version>3.8.0</version>
    </dependency>

    <dependency>
        <groupId>edu.stanford.nlp</groupId>
        <artifactId>stanford-corenlp</artifactId>
        <version>3.8.0</version>
        <classifier>models</classifier>
    </dependency>
The result is different from CoreNlp.run(process the same sentence),and I want to get the same result like CoreNlp.run, what should i do?Thanks!"
368,https://github.com/stanfordnlp/CoreNLP/issues/468,468,[],closed,2017-06-20 22:15:26+00:00,,CoreNLP annotate giving bad formatted JSON,"I have simple .txt file with this content:


```
Hunting has a devastating effect on animal populations ‚Äî and the impact could worsen as development spreads in the future, according to research done in developing countries. Strategies to sustainably manage wild meat hunting in both protected and unprotected tropical ecosystems are urgently needed.

Researchers found that hunting on average leads to an 83% reduction in mammal populations within 25 miles of hunter access points like roads and towns. Of course, hunting is far from the only threat faced by vulnerable animals across the globe.
```




I read paragraphs from file and try to analyze them to get JSON result:

this is core part of my code that gets executed every iteration in loop:

```
print(""text to analyze: "")
print(cp)
print(""start..."")
output_data = nlp.annotate(cp, properties={'timeout': '3600000', 'annotators': 'tokenize, ssplit, pos, depparse, parse', 'outputFormat': 'json'})
print(""end..."")
print(""done generating data"")
print(output_data)
```

first paragraph is ok:
```
text to analyze: 
ÔªøHunting has a devastating effect on animal populations ‚Äî and the impact could worsen as development spreads in the future, according to research done in developing countries. Strategies to sustainably manage wild meat hunting in both protected and unprotected tropical ecosystems are urgently needed. 
start...
end...
done generating data
{""sentences"":[{""index"":0,""parse"":""(ROOT\r\n  (S\r\n    (S\r\n      (NP (NN Hunting))\r\n      (VP (VBZ has)\r\n        (NP\r\n          (NP (DT a) (JJ devastating) (NN effect))\r\n          (PP (IN on)\r\n            (NP (NN animal) (NNS populations))))))\r\n    (: --)\r\n    (CC and)\r\n    (S\r\n      (NP (DT the) (NN impact))\r\n      (VP (MD could)\r\n        (VP (VB worsen)\r\n          (PP (IN as)\r\n            (NP (NN development) (NNS spreads)))\r\n          (PP (IN in)\r\n            (NP (DT the) (NN future)))\r\n          (, ,)\r\n          (PP (VBG according)\r\n            (PP (TO to)\r\n              (NP\r\n                (NP (NN research))\r\n                (VP (VBN done)\r\n                  (PP (IN in)\r\n                    (NP (VBG developing) (NNS countries))))))))))\r\n    (. .)))"",""basic-dependencies"":[{""dep"":""ROOT"",""governor"":0,""governorGloss"":""ROOT"",""dependent"":2,""dependentGloss"":""has""},{""dep"":""nsubj"",""governor"":2,""governorGloss"":""has"",""dependent"":1,""dependentGloss"":""Hunting""},{""dep"":""det"",""governor"":5,""governorGloss"":""effect"",""dependent"":3,""dependentGloss"":""a""},{""dep"":""amod"",""governor"":5,""governorGloss"":""effect"",""dependent"":4,""dependentGloss"":""devastating""},{""dep"":""dobj"",""governor"":2,""governorGloss"":""has"",""dependent"":5,""dependentGloss"":""effect""},{""dep"":""case"",""governor"":8,""governorGloss"":""populations"",""dependent"":6,""dependentGloss"":""on""},{""dep"":""compound"",""governor"":8,""governorGloss"":""populations"",""dependent"":7,""dependentGloss"":""animal""},{""dep"":""nmod"",""governor"":5,""governorGloss"":""effect"",""dependent"":8,""dependentGloss"":""populations""},{""dep"":""punct"",""governor"":2,""governorGloss"":""has"",""dependent"":9,""dependentGloss"":""--""},{""dep"":""cc"",""governor"":2,""governorGloss"":""has"",""dependent"":10,""dependentGloss"":""and""},{""dep"":""det"",""governor"":12,""governorGloss"":""impact"",""dependent"":11,""dependentGloss"":""the""},{""dep"":""nsubj"",""governor"":14,""governorGloss"":""worsen"",""dependent"":12,""dependentGloss"":""impact""},{""dep"":""aux"",""governor"":14,""governorGloss"":""worsen"",""dependent"":13,""dependentGloss"":""could""},{""dep"":""conj"",""governor"":2,""governorGloss"":""has"",""dependent"":14,""dependentGloss"":""worsen""},{""dep"":""case"",""governor"":17,""governorGloss"":""spreads"",""dependent"":15,""dependentGloss"":""as""},{""dep"":""compound"",""governor"":17,""governorGloss"":""spreads"",""dependent"":16,""dependentGloss"":""development""},{""dep"":""nmod"",""governor"":14,""governorGloss"":""worsen"",""dependent"":17,""dependentGloss"":""spreads""},{""dep"":""case"",""governor"":20,""governorGloss"":""future"",""dependent"":18,""dependentGloss"":""in""},{""dep"":""det"",""governor"":20,""governorGloss"":""future"",""dependent"":19,""dependentGloss"":""the""},{""dep"":""nmod"",""governor"":14,""governorGloss"":""worsen"",""dependent"":20,""dependentGloss"":""future""},{""dep"":""punct"",""governor"":14,""governorGloss"":""worsen"",""dependent"":21,""dependentGloss"":"",""},{""dep"":""case"",""governor"":24,""governorGloss"":""research"",""dependent"":22,""dependentGloss"":""according""},{""dep"":""mwe"",""governor"":22,""governorGloss"":""according"",""dependent"":23,""dependentGloss"":""to""},{""dep"":""nmod"",""governor"":14,""governorGloss"":""worsen"",""dependent"":24,""dependentGloss"":""research""},{""dep"":""acl"",""governor"":24,""governorGloss"":""research"",""dependent"":25,""dependentGloss"":""done""},{""dep"":""case"",""governor"":28,""governorGloss"":""countries"",""dependent"":26,""dependentGloss"":""in""},{""dep"":""amod"",""governor"":28,""governorGloss"":""countries"",""dependent"":27,""dependentGloss"":""developing""},{""dep"":""nmod"",""governor"":25,""governorGloss"":""done"",""dependent"":28,""dependentGloss"":""countries""},{""dep"":""punct"",""governor"":2,""governorGloss"":""has"",""dependent"":29,""dependentGloss"":"".""}],""collapsed-dependencies"":[{""dep"":""ROOT"",""governor"":0,""governorGloss"":""ROOT"",""dependent"":2,""dependentGloss"":""has""},{""dep"":""nsubj"",""governor"":2,""governorGloss"":""has"",""dependent"":1,""dependentGloss"":""Hunting""},{""dep"":""det"",""governor"":5,""governorGloss"":""effect"",""dependent"":3,""dependentGloss"":""a""},{""dep"":""amod"",""governor"":5,""governorGloss"":""effect"",""dependent"":4,""dependentGloss"":""devastating""},{""dep"":""dobj"",""governor"":2,""governorGloss"":""has"",""dependent"":5,""dependentGloss"":""effect""},{""dep"":""case"",""governor"":8,""governorGloss"":""populations"",""dependent"":6,""dependentGloss"":""on""},{""dep"":""compound"",""governor"":8,""governorGloss"":""populations"",""dependent"":7,""dependentGloss"":""animal""},{""dep"":""nmod:on"",""governor"":5,""governorGloss"":""effect"",""dependent"":8,""dependentGloss"":""populations""},{""dep"":""punct"",""governor"":2,""governorGloss"":""has"",""dependent"":9,""dependentGloss"":""--""},{""dep"":""cc"",""governor"":2,""governorGloss"":""has"",""dependent"":10,""dependentGloss"":""and""},{""dep"":""det"",""governor"":12,""governorGloss"":""impact"",""dependent"":11,""dependentGloss"":""the""},{""dep"":""nsubj"",""governor"":14,""governorGloss"":""worsen"",""dependent"":12,""dependentGloss"":""impact""},{""dep"":""aux"",""governor"":14,""governorGloss"":""worsen"",""dependent"":13,""dependentGloss"":""could""},{""dep"":""conj:and"",""governor"":2,""governorGloss"":""has"",""dependent"":14,""dependentGloss"":""worsen""},{""dep"":""case"",""governor"":17,""governorGloss"":""spreads"",""dependent"":15,""dependentGloss"":""as""},{""dep"":""compound"",""governor"":17,""governorGloss"":""spreads"",""dependent"":16,""dependentGloss"":""development""},{""dep"":""nmod:as"",""governor"":14,""governorGloss"":""worsen"",""dependent"":17,""dependentGloss"":""spreads""},{""dep"":""case"",""governor"":20,""governorGloss"":""future"",""dependent"":18,""dependentGloss"":""in""},{""dep"":""det"",""governor"":20,""governorGloss"":""future"",""dependent"":19,""dependentGloss"":""the""},{""dep"":""nmod:in"",""governor"":14,""governorGloss"":""worsen"",""dependent"":20,""dependentGloss"":""future""},{""dep"":""punct"",""governor"":14,""governorGloss"":""worsen"",""dependent"":21,""dependentGloss"":"",""},{""dep"":""case"",""governor"":24,""governorGloss"":""research"",""dependent"":22,""dependentGloss"":""according""},{""dep"":""mwe"",""governor"":22,""governorGloss"":""according"",""dependent"":23,""dependentGloss"":""to""},{""dep"":""nmod:according_to"",""governor"":14,""governorGloss"":""worsen"",""dependent"":24,""dependentGloss"":""research""},{""dep"":""acl"",""governor"":24,""governorGloss"":""research"",""dependent"":25,""dependentGloss"":""done""},{""dep"":""case"",""governor"":28,""governorGloss"":""countries"",""dependent"":26,""dependentGloss"":""in""},{""dep"":""amod"",""governor"":28,""governorGloss"":""countries"",""dependent"":27,""dependentGloss"":""developing""},{""dep"":""nmod:in"",""governor"":25,""governorGloss"":""done"",""dependent"":28,""dependentGloss"":""countries""},{""dep"":""punct"",""governor"":2,""governorGloss"":""has"",""dependent"":29,""dependentGloss"":"".""}],""collapsed-ccprocessed-dependencies"":[{""dep"":""ROOT"",""governor"":0,""governorGloss"":""ROOT"",""dependent"":2,""dependentGloss"":""has""},{""dep"":""nsubj"",""governor"":2,""governorGloss"":""has"",""dependent"":1,""dependentGloss"":""Hunting""},{""dep"":""det"",""governor"":5,""governorGloss"":""effect"",""dependent"":3,""dependentGloss"":""a""},{""dep"":""amod"",""governor"":5,""governorGloss"":""effect"",""dependent"":4,""dependentGloss"":""devastating""},{""dep"":""dobj"",""governor"":2,""governorGloss"":""has"",""dependent"":5,""dependentGloss"":""effect""},{""dep"":""case"",""governor"":8,""governorGloss"":""populations"",""dependent"":6,""dependentGloss"":""on""},{""dep"":""compound"",""governor"":8,""governorGloss"":""populations"",""dependent"":7,""dependentGloss"":""animal""},{""dep"":""nmod:on"",""governor"":5,""governorGloss"":""effect"",""dependent"":8,""dependentGloss"":""populations""},{""dep"":""punct"",""governor"":2,""governorGloss"":""has"",""dependent"":9,""dependentGloss"":""--""},{""dep"":""cc"",""governor"":2,""governorGloss"":""has"",""dependent"":10,""dependentGloss"":""and""},{""dep"":""det"",""governor"":12,""governorGloss"":""impact"",""dependent"":11,""dependentGloss"":""the""},{""dep"":""nsubj"",""governor"":14,""governorGloss"":""worsen"",""dependent"":12,""dependentGloss"":""impact""},{""dep"":""aux"",""governor"":14,""governorGloss"":""worsen"",""dependent"":13,""dependentGloss"":""could""},{""dep"":""conj:and"",""governor"":2,""governorGloss"":""has"",""dependent"":14,""dependentGloss"":""worsen""},{""dep"":""case"",""governor"":17,""governorGloss"":""spreads"",""dependent"":15,""dependentGloss"":""as""},{""dep"":""compound"",""governor"":17,""governorGloss"":""spreads"",""dependent"":16,""dependentGloss"":""development""},{""dep"":""nmod:as"",""governor"":14,""governorGloss"":""worsen"",""dependent"":17,""dependentGloss"":""spreads""},{""dep"":""case"",""governor"":20,""governorGloss"":""future"",""dependent"":18,""dependentGloss"":""in""},{""dep"":""det"",""governor"":20,""governorGloss"":""future"",""dependent"":19,""dependentGloss"":""the""},{""dep"":""nmod:in"",""governor"":14,""governorGloss"":""worsen"",""dependent"":20,""dependentGloss"":""future""},{""dep"":""punct"",""governor"":14,""governorGloss"":""worsen"",""dependent"":21,""dependentGloss"":"",""},{""dep"":""case"",""governor"":24,""governorGloss"":""research"",""dependent"":22,""dependentGloss"":""according""},{""dep"":""mwe"",""governor"":22,""governorGloss"":""according"",""dependent"":23,""dependentGloss"":""to""},{""dep"":""nmod:according_to"",""governor"":14,""governorGloss"":""worsen"",""dependent"":24,""dependentGloss"":""research""},{""dep"":""acl"",""governor"":24,""governorGloss"":""research"",""dependent"":25,""dependentGloss"":""done""},{""dep"":""case"",""governor"":28,""governorGloss"":""countries"",""dependent"":26,""dependentGloss"":""in""},{""dep"":""amod"",""governor"":28,""governorGloss"":""countries"",""dependent"":27,""dependentGloss"":""developing""},{""dep"":""nmod:in"",""governor"":25,""governorGloss"":""done"",""dependent"":28,""dependentGloss"":""countries""},{""dep"":""punct"",""governor"":2,""governorGloss"":""has"",""dependent"":29,""dependentGloss"":"".""}],""tokens"":[{""index"":1,""word"":""Hunting"",""originalText"":""Hunting"",""characterOffsetBegin"":1,""characterOffsetEnd"":8,""pos"":""NN"",""before"":""?"",""after"":"" ""},{""index"":2,""word"":""has"",""originalText"":""has"",""characterOffsetBegin"":9,""characterOffsetEnd"":12,""pos"":""VBZ"",""before"":"" "",""after"":"" ""},{""index"":3,""word"":""a"",""originalText"":""a"",""characterOffsetBegin"":13,""characterOffsetEnd"":14,""pos"":""DT"",""before"":"" "",""after"":"" ""},{""index"":4,""word"":""devastating"",""originalText"":""devastating"",""characterOffsetBegin"":15,""characterOffsetEnd"":26,""pos"":""JJ"",""before"":"" "",""after"":"" ""},{""index"":5,""word"":""effect"",""originalText"":""effect"",""characterOffsetBegin"":27,""characterOffsetEnd"":33,""pos"":""NN"",""before"":"" "",""after"":"" ""},{""index"":6,""word"":""on"",""originalText"":""on"",""characterOffsetBegin"":34,""characterOffsetEnd"":36,""pos"":""IN"",""before"":"" "",""after"":"" ""},{""index"":7,""word"":""animal"",""originalText"":""animal"",""characterOffsetBegin"":37,""characterOffsetEnd"":43,""pos"":""NN"",""before"":"" "",""after"":"" ""},{""index"":8,""word"":""populations"",""originalText"":""populations"",""characterOffsetBegin"":44,""characterOffsetEnd"":55,""pos"":""NNS"",""before"":"" "",""after"":"" ""},{""index"":9,""word"":""--"",""originalText"":""¬ó"",""characterOffsetBegin"":56,""characterOffsetEnd"":57,""pos"":"":"",""before"":"" "",""after"":"" ""},{""index"":10,""word"":""and"",""originalText"":""and"",""characterOffsetBegin"":58,""characterOffsetEnd"":61,""pos"":""CC"",""before"":"" "",""after"":"" ""},{""index"":11,""word"":""the"",""originalText"":""the"",""characterOffsetBegin"":62,""characterOffsetEnd"":65,""pos"":""DT"",""before"":"" "",""after"":"" ""},{""index"":12,""word"":""impact"",""originalText"":""impact"",""characterOffsetBegin"":66,""characterOffsetEnd"":72,""pos"":""NN"",""before"":"" "",""after"":"" ""},{""index"":13,""word"":""could"",""originalText"":""could"",""characterOffsetBegin"":73,""characterOffsetEnd"":78,""pos"":""MD"",""before"":"" "",""after"":"" ""},{""index"":14,""word"":""worsen"",""originalText"":""worsen"",""characterOffsetBegin"":79,""characterOffsetEnd"":85,""pos"":""VB"",""before"":"" "",""after"":"" ""},{""index"":15,""word"":""as"",""originalText"":""as"",""characterOffsetBegin"":86,""characterOffsetEnd"":88,""pos"":""IN"",""before"":"" "",""after"":"" ""},{""index"":16,""word"":""development"",""originalText"":""development"",""characterOffsetBegin"":89,""characterOffsetEnd"":100,""pos"":""NN"",""before"":"" "",""after"":"" ""},{""index"":17,""word"":""spreads"",""originalText"":""spreads"",""characterOffsetBegin"":101,""characterOffsetEnd"":108,""pos"":""NNS"",""before"":"" "",""after"":"" ""},{""index"":18,""word"":""in"",""originalText"":""in"",""characterOffsetBegin"":109,""characterOffsetEnd"":111,""pos"":""IN"",""before"":"" "",""after"":"" ""},{""index"":19,""word"":""the"",""originalText"":""the"",""characterOffsetBegin"":112,""characterOffsetEnd"":115,""pos"":""DT"",""before"":"" "",""after"":"" ""},{""index"":20,""word"":""future"",""originalText"":""future"",""characterOffsetBegin"":116,""characterOffsetEnd"":122,""pos"":""NN"",""before"":"" "",""after"":""""},{""index"":21,""word"":"","",""originalText"":"","",""characterOffsetBegin"":122,""characterOffsetEnd"":123,""pos"":"","",""before"":"""",""after"":"" ""},{""index"":22,""word"":""according"",""originalText"":""according"",""characterOffsetBegin"":124,""characterOffsetEnd"":133,""pos"":""VBG"",""before"":"" "",""after"":"" ""},{""index"":23,""word"":""to"",""originalText"":""to"",""characterOffsetBegin"":134,""characterOffsetEnd"":136,""pos"":""TO"",""before"":"" "",""after"":"" ""},{""index"":24,""word"":""research"",""originalText"":""research"",""characterOffsetBegin"":137,""characterOffsetEnd"":145,""pos"":""NN"",""before"":"" "",""after"":"" ""},{""index"":25,""word"":""done"",""originalText"":""done"",""characterOffsetBegin"":146,""characterOffsetEnd"":150,""pos"":""VBN"",""before"":"" "",""after"":"" ""},{""index"":26,""word"":""in"",""originalText"":""in"",""characterOffsetBegin"":151,""characterOffsetEnd"":153,""pos"":""IN"",""before"":"" "",""after"":"" ""},{""index"":27,""word"":""developing"",""originalText"":""developing"",""characterOffsetBegin"":154,""characterOffsetEnd"":164,""pos"":""VBG"",""before"":"" "",""after"":"" ""},{""index"":28,""word"":""countries"",""originalText"":""countries"",""characterOffsetBegin"":165,""characterOffsetEnd"":174,""pos"":""NNS"",""before"":"" "",""after"":""""},{""index"":29,""word"":""."",""originalText"":""."",""characterOffsetBegin"":174,""characterOffsetEnd"":175,""pos"":""."",""before"":"""",""after"":"" ""}]},{""index"":1,""parse"":""(ROOT\r\n  (S\r\n    (NP (NNS Strategies)\r\n      (S\r\n        (VP (TO to)\r\n          (VP\r\n            (ADVP (RB sustainably))\r\n            (VB manage)\r\n            (NP\r\n              (NP (JJ wild) (NN meat) (NN hunting))\r\n              (PP (IN in)\r\n                (NP (DT both) (JJ protected)\r\n                  (CC and)\r\n                  (JJ unprotected) (JJ tropical) (NNS ecosystems))))))))\r\n    (VP (VBP are)\r\n      (ADVP (RB urgently))\r\n      (VP (VBN needed)))\r\n    (. .)))"",""basic-dependencies"":[{""dep"":""ROOT"",""governor"":0,""governorGloss"":""ROOT"",""dependent"":17,""dependentGloss"":""needed""},{""dep"":""nsubjpass"",""governor"":17,""governorGloss"":""needed"",""dependent"":1,""dependentGloss"":""Strategies""},{""dep"":""mark"",""governor"":4,""governorGloss"":""manage"",""dependent"":2,""dependentGloss"":""to""},{""dep"":""advmod"",""governor"":4,""governorGloss"":""manage"",""dependent"":3,""dependentGloss"":""sustainably""},{""dep"":""acl"",""governor"":1,""governorGloss"":""Strategies"",""dependent"":4,""dependentGloss"":""manage""},{""dep"":""amod"",""governor"":7,""governorGloss"":""hunting"",""dependent"":5,""dependentGloss"":""wild""},{""dep"":""compound"",""governor"":7,""governorGloss"":""hunting"",""dependent"":6,""dependentGloss"":""meat""},{""dep"":""dobj"",""governor"":4,""governorGloss"":""manage"",""dependent"":7,""dependentGloss"":""hunting""},{""dep"":""case"",""governor"":10,""governorGloss"":""protected"",""dependent"":8,""dependentGloss"":""in""},{""dep"":""cc:preconj"",""governor"":10,""governorGloss"":""protected"",""dependent"":9,""dependentGloss"":""both""},{""dep"":""nmod"",""governor"":7,""governorGloss"":""hunting"",""dependent"":10,""dependentGloss"":""protected""},{""dep"":""cc"",""governor"":10,""governorGloss"":""protected"",""dependent"":11,""dependentGloss"":""and""},{""dep"":""amod"",""governor"":14,""governorGloss"":""ecosystems"",""dependent"":12,""dependentGloss"":""unprotected""},{""dep"":""amod"",""governor"":14,""governorGloss"":""ecosystems"",""dependent"":13,""dependentGloss"":""tropical""},{""dep"":""conj"",""governor"":10,""governorGloss"":""protected"",""dependent"":14,""dependentGloss"":""ecosystems""},{""dep"":""auxpass"",""governor"":17,""governorGloss"":""needed"",""dependent"":15,""dependentGloss"":""are""},{""dep"":""advmod"",""governor"":17,""governorGloss"":""needed"",""dependent"":16,""dependentGloss"":""urgently""},{""dep"":""punct"",""governor"":17,""governorGloss"":""needed"",""dependent"":18,""dependentGloss"":"".""}],""collapsed-dependencies"":[{""dep"":""ROOT"",""governor"":0,""governorGloss"":""ROOT"",""dependent"":17,""dependentGloss"":""needed""},{""dep"":""nsubjpass"",""governor"":17,""governorGloss"":""needed"",""dependent"":1,""dependentGloss"":""Strategies""},{""dep"":""mark"",""governor"":4,""governorGloss"":""manage"",""dependent"":2,""dependentGloss"":""to""},{""dep"":""advmod"",""governor"":4,""governorGloss"":""manage"",""dependent"":3,""dependentGloss"":""sustainably""},{""dep"":""acl"",""governor"":1,""governorGloss"":""Strategies"",""dependent"":4,""dependentGloss"":""manage""},{""dep"":""amod"",""governor"":7,""governorGloss"":""hunting"",""dependent"":5,""dependentGloss"":""wild""},{""dep"":""compound"",""governor"":7,""governorGloss"":""hunting"",""dependent"":6,""dependentGloss"":""meat""},{""dep"":""dobj"",""governor"":4,""governorGloss"":""manage"",""dependent"":7,""dependentGloss"":""hunting""},{""dep"":""case"",""governor"":10,""governorGloss"":""protected"",""dependent"":8,""dependentGloss"":""in""},{""dep"":""cc:preconj"",""governor"":10,""governorGloss"":""protected"",""dependent"":9,""dependentGloss"":""both""},{""dep"":""nmod:in"",""governor"":7,""governorGloss"":""hunting"",""dependent"":10,""dependentGloss"":""protected""},{""dep"":""cc"",""governor"":10,""governorGloss"":""protected"",""dependent"":11,""dependentGloss"":""and""},{""dep"":""amod"",""governor"":14,""governorGloss"":""ecosystems"",""dependent"":12,""dependentGloss"":""unprotected""},{""dep"":""amod"",""governor"":14,""governorGloss"":""ecosystems"",""dependent"":13,""dependentGloss"":""tropical""},{""dep"":""conj:and"",""governor"":10,""governorGloss"":""protected"",""dependent"":14,""dependentGloss"":""ecosystems""},{""dep"":""auxpass"",""governor"":17,""governorGloss"":""needed"",""dependent"":15,""dependentGloss"":""are""},{""dep"":""advmod"",""governor"":17,""governorGloss"":""needed"",""dependent"":16,""dependentGloss"":""urgently""},{""dep"":""punct"",""governor"":17,""governorGloss"":""needed"",""dependent"":18,""dependentGloss"":"".""}],""collapsed-ccprocessed-dependencies"":[{""dep"":""ROOT"",""governor"":0,""governorGloss"":""ROOT"",""dependent"":17,""dependentGloss"":""needed""},{""dep"":""nsubjpass"",""governor"":17,""governorGloss"":""needed"",""dependent"":1,""dependentGloss"":""Strategies""},{""dep"":""mark"",""governor"":4,""governorGloss"":""manage"",""dependent"":2,""dependentGloss"":""to""},{""dep"":""advmod"",""governor"":4,""governorGloss"":""manage"",""dependent"":3,""dependentGloss"":""sustainably""},{""dep"":""acl"",""governor"":1,""governorGloss"":""Strategies"",""dependent"":4,""dependentGloss"":""manage""},{""dep"":""amod"",""governor"":7,""governorGloss"":""hunting"",""dependent"":5,""dependentGloss"":""wild""},{""dep"":""compound"",""governor"":7,""governorGloss"":""hunting"",""dependent"":6,""dependentGloss"":""meat""},{""dep"":""dobj"",""governor"":4,""governorGloss"":""manage"",""dependent"":7,""dependentGloss"":""hunting""},{""dep"":""case"",""governor"":10,""governorGloss"":""protected"",""dependent"":8,""dependentGloss"":""in""},{""dep"":""cc:preconj"",""governor"":10,""governorGloss"":""protected"",""dependent"":9,""dependentGloss"":""both""},{""dep"":""nmod:in"",""governor"":7,""governorGloss"":""hunting"",""dependent"":10,""dependentGloss"":""protected""},{""dep"":""cc"",""governor"":10,""governorGloss"":""protected"",""dependent"":11,""dependentGloss"":""and""},{""dep"":""amod"",""governor"":14,""governorGloss"":""ecosystems"",""dependent"":12,""dependentGloss"":""unprotected""},{""dep"":""amod"",""governor"":14,""governorGloss"":""ecosystems"",""dependent"":13,""dependentGloss"":""tropical""},{""dep"":""nmod:in"",""governor"":7,""governorGloss"":""hunting"",""dependent"":14,""dependentGloss"":""ecosystems""},{""dep"":""conj:and"",""governor"":10,""governorGloss"":""protected"",""dependent"":14,""dependentGloss"":""ecosystems""},{""dep"":""auxpass"",""governor"":17,""governorGloss"":""needed"",""dependent"":15,""dependentGloss"":""are""},{""dep"":""advmod"",""governor"":17,""governorGloss"":""needed"",""dependent"":16,""dependentGloss"":""urgently""},{""dep"":""punct"",""governor"":17,""governorGloss"":""needed"",""dependent"":18,""dependentGloss"":"".""}],""tokens"":[{""index"":1,""word"":""Strategies"",""originalText"":""Strategies"",""characterOffsetBegin"":176,""characterOffsetEnd"":186,""pos"":""NNS"",""before"":"" "",""after"":"" ""},{""index"":2,""word"":""to"",""originalText"":""to"",""characterOffsetBegin"":187,""characterOffsetEnd"":189,""pos"":""TO"",""before"":"" "",""after"":"" ""},{""index"":3,""word"":""sustainably"",""originalText"":""sustainably"",""characterOffsetBegin"":190,""characterOffsetEnd"":201,""pos"":""RB"",""before"":"" "",""after"":"" ""},{""index"":4,""word"":""manage"",""originalText"":""manage"",""characterOffsetBegin"":202,""characterOffsetEnd"":208,""pos"":""VB"",""before"":"" "",""after"":"" ""},{""index"":5,""word"":""wild"",""originalText"":""wild"",""characterOffsetBegin"":209,""characterOffsetEnd"":213,""pos"":""JJ"",""before"":"" "",""after"":"" ""},{""index"":6,""word"":""meat"",""originalText"":""meat"",""characterOffsetBegin"":214,""characterOffsetEnd"":218,""pos"":""NN"",""before"":"" "",""after"":"" ""},{""index"":7,""word"":""hunting"",""originalText"":""hunting"",""characterOffsetBegin"":219,""characterOffsetEnd"":226,""pos"":""NN"",""before"":"" "",""after"":"" ""},{""index"":8,""word"":""in"",""originalText"":""in"",""characterOffsetBegin"":227,""characterOffsetEnd"":229,""pos"":""IN"",""before"":"" "",""after"":"" ""},{""index"":9,""word"":""both"",""originalText"":""both"",""characterOffsetBegin"":230,""characterOffsetEnd"":234,""pos"":""DT"",""before"":"" "",""after"":"" ""},{""index"":10,""word"":""protected"",""originalText"":""protected"",""characterOffsetBegin"":235,""characterOffsetEnd"":244,""pos"":""JJ"",""before"":"" "",""after"":"" ""},{""index"":11,""word"":""and"",""originalText"":""and"",""characterOffsetBegin"":245,""characterOffsetEnd"":248,""pos"":""CC"",""before"":"" "",""after"":"" ""},{""index"":12,""word"":""unprotected"",""originalText"":""unprotected"",""characterOffsetBegin"":249,""characterOffsetEnd"":260,""pos"":""JJ"",""before"":"" "",""after"":"" ""},{""index"":13,""word"":""tropical"",""originalText"":""tropical"",""characterOffsetBegin"":261,""characterOffsetEnd"":269,""pos"":""JJ"",""before"":"" "",""after"":"" ""},{""index"":14,""word"":""ecosystems"",""originalText"":""ecosystems"",""characterOffsetBegin"":270,""characterOffsetEnd"":280,""pos"":""NNS"",""before"":"" "",""after"":"" ""},{""index"":15,""word"":""are"",""originalText"":""are"",""characterOffsetBegin"":281,""characterOffsetEnd"":284,""pos"":""VBP"",""before"":"" "",""after"":"" ""},{""index"":16,""word"":""urgently"",""originalText"":""urgently"",""characterOffsetBegin"":285,""characterOffsetEnd"":293,""pos"":""RB"",""before"":"" "",""after"":"" ""},{""index"":17,""word"":""needed"",""originalText"":""needed"",""characterOffsetBegin"":294,""characterOffsetEnd"":300,""pos"":""VBN"",""before"":"" "",""after"":""""},{""index"":18,""word"":""."",""originalText"":""."",""characterOffsetBegin"":300,""characterOffsetEnd"":301,""pos"":""."",""before"":"""",""after"":""     ""}]}]}
```

but on second paragraph I am getting bad formatted JSON in single quotes as result:

```

text to analyze: 
Researchers found that hunting on average leads to an 83% reduction in mammal populations within 25 miles of hunter access points like roads and towns. Of course, hunting is far from the only threat faced by vulnerable animals across the globe. 
start...
end...
done generating data
{'sentences': [{'index': 0, 'parse': '(ROOT\r\n  (S\r\n    (NP (NNS Researchers))\r\n    (VP (VBD found)\r\n      (SBAR (IN that)\r\n        (S\r\n          (NP\r\n            (NP (NN hunting))\r\n            (PP (IN on)\r\n              (NP (NN average))))\r\n          (VP (VBZ leads)\r\n            (PP (TO to)\r\n              (NP\r\n                (NP (DT an)\r\n                  (ADJP (CD 83) (NN %))\r\n                  (NN reduction))\r\n                (PP (IN in)\r\n                  (NP (NN mammal) (NNS populations)))))\r\n            (PP (IN within)\r\n              (NP\r\n                (NP (CD 25) (NNS miles))\r\n                (PP (IN of)\r\n                  (NP\r\n                    (NP (NN hunter) (NN access) (NNS points))\r\n                    (PP (IN like)\r\n                      (NP (NNS roads)\r\n                        (CC and)\r\n                        (NNS towns)))))))))))\r\n    (. .)))', 'basic-dependencies': [{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 2, 'dependentGloss': 'found'}, {'dep': 'nsubj', 'governor': 2, 'governorGloss': 'found', 'dependent': 1, 'dependentGloss': 'Researchers'}, {'dep': 'mark', 'governor': 7, 'governorGloss': 'leads', 'dependent': 3, 'dependentGloss': 'that'}, {'dep': 'nsubj', 'governor': 7, 'governorGloss': 'leads', 'dependent': 4, 'dependentGloss': 'hunting'}, {'dep': 'case', 'governor': 6, 'governorGloss': 'average', 'dependent': 5, 'dependentGloss': 'on'}, {'dep': 'nmod', 'governor': 4, 'governorGloss': 'hunting', 'dependent': 6, 'dependentGloss': 'average'}, {'dep': 'ccomp', 'governor': 2, 'governorGloss': 'found', 'dependent': 7, 'dependentGloss': 'leads'}, {'dep': 'case', 'governor': 12, 'governorGloss': 'reduction', 'dependent': 8, 'dependentGloss': 'to'}, {'dep': 'det', 'governor': 12, 'governorGloss': 'reduction', 'dependent': 9, 'dependentGloss': 'an'}, {'dep': 'compound', 'governor': 11, 'governorGloss': '%', 'dependent': 10, 'dependentGloss': '83'}, {'dep': 'amod', 'governor': 12, 'governorGloss': 'reduction', 'dependent': 11, 'dependentGloss': '%'}, {'dep': 'nmod', 'governor': 7, 'governorGloss': 'leads', 'dependent': 12, 'dependentGloss': 'reduction'}, {'dep': 'case', 'governor': 15, 'governorGloss': 'populations', 'dependent': 13, 'dependentGloss': 'in'}, {'dep': 'compound', 'governor': 15, 'governorGloss': 'populations', 'dependent': 14, 'dependentGloss': 'mammal'}, {'dep': 'nmod', 'governor': 12, 'governorGloss': 'reduction', 'dependent': 15, 'dependentGloss': 'populations'}, {'dep': 'case', 'governor': 18, 'governorGloss': 'miles', 'dependent': 16, 'dependentGloss': 'within'}, {'dep': 'nummod', 'governor': 18, 'governorGloss': 'miles', 'dependent': 17, 'dependentGloss': '25'}, {'dep': 'nmod', 'governor': 7, 'governorGloss': 'leads', 'dependent': 18, 'dependentGloss': 'miles'}, {'dep': 'case', 'governor': 22, 'governorGloss': 'points', 'dependent': 19, 'dependentGloss': 'of'}, {'dep': 'compound', 'governor': 22, 'governorGloss': 'points', 'dependent': 20, 'dependentGloss': 'hunter'}, {'dep': 'compound', 'governor': 22, 'governorGloss': 'points', 'dependent': 21, 'dependentGloss': 'access'}, {'dep': 'nmod', 'governor': 18, 'governorGloss': 'miles', 'dependent': 22, 'dependentGloss': 'points'}, {'dep': 'case', 'governor': 24, 'governorGloss': 'roads', 'dependent': 23, 'dependentGloss': 'like'}, {'dep': 'nmod', 'governor': 22, 'governorGloss': 'points', 'dependent': 24, 'dependentGloss': 'roads'}, {'dep': 'cc', 'governor': 24, 'governorGloss': 'roads', 'dependent': 25, 'dependentGloss': 'and'}, {'dep': 'conj', 'governor': 24, 'governorGloss': 'roads', 'dependent': 26, 'dependentGloss': 'towns'}, {'dep': 'punct', 'governor': 2, 'governorGloss': 'found', 'dependent': 27, 'dependentGloss': '.'}], 'collapsed-dependencies': [{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 2, 'dependentGloss': 'found'}, {'dep': 'nsubj', 'governor': 2, 'governorGloss': 'found', 'dependent': 1, 'dependentGloss': 'Researchers'}, {'dep': 'mark', 'governor': 7, 'governorGloss': 'leads', 'dependent': 3, 'dependentGloss': 'that'}, {'dep': 'nsubj', 'governor': 7, 'governorGloss': 'leads', 'dependent': 4, 'dependentGloss': 'hunting'}, {'dep': 'case', 'governor': 6, 'governorGloss': 'average', 'dependent': 5, 'dependentGloss': 'on'}, {'dep': 'nmod:on', 'governor': 4, 'governorGloss': 'hunting', 'dependent': 6, 'dependentGloss': 'average'}, {'dep': 'ccomp', 'governor': 2, 'governorGloss': 'found', 'dependent': 7, 'dependentGloss': 'leads'}, {'dep': 'case', 'governor': 12, 'governorGloss': 'reduction', 'dependent': 8, 'dependentGloss': 'to'}, {'dep': 'det', 'governor': 12, 'governorGloss': 'reduction', 'dependent': 9, 'dependentGloss': 'an'}, {'dep': 'compound', 'governor': 11, 'governorGloss': '%', 'dependent': 10, 'dependentGloss': '83'}, {'dep': 'amod', 'governor': 12, 'governorGloss': 'reduction', 'dependent': 11, 'dependentGloss': '%'}, {'dep': 'nmod:to', 'governor': 7, 'governorGloss': 'leads', 'dependent': 12, 'dependentGloss': 'reduction'}, {'dep': 'case', 'governor': 15, 'governorGloss': 'populations', 'dependent': 13, 'dependentGloss': 'in'}, {'dep': 'compound', 'governor': 15, 'governorGloss': 'populations', 'dependent': 14, 'dependentGloss': 'mammal'}, {'dep': 'nmod:in', 'governor': 12, 'governorGloss': 'reduction', 'dependent': 15, 'dependentGloss': 'populations'}, {'dep': 'case', 'governor': 18, 'governorGloss': 'miles', 'dependent': 16, 'dependentGloss': 'within'}, {'dep': 'nummod', 'governor': 18, 'governorGloss': 'miles', 'dependent': 17, 'dependentGloss': '25'}, {'dep': 'nmod:within', 'governor': 7, 'governorGloss': 'leads', 'dependent': 18, 'dependentGloss': 'miles'}, {'dep': 'case', 'governor': 22, 'governorGloss': 'points', 'dependent': 19, 'dependentGloss': 'of'}, {'dep': 'compound', 'governor': 22, 'governorGloss': 'points', 'dependent': 20, 'dependentGloss': 'hunter'}, {'dep': 'compound', 'governor': 22, 'governorGloss': 'points', 'dependent': 21, 'dependentGloss': 'access'}, {'dep': 'nmod:of', 'governor': 18, 'governorGloss': 'miles', 'dependent': 22, 'dependentGloss': 'points'}, {'dep': 'case', 'governor': 24, 'governorGloss': 'roads', 'dependent': 23, 'dependentGloss': 'like'}, {'dep': 'nmod:like', 'governor': 22, 'governorGloss': 'points', 'dependent': 24, 'dependentGloss': 'roads'}, {'dep': 'cc', 'governor': 24, 'governorGloss': 'roads', 'dependent': 25, 'dependentGloss': 'and'}, {'dep': 'conj:and', 'governor': 24, 'governorGloss': 'roads', 'dependent': 26, 'dependentGloss': 'towns'}, {'dep': 'punct', 'governor': 2, 'governorGloss': 'found', 'dependent': 27, 'dependentGloss': '.'}], 'collapsed-ccprocessed-dependencies': [{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 2, 'dependentGloss': 'found'}, {'dep': 'nsubj', 'governor': 2, 'governorGloss': 'found', 'dependent': 1, 'dependentGloss': 'Researchers'}, {'dep': 'mark', 'governor': 7, 'governorGloss': 'leads', 'dependent': 3, 'dependentGloss': 'that'}, {'dep': 'nsubj', 'governor': 7, 'governorGloss': 'leads', 'dependent': 4, 'dependentGloss': 'hunting'}, {'dep': 'case', 'governor': 6, 'governorGloss': 'average', 'dependent': 5, 'dependentGloss': 'on'}, {'dep': 'nmod:on', 'governor': 4, 'governorGloss': 'hunting', 'dependent': 6, 'dependentGloss': 'average'}, {'dep': 'ccomp', 'governor': 2, 'governorGloss': 'found', 'dependent': 7, 'dependentGloss': 'leads'}, {'dep': 'case', 'governor': 12, 'governorGloss': 'reduction', 'dependent': 8, 'dependentGloss': 'to'}, {'dep': 'det', 'governor': 12, 'governorGloss': 'reduction', 'dependent': 9, 'dependentGloss': 'an'}, {'dep': 'compound', 'governor': 11, 'governorGloss': '%', 'dependent': 10, 'dependentGloss': '83'}, {'dep': 'amod', 'governor': 12, 'governorGloss': 'reduction', 'dependent': 11, 'dependentGloss': '%'}, {'dep': 'nmod:to', 'governor': 7, 'governorGloss': 'leads', 'dependent': 12, 'dependentGloss': 'reduction'}, {'dep': 'case', 'governor': 15, 'governorGloss': 'populations', 'dependent': 13, 'dependentGloss': 'in'}, {'dep': 'compound', 'governor': 15, 'governorGloss': 'populations', 'dependent': 14, 'dependentGloss': 'mammal'}, {'dep': 'nmod:in', 'governor': 12, 'governorGloss': 'reduction', 'dependent': 15, 'dependentGloss': 'populations'}, {'dep': 'case', 'governor': 18, 'governorGloss': 'miles', 'dependent': 16, 'dependentGloss': 'within'}, {'dep': 'nummod', 'governor': 18, 'governorGloss': 'miles', 'dependent': 17, 'dependentGloss': '25'}, {'dep': 'nmod:within', 'governor': 7, 'governorGloss': 'leads', 'dependent': 18, 'dependentGloss': 'miles'}, {'dep': 'case', 'governor': 22, 'governorGloss': 'points', 'dependent': 19, 'dependentGloss': 'of'}, {'dep': 'compound', 'governor': 22, 'governorGloss': 'points', 'dependent': 20, 'dependentGloss': 'hunter'}, {'dep': 'compound', 'governor': 22, 'governorGloss': 'points', 'dependent': 21, 'dependentGloss': 'access'}, {'dep': 'nmod:of', 'governor': 18, 'governorGloss': 'miles', 'dependent': 22, 'dependentGloss': 'points'}, {'dep': 'case', 'governor': 24, 'governorGloss': 'roads', 'dependent': 23, 'dependentGloss': 'like'}, {'dep': 'nmod:like', 'governor': 22, 'governorGloss': 'points', 'dependent': 24, 'dependentGloss': 'roads'}, {'dep': 'cc', 'governor': 24, 'governorGloss': 'roads', 'dependent': 25, 'dependentGloss': 'and'}, {'dep': 'nmod:like', 'governor': 22, 'governorGloss': 'points', 'dependent': 26, 'dependentGloss': 'towns'}, {'dep': 'conj:and', 'governor': 24, 'governorGloss': 'roads', 'dependent': 26, 'dependentGloss': 'towns'}, {'dep': 'punct', 'governor': 2, 'governorGloss': 'found', 'dependent': 27, 'dependentGloss': '.'}], 'tokens': [{'index': 1, 'word': 'Researchers', 'originalText': 'Researchers', 'characterOffsetBegin': 0, 'characterOffsetEnd': 11, 'pos': 'NNS', 'before': '', 'after': ' '}, {'index': 2, 'word': 'found', 'originalText': 'found', 'characterOffsetBegin': 12, 'characterOffsetEnd': 17, 'pos': 'VBD', 'before': ' ', 'after': ' '}, {'index': 3, 'word': 'that', 'originalText': 'that', 'characterOffsetBegin': 18, 'characterOffsetEnd': 22, 'pos': 'IN', 'before': ' ', 'after': ' '}, {'index': 4, 'word': 'hunting', 'originalText': 'hunting', 'characterOffsetBegin': 23, 'characterOffsetEnd': 30, 'pos': 'NN', 'before': ' ', 'after': ' '}, {'index': 5, 'word': 'on', 'originalText': 'on', 'characterOffsetBegin': 31, 'characterOffsetEnd': 33, 'pos': 'IN', 'before': ' ', 'after': ' '}, {'index': 6, 'word': 'average', 'originalText': 'average', 'characterOffsetBegin': 34, 'characterOffsetEnd': 41, 'pos': 'NN', 'before': ' ', 'after': ' '}, {'index': 7, 'word': 'leads', 'originalText': 'leads', 'characterOffsetBegin': 42, 'characterOffsetEnd': 47, 'pos': 'VBZ', 'before': ' ', 'after': ' '}, {'index': 8, 'word': 'to', 'originalText': 'to', 'characterOffsetBegin': 48, 'characterOffsetEnd': 50, 'pos': 'TO', 'before': ' ', 'after': ' '}, {'index': 9, 'word': 'an', 'originalText': 'an', 'characterOffsetBegin': 51, 'characterOffsetEnd': 53, 'pos': 'DT', 'before': ' ', 'after': ' '}, {'index': 10, 'word': '83', 'originalText': '83', 'characterOffsetBegin': 54, 'characterOffsetEnd': 56, 'pos': 'CD', 'before': ' ', 'after': ''}, {'index': 11, 'word': '%', 'originalText': '%', 'characterOffsetBegin': 56, 'characterOffsetEnd': 57, 'pos': 'NN', 'before': '', 'after': ' '}, {'index': 12, 'word': 'reduction', 'originalText': 'reduction', 'characterOffsetBegin': 58, 'characterOffsetEnd': 67, 'pos': 'NN', 'before': ' ', 'after': ' '}, {'index': 13, 'word': 'in', 'originalText': 'in', 'characterOffsetBegin': 68, 'characterOffsetEnd': 70, 'pos': 'IN', 'before': ' ', 'after': ' '}, {'index': 14, 'word': 'mammal', 'originalText': 'mammal', 'characterOffsetBegin': 71, 'characterOffsetEnd': 77, 'pos': 'NN', 'before': ' ', 'after': ' '}, {'index': 15, 'word': 'populations', 'originalText': 'populations', 'characterOffsetBegin': 78, 'characterOffsetEnd': 89, 'pos': 'NNS', 'before': ' ', 'after': ' '}, {'index': 16, 'word': 'within', 'originalText': 'within', 'characterOffsetBegin': 90, 'characterOffsetEnd': 96, 'pos': 'IN', 'before': ' ', 'after': ' '}, {'index': 17, 'word': '25', 'originalText': '25', 'characterOffsetBegin': 97, 'characterOffsetEnd': 99, 'pos': 'CD', 'before': ' ', 'after': ' '}, {'index': 18, 'word': 'miles', 'originalText': 'miles', 'characterOffsetBegin': 100, 'characterOffsetEnd': 105, 'pos': 'NNS', 'before': ' ', 'after': ' '}, {'index': 19, 'word': 'of', 'originalText': 'of', 'characterOffsetBegin': 106, 'characterOffsetEnd': 108, 'pos': 'IN', 'before': ' ', 'after': ' '}, {'index': 20, 'word': 'hunter', 'originalText': 'hunter', 'characterOffsetBegin': 109, 'characterOffsetEnd': 115, 'pos': 'NN', 'before': ' ', 'after': ' '}, {'index': 21, 'word': 'access', 'originalText': 'access', 'characterOffsetBegin': 116, 'characterOffsetEnd': 122, 'pos': 'NN', 'before': ' ', 'after': ' '}, {'index': 22, 'word': 'points', 'originalText': 'points', 'characterOffsetBegin': 123, 'characterOffsetEnd': 129, 'pos': 'NNS', 'before': ' ', 'after': ' '}, {'index': 23, 'word': 'like', 'originalText': 'like', 'characterOffsetBegin': 130, 'characterOffsetEnd': 134, 'pos': 'IN', 'before': ' ', 'after': ' '}, {'index': 24, 'word': 'roads', 'originalText': 'roads', 'characterOffsetBegin': 135, 'characterOffsetEnd': 140, 'pos': 'NNS', 'before': ' ', 'after': ' '}, {'index': 25, 'word': 'and', 'originalText': 'and', 'characterOffsetBegin': 141, 'characterOffsetEnd': 144, 'pos': 'CC', 'before': ' ', 'after': ' '}, {'index': 26, 'word': 'towns', 'originalText': 'towns', 'characterOffsetBegin': 145, 'characterOffsetEnd': 150, 'pos': 'NNS', 'before': ' ', 'after': ''}, {'index': 27, 'word': '.', 'originalText': '.', 'characterOffsetBegin': 150, 'characterOffsetEnd': 151, 'pos': '.', 'before': '', 'after': ' '}]}, {'index': 1, 'parse': '(ROOT\r\n  (S\r\n    (PP (IN Of)\r\n      (NP (NN course)))\r\n    (, ,)\r\n    (NP (NN hunting))\r\n    (VP (VBZ is)\r\n      (ADVP (RB far)\r\n        (PP (IN from)\r\n          (NP (DT the) (JJ only) (NN threat))))\r\n      (VP (VBN faced)\r\n        (PP (IN by)\r\n          (NP (JJ vulnerable) (NNS animals)))\r\n        (PP (IN across)\r\n          (NP (DT the) (NN globe)))))\r\n    (. .)))', 'basic-dependencies': [{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 11, 'dependentGloss': 'faced'}, {'dep': 'case', 'governor': 2, 'governorGloss': 'course', 'dependent': 1, 'dependentGloss': 'Of'}, {'dep': 'nmod', 'governor': 11, 'governorGloss': 'faced', 'dependent': 2, 'dependentGloss': 'course'}, {'dep': 'punct', 'governor': 11, 'governorGloss': 'faced', 'dependent': 3, 'dependentGloss': ','}, {'dep': 'nsubjpass', 'governor': 11, 'governorGloss': 'faced', 'dependent': 4, 'dependentGloss': 'hunting'}, {'dep': 'auxpass', 'governor': 11, 'governorGloss': 'faced', 'dependent': 5, 'dependentGloss': 'is'}, {'dep': 'advmod', 'governor': 11, 'governorGloss': 'faced', 'dependent': 6, 'dependentGloss': 'far'}, {'dep': 'case', 'governor': 10, 'governorGloss': 'threat', 'dependent': 7, 'dependentGloss': 'from'}, {'dep': 'det', 'governor': 10, 'governorGloss': 'threat', 'dependent': 8, 'dependentGloss': 'the'}, {'dep': 'amod', 'governor': 10, 'governorGloss': 'threat', 'dependent': 9, 'dependentGloss': 'only'}, {'dep': 'nmod', 'governor': 6, 'governorGloss': 'far', 'dependent': 10, 'dependentGloss': 'threat'}, {'dep': 'case', 'governor': 14, 'governorGloss': 'animals', 'dependent': 12, 'dependentGloss': 'by'}, {'dep': 'amod', 'governor': 14, 'governorGloss': 'animals', 'dependent': 13, 'dependentGloss': 'vulnerable'}, {'dep': 'nmod', 'governor': 11, 'governorGloss': 'faced', 'dependent': 14, 'dependentGloss': 'animals'}, {'dep': 'case', 'governor': 17, 'governorGloss': 'globe', 'dependent': 15, 'dependentGloss': 'across'}, {'dep': 'det', 'governor': 17, 'governorGloss': 'globe', 'dependent': 16, 'dependentGloss': 'the'}, {'dep': 'nmod', 'governor': 11, 'governorGloss': 'faced', 'dependent': 17, 'dependentGloss': 'globe'}, {'dep': 'punct', 'governor': 11, 'governorGloss': 'faced', 'dependent': 18, 'dependentGloss': '.'}], 'collapsed-dependencies': [{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 11, 'dependentGloss': 'faced'}, {'dep': 'case', 'governor': 2, 'governorGloss': 'course', 'dependent': 1, 'dependentGloss': 'Of'}, {'dep': 'nmod:of', 'governor': 11, 'governorGloss': 'faced', 'dependent': 2, 'dependentGloss': 'course'}, {'dep': 'punct', 'governor': 11, 'governorGloss': 'faced', 'dependent': 3, 'dependentGloss': ','}, {'dep': 'nsubjpass', 'governor': 11, 'governorGloss': 'faced', 'dependent': 4, 'dependentGloss': 'hunting'}, {'dep': 'auxpass', 'governor': 11, 'governorGloss': 'faced', 'dependent': 5, 'dependentGloss': 'is'}, {'dep': 'case', 'governor': 10, 'governorGloss': 'threat', 'dependent': 6, 'dependentGloss': 'far'}, {'dep': 'mwe', 'governor': 6, 'governorGloss': 'far', 'dependent': 7, 'dependentGloss': 'from'}, {'dep': 'det', 'governor': 10, 'governorGloss': 'threat', 'dependent': 8, 'dependentGloss': 'the'}, {'dep': 'amod', 'governor': 10, 'governorGloss': 'threat', 'dependent': 9, 'dependentGloss': 'only'}, {'dep': 'nmod:far_from', 'governor': 11, 'governorGloss': 'faced', 'dependent': 10, 'dependentGloss': 'threat'}, {'dep': 'case', 'governor': 14, 'governorGloss': 'animals', 'dependent': 12, 'dependentGloss': 'by'}, {'dep': 'amod', 'governor': 14, 'governorGloss': 'animals', 'dependent': 13, 'dependentGloss': 'vulnerable'}, {'dep': 'nmod:agent', 'governor': 11, 'governorGloss': 'faced', 'dependent': 14, 'dependentGloss': 'animals'}, {'dep': 'case', 'governor': 17, 'governorGloss': 'globe', 'dependent': 15, 'dependentGloss': 'across'}, {'dep': 'det', 'governor': 17, 'governorGloss': 'globe', 'dependent': 16, 'dependentGloss': 'the'}, {'dep': 'nmod:across', 'governor': 11, 'governorGloss': 'faced', 'dependent': 17, 'dependentGloss': 'globe'}, {'dep': 'punct', 'governor': 11, 'governorGloss': 'faced', 'dependent': 18, 'dependentGloss': '.'}], 'collapsed-ccprocessed-dependencies': [{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 11, 'dependentGloss': 'faced'}, {'dep': 'case', 'governor': 2, 'governorGloss': 'course', 'dependent': 1, 'dependentGloss': 'Of'}, {'dep': 'nmod:of', 'governor': 11, 'governorGloss': 'faced', 'dependent': 2, 'dependentGloss': 'course'}, {'dep': 'punct', 'governor': 11, 'governorGloss': 'faced', 'dependent': 3, 'dependentGloss': ','}, {'dep': 'nsubjpass', 'governor': 11, 'governorGloss': 'faced', 'dependent': 4, 'dependentGloss': 'hunting'}, {'dep': 'auxpass', 'governor': 11, 'governorGloss': 'faced', 'dependent': 5, 'dependentGloss': 'is'}, {'dep': 'case', 'governor': 10, 'governorGloss': 'threat', 'dependent': 6, 'dependentGloss': 'far'}, {'dep': 'mwe', 'governor': 6, 'governorGloss': 'far', 'dependent': 7, 'dependentGloss': 'from'}, {'dep': 'det', 'governor': 10, 'governorGloss': 'threat', 'dependent': 8, 'dependentGloss': 'the'}, {'dep': 'amod', 'governor': 10, 'governorGloss': 'threat', 'dependent': 9, 'dependentGloss': 'only'}, {'dep': 'nmod:far_from', 'governor': 11, 'governorGloss': 'faced', 'dependent': 10, 'dependentGloss': 'threat'}, {'dep': 'case', 'governor': 14, 'governorGloss': 'animals', 'dependent': 12, 'dependentGloss': 'by'}, {'dep': 'amod', 'governor': 14, 'governorGloss': 'animals', 'dependent': 13, 'dependentGloss': 'vulnerable'}, {'dep': 'nmod:agent', 'governor': 11, 'governorGloss': 'faced', 'dependent': 14, 'dependentGloss': 'animals'}, {'dep': 'case', 'governor': 17, 'governorGloss': 'globe', 'dependent': 15, 'dependentGloss': 'across'}, {'dep': 'det', 'governor': 17, 'governorGloss': 'globe', 'dependent': 16, 'dependentGloss': 'the'}, {'dep': 'nmod:across', 'governor': 11, 'governorGloss': 'faced', 'dependent': 17, 'dependentGloss': 'globe'}, {'dep': 'punct', 'governor': 11, 'governorGloss': 'faced', 'dependent': 18, 'dependentGloss': '.'}], 'tokens': [{'index': 1, 'word': 'Of', 'originalText': 'Of', 'characterOffsetBegin': 152, 'characterOffsetEnd': 154, 'pos': 'IN', 'before': ' ', 'after': ' '}, {'index': 2, 'word': 'course', 'originalText': 'course', 'characterOffsetBegin': 155, 'characterOffsetEnd': 161, 'pos': 'NN', 'before': ' ', 'after': ''}, {'index': 3, 'word': ',', 'originalText': ',', 'characterOffsetBegin': 161, 'characterOffsetEnd': 162, 'pos': ',', 'before': '', 'after': ' '}, {'index': 4, 'word': 'hunting', 'originalText': 'hunting', 'characterOffsetBegin': 163, 'characterOffsetEnd': 170, 'pos': 'NN', 'before': ' ', 'after': ' '}, {'index': 5, 'word': 'is', 'originalText': 'is', 'characterOffsetBegin': 171, 'characterOffsetEnd': 173, 'pos': 'VBZ', 'before': ' ', 'after': ' '}, {'index': 6, 'word': 'far', 'originalText': 'far', 'characterOffsetBegin': 174, 'characterOffsetEnd': 177, 'pos': 'RB', 'before': ' ', 'after': ' '}, {'index': 7, 'word': 'from', 'originalText': 'from', 'characterOffsetBegin': 178, 'characterOffsetEnd': 182, 'pos': 'IN', 'before': ' ', 'after': ' '}, {'index': 8, 'word': 'the', 'originalText': 'the', 'characterOffsetBegin': 183, 'characterOffsetEnd': 186, 'pos': 'DT', 'before': ' ', 'after': ' '}, {'index': 9, 'word': 'only', 'originalText': 'only', 'characterOffsetBegin': 187, 'characterOffsetEnd': 191, 'pos': 'JJ', 'before': ' ', 'after': ' '}, {'index': 10, 'word': 'threat', 'originalText': 'threat', 'characterOffsetBegin': 192, 'characterOffsetEnd': 198, 'pos': 'NN', 'before': ' ', 'after': ' '}, {'index': 11, 'word': 'faced', 'originalText': 'faced', 'characterOffsetBegin': 199, 'characterOffsetEnd': 204, 'pos': 'VBN', 'before': ' ', 'after': ' '}, {'index': 12, 'word': 'by', 'originalText': 'by', 'characterOffsetBegin': 205, 'characterOffsetEnd': 207, 'pos': 'IN', 'before': ' ', 'after': ' '}, {'index': 13, 'word': 'vulnerable', 'originalText': 'vulnerable', 'characterOffsetBegin': 208, 'characterOffsetEnd': 218, 'pos': 'JJ', 'before': ' ', 'after': ' '}, {'index': 14, 'word': 'animals', 'originalText': 'animals', 'characterOffsetBegin': 219, 'characterOffsetEnd': 226, 'pos': 'NNS', 'before': ' ', 'after': ' '}, {'index': 15, 'word': 'across', 'originalText': 'across', 'characterOffsetBegin': 227, 'characterOffsetEnd': 233, 'pos': 'IN', 'before': ' ', 'after': ' '}, {'index': 16, 'word': 'the', 'originalText': 'the', 'characterOffsetBegin': 234, 'characterOffsetEnd': 237, 'pos': 'DT', 'before': ' ', 'after': ' '}, {'index': 17, 'word': 'globe', 'originalText': 'globe', 'characterOffsetBegin': 238, 'characterOffsetEnd': 243, 'pos': 'NN', 'before': ' ', 'after': ''}, {'index': 18, 'word': '.', 'originalText': '.', 'characterOffsetBegin': 243, 'characterOffsetEnd': 244, 'pos': '.', 'before': '', 'after': ' '}]}]}
```

Anyone experienced this issue before ? I am using CoreNLP 3.6 version.

"
369,https://github.com/stanfordnlp/CoreNLP/issues/469,469,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 706065874, 'node_id': 'MDU6TGFiZWw3MDYwNjU4NzQ=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/cleanxml', 'name': 'cleanxml', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2017-06-23 05:27:52+00:00,,CleanXmlAnnotator Exception,"For these two documents (from kbp 2016 source collection):
```
LDC2016E63_TAC_KBP_2016_Evaluation_Source_Corpus_V1.1/data/cmn/df/CMN_DF_000191_20160310_G00A0CFR9.xml
LDC2016E63_TAC_KBP_2016_Evaluation_Source_Corpus_V1.1/data/cmn/df/CMN_DF_000311_20160317_G00A0DM2Y.xml
```

I got the following exception:
```
Exception in thread ""main"" java.lang.IllegalArgumentException: Got a close tag a which does not match any open tag
        at edu.stanford.nlp.pipeline.CleanXmlAnnotator.process(CleanXmlAnnotator.java:832)
        at edu.stanford.nlp.pipeline.CleanXmlAnnotator.annotate(CleanXmlAnnotator.java:345)
        at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:599)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:609)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1172)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:945)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1274)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1345)
```

But it looks like 'tag a' is aligned perfect in these two documents."
370,https://github.com/stanfordnlp/CoreNLP/issues/471,471,[],closed,2017-06-29 03:06:17+00:00,,Chinese coref model not work,I use coref model to deal with chineseÔºåthe properties is use defaultÔºåbut don't find result 
371,https://github.com/stanfordnlp/CoreNLP/issues/472,472,[],closed,2017-06-30 07:36:53+00:00,,NER and TokensRegex not working together in StanfordCoreNLPServer 3.8.0,"(copying over my question from StackOverflow in the hope to get a quicker answer)

In brief: I have observed that `StanfordCoreNLP` and `StanfordCoreNLPServer` (v.3.8.0) work differently when annotators include both `ner` and `tokensregex`. More specifically, in the server mode results of `tokensregex` and `ner` are not combined and only results of `ner` are outputted.

Now description at length:

The server `StanfordCoreNLPServer` is started with `-serverProperties` option pointing to the file with the following content:

    annotators = tokenize,ssplit,pos,lemma,ner,tokensregex

    pos.model = edu/stanford/nlp/models/pos-tagger/german/german-hgc.tagger

    tokenize.language = de
    tokenize.options = normalizeCurrency=false

    ner.model = edu/stanford/nlp/models/ner/german.conll.hgc_175m_600.crf.ser.gz
    ner.applyNumericClassifiers = false
    ner.useSUTime = false

    customAnnotatorClass.tokensregex = edu.stanford.nlp.pipeline.TokensRegexAnnotator
    tokensregex.rules = deu/ner.money.tre.txt

The rule is deliberately made super simple and dumb. There is nothing else but these lines in the file `deu/ner.money.tre.txt`:

    ne = { type: ""CLASS"", value: ""edu.stanford.nlp.ling.CoreAnnotations$NamedEntityTagAnnotation"" }

    { ruleType: ""tokens"",
      pattern: ( /Millionen/ ),
      action: ( Annotate($0, ne, ""HUGE"") ) }

The text snippet to tag is as follows:

     mit einem Gesamtwert von mehr als ‚Ç¨ 5 Millionen in Frankfurt am Main .

Experiment 1. Console `StanfordCoreNLP` works as expected outputting both `ner` and `tokensregex` tags. The option `-props` points to the file with properties as described above.

    java -cp ""*:/path/to/models-2017-06-09/*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -props ./deu/StanfordCoreNLP-deu.properties -file text.in -outputFormat json

grepped output contains all correct NEs

    ...
    { ""word"": ""Millionen"", ""ner"": ""HUGE"" },
    ...
    { ""word"": ""Frankfurt"", ""ner"": ""I-LOC"" },
    { ""word"": ""am"",        ""ner"": ""I-LOC"" },
    { ""word"": ""Main"",      ""ner"": ""I-LOC"" },
    ...

Experiment 2. The goal of this experiment is to determine whether `StanfordCoreNLPServer` is able of assigning tags by `tokensregex`. I can confirm it does.

The server is sent properties as below together in the request.

    {
      ""pipelineLanguage"" : ""german"",
      ""annotators"" : ""tokenize,ssplit,pos,tokensregex"",
      ""outputFormat"" : ""json"",
      ""tokenize.whitespace"" : ""true"",
      ""ssplit.eolonly"" : ""true""
    }

As can be seen, the returned json contains tag `HUGE` assigned by `tokensregex`

    ...
    { ""word"": ""Millionen"", ""ner"": ""HUGE"" },
    ...
    { ""word"": ""Frankfurt"", ""ner"": null },
    { ""word"": ""am"",        ""ner"": null },
    { ""word"": ""Main"",      ""ner"": null },
    ...

Similarly, all three `I-LOC` are assigned when `ner` is used solely, without `tokensregex`.

Experiment 3. Looks like the *combination* of `ner` and `tokensregex` does not work in server mode. The server is sent the request that contains properties like this:

    {
      ""pipelineLanguage"" : ""german"",
      ""annotators"" : ""tokenize,ssplit,pos,ner,tokensregex"",
      ""outputFormat"" : ""json"",
      ""tokenize.whitespace"" : ""true"",
      ""ssplit.eolonly"" : ""true""
    }

In the returned json the target word `Millionen` is tagged to as `O` to my great disappointment.

    ...
    { ""word"": ""Millionen"", ""ner"": ""O"" },
    ...
    { ""word"": ""Frankfurt"", ""ner"": ""I-LOC"" },
    { ""word"": ""am"",        ""ner"": ""I-LOC"" },
    { ""word"": ""Main"",      ""ner"": ""I-LOC"" },
    ...

Experiment 4 that blew my mind. Turns out that giving less annotators in the query does produce correct results that combine both `ner` and `tokensregex` tags. The query now looks like this:

    {
      ""pipelineLanguage"" : ""german"",
      ""annotators"" : ""ner,tokensregex"",
      ""outputFormat"" : ""json"",
      ""tokenize.whitespace"" : ""true"",
      ""ssplit.eolonly"" : ""true""
    }

My server is started in preload mode with `-preload tokenize,ssplit,pos,lemma,ner,regexner,tokensregex` option. To my understanding this explains why specifying an incomplete list of annotators works. To be on the safe side I would prefer to always send the complete list of annotators in the request to the server.

I have played around with the list of annotators. Suffice it to add `pos` for `tokensregex` tags to be overriden with `O` tag. WTF?

    {
      ""annotators"" : ""pos,ner,tokensregex"",
    }

Is it an issue with `StanfordCoreNLPServer` or am I just doing something wrong?
"
372,https://github.com/stanfordnlp/CoreNLP/issues/474,474,[],closed,2017-07-03 08:22:11+00:00,,How to load CoreNLP for multiple languages,"I'm running one `CoreNLP` instance in my system. I was using english only, so the parser, the taggers worked as expected.

Now I'm using more than a language:

```
‚îú‚îÄ‚îÄ stanford-arabic-corenlp-2017-06-09-models.jar
‚îú‚îÄ‚îÄ stanford-chinese-corenlp-2017-06-09-models.jar
‚îú‚îÄ‚îÄ stanford-corenlp-3.8.0-models.jar
‚îú‚îÄ‚îÄ stanford-corenlp-3.8.0.jar
‚îú‚îÄ‚îÄ stanford-english-corenlp-2017-06-09-models.jar
‚îú‚îÄ‚îÄ stanford-english-kbp-corenlp-2017-06-09-models.jar
‚îú‚îÄ‚îÄ stanford-french-corenlp-2017-06-09-models.jar
‚îú‚îÄ‚îÄ stanford-german-corenlp-2017-06-09-models.jar
‚îú‚îÄ‚îÄ stanford-spanish-corenlp-2017-06-09-models.jar
```

so having more languages, now I'm detecting the language with Google's `CLD2`, and setting up the `CoreNLP` instance for each language as you can see in the logs:

```
LANG ENGLISH en
09:54:24.360 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
09:54:24.382 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
09:54:24.388 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
09:54:25.108 [main] INFO  e.s.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.7 sec].
09:54:25.108 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
09:54:25.110 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
09:54:26.982 [main] INFO  e.s.n.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.8 sec].
09:54:28.185 [main] INFO  e.s.n.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.2 sec].
09:54:28.745 [main] INFO  e.s.n.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.6 sec].
09:54:28.750 [main] INFO  e.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
09:54:29.482 [main] DEBUG e.s.n.l.t.CoreMapExpressionExtractor - Ignoring inactive rule: null
09:54:29.482 [main] DEBUG e.s.n.l.t.CoreMapExpressionExtractor - Ignoring inactive rule: temporal-composite-8:ranges

...

LANG GERMAN de
09:54:29.501 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""ssplit"" with signature [ssplit.newlineIsSentenceBreak:always;tokenize.language:en;] with new annotator with signature [ssplit.newlineIsSentenceBreak:always;tokenize.language:de;]
09:54:29.502 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""tokenize"" with signature [ssplit.newlineIsSentenceBreak:always;tokenize.language:en;] with new annotator with signature [ssplit.newlineIsSentenceBreak:always;tokenize.language:de;]
09:54:29.502 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""pos"" with signature [] with new annotator with signature [pos.model:edu/stanford/nlp/models/pos-tagger/german/german-fast.tagger;]
09:54:29.502 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""ner"" with signature [] with new annotator with signature [ner.model:edu/stanford/nlp/models/ner/german.conll.hgc_175m_600.crf.ser.gz;]
09:54:29.502 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""parse"" with signature [parse.model:edu/stanford/nlp/models/srparser/englishSR.ser.gz;] with new annotator with signature [parse.model:edu/stanford/nlp/models/srparser/germanSR.ser.gz;]
09:54:29.503 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
09:54:29.504 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
09:54:29.504 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
09:54:29.650 [main] INFO  e.s.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/german/german-fast.tagger ... done [0.1 sec].
09:54:29.650 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
09:54:29.650 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
09:54:30.599 [main] INFO  e.s.n.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/german.conll.hgc_175m_600.crf.ser.gz ... done [0.9 sec].
09:54:30.599 [main] INFO  e.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
09:54:32.129 [main] DEBUG e.s.n.l.t.CoreMapExpressionExtractor - Ignoring inactive rule: null
09:54:32.129 [main] DEBUG e.s.n.l.t.CoreMapExpressionExtractor - Ignoring inactive rule: temporal-composite-8:ranges

...

LANG SPANISH es
09:54:32.133 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""ssplit"" with signature [ssplit.newlineIsSentenceBreak:always;tokenize.language:de;] with new annotator with signature [tokenize.language:de;]
09:54:32.133 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""tokenize"" with signature [ssplit.newlineIsSentenceBreak:always;tokenize.language:de;] with new annotator with signature [tokenize.language:de;]
09:54:32.133 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""pos"" with signature [pos.model:edu/stanford/nlp/models/pos-tagger/german/german-fast.tagger;] with new annotator with signature []
09:54:32.133 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""ner"" with signature [ner.model:edu/stanford/nlp/models/ner/german.conll.hgc_175m_600.crf.ser.gz;] with new annotator with signature []
09:54:32.134 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
09:54:32.134 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
09:54:32.134 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
09:54:32.679 [main] INFO  e.s.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.5 sec].
09:54:32.679 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
09:54:32.679 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
09:54:33.518 [main] INFO  e.s.n.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.8 sec].
09:54:34.133 [main] INFO  e.s.n.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].
09:54:34.596 [main] INFO  e.s.n.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
09:54:34.597 [main] INFO  e.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
09:54:34.635 [main] DEBUG e.s.n.l.t.CoreMapExpressionExtractor - Ignoring inactive rule: null
09:54:34.636 [main] DEBUG e.s.n.l.t.CoreMapExpressionExtractor - Ignoring inactive rule: temporal-composite-8:ranges


...

LANG ChineseT zh-Hant
09:54:34.640 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""ssplit"" with signature [tokenize.language:de;] with new annotator with signature [tokenize.language:zh;ssplit.boundaryTokenRegex :[.]|[!?]+|[„ÄÇ]|[ÔºÅÔºü]+;]
09:54:34.641 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""tokenize"" with signature [tokenize.language:de;] with new annotator with signature [tokenize.language:zh;ssplit.boundaryTokenRegex :[.]|[!?]+|[„ÄÇ]|[ÔºÅÔºü]+;]
09:54:34.641 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""pos"" with signature [] with new annotator with signature [pos.model:edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger;]
09:54:34.641 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""ner"" with signature [] with new annotator with signature [ner.applyNumericClassifiers:false;ner.useSUTime:false;ner.model:edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz;]
09:54:34.642 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""parse"" with signature [parse.model:edu/stanford/nlp/models/srparser/germanSR.ser.gz;] with new annotator with signature [parse.model:edu/stanford/nlp/models/lexparser/chineseFactored.ser.gz;]
09:54:34.642 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Registering annotator segment with class edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator
09:54:34.643 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
09:54:45.437 [main] INFO  e.s.nlp.wordseg.ChineseDictionary - Loading Chinese dictionaries from 1 file:
09:54:45.437 [main] INFO  e.s.nlp.wordseg.ChineseDictionary -   edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz
09:54:45.624 [main] INFO  e.s.nlp.wordseg.ChineseDictionary - Done. Unique words in ChineseDictionary is: 423200.
09:54:45.625 [main] INFO  e.s.n.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/segmenter/chinese/ctb.gz ... done [11.0 sec].
09:54:45.625 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
09:54:45.625 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
09:54:51.916 [main] INFO  e.s.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger ... done [6.3 sec].
09:54:51.916 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
09:54:51.916 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
```

This means that I have to keep in memory different instance of `CoreNLP`, one for each language with its options like:

```javascript
var options = {
                lang: detection.code,
                annotators: ""tokenize, ssplit, pos, lemma, ner, parse, dcoref""
            };
```
 
Now, I have seen that `CoreNLP` uses for the supported languages pre-defined properties files in the repo ([here](https://github.com/stanfordnlp/CoreNLP/tree/master/src/edu/stanford/nlp/pipeline), so that I do not need to manually specify those properties for `Chinese`, `Spanish`, `Arabic`, `German`, `French` and `English` of course. By the way I'm passing these options currently for the detected language:

```javascript
/**
         * NLP settings by language
         * @see http://stanfordnlp.github.io/CoreNLP/human-languages.html
        */
        this.langs = {
            // english
            ""en"" : {
                
                ""parse.model"": ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"",

                ""tokenize.language"" : ""en"",
                /**
                 * @see http://stanfordnlp.github.io/CoreNLP/ssplit.html
                 * @param string := always | never | two
                 Whether to treat newlines as sentence breaks. This property has 3 legal values: ‚Äúalways‚Äù, ‚Äúnever‚Äù, or ‚Äútwo‚Äù. The default is ‚Äúnever‚Äù. ‚Äúalways‚Äù means that a newline is always a sentence break (but there still may be multiple sentences per line). This is often appropriate for texts with soft line breaks. ‚Äúnever‚Äù means to ignore newlines for the purpose of sentence splitting. This is appropriate when just the non-whitespace characters should be used to determine sentence breaks. ‚Äútwo‚Äù means that two or more consecutive newlines will be treated as a sentence break. This option can be appropriate when dealing with text with hard line breaking, and a blank line between paragraphs. Note: A side-effect of setting ssplit.newlineIsSentenceBreak to ‚Äútwo‚Äù or ‚Äúalways‚Äù is that the tokenizer will tokenize newlines.
                 */
                ""ssplit.newlineIsSentenceBreak"": ""always""
            },
            // german
            ""de"" : {
                ""pos.model"": ""edu/stanford/nlp/models/pos-tagger/german/german-fast.tagger"",
                ""parse.model"": ""edu/stanford/nlp/models/srparser/germanSR.ser.gz"",
                ""ner.model"":  ""edu/stanford/nlp/models/ner/german.conll.hgc_175m_600.crf.ser.gz"",
                ""tokenize.language"" : ""de"",
                ""ssplit.newlineIsSentenceBreak"": ""always""    
            },
            // spanish
            ""sp"" : {
                ""pos.model"": ""edu/stanford/nlp/models/pos-tagger/spanish/spanish.tagger"",
                ""parse.model"": ""edu/stanford/nlp/models/srparser/spanishSR.ser.gz"",
                ""ner.model"":  ""edu/stanford/nlp/models/ner/spanish.ancora.distsim.s512.crf.ser.gz"",
                ""tokenize.language"" : ""sp"",
                ""ssplit.newlineIsSentenceBreak"": ""always""    
            },
            // french: ner not supported
            ""fr"" : {
                ""parse.model"": ""edu/stanford/nlp/models/srparser/frenchSR.ser.gz"",
                ""tokenize.language"" : ""fr"",
                ""ssplit.newlineIsSentenceBreak"": ""always"" 
            },
            // chinese
            ""zh-Hant"" : {

                ""customAnnotatorClass.segment"" : ""edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator"",
                
                ""segment.model"" : ""edu/stanford/nlp/models/segmenter/chinese/ctb.gz"",
                ""segment.sighanCorporaDict"" : ""edu/stanford/nlp/models/segmenter/chinese"",
                ""segment.serDictionary"" : ""edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz"",
                ""segment.sighanPostProcessing"" : true,
                
                ""ssplit.boundaryTokenRegex "" : ""[.]|[!?]+|[„ÄÇ]|[ÔºÅÔºü]+"",

                ""ner.applyNumericClassifiers"" : false,
                ""ner.useSUTime"" : false,
                ""pos.model"": ""edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger"",
                ""parse.model"": ""edu/stanford/nlp/models/lexparser/chineseFactored.ser.gz"",
                ""ner.model"":  ""edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz"",

                ""tokenize.language"" : ""zh""    
            },
            // arabic
            ""ar"" : {
                ""parse.model"": ""edu/stanford/nlp/models/srparser/arabicSR.ser.gz "",

                ""tokenize.language"" : ""de"",
                ""ssplit.newlineIsSentenceBreak"": ""always""     
            }
        };
```

Is this approach correct? Is there any way to have multiple configurations for a `CoreNLP` instance i.e. a way to load `edu.stanford.nlp.pipeline.StanfordCoreNLP` once?

Thank you."
373,https://github.com/stanfordnlp/CoreNLP/issues/475,475,[],closed,2017-07-04 16:54:32+00:00,,Issues with NER in French,"I'm using `fr` and the following `""annotators"": ""tokenize,ssplit,pos,lemma,ner""`, where my configuration was like

```
""fr"": {
    ""parse.model"": ""edu/stanford/nlp/models/srparser/frenchSR.ser.gz"",
     ""tokenize.language"" : ""fr"",
     ""ssplit.newlineIsSentenceBreak"": ""always"" 
}
 ```

while, by example in spanish `es` I have specified the `ner.model` along with the other models:

```
// spanish
 ""es"" : {
  ""pos.model"": ""edu/stanford/nlp/models/pos-tagger/spanish/spanish.tagger"",
  ""parse.model"": ""edu/stanford/nlp/models/srparser/spanishSR.ser.gz"",
   ""ner.model"":  ""edu/stanford/nlp/models/ner/spanish.ancora.distsim.s512.crf.ser.gz"",
    ""tokenize.language"" : ""es"",
     ""ssplit.newlineIsSentenceBreak"": ""always""    
}
```

So in french NER results seems to be totally wrong. 

![screencapture-localhost-3000-track-63654076-1499186846303](https://user-images.githubusercontent.com/163333/27838264-b4f3c734-60e9-11e7-9afe-60709240c43b.png)

Looking at the default properties for french in this repo's `pipeline` folder, this seems to be confirmed:

```
annotators = tokenize, ssplit, pos, parse

tokenize.language = fr

pos.model = edu/stanford/nlp/models/pos-tagger/french/french.tagger

parse.model = edu/stanford/nlp/models/lexparser/frenchFactored.ser.gz

# dependency parser
depparse.model = edu/stanford/nlp/models/parser/nndep/UD_French.gz
depparse.language = french
```

Does this means that `NER` not supported in `fr`? Any other custom model for french?
```
-rw-r--r--@ 1 loretoparisi  staff      214038 28 Feb 18:47 /root/jollyday-0.5.1.jar
-rw-r--r--@ 1 loretoparisi  staff    14362014 13 Giu 10:50 /root/stanford-arabic-corenlp-2017-06-09-models.jar
-rw-r--r--@ 1 loretoparisi  staff   821613963 13 Giu 10:59 /root/stanford-chinese-corenlp-2017-06-09-models.jar
-rw-rw-r--@ 1 loretoparisi  staff   362527920 12 Giu 21:41 /root/stanford-corenlp-3.8.0-models.jar
-rw-rw-r--@ 1 loretoparisi  staff     8011820 12 Giu 21:40 /root/stanford-corenlp-3.8.0.jar
-rw-r--r--@ 1 loretoparisi  staff  1039009129 13 Giu 11:00 /root/stanford-english-corenlp-2017-06-09-models.jar
-rw-r--r--@ 1 loretoparisi  staff   474001837 13 Giu 10:59 /root/stanford-english-kbp-corenlp-2017-06-09-models.jar
-rw-r--r--@ 1 loretoparisi  staff   138392747 13 Giu 10:53 /root/stanford-french-corenlp-2017-06-09-models.jar
-rw-r--r--@ 1 loretoparisi  staff   128259316 13 Giu 10:52 /root/stanford-german-corenlp-2017-06-09-models.jar
-rw-r--r--@ 1 loretoparisi  staff   213169413 13 Giu 10:54 /root/stanford-spanish-corenlp-2017-06-09-models.jar
```
"
374,https://github.com/stanfordnlp/CoreNLP/issues/477,477,[],open,2017-07-07 20:53:15+00:00,,Chinese Segmenter k-best function,"Does Stanford Chinese Segmenter - v3.8.0 have the function of returning k-best segmentation? I tried some like -- 
bash /stanford/segment.sh ctb input.txt.zh UTF-8 0 > output.txt
bash /stanford/segment.sh ctb input.txt.zh UTF-8 1 > output.txt
bash /stanford/segment.sh ctb input.txt.zh UTF-8 2 > output.txt

and I got all the same segmentation results. Is there anything I did wrong here? 
"
375,https://github.com/stanfordnlp/CoreNLP/issues/478,478,[],closed,2017-07-10 10:25:18+00:00,,"Replacing old annotator ""pos"" with signature [] with new annotator with signature","I have this warning for different annotators like `pos`, `ner`, `tokenize`, `ssplit`, etc:

```
12:22:33.562 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""ssplit"" with signature [ssplit.newlineIsSentenceBreak:always;tokenize.language:en;] with new annotator with signature [tokenize.language:de;ssplit.newlineIsSentenceBreak:always;]
12:22:33.562 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""depparse"" with signature [] with new annotator with signature [depparse.language:german;depparse.model:edu/stanford/nlp/models/parser/nndep/UD_German.gz;]
12:22:33.563 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""tokenize"" with signature [ssplit.newlineIsSentenceBreak:always;tokenize.language:en;] with new annotator with signature [tokenize.language:de;ssplit.newlineIsSentenceBreak:always;]
12:22:33.563 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""pos"" with signature [] with new annotator with signature [pos.model:edu/stanford/nlp/models/pos-tagger/german/german-hgc.tagger;]
12:22:33.563 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""ner"" with signature [] with new annotator with signature [ner.applyNumericClassifiers:false;ner.useSUTime:false;ner.model:edu/stanford/nlp/models/ner/german.conll.hgc_175m_600.crf.ser.gz;]
12:22:33.563 [main] INFO  e.s.nlp.pipeline.AnnotatorPool - Replacing old annotator ""parse"" with signature [parse.model:edu/stanford/nlp/models/srparser/englishSR.ser.gz;] with new annotator with signature [parse.model:edu/stanford/nlp/models/lexparser/germanFactored.ser.gz;]
```

In my configuration, I'm using the default annotators. Following the configurations by language:

```javascript
""en"" : {
                
                ""parse.model"": ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"",

                ""tokenize.language"" : ""en"",
                /**
                 * @see http://stanfordnlp.github.io/CoreNLP/ssplit.html
                 * @param string := always | never | two
                 Whether to treat newlines as sentence breaks. This property has 3 legal values: ‚Äúalways‚Äù, ‚Äúnever‚Äù, or ‚Äútwo‚Äù. The default is ‚Äúnever‚Äù. ‚Äúalways‚Äù means that a newline is always a sentence break (but there still may be multiple sentences per line). This is often appropriate for texts with soft line breaks. ‚Äúnever‚Äù means to ignore newlines for the purpose of sentence splitting. This is appropriate when just the non-whitespace characters should be used to determine sentence breaks. ‚Äútwo‚Äù means that two or more consecutive newlines will be treated as a sentence break. This option can be appropriate when dealing with text with hard line breaking, and a blank line between paragraphs. Note: A side-effect of setting ssplit.newlineIsSentenceBreak to ‚Äútwo‚Äù or ‚Äúalways‚Äù is that the tokenizer will tokenize newlines.
                 */
                ""ssplit.newlineIsSentenceBreak"": ""always""
            },
            // german
            ""de"" : {
                ""pos.model"" : ""edu/stanford/nlp/models/pos-tagger/german/german-hgc.tagger"",
                ""ner.model"" : ""edu/stanford/nlp/models/ner/german.conll.hgc_175m_600.crf.ser.gz"",
                ""ner.applyNumericClassifiers"" : false,
                ""ner.useSUTime"" : false,

                ""parse.model"" : ""edu/stanford/nlp/models/lexparser/germanFactored.ser.gz"",

                // depparse
                ""depparse.model"" : ""edu/stanford/nlp/models/parser/nndep/UD_German.gz"",
                ""depparse.language"" : ""german"",
                ""ssplit.newlineIsSentenceBreak"": ""always""    
            },
            // spanish
            ""es"" : {
                ""tokenize.language"" : ""es"",

                ""pos.model"" : ""edu/stanford/nlp/models/pos-tagger/spanish/spanish-distsim.tagger"",

                ""ner.model"" : ""edu/stanford/nlp/models/ner/spanish.ancora.distsim.s512.crf.ser.gz"",
                ""ner.applyNumericClassifiers"" : false,
                ""ner.useSUTime"" : false,

                ""parse.model"" : ""edu/stanford/nlp/models/lexparser/spanishPCFG.ser.gz"",

                ""depparse.model"" : ""edu/stanford/nlp/models/parser/nndep/UD_Spanish.gz"",
                ""depparse.language"" : ""spanish""

  
            },
            // french: custom ner
            // @see http://lab.kbresearch.nl/static/html/eunews.html
            ""fr"" : {
                ""tokenize.language"" : ""fr"",
                ""pos.model"" : ""edu/stanford/nlp/models/pos-tagger/french/french.tagger"",
                ""parse.model"" : ""edu/stanford/nlp/models/lexparser/frenchFactored.ser.gz"",
                // dependency parser
                ""depparse.model"" : ""edu/stanford/nlp/models/parser/nndep/UD_French.gz"",
                ""depparse.language"" : ""french"",
                ""ssplit.newlineIsSentenceBreak"": ""always"" 
            },
            // chinese
            ""zh-Hant"" : {

                ""customAnnotatorClass.segment"" : ""edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator"",
                
                ""segment.model"" : ""edu/stanford/nlp/models/segmenter/chinese/ctb.gz"",
                ""segment.sighanCorporaDict"" : ""edu/stanford/nlp/models/segmenter/chinese"",
                ""segment.serDictionary"" : ""edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz"",
                ""segment.sighanPostProcessing"" : true,
                
                ""ssplit.boundaryTokenRegex "" : ""[.„ÄÇ]|[!?ÔºÅÔºü]+"",

                ""ner.applyNumericClassifiers"" : false,
                ""ner.useSUTime"" : false,
                ""pos.model"": ""edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger"",
                ""parse.model"": ""edu/stanford/nlp/models/lexparser/chineseFactored.ser.gz"",
                ""ner.model"":  ""edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz"",

                ""tokenize.language"" : ""zh""    
            },
            // chinese default
            ""zh"" : {

                ""customAnnotatorClass.segment"" : ""edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator"",
                
                ""segment.model"" : ""edu/stanford/nlp/models/segmenter/chinese/ctb.gz"",
                ""segment.sighanCorporaDict"" : ""edu/stanford/nlp/models/segmenter/chinese"",
                ""segment.serDictionary"" : ""edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz"",
                ""segment.sighanPostProcessing"" : true,
                
                ""ssplit.boundaryTokenRegex "" : ""[.„ÄÇ]|[!?ÔºÅÔºü]+"",

                ""ner.applyNumericClassifiers"" : false,
                ""ner.useSUTime"" : false,
                ""pos.model"": ""edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger"",
                ""parse.model"": ""edu/stanford/nlp/models/lexparser/chineseFactored.ser.gz"",
                ""ner.model"":  ""edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz"",

                ""tokenize.language"" : ""zh""    
            },
            // arabic
            ""ar"" : {
                // segment
                ""tokenize.language"" : ""ar"",
                ""segment.model"" : ""edu/stanford/nlp/models/segmenter/arabic/arabic-segmenter-atb+bn+arztrain.ser.gz"",

                // sentence split
                ""ssplit.boundaryTokenRegex"" : ""[.]|[!?]+|[!\u061F]+"",

                // pos
                ""pos.model"" : ""edu/stanford/nlp/models/pos-tagger/arabic/arabic.tagger"",

                // parse
                ""parse.model "" : ""edu/stanford/nlp/models/lexparser/arabicFactored.ser.gz""  
            }
        };
```"
376,https://github.com/stanfordnlp/CoreNLP/issues/480,480,[],closed,2017-07-10 18:11:30+00:00,,Russian tagging and parsing models for CoreNLP,"[we previously mailed the same letter to: parser-support@lists.stanford.edu ]

Hi! 

My colleagues and I have implemented Russian models for tagging and dependency parsing in Stanford CoreNLP and kindly ask for advice.

By now we‚Äôve done the following:
1. Tagger:
1.1 The model was trained on 200k tokens sample from Russian parts of parallel corpora for statistical machine translation (at http://statmt.org/). 
Total size of morphologically annotated subcorpus is 10 mln tokens. This subcorpus was annotated using SemSin parser. Each token was ascribed the following morphological information: Lemma + POS tag + morphological features, homonymy was resolved  during parsing. 
SemSin is a slow but comparatively accurate dependency parser (tagging quality is above 0.96, best LAS is slightly above 0.80, both evaluated on news texts) which combines surface syntax and semantic role labels (but both types of labels do not follow any standard). SemSin descriptions are available in Russian language only, some information in English can be found in the abstract of this paper: [http://www.dialog-21.ru/media/1394/kanevsky.pdf].  

1.2 SemSin tagset was mapped to Universal Dependencies (universal POS-tags and universal features)

Results: Russian tagger properties file: ud-tagger.props, trained Russian tagging  model (labels input sentences with POS tags only).

Questions on tagger: 

What is the best way to add information about morphological features and lexemes to the model? How this information should be represented in the training set? We have a large annotated training set with  POS, morphofeatures and lemmas in UD v2 format and want to add all the information to the tagging model, because it is important for analysis of a morphologically rich language, like Russian.

2. Parser
2.1 Following the guidelines on training neural network dependency parser, we wrote RussianMorphoFeatureSpecification, RussianTreebankLanguagePack and several HeadFinders. The last RussianHeadFinder was written for finding heads in trees following UD representation and distinguishing main and subordinate clause, noun and prepositional phrase in Russian has become a problem. So, this version of the RussianHeadFinder  is just a prototype where the structure of only the most frequent phrases is described. 

2.2 To train the parser we used 1) a sample of  12,000 sentences in conll-u format as training set and a sample of 6,500 as dev set from SynTagRus treebank, 2) a small treebank of 1,000 sentences parsed with SemSin, automatically converted to UD and manually checked.

Word embeddings were built using the corpora at http://statmt.org/.

2.3 The following parameters were used for training : 
-tlp edu.stanford.nlp.trees.international.russian.RussianTreebankLanguagePack -trainFile ud-SytTagRus2.conllu -embedFile model400.txt -embeddingSize 100 -model nndep.rus.model.txt.gz -maxIter 20000 -numPreComputed 10000 -batchSize 1000 -dropProb 0.25 -hiddenSize 200 -initRange 0.005  -trainingThreads 4 -evalPerIter 2000 -devFile ru_syntagrus-ud-dev.conllu -language Russian

Results.
UAS = 73,83
LAS = 67,67

At present we continue experiments with larger embeddings size, training set and number of iterations.


Questions.
Is it possible to use morphological features in HeadFinder and as features for parser training? (In PennTreebank tagset different POS tags are used to denote morphological characteristics of words belonging to the same part of speech, but in UD grammatical properties are moved to morphologicalfeatures, which are not used by the parser). Therefore, if we‚Äôre not mistaken, during parser training on data in UD representation, a lot of information, which is useful for building feature templates and POS embeddings for a morphologically rich language, is lost. 


Can we get any guidelines from you to elaborate and improve the models and contribute them to CoreNLP?

Link to project with all mentioned classes and models:
 https://github.com/MANASLU8/CoreNLP 
https://github.com/MANASLU8/CoreNLPRusModels


*********
About us: we are  NLP group in the Laboratory of Information Science and Semantic Technologies at the Department of Informatics and Applied Mathematics, ITMO University, Saint-Petersburg, Russia: http://iam.ifmo.ru/en/. 
Main research areas in NLP: linguistic resources for Russian, ontology population, grammar inference for spoken Russian language, voice interfaces for IoT
"
377,https://github.com/stanfordnlp/CoreNLP/issues/481,481,"[{'id': 706059615, 'node_id': 'MDU6TGFiZWw3MDYwNTk2MTU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/ner', 'name': 'ner', 'color': 'c5def5', 'default': False, 'description': None}, {'id': 706064425, 'node_id': 'MDU6TGFiZWw3MDYwNjQ0MjU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/algorithm-error', 'name': 'algorithm-error', 'color': 'f9d0c4', 'default': False, 'description': None}]",open,2017-07-12 15:38:46+00:00,,Named entity recognition doesn't identify entities that are used as hashtags,"The named entity annotator fails to identify entities that are used as a hashtag. 

For example `I like Jeremy` would correctly identify Jeremy as a named entity. However, `I like #Jeremy` would not.

Is this behaviour intended?

"
378,https://github.com/stanfordnlp/CoreNLP/issues/482,482,[],closed,2017-07-15 15:07:58+00:00,,Issue with Quote Attribution (English).,"Is the property: quoteattribution (`'annotators': 'quoteattribution'`  for English) \ annotator class name: QuoteAttributionAnnotator (https://stanfordnlp.github.io/CoreNLP/quoteattribution.html) not yet supported with the Version 3.8 available on the page? 

I also tried with the code from GitHub (built with Ant, downloaded the model jars to the CoreNLP folder), using `py-corenlp` but I get a `Null Pointer Exception`, details follow :

> Setting quotes.
> java.util.concurrent.ExecutionException: java.lang.NullPointerException
> 	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
> 	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
> 	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:676)
> 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
> 	at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
> 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
> 	at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
> 	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
> 	at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
> 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
> 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
> 	at java.lang.Thread.run(Thread.java:745)
> Caused by: java.lang.NullPointerException
> 	at edu.stanford.nlp.quoteattribution.Sieves.Sieve.createNameMatcher(Sieve.java:128)
> 	at edu.stanford.nlp.quoteattribution.Sieves.Sieve.<init>(Sieve.java:40)
> 	at edu.stanford.nlp.quoteattribution.Sieves.QMSieves.QMSieve.<init>(QMSieve.java:32)
> 	at edu.stanford.nlp.quoteattribution.Sieves.QMSieves.TrigramSieve.<init>(TrigramSieve.java:23)
> 	at edu.stanford.nlp.pipeline.QuoteAttributionAnnotator.getQMMapping(QuoteAttributionAnnotator.java:220)
> 	at edu.stanford.nlp.pipeline.QuoteAttributionAnnotator.annotate(QuoteAttributionAnnotator.java:206)
> 	at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
> 	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:648)
> 	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.lambda$handle$0(StanfordCoreNLPServer.java:658)
> 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
> 	... 3 more
"
379,https://github.com/stanfordnlp/CoreNLP/issues/483,483,[],closed,2017-07-17 20:51:23+00:00,,Adding custom annotators to CoreNLP Server,"I know that custom annotators can be added to CoreNLP, but I can't find a way to enable custom annotators within CoreNLP server. I need to be able to add features such as ClausIE to CoreNLP, and have them available in the same manner OpenIE or NER is usable via a network interface, but I haven't been able to find how. Is it possible to add custom annotators to CoreNLP Server?"
380,https://github.com/stanfordnlp/CoreNLP/issues/484,484,[],closed,2017-07-18 04:38:39+00:00,,chinese segment error,"This is my code:
#coding:utf-8
from nltk.tokenize.stanford_segmenter import StanfordSegmenter
from nltk.tag import StanfordNERTagger
import nltk

segmenter = StanfordSegmenter(
    path_to_jar='/home/kenwood/stanford/segmenter/stanford-segmenter.jar',
    path_to_slf4j='/home/kenwood/stanford/segmenter/slf4j-api.jar',
    path_to_sihan_corpora_dict='/home/kenwood/stanford/segmenter/data',
    path_to_model='/home/kenwood/stanford/segmenter/data/pku.gz',
    path_to_dict='/home/kenwood/stanford/segmenter/data/dict-chris6.ser.gz'
)

chi_tagger = StanfordNERTagger(path_to_jar='/home/kenwood/stanford/ner/stanford-ner.jar',model_filename='/home/kenwood/stanford/ner/classifiers/chinese.misc.distsim.crf.ser.gz')

sentence = ""ËøôÊòØÊñØÂù¶Á¶è‰∏≠ÊñáÂàÜËØçÂô®ÊµãËØï""
result = segmenter.segment(sentence)
print (result)
 
The error message :
Traceback (most recent call last):
  File ""/home/kenwood/PycharmProjects/wordcut/toolpackage/ntlk_module.py"", line 17, in <module>
    result = segmenter.segment(sentence)
  File ""/usr/lib/python3.6/site-packages/nltk/tokenize/stanford_segmenter.py"", line 164, in segment
    return self.segment_sents([tokens])
  File ""/usr/lib/python3.6/site-packages/nltk/tokenize/stanford_segmenter.py"", line 192, in segment_sents
    stdout = self._execute(cmd)
  File ""/usr/lib/python3.6/site-packages/nltk/tokenize/stanford_segmenter.py"", line 211, in _execute
    stdout, _stderr = java(cmd, classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE)
  File ""/usr/lib/python3.6/site-packages/nltk/internals.py"", line 129, in java
    p = subprocess.Popen(cmd, stdin=stdin, stdout=stdout, stderr=stderr)
  File ""/usr/lib/python3.6/subprocess.py"", line 707, in __init__
    restore_signals, start_new_session)
  File ""/usr/lib/python3.6/subprocess.py"", line 1260, in _execute_child
    restore_signals, start_new_session, preexec_fn)
TypeError: expected str, bytes or os.PathLike object, not NoneType

Someone can help me? thank you!
"
381,https://github.com/stanfordnlp/CoreNLP/issues/485,485,[],closed,2017-07-18 14:06:57+00:00,,NLP not able to identify billion or million or Rupees given in short form ,"Hi,
Considering the below sentences for example:
1) software company turn over is  **$ 1.8 bln** for the current year.
2) software company turn over is **Rs16,189-crore** ($ 2.5 billion) previous year.
3) software company turn over is expected to be **$ 728 mil** next year.
4) Over all profit seen in this year is **RM10 million**.

**NLP is not able to identify** 
- bln as Billion
- Rs16,189-crore as money itself
- mil as million
- RM10 million as money iteself.

NLP identifying Money entities just as $1.8, $728 without billion and million
Other two cases of RM10 million and Rs16,189-crore is not getting identified as Money at all
If full form like billion/million is given, it is able to identify. 

Could you please suggest a best solution that we can try out to make these cases success. 
Thank you in advance."
382,https://github.com/stanfordnlp/CoreNLP/issues/486,486,[],closed,2017-07-24 18:18:03+00:00,,Inconsistent NER execution,"I'm using the latest version of CoreNLP server in English, with 7 gigs of memory available. The following sentence has parsing issues: 

'In ""Desperate Living""; a crazy woman and her maid accidentally kill the crazy woman's husband, and faced with jail or Mortville, opt to live in Mortville, which is populated by criminals, perverts, nudists, and the evil corrupt cops who serve the fascist evil ruler of Mortville, Queen Carlota.'

The word ""Mortville"" is not tagged consistently with NER. The first time it's tagged, it's tagged as a person; but the other two times, it's tagged as a location. It's a location. The NER results should be internally consistent."
383,https://github.com/stanfordnlp/CoreNLP/issues/487,487,"[{'id': 706064425, 'node_id': 'MDU6TGFiZWw3MDYwNjQ0MjU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/algorithm-error', 'name': 'algorithm-error', 'color': 'f9d0c4', 'default': False, 'description': None}, {'id': 711772827, 'node_id': 'MDU6TGFiZWw3MTE3NzI4Mjc=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/sentiment', 'name': 'sentiment', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2017-07-25 13:45:45+00:00,,Sentiment dependency on tense,"The sentiment for sentence :-

> the movie itself is excellent but i expected more from the director

is positive 

whereas for sentence :-
> the movie itself was excellent but i expected more from the director

is negative, 
just because the tense is changed, whereas in the training data (tress/train.txt) we see the word ""was"" being tagged as neutral 

![screen shot](https://user-images.githubusercontent.com/15106140/28575966-c43111d8-7152-11e7-95e1-028a1ebe2941.png)

![screenshot](https://user-images.githubusercontent.com/15106140/28576083-1eea26c8-7153-11e7-8b41-bd27f65c733b.png)


Could this be related to fact that we have more negative and very negative reviews than positive reviews for the past tense in the training data  ?

y axis - count of reviews , x axis - sentiment from 0 to 4 (very -ve to very +ve)
![image](https://user-images.githubusercontent.com/15106140/28576002-dcf461e8-7152-11e7-80a1-a48fbed79bb3.png)
"
384,https://github.com/stanfordnlp/CoreNLP/issues/488,488,[],closed,2017-07-26 06:14:45+00:00,,online training for NER?,I am curious if online training is possible to implement especially for NER? Is it a matter of writing the code to load models or are there algorithmic or structural reasons for the limitation?
385,https://github.com/stanfordnlp/CoreNLP/issues/489,489,[],closed,2017-07-26 13:20:08+00:00,,Training relation extractor with custom data,"Hello All,
As a test I want to train my own model for extracting relations between entities in the financial domain. So far I have been able to trail a NER-model in order to recognize custom entities. The next step in my project is to recognize the relations. Theoretically, I know how the training process works, but I need some help with the relation data for training and how to integrate all this in stanfordCoreNLP.

**What I know until now:**
1. I need to train a ner-model for recognizing new entities (this step is already implemented and is working fine).
2. I need to create training data in the format IO similar to conll04.corp, e.g.:
1	O	0	O	IN	By	O	O	O
1	Peop	1	O	NNP/NNP	RONI/RABIN	O	O	O
2	Org	0	O	NNP/NNP	Associated/Press	O	O	O
2	O	1	O	NNP	Writer	O	O	O
3	O	0	O	DT	The	O	O	O
3	O	1	O	JJ	long	O	O	O
3	Other	2	O	JJ	Palestinian	O	O	O
3	O	3	O	NN	uprising	O	O	O
3	O	4	O	VBZ	has	O	O	O
3	O	5	O	VBN	brought	O	O	O
3	O	6	O	NN	bitterness	O	O	O
3	O	7	O	TO	to	O	O	O

3. Change use new ""possibleEntities"" attribute in the properties file
4. Re-compile library (?)
5. Train and test your model

**Where do I need help:**
a. Is there a script which could help me to generate the training data layout for the relation extractor. I hope I don't have to create this file manually. And do I run the script in order to get the desired output.

b.  Where do I have to do changes in the original code/classes?

c. After changes are done, do I have to recompile the library indeed?

Please advise.

Thanks "
386,https://github.com/stanfordnlp/CoreNLP/issues/490,490,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 706055902, 'node_id': 'MDU6TGFiZWw3MDYwNTU5MDI=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/tokenize', 'name': 'tokenize', 'color': 'c5def5', 'default': False, 'description': None}]",open,2017-07-26 16:30:04+00:00,,Chinese Segmentation Pipeline IndexOutOfBoundsException ,"Hello all,

The segmentation pipeline seems to output more IndexOutOfBoundsException in latest 3.8.0 version. I got an IndexOutOfBoundsException on some sentences, especially the ones with weird chars like emojis but those used to work in 3.7.0. Here is the stack : 

`java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 220, Size: 220 at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:466) at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:432) at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) at org.apache.storm.daemon.executor$fn__4973$fn__4986$fn__5039.invoke(executor.clj:846) at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) at clojure.lang.AFn.run(AFn.java:22) at java.lang.Thread.run(Thread.java:748) Caused by: java.lang.IndexOutOfBoundsException: Index: 220, Size: 220 at java.util.ArrayList.rangeCheck(ArrayList.java:653) at java.util.ArrayList.get(ArrayList.java:429) at edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator.runSegmentation(ChineseSegmenterAnnotator.java:306) at edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator.doOneSentence(ChineseSegmenterAnnotator.java:124) at edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator.annotate(ChineseSegmenterAnnotator.java:118) at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76) at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:599) `

It happens during annotation of sentences with a pipeline with this conf : 

`""annotators"" ""segment, ssplit, ner""
   ""customAnnotatorClass.segment"" ""edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator""
   ""segment.model"" ""xx/stanford-chinese-corenlp-2017-06-09-models/edu/stanford/nlp/models/segmenter/chinese/ctb.gz""
   ""segment.sighanCorporaDict"" ""xx/stanford-chinese-corenlp-2017-06-09-models/edu/stanford/nlp/models/segmenter/chinese""
   ""segment.serDictionary"" ""xx/stanford-chinese-corenlp-2017-06-09-models/edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz""
   ""segment.sighanPostProcessing"" ""true""
   ""ssplit.boundaryTokenRegex"" ""[.]|[!?]+|[„ÄÇ]|[ÔºÅÔºü]+""
   ""ner.model"" ""xx/stanford-chinese-corenlp-2017-06-09-models/edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz""
   ""ner.applyNumericClassifiers"" ""false""
   ""ner.useSUTime"" ""false""`

In 3.7.0 I usually had warnings like this one 
`ChineseUtils.normalize warning: private use area codepoint U+e310 in ÓåêÊØè‰∏Ä‰∏™ËÆ§Áúü` and few IndexOutOfBoundsException (in my test dump : ~20 Exception over 500 000 sentences), in 3.8.0 it's more ~100 000Exceptions over the same 500 000 sentences. 

It might be an improvement as these sentence cannot definitely be segmented, and an error might be more significant than a warning.

Any advice ? 

Thanks."
387,https://github.com/stanfordnlp/CoreNLP/issues/491,491,[],closed,2017-07-27 13:06:48+00:00,,CorefExample memory issue,"Hello

I have tried running the the [example](https://stanfordnlp.github.io/CoreNLP/coref.html) program that demonstrates the CorefAnnotator functionality.

This program produced an OutOfMemoryError on my (modest) machine.
Is this behavior to be expected for processing such a short String?

stanford-corenlp version used: 3.8.0

Sina

Edit: Resolved issue."
388,https://github.com/stanfordnlp/CoreNLP/issues/492,492,[],closed,2017-07-28 01:59:06+00:00,,How can I get train file of english tagger model?,"I am using CoreNLP to tag word's part of speech in sentence. It's pretty awesome. But some word was tagged inappropriately.
I know how to train tagger model for CoreNLP. But my problem is that I couldn't find 3 files ""train-wsj-0-18"", ""train-extra-english"", ""train-tech-english"". 
How could I get 3 files to train CoreNLP tagger model to add some miss train rule and rebuild tagger model ? Also arch files ""egw4-reut.512.clusters,-1,1""
Thank you very much"
389,https://github.com/stanfordnlp/CoreNLP/issues/493,493,[],closed,2017-07-28 03:25:33+00:00,,question about Chinese NER,"I cloned the CoreNLP source code from GitHub,but there is no model and the path to model.May I need to creat a path,and download models for it.
I want to use Chinese NER function,where can I download all models I need?"
390,https://github.com/stanfordnlp/CoreNLP/issues/494,494,[],closed,2017-07-30 17:29:33+00:00,,"Replacing old annotator ""tokenize"" ..language:es with language:en","I have this code to run corenlp with spanish language. I use the databricks api in scala:

var props: Properties = new Properties()
  props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner"") 
  props.setProperty(""tokenize.language"", ""es"")
  props.setProperty(""tokenize.verbose"", ""true"")
  props.setProperty(""pos.model"", ""edu/stanford/nlp/models/pos-tagger/spanish/spanish-distsim.tagger"")
  props.setProperty(""ner.model"", ""edu/stanford/nlp/models/ner/spanish.ancora.distsim.s512.crf.ser.gz"")
  props.setProperty(""parse.model"", ""edu/stanford/nlp/models/lexparser/spanishPCFG.ser.gz"")
val sentimentPipeline = new StanfordCoreNLP(props)
    val output = df
      .select(explode(ssplit('_c3)).as('sen))
      .select('sen, tokenize('sen).as('words)  , ner('sen).as('nerTags)   )
output.show(truncate = false)


My POM.xml file look like this:

    <dependency>
      <groupId>edu.stanford.nlp</groupId>
      <artifactId>stanford-corenlp</artifactId>
      <version>3.8.0</version>
    </dependency>
    <dependency>
      <groupId>edu.stanford.nlp</groupId>
      <artifactId>stanford-corenlp</artifactId>
      <version>3.8.0</version>
      <classifier>models-spanish</classifier>
    </dependency>
    <dependency>
      <groupId>databricks</groupId>
      <artifactId>spark-corenlp</artifactId>
      <version>0.2.0-s_2.10</version>
    </dependency>

i get this error:
Caused by: java.io.IOException: Unable to open ""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"" as class path, filename or URL

I saw in my log this before the error:

17/07/30 19:03:04 INFO AnnotatorPool: Replacing old annotator ""tokenize"" with signature [tokenize.language:es;tokenize.verbose:true;] with new annotator with signature [ssplit.isOneSentence:true;tokenize.language:en;tokenize.class:PTBTokenizer;]
17/07/30 19:03:04 INFO AnnotatorPool: Replacing old annotator ""ssplit"" with signature [tokenize.language:es;tokenize.verbose:true;] with new annotator with signature [ssplit.isOneSentence:true;tokenize.language:en;tokenize.class:PTBTokenizer;]

I think this is the reason of my error because the language has been replaced ""automatically""¬ø?
thanks
"
391,https://github.com/stanfordnlp/CoreNLP/issues/495,495,[],closed,2017-07-31 07:17:28+00:00,,How to over ride the original entities fetched with RegexNER,"Hi Team,
 In the below sentence
Mega First Corp Bhd has A21 a major stake in a Perak mining company for **RM10 million**

RM10 million is identified as NUMBER by NLP default. However we wrote a regex using RegNER to fetch it as Money. The regex is as follows:

**RM[0-9]{2}\s*million	Money**

But still NLP is identifying it as NUMBER instead of MONEY. Could you please tell how to override this behavior and fetch the value as Money.

Thank you advance."
392,https://github.com/stanfordnlp/CoreNLP/issues/496,496,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 732360245, 'node_id': 'MDU6TGFiZWw3MzIzNjAyNDU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/protobuf', 'name': 'protobuf', 'color': 'c5def5', 'default': False, 'description': None}]",open,2017-08-01 11:59:13+00:00,,CoreNLPProtos::registerAllExtensions method is not actually registering the extensions.,"I was trying to implement serialization of my custom annotations through CoreNLP's ProtoBuf serializer. In the JavaDoc of [`ProtobufAnnotationSerializer`](https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializer.html), it was mentioned that the extension field should be registered with the CoreNLPProtos through the following code:
```
static {
    ExtensionRegistry registry = ExtensionRegistry.newInstance();
    registry.add(MyAppProtos.myNewField);
    CoreNLPProtos.registerAllExtensions(registry);
}
```
The serialization went without problem. However, ProtoBuf fails to recognize the custom fields when deserializing; the custom fields were instead categorized as UnkownFields and could not be read properly when overriding the `ProtobufAnnotationSerializer::fromProtoNoTokens` method.

After some debugging, I found two potential problems:

1. The `CoreNLPProtos::registerAllExtensions` method is in fact empty --- my understanding is that this method should remember the registry passed into it when the above static code block is executed.
2. The `ProtobufAnnotationSerializer::read(InputStream is)` method calls `CoreNLPProtos.Document::parseDelimitedFrom(java.io.InputStream input)` during deserialization. Since `CoreNLPProtos::registerAllExtensions` did nothing, the registry was not available inside `CoreNLPProtos.Document::parseDelimitedFrom(java.io.InputStream input)` and so `com.google.protobuf.GeneratedMessageV3::parseDelimitedWithIOException(arser<M> parser,
      InputStream input)` was called instead of `com.google.protobuf.GeneratedMessageV3::parseDelimitedWithIOException(Parser<M> parser,
      InputStream input, ExtensionRegistryLite extensions)`. The former method initializes an empty extension registry as a default so the information about the user extensions is lost to the underlying ProtoBuf framework.

I could temporarily bypass this issue by keeping the registry when overriding the `ProtobufAnnotationSerializer` and also overriding the `ProtobufAnnotationSerializer::read(InputStream is)` method to pass on the extension registry manually to the `CoreNLPProtos.Document::parseDelimitedFrom` method:

```
	@Override
	public Pair<Annotation, InputStream> read(InputStream is)
			throws IOException, ClassNotFoundException, ClassCastException {
		CoreNLPProtos.Document doc = CoreNLPProtos.Document
				.parseDelimitedFrom(is, registry);
		return Pair.makePair(fromProto(doc), is);
	}
```"
393,https://github.com/stanfordnlp/CoreNLP/issues/497,497,[],closed,2017-08-03 04:14:52+00:00,,Very slow processing time for sentiment annotation,"I am seeing very slow processing times (~3 to 10 seconds or more per string) when using StanfordCoreNLP for sentiment annotation, as was discussed [here](https://mailman.stanford.edu/pipermail/java-nlp-user/2013-January/003024.html) in 2013.

I am using Maven to retrieve stanford-corenlp version 3.8.0 (with models), and the code I'm using is:

`Properties props = new Properties();`
`props.setProperty(""annotators"", ""tokenize,ssplit,parse,sentiment"");`
`StanfordCoreNLP pipeline = new StanfordCoreNLP(props);`
`String text = ""... very long UTF-8 encoded string containing 1000-2000 chars....."";`
`Annotation annotation = pipeline.process(text);`
`List<String> sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class).stream().map(sentence -> sentence.toString() + "" ["" + Sentiment.parse(sentence.get(SentimentCoreAnnotations.SentimentClass.class)).getRepresentation() + ""]"").collect(Collectors.toList());`

Re-using `pipeline.process(text)` to process similarly long strings produces similar results. JVisualVM reports that the most time is spent inside these methods:
                 
`edu.stanford.nlp.parser.lexparser.ExhaustivePCFGParser.doInsideScores()`
`edu.stanford.nlp.parser.lexparser.ExhaustivePCFGParser.doInsideChartCell()`

Am I using the code properly to perform sentiment annotation?"
394,https://github.com/stanfordnlp/CoreNLP/issues/498,498,[],closed,2017-08-03 07:39:54+00:00,,Conditions and Guidelines to make changes to the Standford core NLP project,"Hi Team,

We would like to know if there is any chance that we can do some enhancements or changes to the Standford core NLP project.  
If its allowed 
Will the changes be committed to your core project and released ? 
Any restrictions on the same ?
What are the things to be noted or minded in doing so ?

Could you please give more information.

Thank you in advance.
"
395,https://github.com/stanfordnlp/CoreNLP/issues/499,499,[],closed,2017-08-07 07:18:32+00:00,,Getting Exception in thread ‚Äúmain‚Äù java.lang.ArrayIndexOutOfBoundsException while retraining the stanford sentiment RNN,"
I am trying to retrain the Stanford sentiment RNN with my own dataset which I have converted into PTB format using BuildBinarizedDataset from the Stanford-core-nlp jar (3.8) 

Here are the first few lines of my training data:


(9 (9 (5 very) (7 well)) (5 handled))

(7 (6 engineers) (4 (4 to) (5 (6 answer) (6 (5 very) (3 carefully)))))

(9 (9 (9 (8 fast) (5 and)) (9 reliable)) (9 (4 response) (9 great)))

(8 (7 prompt) (7 (5 responses) (8 (5 fixed) (1 (4 the) (0 issue)))))

This is the command I used :

java -cp ""*"" 

-mx8g edu.stanford.nlp.sentiment.SentimentTraining 

-gradientcheck 

-trainPath train_s.txt -equivalenceClasses 0,1,2,3,4,5;6,7,8,9,10 

-equivalenceClassNames Negative,Neutral,Positive

-numHid 25 

-batchSize 78 

-numClasses 2 

-classNames Negative,Positive 

-train 

-model modeltrial.ser.gz

And this is my exception:


Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 9
   at org.ejml.data.D1Matrix64F.set(Unknown Source)

   at org.ejml.simple.SimpleBase.set(Unknown Source)

   at edu.stanford.nlp.sentiment.SentimentCostAndGradient.backpropDerivativesAndError(SentimentCostAndGradient.java:376)

   at edu.stanford.nlp.sentiment.SentimentCostAndGradient.backpropDerivativesAndError(SentimentCostAndGradient.java:354)

   at edu.stanford.nlp.sentiment.SentimentCostAndGradient.scoreDerivatives(SentimentCostAndGradient.java:223)

   at edu.stanford.nlp.sentiment.SentimentCostAndGradient.calculate(SentimentCostAndGradient.java:249)

   at edu.stanford.nlp.optimization.AbstractCachingDiffFunction.ensure(AbstractCachingDiffFunction.java:140)

   at edu.stanford.nlp.optimization.AbstractCachingDiffFunction.derivativeAt(AbstractCachingDiffFunction.java:151)

   at edu.stanford.nlp.optimization.AbstractCachingDiffFunction.gradientCheck(AbstractCachingDiffFunction.java:39)

   at edu.stanford.nlp.sentiment.SentimentTraining.runGradientCheck(SentimentTraining.java:129)

   at edu.stanford.nlp.sentiment.SentimentTraining.main(SentimentTraining.java:226)


These are the options I am working with

GENERAL OPTIONS


randomSeed=1659449035

wordVectors=null

unkWord=UNK

randomWordVectors=true

numHid=25

numClasses=2

lowercaseWordVectors=false

useTensors=true

simplifiedModel=true

combineClassification=true

classNames=Negative,Positive

equivalenceClasses=0,1,2,3,4,5;6,7,8,9,10

equivalenceClassNames=Negative,Neutral,Positive

TRAIN OPTIONS

batchSize=78

epochs=400

debugOutputEpochs=8

maxTrainTimeSeconds=86400

learningRate=0.01

scalingForInit=1.0

classWeights=null

regTransformMatrix=0.001

regTransformTensor=0.001

regClassification=1.0E-4

regWordVector=1.0E-4

initialAdagradWeight=0.0

adagradResetFrequency=1

shuffleMatrices=true

initialMatrixLogPath=null

nThreads=1

TEST OPTIONS

ngramRecordSize=0

ngramRecordMaximumLength=0

printLengthAccuracies=false

My training data has around 4800 sentences. This is the first time I am working with Stanford core NLP and so far no other links have helped me.
 
stack overflow link : https://stackoverflow.com/questions/45322871/getting-exception-in-thread-main-java-lang-arrayindexoutofboundsexception-whil"
396,https://github.com/stanfordnlp/CoreNLP/issues/500,500,[],closed,2017-08-07 10:44:08+00:00,,Issue in emojis tokenization,"Hello,

I think I might have spotted an issue in the tokenization process when an emoji appears in the text. I'm using the last [Stanford CoreNLP 3.8.0 ](http://nlp.stanford.edu/software/stanford-corenlp-full-2017-06-09.zip)version. As example I take the following tweet:

```
üì∑ .@MatthewDaddario at #StarWars #TheForceAwakens premiere Part 1 #Shadowhunters Photo : Getty Images https://t.co/LVLghZyOnr
```

I get the following tokenization:

```
[Text=üì∑ CharacterOffsetBegin=0 CharacterOffsetEnd=2]
[Text=.@MatthewDaddario CharacterOffsetBegin=3 CharacterOffsetEnd=20]
[Text=at CharacterOffsetBegin=21 CharacterOffsetEnd=23]
[Text=#StarWars CharacterOffsetBegin=24 CharacterOffsetEnd=33]
[Text=#TheForceAwakens CharacterOffsetBegin=34 CharacterOffsetEnd=50]
[Text=premiere CharacterOffsetBegin=51 CharacterOffsetEnd=59]
[Text=Part CharacterOffsetBegin=60 CharacterOffsetEnd=64]
[Text=1 CharacterOffsetBegin=65 CharacterOffsetEnd=66]
[Text=#Shadowhunters CharacterOffsetBegin=67 CharacterOffsetEnd=81]
[Text=Photo CharacterOffsetBegin=82 CharacterOffsetEnd=87]
[Text=: CharacterOffsetBegin=88 CharacterOffsetEnd=89]
[Text=Getty CharacterOffsetBegin=90 CharacterOffsetEnd=95]
[Text=Images CharacterOffsetBegin=96 CharacterOffsetEnd=102]
[Text=https://t.co/LVLghZyOnr CharacterOffsetBegin=103 CharacterOffsetEnd=126]
```

One can see that the offset of the second token is ""3-20"" instead of ""2-19"".

And when I use the last compiled version provided [here](http://nlp.stanford.edu/software/stanford-corenlp-2017-04-14-build.jar), I get the following warning:

```
Untokenizable: ? (U+D83D, decimal: 55357)
```"
397,https://github.com/stanfordnlp/CoreNLP/issues/501,501,"[{'id': 45387509, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOQ==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/wontfix', 'name': 'wontfix', 'color': 'eeeeee', 'default': True, 'description': None}, {'id': 103162424, 'node_id': 'MDU6TGFiZWwxMDMxNjI0MjQ=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/request', 'name': 'request', 'color': '94c5e9', 'default': False, 'description': None}]",closed,2017-08-07 13:16:34+00:00,,Tokenization emojis,"Hello,

As referenced in the issue #248, emojis are now handled in CoreNLP. But instead of being considered as ""real UTF-8 token"" they are considered as ""2 tokens"" in the offset all the time. For example:

```
üì∑
```
Has the offset [0:2] instead of [0:1].

But:

```
‚ù§Ô∏è
```

Has really the offset [0:2]. Basically considering the real unicode size like Python 3 does. How to make CoreNLP able to understand an emoji like this?

Thanks."
398,https://github.com/stanfordnlp/CoreNLP/issues/502,502,[],closed,2017-08-10 03:08:09+00:00,,"How to get the universal dependency, NOT enhanced just like the demo online from command line? ","In ""stanford-parser-full-2016-10-31"", I try the command line:

####
java -mx1000m -cp ""$scriptdir/*:"" edu.stanford.nlp.parser.lexparser.LexicalizedParser -sentences newline -outputFormat ""typedDependencies"" -originalDependencies edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz $*
#### 

and  get the universal dependency, enhanced;  but I don't know how to get the NOT enhanced universal dependency on command line.
Could anyone help me? Thanks a lot.  
"
399,https://github.com/stanfordnlp/CoreNLP/issues/503,503,"[{'id': 626016953, 'node_id': 'MDU6TGFiZWw2MjYwMTY5NTM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/analysis-bug', 'name': 'analysis-bug', 'color': 'f98685', 'default': False, 'description': None}]",closed,2017-08-10 08:20:48+00:00,,Issue in tokenization,"Hello,

I have noticed few bugs when tokenizing tweets with CoreNLP 3.8.0 released version. I'm providing several examples.

Example 1:

```
Happy #StarWars week! Ever wonder what was going on with Uncle Owen's dad? Check out .@WHMPodcast's rant on Ep2 https://t.co/9iJMMkAokT
```

The tokenization of this tweet I get is the following:

```
[Text=Happy CharacterOffsetBegin=0 CharacterOffsetEnd=5]
[Text=#StarWars CharacterOffsetBegin=6 CharacterOffsetEnd=15]
[Text=week CharacterOffsetBegin=16 CharacterOffsetEnd=20]
[Text=! CharacterOffsetBegin=20 CharacterOffsetEnd=21]
[Text=Ever CharacterOffsetBegin=22 CharacterOffsetEnd=26]
[Text=wonder CharacterOffsetBegin=27 CharacterOffsetEnd=33]
[Text=what CharacterOffsetBegin=34 CharacterOffsetEnd=38]
[Text=was CharacterOffsetBegin=39 CharacterOffsetEnd=42]
[Text=going CharacterOffsetBegin=43 CharacterOffsetEnd=48]
[Text=on CharacterOffsetBegin=49 CharacterOffsetEnd=51]
[Text=with CharacterOffsetBegin=52 CharacterOffsetEnd=56]
[Text=Uncle CharacterOffsetBegin=57 CharacterOffsetEnd=62]
[Text=Owen CharacterOffsetBegin=63 CharacterOffsetEnd=67]
[Text='s CharacterOffsetBegin=67 CharacterOffsetEnd=69]
[Text=dad CharacterOffsetBegin=70 CharacterOffsetEnd=73]
[Text=? CharacterOffsetBegin=73 CharacterOffsetEnd=74]
[Text=Check CharacterOffsetBegin=75 CharacterOffsetEnd=80]
[Text=out CharacterOffsetBegin=81 CharacterOffsetEnd=84]
[Text=.@WHMPodcast CharacterOffsetBegin=85 CharacterOffsetEnd=97]
[Text='s CharacterOffsetBegin=97 CharacterOffsetEnd=99]
[Text=rant CharacterOffsetBegin=100 CharacterOffsetEnd=104]
[Text=on CharacterOffsetBegin=105 CharacterOffsetEnd=107]
[Text=Ep2 CharacterOffsetBegin=108 CharacterOffsetEnd=111]
[Text=https://t.co/9iJMMkAokT CharacterOffsetBegin=112 CharacterOffsetEnd=135]
```

As one can see, there is a problem with the token number 19: ```.@WHMPodcast``` it should be split into two tokens ```.``` and ```@WHMPodcast```.

Example 2:

```
Believe me, I #boycott disgusting corporate #media #ABCNews #Trump trash and their advertisers. .@mmfa -- @BernieSanders, you rock!
```

Same case than the example 1 with ```.@mmfa```

Example 3:

```
RT @JoshEstrin: #PHOTOS -  #StarWars: #TheForceAwakens is it all hype? https://t.co/kFp9mnMv8l via .@JoshEstrin
```

Same case than the example 1 with ```.@JoshEstrin```

Example 4:

```
üì∑ .@MatthewDaddario at #StarWars #TheForceAwakens premiere Part 1 #Shadowhunters Photo : Getty Images https://t.co/LVLghZyOnr
```

Same case than the example 1 with ```.@MatthewDaddario```

Example 5:

```
.@StarWarsAUNZ I have chosen the #Darkside & have my tix for #StarWars #TheForceAwakens on Dec 17! https://t.co/I9Inmd4iuV
```

Same case than the example 1 with ```.@StarWarsAUNZ```

Example 6:

```
RT @Suns: What happens when you combine @50cent, #StarWars and introductions at an @NBA game? This.
```

The tokenization of this tweet I get is the following:

```
[Text=RT CharacterOffsetBegin=0 CharacterOffsetEnd=2]
[Text=@Suns CharacterOffsetBegin=3 CharacterOffsetEnd=8]
[Text=: CharacterOffsetBegin=8 CharacterOffsetEnd=9]
[Text=What CharacterOffsetBegin=10 CharacterOffsetEnd=14]
[Text=happens CharacterOffsetBegin=15 CharacterOffsetEnd=22]
[Text=when CharacterOffsetBegin=23 CharacterOffsetEnd=27]
[Text=you CharacterOffsetBegin=28 CharacterOffsetEnd=31]
[Text=combine CharacterOffsetBegin=32 CharacterOffsetEnd=39]
[Text=@ CharacterOffsetBegin=40 CharacterOffsetEnd=41]
[Text=50cent CharacterOffsetBegin=41 CharacterOffsetEnd=47]
[Text=, CharacterOffsetBegin=47 CharacterOffsetEnd=48]
[Text=#StarWars CharacterOffsetBegin=49 CharacterOffsetEnd=58]
[Text=and CharacterOffsetBegin=59 CharacterOffsetEnd=62]
[Text=introductions CharacterOffsetBegin=63 CharacterOffsetEnd=76]
[Text=at CharacterOffsetBegin=77 CharacterOffsetEnd=79]
[Text=an CharacterOffsetBegin=80 CharacterOffsetEnd=82]
[Text=@NBA CharacterOffsetBegin=83 CharacterOffsetEnd=87]
[Text=game CharacterOffsetBegin=88 CharacterOffsetEnd=92]
[Text=? CharacterOffsetBegin=92 CharacterOffsetEnd=93]
[Text=This CharacterOffsetBegin=94 CharacterOffsetEnd=98]
[Text=. CharacterOffsetBegin=98 CharacterOffsetEnd=99]
```

As one can see there is a problem for the user mention ```@50cent``` that has been split into two tokens instead of keeping it as it is.

Example 7:

```
RT @BiIlionaires: #TheForceAwakens¬†inspired vehicles are a big hit in LA.
```

The tokenization of this tweet I get is the following:

```
[Text=RT CharacterOffsetBegin=0 CharacterOffsetEnd=2]
[Text=@BiIlionaires CharacterOffsetBegin=3 CharacterOffsetEnd=16]
[Text=: CharacterOffsetBegin=16 CharacterOffsetEnd=17]
[Text=#TheForceAwakens CharacterOffsetBegin=18 CharacterOffsetEnd=34]
[Text=inspired CharacterOffsetBegin=35 CharacterOffsetEnd=43]
[Text=vehicles CharacterOffsetBegin=44 CharacterOffsetEnd=52]
[Text=are CharacterOffsetBegin=53 CharacterOffsetEnd=56]
[Text=a CharacterOffsetBegin=57 CharacterOffsetEnd=58]
[Text=big CharacterOffsetBegin=59 CharacterOffsetEnd=62]
[Text=hit CharacterOffsetBegin=63 CharacterOffsetEnd=66]
[Text=in CharacterOffsetBegin=67 CharacterOffsetEnd=69]
[Text=LA. CharacterOffsetBegin=70 CharacterOffsetEnd=72]
[Text=. CharacterOffsetBegin=72 CharacterOffsetEnd=73]
```

As one can see, there is a problem with the last token the ```.``` is still attached to the token ```LA``` whereas the offset has been properly computed."
400,https://github.com/stanfordnlp/CoreNLP/issues/504,504,"[{'id': 45387509, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOQ==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/wontfix', 'name': 'wontfix', 'color': 'eeeeee', 'default': True, 'description': None}]",closed,2017-08-12 03:55:20+00:00,,Circle in dependency parser?,"When I am parsing sentences through 
"" java -mx1000m -cp ""$scriptdir/*:"" edu.stanford.nlp.parser.lexparser.LexicalizedParser -sentences newline \
 -outputFormat ""typedDependencies"" edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz $*
"" circle occurs in the result? The details are as follows. Could anyone help me?

circle
8 -> 11 -> 22 -> 8

neg276: 
why spend $9 on the same stuff you can get for a buck or so in that greasy little vidgame pit in the theater lobby ? 

parser:
advmod(spend-2, why-1)
root(ROOT-0, spend-2)
dep(9-4, $-3)
dobj(spend-2, 9-4)
case(stuff-8, on-5)
det(stuff-8, the-6)
amod(stuff-8, same-7)
nmod:on(spend-2, stuff-8)
det(pit-22, stuff-8)
nsubj(get-11, you-9)
aux(get-11, can-10)
acl:relcl(stuff-8, get-11)
case(buck-14, for-12)
det(buck-14, a-13)
nmod:for(get-11, buck-14)
cc(buck-14, or-15)
advmod(buck-14, so-16)
case(pit-22, in-17)
ref(stuff-8, that-18)
amod(pit-22, greasy-19)
amod(pit-22, little-20)
compound(pit-22, vidgame-21)
nmod:in(get-11, pit-22)
case(lobby-26, in-23)
det(lobby-26, the-24)
compound(lobby-26, theater-25)
nmod:in(pit-22, lobby-26)"
401,https://github.com/stanfordnlp/CoreNLP/issues/505,505,"[{'id': 706056248, 'node_id': 'MDU6TGFiZWw3MDYwNTYyNDg=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/ssplit', 'name': 'ssplit', 'color': 'c5def5', 'default': False, 'description': None}, {'id': 706064425, 'node_id': 'MDU6TGFiZWw3MDYwNjQ0MjU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/algorithm-error', 'name': 'algorithm-error', 'color': 'f9d0c4', 'default': False, 'description': None}]",open,2017-08-13 21:04:16+00:00,,WordToSentenceProcessor problem with listings,"Hi,

The sentence processor does not handle well this case

```
I have two pets:
1. a dog;
1. a cat.
```

Results as 3 sentences:
S1: Hi have two pets: \n1.
S2: a dog;\n2.
S3: a cat.

It should result only one sentence:
S1: Hi have two pets: \n1. a dog;\n2. a cat.

I am not sure how to configure the parser in order it works fine. 
This kind of enumeration is typically used in markdown when listing."
402,https://github.com/stanfordnlp/CoreNLP/issues/506,506,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 732360245, 'node_id': 'MDU6TGFiZWw3MzIzNjAyNDU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/protobuf', 'name': 'protobuf', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2017-08-14 07:21:54+00:00,,stanford-corenlp-3.8.0.jar fail in Spark2 BUT stanford-corenlp-3.7.0.jar works fine (with models in 3.8.0),"This execution fails:
spark2-shell --jars protobuf-java-3.2.0.jar,stanford-corenlp-3.8.0.jar,stanford-corenlp-3.8.0-models.jar
import edu.stanford.nlp.simple.{Document, Sentence}
new Sentence(""hello world!"")

java.lang.VerifyError: Bad type on operand stack
Exception Details:
  Location:
    com/google/protobuf/GeneratedMessageV3$ExtendableMessage.getExtensionCount(Lcom/google/protobuf/GeneratedMessage$GeneratedExtension;)I @2: invokevirtual
  Reason:
    Type 'com/google/protobuf/GeneratedMessage$GeneratedExtension' (current frame, stack[1]) is not assignable to 'com/google/protobuf/ExtensionLite'
  Current Frame:
    bci: @2
    flags: { }
    locals: { 'com/google/protobuf/GeneratedMessageV3$ExtendableMessage', 'com/google/protobuf/GeneratedMessage$GeneratedExtension' }
    stack: { 'com/google/protobuf/GeneratedMessageV3$ExtendableMessage', 'com/google/protobuf/GeneratedMessage$GeneratedExtension' }
  Bytecode:
    0x0000000: 2a2b b600 22ac

  at edu.stanford.nlp.simple.Document.<init>(Document.java:433)
  at edu.stanford.nlp.simple.Sentence.<init>(Sentence.java:118)
  at edu.stanford.nlp.simple.Sentence.<init>(Sentence.java:126)
  ... 48 elided


And this works fine:
spark2-shell --jars protobuf-java-3.2.0.jar,stanford-corenlp-3.**7**.0.jar,stanford-corenlp-3.8.0-models.jar
"
403,https://github.com/stanfordnlp/CoreNLP/issues/507,507,[],closed,2017-08-17 07:32:24+00:00,,Tagging values with decimal points to Money using Regex ner,"Hi Team,

Example Sentence : DRB-Hicom Bhd and Zhejiang Geely Holdings Group have signed a definitive agreement for Geely to acquire a 49.9% stake in the national car manufacturer in a deal worth **RM460.3 million.**

In the above sentence we want to tag RM460.3 million as money entity. The below is the regex which we used in regex ner of Stanford NLP:

Regex 1: RM[0-9]+.[0-9]+ million MONEY

Regex 2: RM[0-9]+(.[0-9]+) million MONEY

Regex 3: RM[0-9].*million MONEY

None of the above regex matched and RM460.3 million is not getting captured as Money. 
Instead NLP is just identifying **million** as money by default and resulting **NER:100000.0**
We are not sure where is the issue. Could anybody please help us in tagging whole **RM460.3 million** as money using regexner of Stanford NLP.

Thank you in advance."
404,https://github.com/stanfordnlp/CoreNLP/issues/508,508,[],closed,2017-08-18 22:06:09+00:00,,regexner.ignoreCase is an invalid property,Docs says regexner.ignoreCase however the right property is regexner.ignorecase.
405,https://github.com/stanfordnlp/CoreNLP/issues/509,509,[],closed,2017-08-18 22:31:22+00:00,,truecase + ner not working as expected,"I have tried the demo in the docs. Truecase efectively uppercases the text but the ner can't see it. I changed the param truecase.overwrite but didn't work.

Here is my properties file
```
annotators = tokenize, ssplit, truecase, pos, lemma, ner
truecase.overwrite = true
ner.model = edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz
```

I'm running the server
```
java -mx1g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout 15000 -port 9000 -props myprops.prop
```

Output
```{
    ""sentences"": [
        {
            ""index"": 0,
            ""tokens"": [
                {
                    ""index"": 1,
                    ""word"": ""lonzo"",
                    ""originalText"": ""lonzo"",
                    ""lemma"": ""lonzo"",
                    ""characterOffsetBegin"": 0,
                    ""characterOffsetEnd"": 5,
                    ""pos"": ""NN"",
                    ""ner"": ""O"",
                    ""truecase"": ""INIT_UPPER"",
                    ""truecaseText"": ""Lonzo"",
                    ""before"": """",
                    ""after"": "" ""
                },
                {
                    ""index"": 2,
                    ""word"": ""ball"",
                    ""originalText"": ""ball"",
                    ""lemma"": ""ball"",
                    ""characterOffsetBegin"": 6,
                    ""characterOffsetEnd"": 10,
                    ""pos"": ""NN"",
                    ""ner"": ""O"",
                    ""truecase"": ""LOWER"",
                    ""truecaseText"": ""ball"",
                    ""before"": "" "",
                    ""after"": "" ""
                },
                {
                    ""index"": 3,
                    ""word"": ""talked"",
                    ""originalText"": ""talked"",
                    ""lemma"": ""talk"",
                    ""characterOffsetBegin"": 11,
                    ""characterOffsetEnd"": 17,
                    ""pos"": ""VBD"",
                    ""ner"": ""O"",
                    ""truecase"": ""LOWER"",
                    ""truecaseText"": ""talked"",
                    ""before"": "" "",
                    ""after"": "" ""
                },
                {
                    ""index"": 4,
                    ""word"": ""about"",
                    ""originalText"": ""about"",
                    ""lemma"": ""about"",
                    ""characterOffsetBegin"": 18,
                    ""characterOffsetEnd"": 23,
                    ""pos"": ""IN"",
                    ""ner"": ""O"",
                    ""truecase"": ""LOWER"",
                    ""truecaseText"": ""about"",
                    ""before"": "" "",
                    ""after"": "" ""
                },
                {
                    ""index"": 5,
                    ""word"": ""kobe"",
                    ""originalText"": ""kobe"",
                    ""lemma"": ""kobe"",
                    ""characterOffsetBegin"": 24,
                    ""characterOffsetEnd"": 28,
                    ""pos"": ""NN"",
                    ""ner"": ""O"",
                    ""truecase"": ""INIT_UPPER"",
                    ""truecaseText"": ""Kobe"",
                    ""before"": "" "",
                    ""after"": "" ""
                },
                {
                    ""index"": 6,
                    ""word"": ""bryant"",
                    ""originalText"": ""bryant"",
                    ""lemma"": ""bryant"",
                    ""characterOffsetBegin"": 29,
                    ""characterOffsetEnd"": 35,
                    ""pos"": ""NN"",
                    ""ner"": ""O"",
                    ""truecase"": ""INIT_UPPER"",
                    ""truecaseText"": ""Bryant"",
                    ""before"": "" "",
                    ""after"": "" ""
                },
                {
                    ""index"": 7,
                    ""word"": ""after"",
                    ""originalText"": ""after"",
                    ""lemma"": ""after"",
                    ""characterOffsetBegin"": 36,
                    ""characterOffsetEnd"": 41,
                    ""pos"": ""IN"",
                    ""ner"": ""O"",
                    ""truecase"": ""LOWER"",
                    ""truecaseText"": ""after"",
                    ""before"": "" "",
                    ""after"": "" ""
                },
                {
                    ""index"": 8,
                    ""word"": ""the"",
                    ""originalText"": ""the"",
                    ""lemma"": ""the"",
                    ""characterOffsetBegin"": 42,
                    ""characterOffsetEnd"": 45,
                    ""pos"": ""DT"",
                    ""ner"": ""O"",
                    ""truecase"": ""LOWER"",
                    ""truecaseText"": ""the"",
                    ""before"": "" "",
                    ""after"": "" ""
                },
                {
                    ""index"": 9,
                    ""word"": ""lakers"",
                    ""originalText"": ""lakers"",
                    ""lemma"": ""laker"",
                    ""characterOffsetBegin"": 46,
                    ""characterOffsetEnd"": 52,
                    ""pos"": ""NNS"",
                    ""ner"": ""O"",
                    ""truecase"": ""INIT_UPPER"",
                    ""truecaseText"": ""Lakers"",
                    ""before"": "" "",
                    ""after"": "" ""
                },
                {
                    ""index"": 10,
                    ""word"": ""game"",
                    ""originalText"": ""game"",
                    ""lemma"": ""game"",
                    ""characterOffsetBegin"": 53,
                    ""characterOffsetEnd"": 57,
                    ""pos"": ""NN"",
                    ""ner"": ""O"",
                    ""truecase"": ""LOWER"",
                    ""truecaseText"": ""game"",
                    ""before"": "" "",
                    ""after"": """"
                },
                {
                    ""index"": 11,
                    ""word"": ""."",
                    ""originalText"": ""."",
                    ""lemma"": ""."",
                    ""characterOffsetBegin"": 57,
                    ""characterOffsetEnd"": 58,
                    ""pos"": ""."",
                    ""ner"": ""O"",
                    ""truecase"": ""O"",
                    ""truecaseText"": ""."",
                    ""before"": """",
                    ""after"": """"
                }
            ]
        }
    ]
}
```"
406,https://github.com/stanfordnlp/CoreNLP/issues/510,510,"[{'id': 706064425, 'node_id': 'MDU6TGFiZWw3MDYwNjQ0MjU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/algorithm-error', 'name': 'algorithm-error', 'color': 'f9d0c4', 'default': False, 'description': None}, {'id': 706094173, 'node_id': 'MDU6TGFiZWw3MDYwOTQxNzM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/lemma', 'name': 'lemma', 'color': 'c5def5', 'default': False, 'description': None}]",open,2017-08-24 16:20:01+00:00,,"Lemma for ""seconds"" is wrong","The lemma from Corenlp for ""seconds"" is ""seconds"" instead of ""second""."
407,https://github.com/stanfordnlp/CoreNLP/issues/511,511,[],closed,2017-08-25 15:34:43+00:00,,"Sentence.openieTriples() : Exception in thread ""main"" java.lang.IndexOutOfBoundsException: Index: 1, Size: 1","I'm getting this exception when iterating over sentences of a document. I can get the openie Triples just for the first sentence !

at java.util.ArrayList.rangeCheck(ArrayList.java:653)
	at java.util.ArrayList.get(ArrayList.java:429)
	at edu.stanford.nlp.simple.Sentence.asCoreMap(Sentence.java:1044)
	at edu.stanford.nlp.simple.Sentence.asCoreLabels(Sentence.java:1064)
	at edu.stanford.nlp.simple.Sentence.openieTriples(Sentence.java:892)
	at edu.stanford.nlp.simple.Sentence.openieTriples(Sentence.java:900)

code snippet:
List<CoreMap> sentences = doc.get(SentencesAnnotation.class);
for (CoreMap sentence : sentences) {
			Sentence simpleSentence = new Sentence(sentence);
			System.out.println( simpleSentence.openieTriples());		
		}
"
408,https://github.com/stanfordnlp/CoreNLP/issues/512,512,[],closed,2017-08-28 09:31:51+00:00,,Can you please provide some examples of training a dependency parser?,"We need to parse e-mail messages in our project to extract information to generate orders (involving a code, description, quantity). We are using Named Entity Recognition to identify the key domain entities, and have been quite successful. Now we need to build dependency graphs, to establish relations and associations, for eg.
common information, say 1% rate of interest
first order information
second order information
third order information
In this example, the 1% rate of interest is applicable to all the orders. In case any order has a specific rate of interest, it overrides the value 1% (as is in this context)

I have not found any examples anywhere across the web, with searches done so far. Could you please advise?

Thanks!"
409,https://github.com/stanfordnlp/CoreNLP/issues/513,513,[],closed,2017-08-29 17:39:14+00:00,,StanfordCoreNLPServer & StanfordCoreNLP - different output with same settings,"By running `StanfordCoreNLP` with `-annotators tokenize,ssplit,parse` the PCFG parser is correctly used to do the PoS tagging. However, when `StanfordCoreNLPServer` with `-annotators tokenize,ssplit,parse` is run, the `pos` annotator is automatically added, even though it was not specified in parameters, and thus the MaxentTagger is used for PoS tagging. It would be great (and consistent) if the server version supported PoS-tagging with PCFG parser too.

### StanfordCoreNLP (run in interactive mode)
- started with: `java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,parse`
- input: `'Coach Harvey hands Mike a towel.'`
- output: 
```
Sentence #1 (7 tokens):
Coach Harvey hands Mike a towel.
[Text=Coach CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=NNP]
[Text=Harvey CharacterOffsetBegin=6 CharacterOffsetEnd=12 PartOfSpeech=NNP]
[Text=hands CharacterOffsetBegin=13 CharacterOffsetEnd=18 PartOfSpeech=VBZ]
[Text=Mike CharacterOffsetBegin=19 CharacterOffsetEnd=23 PartOfSpeech=NNP]
[Text=a CharacterOffsetBegin=24 CharacterOffsetEnd=25 PartOfSpeech=DT]
[Text=towel CharacterOffsetBegin=26 CharacterOffsetEnd=31 PartOfSpeech=NN]
[Text=. CharacterOffsetBegin=31 CharacterOffsetEnd=32 PartOfSpeech=.]
(ROOT
  (S
    (NP (NNP Coach) (NNP Harvey))
    (VP (VBZ hands)
      (S
        (NP (NNP Mike))
        (NP (DT a) (NN towel))))
    (. .)))

root(ROOT-0, hands-3)
compound(Harvey-2, Coach-1)
nsubj(hands-3, Harvey-2)
nsubj(towel-6, Mike-4)
det(towel-6, a-5)
xcomp(hands-3, towel-6)
punct(hands-3, .-7)
```

### StanfordCoreNLPServer:
- started with: `java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000 -annotators tokenize,ssplit,parse`
- input: `'Coach Harvey hands Mike a towel.'`
- request: `wget --post-data 'Coach Harvey hands Mike a towel.' 'localhost:9000/?properties={""annotators"":""tokenize,ssplit,parse"", ""outputFormat"":""json""}' -O -`
- server log: 
```
[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - setting default constituency parser
[main] INFO CoreNLP - warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP - using: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz instead
[main] INFO CoreNLP - to use shift reduce parser download English models jar from:
[main] INFO CoreNLP - http://stanfordnlp.github.io/CoreNLP/download.html
[main] INFO CoreNLP -     Threads: 4
[main] INFO CoreNLP - Starting server...
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000
[pool-1-thread-1] INFO CoreNLP - [/0:0:0:0:0:0:0:1:49993] API call w/annotators tokenize,ssplit,pos,parse
Coach Harvey hands Mike a towel.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[pool-1-thread-1] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.0 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[pool-1-thread-1] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.7 sec].
```
- output: 
```
{
   ""sentences"":[
      {
         ""index"":0,
         ""parse"":""(ROOT\n  (S\n    (VP (VB Coach)\n      (S\n        (NP (NNP Harvey) (NNS hands))\n        (NP\n          (NP (NNP Mike))\n          (NP (DT a) (NN towel)))))\n    (. .)))"",
         ""basicDependencies"":[
            {
               ""dep"":""ROOT"",
               ""governor"":0,
               ""governorGloss"":""ROOT"",
               ""dependent"":1,
               ""dependentGloss"":""Coach""
            },
            {
               ""dep"":""compound"",
               ""governor"":3,
               ""governorGloss"":""hands"",
               ""dependent"":2,
               ""dependentGloss"":""Harvey""
            },
            {
               ""dep"":""nsubj"",
               ""governor"":4,
               ""governorGloss"":""Mike"",
               ""dependent"":3,
               ""dependentGloss"":""hands""
            },
            {
               ""dep"":""xcomp"",
               ""governor"":1,
               ""governorGloss"":""Coach"",
               ""dependent"":4,
               ""dependentGloss"":""Mike""
            },
            {
               ""dep"":""det"",
               ""governor"":6,
               ""governorGloss"":""towel"",
               ""dependent"":5,
               ""dependentGloss"":""a""
            },
            {
               ""dep"":""dep"",
               ""governor"":4,
               ""governorGloss"":""Mike"",
               ""dependent"":6,
               ""dependentGloss"":""towel""
            },
            {
               ""dep"":""punct"",
               ""governor"":1,
               ""governorGloss"":""Coach"",
               ""dependent"":7,
               ""dependentGloss"":"".""
            }
         ],
         ""enhancedDependencies"":[
            {
               ""dep"":""ROOT"",
               ""governor"":0,
               ""governorGloss"":""ROOT"",
               ""dependent"":1,
               ""dependentGloss"":""Coach""
            },
            {
               ""dep"":""compound"",
               ""governor"":3,
               ""governorGloss"":""hands"",
               ""dependent"":2,
               ""dependentGloss"":""Harvey""
            },
            {
               ""dep"":""nsubj"",
               ""governor"":4,
               ""governorGloss"":""Mike"",
               ""dependent"":3,
               ""dependentGloss"":""hands""
            },
            {
               ""dep"":""xcomp"",
               ""governor"":1,
               ""governorGloss"":""Coach"",
               ""dependent"":4,
               ""dependentGloss"":""Mike""
            },
            {
               ""dep"":""det"",
               ""governor"":6,
               ""governorGloss"":""towel"",
               ""dependent"":5,
               ""dependentGloss"":""a""
            },
            {
               ""dep"":""dep"",
               ""governor"":4,
               ""governorGloss"":""Mike"",
               ""dependent"":6,
               ""dependentGloss"":""towel""
            },
            {
               ""dep"":""punct"",
               ""governor"":1,
               ""governorGloss"":""Coach"",
               ""dependent"":7,
               ""dependentGloss"":"".""
            }
         ],
         ""enhancedPlusPlusDependencies"":[
            {
               ""dep"":""ROOT"",
               ""governor"":0,
               ""governorGloss"":""ROOT"",
               ""dependent"":1,
               ""dependentGloss"":""Coach""
            },
            {
               ""dep"":""compound"",
               ""governor"":3,
               ""governorGloss"":""hands"",
               ""dependent"":2,
               ""dependentGloss"":""Harvey""
            },
            {
               ""dep"":""nsubj"",
               ""governor"":4,
               ""governorGloss"":""Mike"",
               ""dependent"":3,
               ""dependentGloss"":""hands""
            },
            {
               ""dep"":""xcomp"",
               ""governor"":1,
               ""governorGloss"":""Coach"",
               ""dependent"":4,
               ""dependentGloss"":""Mike""
            },
            {
               ""dep"":""det"",
               ""governor"":6,
               ""governorGloss"":""towel"",
               ""dependent"":5,
               ""dependentGloss"":""a""
            },
            {
               ""dep"":""dep"",
               ""governor"":4,
               ""governorGloss"":""Mike"",
               ""dependent"":6,
               ""dependentGloss"":""towel""
            },
            {
               ""dep"":""punct"",
               ""governor"":1,
               ""governorGloss"":""Coach"",
               ""dependent"":7,
               ""dependentGloss"":"".""
            }
         ],
         ""tokens"":[
            {
               ""index"":1,
               ""word"":""Coach"",
               ""originalText"":""Coach"",
               ""characterOffsetBegin"":0,
               ""characterOffsetEnd"":5,
               ""pos"":""VB"",
               ""before"":"""",
               ""after"":"" ""
            },
            {
               ""index"":2,
               ""word"":""Harvey"",
               ""originalText"":""Harvey"",
               ""characterOffsetBegin"":6,
               ""characterOffsetEnd"":12,
               ""pos"":""NNP"",
               ""before"":"" "",
               ""after"":"" ""
            },
            {
               ""index"":3,
               ""word"":""hands"",
               ""originalText"":""hands"",
               ""characterOffsetBegin"":13,
               ""characterOffsetEnd"":18,
               ""pos"":""NNS"",
               ""before"":"" "",
               ""after"":"" ""
            },
            {
               ""index"":4,
               ""word"":""Mike"",
               ""originalText"":""Mike"",
               ""characterOffsetBegin"":19,
               ""characterOffsetEnd"":23,
               ""pos"":""NNP"",
               ""before"":"" "",
               ""after"":"" ""
            },
            {
               ""index"":5,
               ""word"":""a"",
               ""originalText"":""a"",
               ""characterOffsetBegin"":24,
               ""characterOffsetEnd"":25,
               ""pos"":""DT"",
               ""before"":"" "",
               ""after"":"" ""
            },
            {
               ""index"":6,
               ""word"":""towel"",
               ""originalText"":""towel"",
               ""characterOffsetBegin"":26,
               ""characterOffsetEnd"":31,
               ""pos"":""NN"",
               ""before"":"" "",
               ""after"":""""
            },
            {
               ""index"":7,
               ""word"":""."",
               ""originalText"":""."",
               ""characterOffsetBegin"":31,
               ""characterOffsetEnd"":32,
               ""pos"":""."",
               ""before"":"""",
               ""after"":""""
            }
         ]
      }
   ]
}
```
"
410,https://github.com/stanfordnlp/CoreNLP/issues/514,514,[],closed,2017-08-31 22:18:11+00:00,,MaxentTagger throws RuntimeInterruptedException,"I am wondering what is the reason behind loc 334-336 and loc 347-349 for `TestSentence.java`, i.e., 

```java
    if (Thread.interrupted()) {  // Allow interrupting
      throw new RuntimeInterruptedException();
    }
```

With this in place, `MaxentTagger` throws exception in a multi-thread environment from time to time. Why is it necessary to do this?"
411,https://github.com/stanfordnlp/CoreNLP/issues/515,515,[],closed,2017-09-05 09:41:55+00:00,,StanfordCoreNLPClient host parameter must contain protocol,"For StanfordCoreNLPClient(property,host,port), host is a String that must contain protocol too.
So host must begin with ""http://"". See #416 

The documentation is wrong and should be changed : 
https://stanfordnlp.github.io/CoreNLP/corenlp-server.html#java-client (see first java example, host is ""localhost"" and should be ""http://localhost"")"
412,https://github.com/stanfordnlp/CoreNLP/issues/516,516,"[{'id': 626016953, 'node_id': 'MDU6TGFiZWw2MjYwMTY5NTM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/analysis-bug', 'name': 'analysis-bug', 'color': 'f98685', 'default': False, 'description': None}]",closed,2017-09-07 20:05:57+00:00,,Inconsistency of NER tagging with 3.6 and 3.8,"I asked the question on stackoverflow and also here. 

I am really puzzled by the results I get from CoreNLP (I am still using 3.6, currently upgrading to 3.8, and I used the online tool on the website to test with 3.8).

Look for example at the sentences below:

1) The Audit Committee has selected PricewaterhouseCoopers, which has audited our financial statements annually since 2004, to serve as our independent registered public accounting firm for Fiscal 2018.
2) Our lead audit partner at PricewaterhouseCoopers serves no more than five consecutive years in that role.
In (2), PricewaterhouseCoopers is tagged as as ""ORGANIZATION"" (which is what I want), HOWEVER it is NOT tagged as such in (1). **What am I missing here?**

Now if I add to the mix:

3) If our stockholders do not ratify the selection, the AC will reconsider whether or not to retain PricewaterhouseCoopers.
4) The AC believes it is in the best interests of NVIDIA and our stockholders to retain 
PricewaterhouseCoopers.

PricewaterhouseCoopers is not tagged correctly in either (3) or (4). In (4) NVIDIA is tagged correctly though. Why is that?"
413,https://github.com/stanfordnlp/CoreNLP/issues/518,518,[],closed,2017-09-08 13:46:48+00:00,,NER training get stuck since more than 10 days,"Hello,

I ran the following command line 10 days ago:

```
java -Xmx30g -cp stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop stanford.prop -trainFile wikipedia.conll -serializeTo wikipedia_fr.ser.gz
```

In order to train a model for French. Nevertheless, I have the feeling that the training has never started because my output looks like this:

```
java -Xmx30g -cp stanford-corenlp-full-2017-06-09/stanford-corenlp-3.8.0.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop stanford.prop -trainFile wikipedia.conll -serializeTo wikipedia_fr.ser.gz
Invoked on Mon Aug 28 10:26:51 CEST 2017 with arguments: -prop stanford.prop -trainFile wikipedia.conll -serializeTo wikipedia_fr.ser.gz
usePrevSequences=true
useQN=true
useObservedSequencesOnly=true
useTitle=true
useLastRealWord=true
useClassFeature=true
type=crf
useTypeSeqs2=true
featureDiffThresh=0.05
useSequences=true
useNextRealWord=true
disjunctionWidth=4
wordShape=dan2useLC
saveFeatureIndexToDisk=true
useTypeySequences=true
QNsize=25
useDisjunctive=true
noMidNGrams=true
sigma=20
serializeTo=wikipedia_fr.ser.gz
useNGrams=true
normalize=true
usePrev=true
readerAndWriter=edu.stanford.nlp.sequences.ColumnDocumentReaderAndWriter
maxLeft=1
useNext=true
trainFile=wikipedia.conll
useLongSequences=true
useOccurrencePatterns=true
map=word=0,answer=1
useWord=true
useTypeSeqs=true
```

My training file is pretty big, it has around 7 Million rows. This might be the reason?

Thanks in advance for any help."
414,https://github.com/stanfordnlp/CoreNLP/issues/520,520,"[{'id': 732360245, 'node_id': 'MDU6TGFiZWw3MzIzNjAyNDU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/protobuf', 'name': 'protobuf', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2017-09-11 23:43:02+00:00,,StanfordCoreNLPServer 3.8 : Exception for some specific words,"Using StanfordCoreNLPServer and StanfordCoreNLPClient for French models and specific words (like **aujourd'hui** or **l√†-bas** ) is problematic and returns an exception on the server side:

    edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer$LossySerializationException: Keys are not being serialized: class edu.stanford.nlp.ling.CoreAnnotations$ParentAnnotation
        at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProto(ProtobufAnnotationSerializer.java:240)
        ...

Here is a client code example for a server running on localhost, port 8135:

    import java.util.Properties;

    import edu.stanford.nlp.pipeline.Annotation;
    import edu.stanford.nlp.pipeline.StanfordCoreNLPClient;

    public class test2 {
        public static void main(String[] args) {
            Properties props = new Properties();
            
            props.setProperty(""annotators"", ""tokenize, ssplit, pos, depparse"");             
            props.setProperty(""tokenize.language"",""fr"");
            props.setProperty(""pos.model"",""edu/stanford/nlp/models/pos-tagger/french/french.tagger"");
            props.setProperty(""depparse.model"",""edu/stanford/nlp/models/parser/nndep/UD_French.gz"");    
            props.setProperty(""depparse.language"",""french"");
            
            StanfordCoreNLPClient pipeline = new StanfordCoreNLPClient(props,""http://localhost"",8135);
            Annotation annotation = pipeline.process(""aujourd'hui"");
        }
    }

Is there a way to fix it (seems it's not possible to change client/server serializer) ?
"
415,https://github.com/stanfordnlp/CoreNLP/issues/521,521,[],closed,2017-09-12 15:57:25+00:00,,"How to use ""batch processing"" when training in CoreNLP?","I am trying to train this huge dataset of addresses (NER with 11 target classes). There are ~220 million lines in the training file, with each document consisting of 10 lines on average... I have access to 2 Titan X GPUs, but I still keep running into ""Out of Memory"" errors. 
So, Is there a way to batch process the training in CoreNLP? If yes, how to do that from command line?

I am using this command to run the training from command line. I have tried using up till 20Gb as the memory flag, but to no avail.
<java -mx7g -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop my.prop>

=================================
                              ERROR 
=================================
Exception in thread ""main"" java.lang.OutOfMemoryError
	at java.lang.AbstractStringBuilder.hugeCapacity(java.base@9-internal/AbstractStringBuilder.java:188)
	at java.lang.AbstractStringBuilder.newCapacity(java.base@9-internal/AbstractStringBuilder.java:180)
	at java.lang.AbstractStringBuilder.ensureCapacityInternal(java.base@9-internal/AbstractStringBuilder.java:147)
	at java.lang.AbstractStringBuilder.append(java.base@9-internal/AbstractStringBuilder.java:510)
	at java.lang.StringBuilder.append(java.base@9-internal/StringBuilder.java:141)
	at edu.stanford.nlp.objectbank.DelimitRegExIterator.<init>(DelimitRegExIterator.java:40)
	at edu.stanford.nlp.objectbank.DelimitRegExIterator$DelimitRegExIteratorFactory.getIterator(DelimitRegExIterator.java:122)
	at edu.stanford.nlp.sequences.ColumnDocumentReaderAndWriter.getIterator(ColumnDocumentReaderAndWriter.java:58)
	at edu.stanford.nlp.objectbank.ObjectBank$OBIterator.setNextObject(ObjectBank.java:436)
	at edu.stanford.nlp.objectbank.ObjectBank$OBIterator.<init>(ObjectBank.java:415)
	at edu.stanford.nlp.objectbank.ObjectBank.iterator(ObjectBank.java:253)
	at edu.stanford.nlp.sequences.ObjectBankWrapper.iterator(ObjectBankWrapper.java:45)
	at edu.stanford.nlp.ie.crf.CRFClassifier.train(CRFClassifier.java:1585)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequenceClassifier.java:765)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequenceClassifier.java:753)
	at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3012)



"
416,https://github.com/stanfordnlp/CoreNLP/issues/522,522,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2017-09-13 19:03:52+00:00,,Invalid json in output,"### Sample input I will use below:
```
$ echo -n $'Th\x10e' | xxd
0000000: 5468 1065                                Th.e
```

### Let's use `tokenize,ssplit` annotator:
```
$ echo $'Th\x10e' | /usr/bin/java -mx1g -cp \* edu.stanford.nlp.pipeline.StanfordCoreNLP -outputFormat json  -annotators tokenize,ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit

Entering interactive shell. Type q RETURN or EOF to quit.
NLP> Untokenizable:  (U+10, decimal: 16)
{
  ""sentences"": [
    {
      ""index"": 0,
      ""tokens"": [
        {
          ""index"": 1,
          ""word"": ""Th"",
          ""originalText"": ""Th"",
          ""characterOffsetBegin"": 0,
          ""characterOffsetEnd"": 2,
          ""before"": """",
          ""after"": """"  <------ ascii 0x10
        },
        {
          ""index"": 2,
          ""word"": ""e"",
          ""originalText"": ""e"",
          ""characterOffsetBegin"": 3,
          ""characterOffsetEnd"": 4,
          ""before"": """",  <------ ascii 0x10
          ""after"": """"
        }
      ]
    }
  ]
}
NLP> 
Annotation pipeline timing information:
TokenizerAnnotator: 0.1 sec.
WordsToSentencesAnnotator: 0.0 sec.
TOTAL: 0.1 sec. for 2 tokens at 38.5 tokens/sec.
Pipeline setup: 0.0 sec.
Total time for StanfordCoreNLP pipeline: 0.1 sec.
```

We've got ascii 0x10 raw dumped in json under _after_ and _before_ keys. I'm not sure if printing **raw** ascii control characters in json is valid but php's `json_decode()` returns `null` for any json containing that so I guess it is not a common practice at the very least."
417,https://github.com/stanfordnlp/CoreNLP/issues/523,523,[],closed,2017-09-14 02:13:58+00:00,,"When I train my own NER model, got the java.lang.OutofMemoryError: GC overhead limit exceeded","Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceede
d
        at java.util.Arrays.copyOfRange(Unknown Source)
        at java.lang.String.<init>(Unknown Source)
        at java.lang.StringBuilder.toString(Unknown Source)
        at edu.stanford.nlp.process.WordShapeClassifier.wordShapeChris2Short(Wor
dShapeClassifier.java:408)
        at edu.stanford.nlp.process.WordShapeClassifier.wordShapeChris2(WordShap
eClassifier.java:364)
        at edu.stanford.nlp.process.WordShapeClassifier.wordShape(WordShapeClass
ifier.java:180)
        at edu.stanford.nlp.sequences.ObjectBankWrapper.doBasicStuff(ObjectBankW
rapper.java:141)
        at edu.stanford.nlp.sequences.ObjectBankWrapper.processDocument(ObjectBa
nkWrapper.java:92)
        at edu.stanford.nlp.sequences.ObjectBankWrapper$WrappedIterator.next(Obj
ectBankWrapper.java:85)
        at edu.stanford.nlp.sequences.ObjectBankWrapper$WrappedIterator.next(Obj
ectBankWrapper.java:49)
        at edu.stanford.nlp.ie.crf.CRFClassifier.train(CRFClassifier.java:1585)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequence
Classifier.java:765)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequence
Classifier.java:753)
        at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3012)"
418,https://github.com/stanfordnlp/CoreNLP/issues/524,524,"[{'id': 706064425, 'node_id': 'MDU6TGFiZWw3MDYwNjQ0MjU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/algorithm-error', 'name': 'algorithm-error', 'color': 'f9d0c4', 'default': False, 'description': None}, {'id': 706083198, 'node_id': 'MDU6TGFiZWw3MDYwODMxOTg=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/sutime', 'name': 'sutime', 'color': 'c5def5', 'default': False, 'description': None}]",open,2017-09-15 05:50:23+00:00,,Two consecutive dates being identified as single date,"Hi Team,

Consider the below example sentence
Peaceful Union Acquires US Biotech Sharklet Technologies M&A Navigator **_05/19/2017 18 May 2017_** - Chinese medical device maker Peaceful Union has closed the acquisition of US-based biotechnology company Sharklet Technologies, Inc, the companies said.

05/19/2017 and 18 May 2017 are two different dates. But NLP is identifying them as single date as you can see below
![twoconsecutivedateassingle](https://user-images.githubusercontent.com/20786984/30468243-b68d1184-9a07-11e7-9db4-0c2890b0e690.png)


Could you please help us in overcoming this type of issue.
Thank you in advance.

"
419,https://github.com/stanfordnlp/CoreNLP/issues/525,525,[],closed,2017-09-20 08:12:35+00:00,,Wikipedia entities pre-load,"When using the annotator for **Wikipedia Entities**, the loading times take a while:

```
10:04:11.736 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator entitylink
10:04:11.889 [main] INFO  e.s.nlp.pipeline.WikidictAnnotator - Reading Wikidict from edu/stanford/nlp/models/kbp/wikidict.tab.gz
10:04:11.894 [main] INFO  e.s.nlp.pipeline.WikidictAnnotator - Loaded 0 entries from Wikidict [1564MB memory used; 0.0003 seconds elapsed]
10:04:17.241 [main] INFO  e.s.nlp.pipeline.WikidictAnnotator - Loaded 1000000 entries from Wikidict [1656MB memory used; 5.0352 seconds elapsed]
10:04:20.106 [main] INFO  e.s.nlp.pipeline.WikidictAnnotator - Loaded 2000000 entries from Wikidict [1828MB memory used; 8.0217 seconds elapsed]
10:04:24.148 [main] INFO  e.s.nlp.pipeline.WikidictAnnotator - Loaded 3000000 entries from Wikidict [2014MB memory used; 12.0258 seconds elapsed]
10:04:28.895 [main] INFO  e.s.nlp.pipeline.WikidictAnnotator - Loaded 4000000 entries from Wikidict [2185MB memory used; 17.0006 seconds elapsed]
10:04:34.346 [main] INFO  e.s.nlp.pipeline.WikidictAnnotator - Loaded 5000000 entries from Wikidict [2362MB memory used; 22.0457 seconds elapsed]
10:04:45.852 [main] INFO  e.s.nlp.pipeline.WikidictAnnotator - Loaded 6000000 entries from Wikidict [2525MB memory used; 33.0962 seconds elapsed]
10:04:51.871 [main] INFO  e.s.nlp.pipeline.WikidictAnnotator - Loaded 7000000 entries from Wikidict [2698MB memory used; 39.0982 seconds elapsed]
10:04:58.646 [main] INFO  e.s.nlp.pipeline.WikidictAnnotator - Loaded 8000000 entries from Wikidict [2860MB memory used; 46.0757 seconds elapsed]
10:05:12.147 [main] INFO  e.s.nlp.pipeline.WikidictAnnotator - Loaded 9000000 entries from Wikidict [2696MB memory used; 01:00.0258 minutes elapsed]
10:05:26.907 [main] INFO  e.s.nlp.pipeline.WikidictAnnotator - Loaded 10000000 entries from Wikidict [2863MB memory used; 01:15.0018 minutes elapsed]
10:05:41.174 [main] INFO  e.s.nlp.pipeline.WikidictAnnotator - Loaded 11000000 entries from Wikidict [3074MB memory used; 01:29.0284 minutes elapsed]
10:06:08.225 [main] INFO  e.s.nlp.pipeline.WikidictAnnotator - Loaded 12000000 entries from Wikidict [3120MB memory used; 01:56.0336 minutes elapsed]
```

Given this configuration:

```javascript
var options = {
   lang: ""en"",
   annotators: ""tokenize, ssplit, pos, lemma, ner, parse, sentiment, entitylink""
};
```

is it possibile to speed up `WikidictAnnotator` loading in the pipeline?"
420,https://github.com/stanfordnlp/CoreNLP/issues/526,526,[],closed,2017-09-20 13:17:07+00:00,,Capturing as money from Decimal representation numbers along with their scales,"Example: RM460.35 million, RM460.354 million

With RegeNER : **RM[0-9]+ .[0-9] million MONEY MISC,NUMBER 1**  we could capture **RM460.3** as money
But
Could you please help us in capturing **RM460.35 million** money as whole"
421,https://github.com/stanfordnlp/CoreNLP/issues/528,528,"[{'id': 706089022, 'node_id': 'MDU6TGFiZWw3MDYwODkwMjI=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/windows', 'name': 'windows', 'color': '1d76db', 'default': False, 'description': None}]",closed,2017-09-21 23:29:38+00:00,,Errors parsing Web Treebank to Universal Dependency,"Hi there,

I was using my Windows system to use both stanford-parser and coreNLP to parse Web Treebank (in bracket form to UD Conllx form)

The following code fails with stanford-parser with the following error:
C:\Users\hp\Documents\Georgetown\GUM\SD-UD_converter_test\stanford-parser-full-2017-06-09\stanford-parser-full-2017-06-09>(java -cp ""*;"" -mx1g edu.stanford.nlp.trees.ud.UniversalDependenciesConverter -treeFile  20070404104007AAY1Chs_ans.xml.tree  1>20070404104007AAY1Chs_ans.xml_UD.conllu )
Exception in thread ""main"" java.lang.RuntimeException: Error loading flags.readerAndWriter: 'edu.stanford.nlp.sequences.ColumnDocumentReaderAndWriter'
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.makeReaderAndWriter(AbstractSequenceClassifier.java:227)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.reinit(AbstractSequenceClassifier.java:194)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.<init>(AbstractSequenceClassifier.java:171)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.<init>(AbstractSequenceClassifier.java:139)
        at edu.stanford.nlp.ie.ClassifierCombiner.<init>(ClassifierCombiner.java:139)
        at edu.stanford.nlp.ie.NERClassifierCombiner.<init>(NERClassifierCombiner.java:128)
        at edu.stanford.nlp.ie.NERClassifierCombiner.createNERClassifierCombiner(NERClassifierCombiner.java:273)
        at edu.stanford.nlp.ie.NERClassifierCombiner.createNERClassifierCombiner(NERClassifierCombiner.java:212)
        at edu.stanford.nlp.trees.ud.UniversalDependenciesConverter.addNERTags(UniversalDependenciesConverter.java:144)
        at edu.stanford.nlp.trees.ud.UniversalDependenciesConverter.convertTreeToBasic(UniversalDependenciesConverter.java:68)
        at edu.stanford.nlp.trees.ud.UniversalDependenciesConverter.access$000(UniversalDependenciesConverter.java:27)
        at edu.stanford.nlp.trees.ud.UniversalDependenciesConverter$TreeToSemanticGraphIterator.next(UniversalDependenciesConverter.java:99)
        at edu.stanford.nlp.trees.ud.UniversalDependenciesConverter$TreeToSemanticGraphIterator.next(UniversalDependenciesConverter.java:81)
        at edu.stanford.nlp.trees.ud.UniversalDependenciesConverter.main(UniversalDependenciesConverter.java:196)
Caused by: edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: Error creating edu.stanford.nlp.sequences.ColumnDocumentReaderAndWriter
        at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:40)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.makeReaderAndWriter(AbstractSequenceClassifier.java:225)
        ... 13 more
Caused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: java.lang.ClassNotFoundException: edu.stanford.nlp.sequences.ColumnDocumentReaderAndWriter
        at edu.stanford.nlp.util.MetaClass.createFactory(MetaClass.java:364)
        at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:381)
        at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:38)
        ... 14 more
Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.sequences.ColumnDocumentReaderAndWriter
        at java.net.URLClassLoader.findClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
        at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Unknown Source)
        at edu.stanford.nlp.util.MetaClass$ClassFactory.construct(MetaClass.java:135)
        at edu.stanford.nlp.util.MetaClass$ClassFactory.<init>(MetaClass.java:202)
        at edu.stanford.nlp.util.MetaClass$ClassFactory.<init>(MetaClass.java:69)
        at edu.stanford.nlp.util.MetaClass.createFactory(MetaClass.java:360)
        ... 16 more


It somehow runs with coreNLP but takes 10 seconds per file due to loadings:
>java -mx1024m -cp ""*;"" edu.stanford.nlp.trees.ud.UniversalDependenciesConverter -treeFile 20070404104007AAY1Chs_ans.xml.tree
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [7.2 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [2.1 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [5.1 sec].
[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
1       where   where   ADV     WRB     _       4       advmod  _       _
2       can     can     AUX     MD      _       4       aux     _       _
3       I       I       PRON    PRP     _       4       nsubj   _       _




I would like to get some help with this issue (hopefully it could run on both stanford-parser and coreNLP).

Thank you all.
Best,
Logan Peng"
422,https://github.com/stanfordnlp/CoreNLP/issues/529,529,[],closed,2017-09-22 13:10:28+00:00,,How to retrain an existing model,"We would like to train the existing POS tagger French models by removing accents from the training set.
Is it a way to remove accents directly without training the model again (from the .tagger file, or an option to the POS tagger annotator), or do i need to remove accents from the train File before training my own model ?

I cann't find the original trainfile (/u/nlp/data/lexparser/trees/FrenchCC/FTB-Train.utf8.txt), is it available somewhere?
"
423,https://github.com/stanfordnlp/CoreNLP/issues/531,531,[],closed,2017-09-24 08:58:38+00:00,,"edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator.loadModel(String,Properties) failed","my problem is that ,I have downloaded the standford-corenlp-3.7.0-chinese-models.jar by maven,but when I run my project ,I meet the error as follow:

> Caused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: MetaClass couldn't create public edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator(java.lang.String,java.util.Properties) with args [segment, {segment.sighanPostProcessing=true, coref.input.type=raw, segment.serDictionary=edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz, ner.language=chinese, regexner.noDefaultOverwriteLabels=CITY, coref.path.word2vec=, tokenize.language=zh, coref.defaultPronounAgreement=true, kbp.tokensregex=edu/stanford/nlp/models/kbp/chinese/tokensregex, ner.useSUTime=false, customAnnotatorClass.segment=edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator, coref.md.type=RULE, coref.postprocessing=true, kbp.model=none]
	at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:237)
	at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:382)
	at edu.stanford.nlp.pipeline.AnnotatorImplementations.custom(AnnotatorImplementations.java:188)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP$1.create(StanfordCoreNLP.java:562)
	at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:152)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:451)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:170)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:162)
	at com.nlp.job.Standalone.<init>(Standalone.java:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:147)
	... 44 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:233)
	... 57 more
Caused by: java.lang.RuntimeException: java.io.IOException: Unable to open ""edu/stanford/nlp/models/segmenter/chinese/ctb.gz"" as class path, filename or URL
	at edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator.loadModel(ChineseSegmenterAnnotator.java:105)
	at edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator.<init>(ChineseSegmenterAnnotator.java:83)
	... 62 more
Caused by: java.io.IOException: Unable to open ""edu/stanford/nlp/models/segmenter/chinese/ctb.gz"" as class path, filename or URL
	at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:470)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1492)
	at edu.stanford.nlp.ie.crf.CRFClassifier.getClassifier(CRFClassifier.java:2963)
	at edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator.loadModel(ChineseSegmenterAnnotator.java:101)
	... 63 more

this is my corenlp.properties : 

> # Pipeline options - lemma is no-op for Chinese but currently needed because coref demands it (bad old requirements system)
#annotators = tokenize, ssplit, pos, lemma, ner, parse, depparse, relation
annotators = segment, ssplit, pos, lemma, ner

# segment
customAnnotatorClass.segment = edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator

tokenize.language = zh

segment.model =edu/stanford/nlp/models/segmenter/chinese/ctb.gz
segment.sighanCorporaDict = edu/stanford/nlp/models/segmenter/chinese
segment.serDictionary = edu/stanford/nlp/models/segmenter/chinese/dict-chris6.ser.gz
segment.sighanPostProcessing = true
...

I guess my problem maybe caused by the path of segment.model,but I just copy the tutorial of
Standford-NLP,properties ,so I don't know how to make it   .


"
424,https://github.com/stanfordnlp/CoreNLP/issues/532,532,[],closed,2017-09-25 19:02:59+00:00,,How to use WordToSentenceProcessor in Python?,"I want to split a list of words into a list of sentences, with each sentence being a list of words.
This can be done by your WordToSentenceProcessor, but how can I use it in Python?
Can you give me an example?"
425,https://github.com/stanfordnlp/CoreNLP/issues/534,534,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}, {'id': 706083198, 'node_id': 'MDU6TGFiZWw3MDYwODMxOTg=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/sutime', 'name': 'sutime', 'color': 'c5def5', 'default': False, 'description': None}]",open,2017-09-28 02:47:28+00:00,,Chinese version of SUTime support ?,Any plan for Chinese version of SUTime support ?
426,https://github.com/stanfordnlp/CoreNLP/issues/535,535,"[{'id': 706064425, 'node_id': 'MDU6TGFiZWw3MDYwNjQ0MjU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/algorithm-error', 'name': 'algorithm-error', 'color': 'f9d0c4', 'default': False, 'description': None}, {'id': 706082923, 'node_id': 'MDU6TGFiZWw3MDYwODI5MjM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/pos', 'name': 'pos', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2017-10-01 10:49:15+00:00,,[Part of speech] Inaccurate proper nouns tagging.,"Could you explain why sometimes the proper nouns tagging (NNP) is inaccurate.
For instance in this sentence: ""Greater exposure to the market is required"".
**Greater** is a `NNP` (inaccurate).
While in this sentence: ""Better exposure to the market is required"".
**Better** is a `RBR` (accurate)

Another example:
In this sentence:  ""Automatic mail sending is an important feature.""
**Automatic** is a `NNP` (inaccurate).
While in this sentence: ""Instant mail sending is an important feature.""
**Instant** is a `JJ` (accurate).

Here is a screenshot of the NLP Core online demo:
![image](https://user-images.githubusercontent.com/955710/31053853-be11a9de-a6ae-11e7-9a2c-e23db76d05b9.png)
"
427,https://github.com/stanfordnlp/CoreNLP/issues/537,537,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 706055902, 'node_id': 'MDU6TGFiZWw3MDYwNTU5MDI=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/tokenize', 'name': 'tokenize', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2017-10-04 23:11:38+00:00,,"""tokenize.whitespace"" should not deal with unicode whitespaces","I have a pre-tokenized text, so I join the tokens by "" "", and set the ""tokenize.whitespace"" to ""true"" to skip the tokenization. But since there are some unicode whitespaces (U+200B: \xe2\x80\x8b) in some tokens, the tokenizer still split each of these tokens into two or more tokens. How can I stop the tokenizer from doing this?"
428,https://github.com/stanfordnlp/CoreNLP/issues/539,539,"[{'id': 712128209, 'node_id': 'MDU6TGFiZWw3MTIxMjgyMDk=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/build', 'name': 'build', 'color': 'c5def5', 'default': False, 'description': None}]",open,2017-10-06 07:51:52+00:00,,Building project generates a large number of warnings related to CoreNLPProtos,"Building the project results in a large number of warnings like following:

```
CoreNLPProtos.java:40451: warning: [cast] redundant cast to Builder
    [javac]         return (Builder) super.clearOneof(oneof);
```"
429,https://github.com/stanfordnlp/CoreNLP/issues/540,540,[],closed,2017-10-09 05:03:37+00:00,,Rodeo is not imorting seaborn even it is installed and my os is windows 10,"this is the error i am getting it :
ImportError: No module named 'seaborn'
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-19-ed9806ce3570> in <module>()
----> 1 import seaborn as sns
ImportError: No module named 'seaborn'

plz help me i like rodeo"
430,https://github.com/stanfordnlp/CoreNLP/issues/541,541,[],closed,2017-10-09 21:16:20+00:00,,I need VerbTense.java,"I want to change verb's tense using this https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/naturalli/VerbTense.java

but it's currently not in 3.8.0 version. Why ? And when can it be avaliable ? If not, is there any workaround for this ?

thank you"
431,https://github.com/stanfordnlp/CoreNLP/issues/547,547,[],open,2017-10-16 05:39:19+00:00,,java.lang.NumberFormatException: Bad number put into wordToNumber,"Same exceptions over and over again. 

2017-10-15 15:36:02 WARN  NumberNormalizer:81 - java.lang.NumberFormatException: Bad number put into wordToNumber.  Word is: ""2.7million"", originally part of ""2.7million"", piece # 0
  edu.stanford.nlp.ie.NumberNormalizer.wordToNumber(NumberNormalizer.java:294)
  edu.stanford.nlp.ie.NumberNormalizer.findNumbers(NumberNormalizer.java:636)
  edu.stanford.nlp.ie.NumberNormalizer.findAndMergeNumbers(NumberNormalizer.java:725)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressions(TimeExpressionExtractorImpl.java:189)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressions(TimeExpressionExtractorImpl.java:183)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressionCoreMaps(TimeExpressionExtractorImpl.java:114)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressionCoreMaps(TimeExpressionExtractorImpl.java:104)
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.runSUTime(NumberSequenceClassifier.java:345)
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.classifyWithSUTime(NumberSequenceClassifier.java:143)
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.classifyWithGlobalInformation(NumberSequenceClassifier.java:106)
  edu.stanford.nlp.ie.NERClassifierCombiner.recognizeNumberSequences(NERClassifierCombiner.java:369)
  edu.stanford.nlp.ie.NERClassifierCombiner.classifyWithGlobalInformation(NERClassifierCombiner.java:312)
  edu.stanford.nlp.ie.NERClassifierCombiner.classify(NERClassifierCombiner.java:299)
  edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyToCharacterOffsets(AbstractSequenceClassifier.java:618)


2017-10-15 15:38:22 WARN  NumberNormalizer:81 - java.lang.NumberFormatException: Bad number put into wordToNumber.  Word is: ""2.5million"", originally part of ""2.5million"", piece # 0
  edu.stanford.nlp.ie.NumberNormalizer.wordToNumber(NumberNormalizer.java:294)
  edu.stanford.nlp.ie.NumberNormalizer.findNumbers(NumberNormalizer.java:636)
  edu.stanford.nlp.ie.NumberNormalizer.findAndMergeNumbers(NumberNormalizer.java:725)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressions(TimeExpressionExtractorImpl.java:189)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressions(TimeExpressionExtractorImpl.java:183)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressionCoreMaps(TimeExpressionExtractorImpl.java:114)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressionCoreMaps(TimeExpressionExtractorImpl.java:104)
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.runSUTime(NumberSequenceClassifier.java:345)
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.classifyWithSUTime(NumberSequenceClassifier.java:143)
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.classifyWithGlobalInformation(NumberSequenceClassifier.java:106)
  edu.stanford.nlp.ie.NERClassifierCombiner.recognizeNumberSequences(NERClassifierCombiner.java:369)
  edu.stanford.nlp.ie.NERClassifierCombiner.classifyWithGlobalInformation(NERClassifierCombiner.java:312)
  edu.stanford.nlp.ie.NERClassifierCombiner.classify(NERClassifierCombiner.java:299)
  edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyToCharacterOffsets(AbstractSequenceClassifier.java:618)

  2017-10-15 15:42:44 WARN  NumberNormalizer:81 - java.lang.NumberFormatException: Bad number put into wordToNumber.  Word is: ""3.5million"", originally part of ""3.5million"", piece # 0
  edu.stanford.nlp.ie.NumberNormalizer.wordToNumber(NumberNormalizer.java:294)
  edu.stanford.nlp.ie.NumberNormalizer.findNumbers(NumberNormalizer.java:636)
  edu.stanford.nlp.ie.NumberNormalizer.findAndMergeNumbers(NumberNormalizer.java:725)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressions(TimeExpressionExtractorImpl.java:189)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressions(TimeExpressionExtractorImpl.java:183)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressionCoreMaps(TimeExpressionExtractorImpl.java:114)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressionCoreMaps(TimeExpressionExtractorImpl.java:104)
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.runSUTime(NumberSequenceClassifier.java:345)
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.classifyWithSUTime(NumberSequenceClassifier.java:143)
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.classifyWithGlobalInformation(NumberSequenceClassifier.java:106)
  edu.stanford.nlp.ie.NERClassifierCombiner.recognizeNumberSequences(NERClassifierCombiner.java:369)
  edu.stanford.nlp.ie.NERClassifierCombiner.classifyWithGlobalInformation(NERClassifierCombiner.java:312)
  edu.stanford.nlp.ie.NERClassifierCombiner.classify(NERClassifierCombiner.java:299)
  edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyToCharacterOffsets(AbstractSequenceClassifier.java:618)


  2017-10-15 16:32:36 WARN  NumberNormalizer:81 - java.lang.NumberFormatException: Bad number put into wordToNumber.  Word is: ""1783.9million"", originally part of ""1,783.9million"", piece # 0
  edu.stanford.nlp.ie.NumberNormalizer.wordToNumber(NumberNormalizer.java:294)
  edu.stanford.nlp.ie.NumberNormalizer.findNumbers(NumberNormalizer.java:636)
  edu.stanford.nlp.ie.NumberNormalizer.findAndMergeNumbers(NumberNormalizer.java:725)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressions(TimeExpressionExtractorImpl.java:189)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressions(TimeExpressionExtractorImpl.java:183)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressionCoreMaps(TimeExpressionExtractorImpl.java:114)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressionCoreMaps(TimeExpressionExtractorImpl.java:104)
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.runSUTime(NumberSequenceClassifier.java:345)
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.classifyWithSUTime(NumberSequenceClassifier.java:143)
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.classifyWithGlobalInformation(NumberSequenceClassifier.java:106)
  edu.stanford.nlp.ie.NERClassifierCombiner.recognizeNumberSequences(NERClassifierCombiner.java:369)
  edu.stanford.nlp.ie.NERClassifierCombiner.classifyWithGlobalInformation(NERClassifierCombiner.java:312)
  edu.stanford.nlp.ie.NERClassifierCombiner.classify(NERClassifierCombiner.java:299)
  edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyToCharacterOffsets(AbstractSequenceClassifier.java:618)
  com.innefu.util.NERSentimentUtil.getStanford(NERSentimentUtil.java:382)

  2017-10-15 16:32:36 WARN  NumberNormalizer:81 - java.lang.NumberFormatException: Bad number put into wordToNumber.  Word is: ""356.8million"", originally part of ""356.8million"", piece # 0
  edu.stanford.nlp.ie.NumberNormalizer.wordToNumber(NumberNormalizer.java:294)
  edu.stanford.nlp.ie.NumberNormalizer.findNumbers(NumberNormalizer.java:636)
  edu.stanford.nlp.ie.NumberNormalizer.findAndMergeNumbers(NumberNormalizer.java:725)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressions(TimeExpressionExtractorImpl.java:189)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressions(TimeExpressionExtractorImpl.java:183)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressionCoreMaps(TimeExpressionExtractorImpl.java:114)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressionCoreMaps(TimeExpressionExtractorImpl.java:104)
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.runSUTime(NumberSequenceClassifier.java:345)
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.classifyWithSUTime(NumberSequenceClassifier.java:143)
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.classifyWithGlobalInformation(NumberSequenceClassifier.java:106)
  edu.stanford.nlp.ie.NERClassifierCombiner.recognizeNumberSequences(NERClassifierCombiner.java:369)
  edu.stanford.nlp.ie.NERClassifierCombiner.classifyWithGlobalInformation(NERClassifierCombiner.java:312)
  edu.stanford.nlp.ie.NERClassifierCombiner.classify(NERClassifierCombiner.java:299)
  edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyToCharacterOffsets(AbstractSequenceClassifier.java:618)


  2017-10-16 00:20:12 WARN  NumberNormalizer:81 - java.lang.NumberFormatException: Bad number put into wordToNumber.  Word is: ""1.7billion"", originally part of ""1.7billion"", piece # 0
  edu.stanford.nlp.ie.NumberNormalizer.wordToNumber(NumberNormalizer.java:294)
  edu.stanford.nlp.ie.NumberNormalizer.findNumbers(NumberNormalizer.java:636)
  edu.stanford.nlp.ie.NumberNormalizer.findAndMergeNumbers(NumberNormalizer.java:725)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressions(TimeExpressionExtractorImpl.java:189)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressions(TimeExpressionExtractorImpl.java:183)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressionCoreMaps(TimeExpressionExtractorImpl.java:114)
  edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressionCoreMaps(TimeExpressionExtractorImpl.java:104)
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.runSUTime(NumberSequenceClassifier.java:345)
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.classifyWithSUTime(NumberSequenceClassifier.java:143)
  edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.classifyWithGlobalInformation(NumberSequenceClassifier.java:106)
  edu.stanford.nlp.ie.NERClassifierCombiner.recognizeNumberSequences(NERClassifierCombiner.java:369)
  edu.stanford.nlp.ie.NERClassifierCombiner.classifyWithGlobalInformation(NERClassifierCombiner.java:312)
  edu.stanford.nlp.ie.NERClassifierCombiner.classify(NERClassifierCombiner.java:299)
  edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyToCharacterOffsets(AbstractSequenceClassifier.java:618)

"
432,https://github.com/stanfordnlp/CoreNLP/issues/548,548,"[{'id': 706089022, 'node_id': 'MDU6TGFiZWw3MDYwODkwMjI=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/windows', 'name': 'windows', 'color': '1d76db', 'default': False, 'description': None}]",open,2017-10-17 09:45:01+00:00,,dcoref always returns null - Coreferencing,"Hi, I am very interested in getting this example to work. I tried this example in Standford.NLP.NET and I keep getting null, for Map<Integer, CorefChain> coref = document.get(CorefChainAnnotation.class).

This is what I am trying to recreate in C#.

JAVA
Map<Integer, CorefChain> coref = document.get(CorefChainAnnotation.class);

for(Map.Entry<Integer, CorefChain> entry : coref.entrySet()) {
CorefChain c = entry.getValue();

//this is because it prints out a lot of self references which aren't that useful
if(c.getCorefMentions().size() <= 1)
    continue;

CorefMention cm = c.getRepresentativeMention();
String clust = """";
List<CoreLabel> tks = document.get(SentencesAnnotation.class).get(cm.sentNum-1).get(TokensAnnotation.class);
for(int i = cm.startIndex-1; i < cm.endIndex-1; i++)
    clust += tks.get(i).get(TextAnnotation.class) + "" "";
clust = clust.trim();
System.out.println(""representative mention: \"""" + clust + ""\"" is mentioned by:"");

for(CorefMention m : c.getCorefMentions()){
    String clust2 = """";
    tks = document.get(SentencesAnnotation.class).get(m.sentNum-1).get(TokensAnnotation.class);
    for(int i = m.startIndex-1; i < m.endIndex-1; i++)
        clust2 += tks.get(i).get(TextAnnotation.class) + "" "";
    clust2 = clust2.trim();
    //don't need the self mention
    if(clust.equals(clust2))
        continue;

    System.out.println(""\t"" + clust2);
}"
433,https://github.com/stanfordnlp/CoreNLP/issues/549,549,"[{'id': 706089022, 'node_id': 'MDU6TGFiZWw3MDYwODkwMjI=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/windows', 'name': 'windows', 'color': '1d76db', 'default': False, 'description': None}]",open,2017-10-18 14:07:12+00:00,,Failure to load language specific properties for en,"I am using Windows 10 with Java 1.8. I downloaded the CoreNLP and the English model jars from [the official website](https://stanfordnlp.github.io/CoreNLP/download.html), then unzip them into the same directory. I also set the enviroment variables ""CLASSPATH"" point to that directory.

Now I follow the instructions [here](https://stanfordnlp.github.io/CoreNLP/corenlp-server.html) to run a CoreNLP Server. 

```
java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000
```

When I open http://localhost:9000/ for testing, an error occur, though the POS result seems correct:

> ""ERROR CoreNLP - Failure to load language specific properties"".

Here's the screenshot while running the server and I outlined the error.

![image](https://user-images.githubusercontent.com/14769033/31723099-7ff4170e-b450-11e7-84a6-90dd883ad7f2.png)

Any ideas of solving it?




"
434,https://github.com/stanfordnlp/CoreNLP/issues/550,550,"[{'id': 706059615, 'node_id': 'MDU6TGFiZWw3MDYwNTk2MTU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/ner', 'name': 'ner', 'color': 'c5def5', 'default': False, 'description': None}, {'id': 706064425, 'node_id': 'MDU6TGFiZWw3MDYwNjQ0MjU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/algorithm-error', 'name': 'algorithm-error', 'color': 'f9d0c4', 'default': False, 'description': None}]",closed,2017-10-20 00:37:59+00:00,,British-style date only recognized as Number and not Date entity,"Testing online, it seems British-style date only recognized as Number and not Date entity:
```
I'll be away 20/10/2017 to 25/10/2017
```

If you can provide some guidance on the fix; I'll implement and do a pull request.  
From my quick inspection of the code, it is possibly an additional line of RegExp; is it correct?  
Also, it will be good to give the corenlp some hints to prioritize British-style date."
435,https://github.com/stanfordnlp/CoreNLP/issues/551,551,[],closed,2017-10-24 11:18:24+00:00,,How to set language for CoreNLP Simple API,"Hi,

I am using Simple API in my Spark applications. It is very fast compare to normal pipeline/annotation. I was wondering how to set a different language for my POS tagger.
Here is how I use it for default English in Scala:

```
val newSentences = new Document(document).sentences().asScala.map(_.text())
val wordsArray = new Sentence(finalSentence).words().asScala
val posTagsArray = new Sentence(finalSentence).posTags().asScala
```

That's being said, I saw in the code there is option to pass properties for Simple API:

```
public Sentence(String text, Properties props) {
    // Set document
    this.document = new Document(text);
    // Set sentence
    if (props.containsKey(""ssplit.isOneSentence"")) {
      this.impl = this.document.sentence(0, props).impl;
    } else {
      Properties modProps = new Properties(props);
      modProps.setProperty(""ssplit.isOneSentence"", ""true"");
      this.impl = this.document.sentence(0, modProps).impl;
    }
    // Set tokens
    this.tokensBuilders = document.sentence(0).tokensBuilders;
    // Asserts
    assert (this.document.sentence(0).impl == this.impl);
    assert (this.document.sentence(0).tokensBuilders == this.tokensBuilders);
  }
```
Even posTags can pas properties variable:

```
public List<String> posTags(Properties props) {
    document.runPOS(props);
    synchronized (impl) {
      return lazyList(tokensBuilders, CoreNLPProtos.Token.Builder::getPos);
    }
  }
```

But neither works when I set fr as a language:

```
val props = new Properties()
props.setProperty(""tokenize.language"", ""fr"")
val posTagsArray = new Sentence(""Au fond, les choses sont assez simples."", props).posTags(props)
```

Does anyone know how to change language for Simple API? 

Many thanks.

"
436,https://github.com/stanfordnlp/CoreNLP/issues/552,552,[],closed,2017-10-25 04:11:07+00:00,,Semgrex { } matches multiple nodes in child patterns,"Semgrex documentation suggests that a pair of curly braces `{ }` represents a single token and that multiple assertions of the same token can be grouped with square brackets `[{ } | { }]`. It appears, however, that the components of an AND `CoordinationPattern` are applied independently, such that a single set of braces or brackets may end up matching multiple tokens.

Today, these patterns are functionally equivalent, but the documentation suggests that the third and fourth should behave differently, ensuring that the `pos` and `word` descriptions apply to the same node:
‚úÖ  `{$} > { pos:JJS } > { word:most }`
‚úÖ  `{$} [> { pos:JJS } & > { word:most }]`
‚ùå  `{$} > { pos:JJS; word:most }`
‚ùå  `{$} > [{ pos:JJS } & { word:most }]`

This example is verifiable at [http://corenlp.run](http://corenlp.run).
Input: `She said hello.`
Pattern: `{$} > { word:She; word:hello }`

Clearly, no node can have the `TextAnnotation` value of ""She"" and at the same time a value of ""hello,"" yet this pattern matches because the root has dependents that match each child of the `CoordinationPattern`.

If this is expanded out, one can name the nodes and see that they both match:
Pattern: `{$} > [{ word:She }=sheNode & { word:hello }=helloNode]`

The issue appears to be particular to child patterns. If one inverts that pattern, it [correctly] no longer matches: `[{ word:She } & { word:hello }] < {$}`

### Why it matters:
Grouping node patterns is the only clear way to assert OR conditions, to assert positive and negative attributes of a node, or both ""rootness"" and any other attribute(s).
```
[{$} & { pos:/NN.*/ }=root] >amod [{ pos:JJS }=sup & !{ word:most }]
```
If the descriptions in `{ ; }` or `[{ } & !{ }]` or `[{$} & { }]` patterns don't reliably refer to the same node, then the power and accuracy of Semgrex is unnecessarily limited.

### A limited workaround:
The only way I've seen to describe the same node in a child pattern is to put each description in its own set of braces `{ }`, name every node, and assert their equality or inequality at the end using backreferences. Even then, I don't believe this fully achieves what a proper fix would do for all cases.
```
[{$} & { pos:/NN.*/ }=root] >amod [{ pos:JJS }=ref1 & !{ word:most }=ref2] : {}=ref1 == {}=ref2
```"
437,https://github.com/stanfordnlp/CoreNLP/issues/553,553,[],closed,2017-10-26 02:29:02+00:00,,Is sentiment treebank the training data? How to prepare my own train data?,"Hi, 
For the sentiment analysis model,  I want to train by myself, I saw this page for instruction, https://nlp.stanford.edu/sentiment/code.html
**Models can be retrained using the following command using the PTB format dataset:
java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath dev.txt -train -model model.ser.gz**

My question is,  what's the relationship between train data here and the sentiment treebank data mentioned in that paper for sentiment analysis model?
And If I make annotated data by myself, how to make that in the PTB format, is there any tools?

thanks."
438,https://github.com/stanfordnlp/CoreNLP/issues/554,554,[],closed,2017-10-26 02:55:45+00:00,,QuoteAttribution Exceptions,"**Context**

- Feature: CoreNLP server QuoteAttribution annotator
- Models: current base, current english, current english kbp (10/24/17)
- Build: ant, JDK .18
- Version: Github (master 10/24/17)
- OS:  Ubuntu @ latest (Docker)
---

**Reproduce**

- Send request to CoreNLP server with annotators set to 'quoteattribution'

---

**Details**

The QuoteAttribution annotator seems to be hard-coded into the CLI and inaccessible from CoreNLP server. Can anyone confirm this is the case? That's my assessment, but I know very little Java. 

Initially, QuoteAttribution couldn't find the required file path props (family words, gender, animations), but I resolved it by changing the default (outdated?) path to each. Adding props or a props file during startup didn't work. Now, I'm getting a null pointer in Sieve.java because of a null character map, which seems to be caused by a missing Person. In the case of a missing character map, ""Person"" should be generated. I've confirmed the person in the text being used is being identified as a person in other methods at http://corenlp.run. I'm also getting the same null pointer error at http://corenlp.run, but I can't verify it's the same issue without a stack trace.

So at this point, I'm wondering if I have an incorrect setup, QuoteAttribution isn't fully integrated, or both. I also welcome any clarification regarding the requirement for a generated BookNLP file and solutions that work dynamically with the CoreNLP server.

I would appreciate any input on the subject. I'm willing to commit any improvements or documentation that come from the discussion. Obviously I don't know Java well, but I imagine those additions would be referential in nature. 

---

**Error**

java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:206)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:770)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
        at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
        at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
        at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
        at edu.stanford.nlp.quoteattribution.Sieves.Sieve.createNameMatcher(Sieve.java:128)
        at edu.stanford.nlp.quoteattribution.Sieves.Sieve.<init>(Sieve.java:40)
        at edu.stanford.nlp.quoteattribution.Sieves.QMSieves.QMSieve.<init>(QMSieve.java:32)
        at edu.stanford.nlp.quoteattribution.Sieves.QMSieves.TrigramSieve.<init>(TrigramSieve.java:23)
        at edu.stanford.nlp.pipeline.QuoteAttributionAnnotator.getQMMapping(QuoteAttributionAnnotator.java:220)
        at edu.stanford.nlp.pipeline.QuoteAttributionAnnotator.annotate(QuoteAttributionAnnotator.java:206)
        at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:652)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.lambda$handle$0(StanfordCoreNLPServer.java:752)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        ... 3 more"
439,https://github.com/stanfordnlp/CoreNLP/issues/555,555,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}, {'id': 706059615, 'node_id': 'MDU6TGFiZWw3MDYwNTk2MTU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/ner', 'name': 'ner', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2017-10-27 05:17:49+00:00,,Merge ner and entitymentions annotators,I think it would make sense to just merge these into one annotator.
440,https://github.com/stanfordnlp/CoreNLP/issues/556,556,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 732360245, 'node_id': 'MDU6TGFiZWw3MzIzNjAyNDU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/protobuf', 'name': 'protobuf', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2017-10-27 16:31:14+00:00,,CoreNLP 3.8 fails in Apache Spark,"Hi,

I can use CoreNLP 3.6 and 3.7 simply by calling these jars in my Spark app (1.6 and 2.2):

spark-shell --master yarn --deploy-mode client --queue multivac --driver-cores 5 --driver-memory 8g --executor-cores 5 --executor-memory 4g --num-executors 30 --jars `/home/jars/stanford-corenlp-3.7.0/ejml-0.23.jar,/home/jars/stanford-corenlp-3.7.0/stanford-corenlp-3.7.0.jar,/home/jars/stanford-corenlp-3.7.0/stanford-corenlp-3.7.0-models.jar,/home/jars/stanford-corenlp-3.7.0/protobuf.jar,/home/jars/stanford-corenlp-3.7.0/jollyday.jar`

But if I try the same set of jars from CoreNLP 3.8 it always fails with this error:

```
scala> import edu.stanford.nlp.simple._
scala> new Sentence(document).words()

java.lang.VerifyError: Bad type on operand stack
Exception Details:
  Location:
    com/google/protobuf/GeneratedMessageV3$ExtendableMessage.getExtension(Lcom/google/protobuf/GeneratedMessage$GeneratedExtension;I)Ljava/lang/Object; @3: invokevirtual
  Reason:
    Type 'com/google/protobuf/GeneratedMessage$GeneratedExtension' (current frame, stack[1]) is not assignable to 'com/google/protobuf/ExtensionLite'
  Current Frame:
    bci: @3
    flags: { }
    locals: { 'com/google/protobuf/GeneratedMessageV3$ExtendableMessage', 'com/google/protobuf/GeneratedMessage$GeneratedExtension', integer }
    stack: { 'com/google/protobuf/GeneratedMessageV3$ExtendableMessage', 'com/google/protobuf/GeneratedMessage$GeneratedExtension', integer }
  Bytecode:
    0x0000000: 2a2b 1cb6 0024 b0

  at edu.stanford.nlp.simple.Document.<init>(Document.java:433)
  at edu.stanford.nlp.simple.Sentence.<init>(Sentence.java:118)
  at edu.stanford.nlp.simple.Sentence.<init>(Sentence.java:126)
  ... 56 elided
```

Any help is appreciated,

Cheers,
Maziyar"
441,https://github.com/stanfordnlp/CoreNLP/issues/558,558,[],closed,2017-10-30 06:35:34+00:00,,NER Classifier unable to classify time,"Hi, I am using the latest CoreNLP jar (stanford-corenlp-models-current.jar), but when I am using the 7 class NER classifier to get the tags, I am not able to get tags for time , eg. for 6 pm ,NER classifies it as 'O' rather than time. Can anyone please help me with the issue and tell me where I might have gone wrong. Thank you."
442,https://github.com/stanfordnlp/CoreNLP/issues/559,559,[],closed,2017-11-01 09:14:51+00:00,,ProtobufAnnotationSerializer will crash on documents with 0 mentions/0 coref chains,There needs to be a distinction between empty list of coref chains and no coref annotation of any sort.
443,https://github.com/stanfordnlp/CoreNLP/issues/560,560,[],closed,2017-11-02 14:20:22+00:00,,Parsing neural coref output (say xml) and resolving the references correspondingly in the original text.,What is the best approach to resolve all the references in a text document (as given from the **neural coref** output) and modify the original document accordingly?
444,https://github.com/stanfordnlp/CoreNLP/issues/562,562,[],closed,2017-11-03 21:23:24+00:00,,Simple Document() constructor does not utilize Properties parameter,"The `edu.stanford.nlp.simple.Document.Document(Properties, String)` constructor does not utilize the Properties parameter as per the JavaDoc.

```
  public Document(Properties props, String text) {
    this.impl = CoreNLPProtos.Document.newBuilder().setText(text);
  }
```
"
445,https://github.com/stanfordnlp/CoreNLP/issues/563,563,[],closed,2017-11-06 00:56:24+00:00,,Remove old TokensRegexNER file and references to it,"Our code still refers to and has a DefaultPaths to `type_map_clean`, but it's really our tackbp2010 regexner. We should everywhere change to the two current files - currently in `kbp/regexner_* but they should maybe be moved out of the kbp dir.
"
446,https://github.com/stanfordnlp/CoreNLP/issues/564,564,[],closed,2017-11-06 07:08:49+00:00,,Contributions to Standford NLP Project,"Hi Team,

We have been using the Standford NLP for textual extraction since 2 years. 
And It is an awesome software built which is helping us in lot many ways.

However we are facing some issues with the identification of NUMBERS and CURRENCIES from RegexNER rule file. 
Some of them we have reported in the GitHub for which we have received the responses. However it was not addressing all the issues.

So the reason why we are writing this mail is that we would like know more information on the below lists

1.	From where should we download the Standford NLP source project ?
2.	Which package can we modify in order to fix the issues with NUMBERS and CURRENCIES format given in RegexNER rule file ?
3.	Should we have to become a member of the group in order to contribute ? If yes how would we become ?
4.	How should we post the changes to the project ?
5.	How can we test the tool on the contributions/fixes made from our end ?
6.	What the conditions to contribute to the project ?
7.	What are the overall in and out procedure in steps to contribute to the project ?
8.	Who is the point of contact to work with for any further information or clarification needed throughout the contribution ? 

We hope you will kindly consider our request and respond positively ASAP.
Looking forward. 
Thank you in advance.
"
447,https://github.com/stanfordnlp/CoreNLP/issues/565,565,[],closed,2017-11-06 07:18:58+00:00,,Numbers represented in words are extracted only half of them,"Hi Team,

Considering the below example sentence
There is an issue in number **seven hundred and seventeen**.

The NormalizedNamedEntityTagAnnotation of seven hundred and seventeen should be **717** but NLP is identifying it as only **100**

![image](https://user-images.githubusercontent.com/20786984/32429424-9c9ae9b0-c2f0-11e7-8d5b-6c07a5ce434e.png)

There are multiple examples with such issues. Please help us in overcoming these types.
Thank you in advance."
448,https://github.com/stanfordnlp/CoreNLP/issues/566,566,[],closed,2017-11-07 09:39:05+00:00,,How to map questions to questions in Stanford NLP,"
I am new to NLP. I am working on a virtual avatar that communicates with the user. I want the system to answer the questions of the users. The Avatar and the entire system has been created with .NET. Therefore I had to choose Stanford NLP for .NET. The problem I have here is a question can be asked in many ways. I want to identify the questions that has similar meaning and map it to one question, therefore I can return the predefined answer. I know this can be done using NLTK or other NLP tools. Can this be done in Stanford NLP. If yes how can I achieve this?


"
449,https://github.com/stanfordnlp/CoreNLP/issues/567,567,[],closed,2017-11-07 09:51:25+00:00,,Emotion Dictionary,"I want to find the emotional dictionary in the project , I hope you can give some advice"
450,https://github.com/stanfordnlp/CoreNLP/issues/568,568,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 706088581, 'node_id': 'MDU6TGFiZWw3MDYwODg1ODE=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/kbp', 'name': 'kbp', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2017-11-08 05:20:59+00:00,,KBP relation extraction breaking with -ssplit.eolonly (ProtobufAnnotationSerializer issue),"Command:

`java -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,regexner,entitymentions,parse,mention,coref,kbp -ssplit.eolonly -file missing-relations.txt -outputFormat text`

Error:

```
Exception in thread ""main"" java.lang.NullPointerException
	at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.fromProto(ProtobufAnnotationSerializer.java:1556)
	at edu.stanford.nlp.simple.Document.asAnnotation(Document.java:1064)
	at edu.stanford.nlp.simple.Sentence.asCoreMap(Sentence.java:1089)
	at edu.stanford.nlp.ie.KBPTokensregexExtractor.classify(KBPTokensregexExtractor.java:88)
	at edu.stanford.nlp.ie.KBPEnsembleExtractor.classify(KBPEnsembleExtractor.java:61)
	at edu.stanford.nlp.pipeline.KBPAnnotator.annotate(KBPAnnotator.java:430)
	at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:652)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:662)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$73/102227435.accept(Unknown Source)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1231)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1002)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1336)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1407)
```"
451,https://github.com/stanfordnlp/CoreNLP/issues/569,569,"[{'id': 706064425, 'node_id': 'MDU6TGFiZWw3MDYwNjQ0MjU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/algorithm-error', 'name': 'algorithm-error', 'color': 'f9d0c4', 'default': False, 'description': None}, {'id': 735987943, 'node_id': 'MDU6TGFiZWw3MzU5ODc5NDM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/depparse', 'name': 'depparse', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2017-11-16 01:46:11+00:00,,Misprediction on dep parse,"Dependency Parse has a misprediction: 
""How far did the sea level drop?"" 
ADVMOD ROOT DEP	DET	COMPOUND COMPOUND DOB
(assuming you want mispredicitons here, as I found no other place). 

"
452,https://github.com/stanfordnlp/CoreNLP/issues/570,570,[],closed,2017-11-16 12:02:42+00:00,,what do maxLeft and maxRight means to NER,"Please help me.
java -cp ../../stanford-corenlp-3.7.0.jar edu.stanford.nlp.ie.crf.CRFClassifier -prop austen.prop

In the austen.prop file for crf ner module, When I set maxLeft and maxRight  both to 5, 
the Precision and Recall both rise to 0.99(from 0.9 when maxLeft = 3).
Why? is it reasonable?
Thanks!"
453,https://github.com/stanfordnlp/CoreNLP/issues/571,571,[],closed,2017-11-18 17:40:51+00:00,,[CoreNLP Server] Order not preserved,"Working with the CoreNLP server's HTTP API I found that, when calling the annotation endpoint with large data (i.e. >= 200 sentences, separated by dots), the order of the sentences is not always preserved. Sometimes sentences that were close togehter in the input seem to be randomly swapped in the respone's `sentences` array. E.g. the sentence with index 100 from my input ends up at position 102 in the output. I only encountered that behaviour for large inputs, while for smaller inputs the order seems to be preserved. Maybe it's a bug, i don't know.

My actual question is: can I pass an identifier-like parameter for each sentence to reference it in the JSON output again?

EDIT: Never mind, I had a mistake in my script. Sorry."
454,https://github.com/stanfordnlp/CoreNLP/issues/572,572,[],closed,2017-11-19 09:32:08+00:00,,text output format and client/server sentiment NullPointerException,"Because the `TextOutputter` assumes the presence of the associated probabilities 
 https://github.com/stanfordnlp/CoreNLP/blob/59064306d4487fec7533172f1e48d31b78d189c1/src/edu/stanford/nlp/pipeline/TextOutputter.java#L127

and because those probabilities are not transmitted from server to client, you may get a
`Exception in thread ""Thread-2"" java.lang.NullPointerException
	at edu.stanford.nlp.neural.rnn.RNNCoreAnnotations.getPredictedClassProb(RNNCoreAnnotations.java:124)
`

In lieu of adding the probabilities vector to the CoreNLP.proto, I just made a quick fix, the downside of which is that all the sentiment probabilities will appear as -1.0

https://github.com/dickmao/CoreNLP/blob/5a0b2693b0046542a72aff2ca38806c500d03f04/src/edu/stanford/nlp/neural/rnn/RNNCoreAnnotations.java#L123"
455,https://github.com/stanfordnlp/CoreNLP/issues/573,573,"[{'id': 45387507, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNw==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/cantreproduce', 'name': 'cantreproduce', 'color': 'dddddd', 'default': False, 'description': None}]",closed,2017-11-20 09:07:36+00:00,,"While loading the MaxentTagger , getting Cast exception","Loading default properties from tagger stanfordmodels/english-left3words-distsim.tagger
Reading POS tagger model from stanfordmodels/english-left3words-distsim.tagger ... Exception in thread ""main"" java.lang.ClassCastException: edu.stanford.nlp.process.AmericanizeFunction cannot be cast to edu.stanford.nlp.util.Function

MaxentTagger tagger = new MaxentTagger(taggerPath);"
456,https://github.com/stanfordnlp/CoreNLP/issues/574,574,[],closed,2017-11-21 16:12:52+00:00,,Modifying order of sentence array creates NPE,"When I process a text with the simple API (which is a really nice API btw), if I change the order of the sentences (via `Collections.reverse()` or `Collections.shuffle()`) and then make calls that require a model e.g. `parse()` those calls generate an NPE.

Workaround is to iterate through items via `get(int)` method on `List<Sentence>` instance from `Document.sentences()` method.

Code to recreate the error:
```
import edu.stanford.nlp.simple.*;
import java.util.*;

public class SimpleApiTest {

    public static void main(String[] whocares) {
        Document d = new Document(""Hey everybody just wanted to say this is a new document. Wonder if this will work? This is another sentence. This is yet another sentence."");

        List<Sentence> sents = d.sentences();

        Collections.reverse(sents);

        for (Sentence aSent : sents)
        {
            System.out.println(""aSent : ""+aSent.text());
            System.out.println(aSent.parse().score());
        }
    }
}
```

stack trace:
```
Exception in thread ""main"" java.lang.NullPointerException
	at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.fromProto(ProtobufAnnotationSerializer.java:1507)
	at edu.stanford.nlp.simple.Document.asAnnotation(Document.java:1091)
	at edu.stanford.nlp.simple.Document.runParse(Document.java:922)
	at edu.stanford.nlp.simple.Sentence.parse(Sentence.java:637)
	at edu.stanford.nlp.simple.Sentence.parse(Sentence.java:645)
	at SimpleApiTest.main(SimpleApiTest.java:15)
```

workaround:

```
import edu.stanford.nlp.simple.*;
import java.util.*;

public class SimpleApiTest {

    public static void main(String[] whocares) {
        Document d = new Document(""Hey everybody just wanted to say this is a new document. Wonder if this will work? This is another sentence. This is yet another sentence."");

        List<Sentence> sents = d.sentences();

        for (int x=sents.size()-1; x>=0; x--)
        {
            System.out.println(""x : ""+x+"" : ""+sents.get(x).parse().score());
        }
    }
}
```"
457,https://github.com/stanfordnlp/CoreNLP/issues/575,575,"[{'id': 706064425, 'node_id': 'MDU6TGFiZWw3MDYwNjQ0MjU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/algorithm-error', 'name': 'algorithm-error', 'color': 'f9d0c4', 'default': False, 'description': None}, {'id': 706082923, 'node_id': 'MDU6TGFiZWw3MDYwODI5MjM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/pos', 'name': 'pos', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2017-11-21 21:12:33+00:00,,"""Shall"" is parsed as a name rather than an auxiliary verb. ","The POS tagging of ""Shall Bob eat pizza?"" is flawed. ""Shall"" is tagged as ""NNP"" when it should be ""MD"":

""Shall Bob eat pizza?"":

This is tokenized as ""Shall"", ""Bob"", ""eat"", ""pizza"", ""?""; with POS values of ""NNP"", ""NNP"", ""VB"", ""NN"", ""."", when it should be ""MD"", ""NNP"", ""VB"", ""NN"", ""."".

This can be reproduced on corenlp.run."
458,https://github.com/stanfordnlp/CoreNLP/issues/576,576,[],closed,2017-11-21 22:00:29+00:00,,Starting auxiliary verbs are tagged as regular verbs,"The POS tagging of ""Is Bob eating pizza?"" is flawed. ""Is"" is tagged as ""VBZ"" when it should be ""MD"":

""Is Bob eat pizza?"": This is tokenized as ""Is"", ""Bob"", ""eat"", ""pizza"", ""?""; with POS values of ""VBZ"", ""NNP"", ""VB"", ""NN"", ""."", when it should be ""MD"", ""NNP"", ""VB"", ""NN"", ""."".

There's a similar issue with ""Did Bob eat pizza?"". ""Did"" gets tagged as ""VBD"".

Additionally, in ""Has Bob eaten pizza?"". ""Has"" gets tagged as ""VBZ""."
459,https://github.com/stanfordnlp/CoreNLP/issues/578,578,"[{'id': 626016953, 'node_id': 'MDU6TGFiZWw2MjYwMTY5NTM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/analysis-bug', 'name': 'analysis-bug', 'color': 'f98685', 'default': False, 'description': None}]",closed,2017-12-01 13:59:37+00:00,,"Distinguish possessive from a contracted ""is""","CoreNLP incorrectly tags contracted-s as possessives. 

Bob's dog ate a bone. (possessive)
Bob's eating a burger. (contraction - ""is eating"") 
"
460,https://github.com/stanfordnlp/CoreNLP/issues/580,580,[],closed,2017-12-01 15:50:23+00:00,,Failure on Infinitives,"Infinitives are incorrectly being treated like verbs. 

Should be: (adverbial phrase), preposition + noun
""To work is honorable.""
""To dream creates opportunity."" 

"
461,https://github.com/stanfordnlp/CoreNLP/issues/581,581,[],closed,2017-12-04 19:11:06+00:00,,constituency parse failure on verb references,"The constituency parse is not parsing ""What is being very fat called?"" in a logical way. 

The tree yielded is: ( What (is (being (very fat called)))), when it should be (what (is ((being (very fat)) called))). ""Called"" is not part of the phrase ""being very fat"". It should be an SBAR of the root verb.

This is what it is as of the latest verison:
![parsefailure](https://user-images.githubusercontent.com/10189447/33570936-75501034-d8fc-11e7-8c16-9ec9aca2ac71.PNG)

This is what it should be:
![thesbarbranchshouldbemoved](https://user-images.githubusercontent.com/10189447/33571099-f3306076-d8fc-11e7-987a-0491cc0b1e88.PNG)
"
462,https://github.com/stanfordnlp/CoreNLP/issues/582,582,[],closed,2017-12-05 01:34:52+00:00,,Error while trying to load the StanfordCoreNLP module,"while trying to run the following i got the below error.Couldnt figure out whats the issue with it.If its with the python version syntax error or any thing else.It would be great if someone who knows this can help me fix this.Thanks for your time
>>> from corenlp import *
>>> corenlp = StanfordCoreNLP()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""corenlp.py"", line 163, in __init__
    self.corenlp = pexpect.spawn(start_corenlp)
  File ""/usr/lib/python2.7/dist-packages/pexpect/pty_spawn.py"", line 194, in __init__
    self._spawn(command, args, preexec_fn, dimensions)
  File ""/usr/lib/python2.7/dist-packages/pexpect/pty_spawn.py"", line 267, in _spawn
    'executable: %s.' % self.command)
pexpect.exceptions.ExceptionPexpect: The command was not found or was not executable: java."
463,https://github.com/stanfordnlp/CoreNLP/issues/583,583,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}, {'id': 45387509, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOQ==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/wontfix', 'name': 'wontfix', 'color': 'eeeeee', 'default': True, 'description': None}]",closed,2017-12-07 16:24:14+00:00,,Improper Lemma Parsing,"There's a flaw in the Stanford Lemma algorithm:

""What is the English translation of tawhid?""

It yields ""translation"" for the lemma of ""translation"".  The parser should yield ""translate"" for the lemma of ""translation""."
464,https://github.com/stanfordnlp/CoreNLP/issues/584,584,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}, {'id': 706059615, 'node_id': 'MDU6TGFiZWw3MDYwNTk2MTU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/ner', 'name': 'ner', 'color': 'c5def5', 'default': False, 'description': None}]",closed,2017-12-08 16:33:40+00:00,,Quality regression git vs. 3.7.0 and 3.8.0,"With CoreNLP 3.8.0 `edu.stanford.nlp.ie.crf.CRFClassifier -prop german.hgc_175m_600.prop`:
```
CRFClassifier tagged 51645 words in 3068 documents at 18664,62 words per second.
         Entity	P	R	F1	TP	FP	FN
            LOC	0,7869	0,6190	0,6929	731	198	450
           MISC	0,7486	0,4069	0,5273	411	138	599
            ORG	0,8260	0,5624	0,6692	698	147	543
            PER	0,7514	0,4768	0,5834	668	221	733
         Totals	0,7808	0,5189	0,6235	2508	704	2325
```
With CoreNLP 3.7.0
```
CRFClassifier tagged 51645 words in 3068 documents at 20477,80 words per second.
         Entity	P	R	F1	TP	FP	FN
            LOC	0,7869	0,6190	0,6929	731	198	450
           MISC	0,7486	0,4069	0,5273	411	138	599
            ORG	0,8260	0,5624	0,6692	698	147	543
            PER	0,7514	0,4768	0,5834	668	221	733
         Totals	0,7808	0,5189	0,6235	2508	704	2325
```

With current git (7fc91e35582e00de2c8b012cfb70138679534943), same properties file again, using :
```
commit 7fc91e35582e00de2c8b012cfb70138679534943 (HEAD -> master, upstream/master)
Author: Christopher Manning <manning@cs.stanford.edu>
Date:   Thu Nov 30 22:05:49 2017 -0800

    4 less javadoc warnings.

CRFClassifier tagged 51645 words in 3068 documents at 20951,32 words per second.
         Entity	P	R	F1	TP	FP	FN
            LOC	0,4514	0,7426	0,5615	877	1066	304
           MISC	0,2324	0,6426	0,3414	649	2143	361
            ORG	0,3447	0,5479	0,4231	680	1293	561
            PER	0,5490	0,5517	0,5504	773	635	628
         Totals	0,3671	0,6164	0,4601	2979	5137	1854
```

So the F1 dropped from 0,6235 to 0,4601. There is a regression in CRFClassifier in current git. Training time also increased substantially, by taking 380 instead of 299 iterations (in both 3.7.0 and 3.8.0)."
465,https://github.com/stanfordnlp/CoreNLP/issues/585,585,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}, {'id': 45387509, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOQ==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/wontfix', 'name': 'wontfix', 'color': 'eeeeee', 'default': True, 'description': None}]",closed,2017-12-08 20:38:35+00:00,,Improper Lemma Parsing in units of measurement,"When one runs ""It alternates between 20 and 25 centimeters."" through the lemma parser, alongside the dependencies; the lemma result for ""centimeters"" is ""centimeter"". The proper lemma should be ""meter"", since ""centi"" is a well known prefix meaning ""1/100th of"". 

I've confirmed the same issue with the following words:

- picometers -> picometer
- milligrams -> milligram
- gigahertz -> gigahertz
- kiloliters -> kiloliter

I suspect this isn't just a matter of limited dataset, because Stanford uses the WSJ dataset, and that has most of the main measurement words."
466,https://github.com/stanfordnlp/CoreNLP/issues/586,586,"[{'id': 45387505, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNQ==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/duplicate', 'name': 'duplicate', 'color': 'bbbbbb', 'default': True, 'description': None}]",closed,2017-12-12 01:58:49+00:00,, edu.stanford.nlp.process.PTBTokenizer failed to input/output files path pairs passed in file with ioFileList parameter,"when I have input/ouput file path pairs in a file like:
input1  output1
input2 output2

only the first pair can be processor. and the program starts to fail from the second pair"
467,https://github.com/stanfordnlp/CoreNLP/issues/587,587,[],closed,2017-12-12 16:54:06+00:00,,Improper POS verb detection,"With the sentence ""How far did the sea level drop in the ice ages?"", CoreNLP 3.8.0 classifies the word part of speech of the word ""drop"" as ""NN"". It should be VB*."
468,https://github.com/stanfordnlp/CoreNLP/issues/590,590,"[{'id': 626016953, 'node_id': 'MDU6TGFiZWw2MjYwMTY5NTM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/analysis-bug', 'name': 'analysis-bug', 'color': 'f98685', 'default': False, 'description': None}]",closed,2017-12-28 13:46:56+00:00,,misprediction: like as a verb,"Case: ""Politicians like regulations more than farmers.""

It's being tagged as a preposition. 
"
469,https://github.com/stanfordnlp/CoreNLP/issues/591,591,[],open,2018-01-03 08:18:05+00:00,,"Can ""GetPatternsFromDataMultiClass.java"" deal with Chinese dataÔºü","I wanna to use ""GetPatternsFromDataMultiClass.java"" to deal with Chinese dataÔºåbut failed"
470,https://github.com/stanfordnlp/CoreNLP/issues/594,594,[],closed,2018-01-05 11:05:30+00:00,,Error with NER on NLP server,"Hello everyone,

I've downloaded CoreNLP 3.8.0 from https://stanfordnlp.github.io/CoreNLP and startet the local server with:

`java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000`

The server is up and running as it should, just as the CoreNLP online demo.

Unfortunately when choosing _named entities_ CoreNLP responds with the following error on the dashboard:

`edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl`

My terminal shows the following when starting the server:
```
[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - setting default constituency parser
[main] INFO CoreNLP - using SR parser: edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP -     Threads: 4
[main] INFO CoreNLP - Starting server...
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000
```

And the following log after entering my text and choosing named entities:
```
[pool-1-thread-2] ERROR CoreNLP - Failure to load language specific properties: StanfordCoreNLP.properties for en
[pool-1-thread-2] INFO CoreNLP - [/0:0:0:0:0:0:0:1:52994] API call w/annotators tokenize,ssplit,pos,lemma,ner
Holy Wood (In the Shadow of the Valley of Death) is the fourth studio album by American rock band Marilyn Manson.
[pool-1-thread-2] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[pool-1-thread-2] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.
[pool-1-thread-2] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[pool-1-thread-2] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[pool-1-thread-2] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [2.5 sec].
[pool-1-thread-2] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[pool-1-thread-2] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[pool-1-thread-2] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [3.3 sec].
[pool-1-thread-2] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.4 sec].
[pool-1-thread-2] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [1.2 sec].
[pool-1-thread-2] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl
	at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:40)
	at edu.stanford.nlp.time.TimeExpressionExtractorFactory.create(TimeExpressionExtractorFactory.java:57)
	at edu.stanford.nlp.time.TimeExpressionExtractorFactory.createExtractor(TimeExpressionExtractorFactory.java:38)
	at edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.<init>(NumberSequenceClassifier.java:86)
	at edu.stanford.nlp.ie.NERClassifierCombiner.<init>(NERClassifierCombiner.java:136)
	at edu.stanford.nlp.pipeline.NERCombinerAnnotator.<init>(NERCombinerAnnotator.java:91)
	at edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(AnnotatorImplementations.java:70)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$44(StanfordCoreNLP.java:498)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getDefaultAnnotatorPool$65(StanfordCoreNLP.java:533)
	at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:118)
	at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
	at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:146)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:447)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:150)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:146)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:133)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.mkStanfordCoreNLP(StanfordCoreNLPServer.java:319)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.access$500(StanfordCoreNLPServer.java:50)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:642)
	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77)
	at jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82)
	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80)
	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:685)
	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77)
	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:657)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
	at java.base/java.lang.Thread.run(Thread.java:844)
Caused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: MetaClass couldn't create public edu.stanford.nlp.time.TimeExpressionExtractorImpl(java.lang.String,java.util.Properties) with args [sutime, {}]
	at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:237)
	at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:382)
	at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:38)
	... 27 more
Caused by: java.lang.reflect.InvocationTargetException
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:488)
	at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:233)
	... 29 more
Caused by: java.lang.NoClassDefFoundError: javax/xml/bind/JAXBException
	at de.jollyday.util.CalendarUtil.<init>(CalendarUtil.java:42)
	at de.jollyday.HolidayManager.<init>(HolidayManager.java:66)
	at de.jollyday.impl.DefaultHolidayManager.<init>(DefaultHolidayManager.java:46)
	at edu.stanford.nlp.time.JollyDayHolidays$MyXMLManager.<init>(JollyDayHolidays.java:148)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:488)
	at java.base/java.lang.Class.newInstance(Class.java:558)
	at de.jollyday.caching.HolidayManagerValueHandler.instantiateManagerImpl(HolidayManagerValueHandler.java:60)
	at de.jollyday.caching.HolidayManagerValueHandler.createValue(HolidayManagerValueHandler.java:41)
	at de.jollyday.caching.HolidayManagerValueHandler.createValue(HolidayManagerValueHandler.java:13)
	at de.jollyday.util.Cache.get(Cache.java:51)
	at de.jollyday.HolidayManager.createManager(HolidayManager.java:168)
	at de.jollyday.HolidayManager.getInstance(HolidayManager.java:148)
	at edu.stanford.nlp.time.JollyDayHolidays.init(JollyDayHolidays.java:57)
	at edu.stanford.nlp.time.Options.<init>(Options.java:90)
	at edu.stanford.nlp.time.TimeExpressionExtractorImpl.init(TimeExpressionExtractorImpl.java:44)
	at edu.stanford.nlp.time.TimeExpressionExtractorImpl.<init>(TimeExpressionExtractorImpl.java:39)
	... 34 more
Caused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:582)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:185)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:496)
	... 53 more
```

Could anyone give me a hint, about what I might be doing wrong or how to solve this issue? I'd be grateful for any help provided!
  "
471,https://github.com/stanfordnlp/CoreNLP/issues/595,595,[],closed,2018-01-10 14:34:27+00:00,,Basic Example From Install Page Not Working,"I went to the download page and downloaded `stanford-corenlp-full-2017-06-09` as the instructions on the [download page](https://stanfordnlp.github.io/CoreNLP/download.html#steps-to-setup-from-the-official-release) said. I set up my class path using:
```
for file in `find . -name ""*.jar""`; do export
CLASSPATH=""$CLASSPATH:`realpath $file`""; done
```

Made an input text file that contained: `the quick brown fox jumped over the lazy dog` per the instructions. But when I ran it I got this, any idea where I went wrong in the setup?

```
bash$ java -mx3g edu.stanford.nlp.pipeline.StanfordCoreNLP -outputFormat json -file input.txt --add-modules java.xml.bind --add-modules java.se.ee
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Searching for resource: StanfordCoreNLP.properties ... not found.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Searching for resource: edu/stanford/nlp/pipeline/StanfordCoreNLP.properties ... found.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.0 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.5 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.8 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].
[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
Exception in thread ""main"" edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl
	at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:38)
	at edu.stanford.nlp.time.TimeExpressionExtractorFactory.create(TimeExpressionExtractorFactory.java:60)
	at edu.stanford.nlp.time.TimeExpressionExtractorFactory.createExtractor(TimeExpressionExtractorFactory.java:43)
	at edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.<init>(NumberSequenceClassifier.java:86)
	at edu.stanford.nlp.ie.NERClassifierCombiner.<init>(NERClassifierCombiner.java:138)
	at edu.stanford.nlp.pipeline.NERCombinerAnnotator.<init>(NERCombinerAnnotator.java:131)
	at edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(AnnotatorImplementations.java:68)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$5(StanfordCoreNLP.java:545)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$constructAnnotatorPool$30(StanfordCoreNLP.java:624)
	at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)
	at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
	at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:494)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:201)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:194)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:181)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1413)
Caused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: MetaClass couldn't create public edu.stanford.nlp.time.TimeExpressionExtractorImpl(java.lang.String,java.util.Properties) with args [sutime, {}]
	at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:237)
	at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:382)
	at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:36)
	... 16 more
Caused by: java.lang.reflect.InvocationTargetException
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:488)
	at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:233)
	... 18 more
Caused by: java.lang.NoClassDefFoundError: javax/xml/bind/JAXBException
	at de.jollyday.util.CalendarUtil.<init>(CalendarUtil.java:42)
	at de.jollyday.HolidayManager.<init>(HolidayManager.java:66)
	at de.jollyday.impl.DefaultHolidayManager.<init>(DefaultHolidayManager.java:46)
	at edu.stanford.nlp.time.JollyDayHolidays$MyXMLManager.<init>(JollyDayHolidays.java:148)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:488)
	at java.base/java.lang.Class.newInstance(Class.java:558)
	at de.jollyday.caching.HolidayManagerValueHandler.instantiateManagerImpl(HolidayManagerValueHandler.java:60)
	at de.jollyday.caching.HolidayManagerValueHandler.createValue(HolidayManagerValueHandler.java:41)
	at de.jollyday.caching.HolidayManagerValueHandler.createValue(HolidayManagerValueHandler.java:13)
	at de.jollyday.util.Cache.get(Cache.java:51)
	at de.jollyday.HolidayManager.createManager(HolidayManager.java:168)
	at de.jollyday.HolidayManager.getInstance(HolidayManager.java:148)
	at edu.stanford.nlp.time.JollyDayHolidays.init(JollyDayHolidays.java:57)
	at edu.stanford.nlp.time.Options.<init>(Options.java:90)
	at edu.stanford.nlp.time.TimeExpressionExtractorImpl.init(TimeExpressionExtractorImpl.java:44)
	at edu.stanford.nlp.time.TimeExpressionExtractorImpl.<init>(TimeExpressionExtractorImpl.java:39)
	... 23 more
Caused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:582)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:185)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:496)
	... 42 more
```

Using Java version:

```
bash$ java --version
java 9.0.1
Java(TM) SE Runtime Environment (build 9.0.1+11)
Java HotSpot(TM) 64-Bit Server VM (build 9.0.1+11, mixed mode)
```

On Mac OSX 10.13.2"
472,https://github.com/stanfordnlp/CoreNLP/issues/596,596,[],closed,2018-01-10 15:32:47+00:00,,German lemmatization and sentiment tagging,Will the German models support lemmatization and sentiment analysis in the near future? That'd be really great!
473,https://github.com/stanfordnlp/CoreNLP/issues/597,597,[],closed,2018-01-10 20:18:29+00:00,,Miscategorization of a noun with a color adjective,"If you run ""The yellow sofa with a brown stain"" through the PartOfSpeech annotation, it labels ""stain"" as a verb (VBP), despite the word having an adjective and a determiner. ""stain"" should have been labeled as ""NN"" or ""NNS""."
474,https://github.com/stanfordnlp/CoreNLP/issues/599,599,[],open,2018-01-11 13:20:11+00:00,,makeDatum with embeddings not using regular C features,"If `flags.useEmbedding && i == 0`, then a special codepath for embeddings is used, instead of the regular codepath.

https://github.com/stanfordnlp/CoreNLP/blob/d558d95d80b36b5b45bc21882cbc0ef7452eda24/src/edu/stanford/nlp/ie/crf/CRFClassifier.java#L992-L995

within this codepath, the clique features are only used if `flags.prependEmbedding` is set:
https://github.com/stanfordnlp/CoreNLP/blob/d558d95d80b36b5b45bc21882cbc0ef7452eda24/src/edu/stanford/nlp/ie/crf/CRFClassifier.java#L1087-L1089

otherwise (i.e. if `flags.useEmbedding` but not `flags.prependEmbedding`),
then `cliqueC` (and thus, NERFeatureFactory.featuresC) will not be used as far as I can tell.

This may be intentional (if prependEmbedding=false is meant to say ""use embedding only"")?

Note that `featureVals` also always contains a bunch of rather useless `null` values (windowsize `null` if not using embeddings, windowsize-1 nulls otherwise). We can conserve memory by not building such lists of `null` values in the first place, and e.g., in 
https://github.com/stanfordnlp/CoreNLP/blob/63c4a2cd1976847afb6eae9e154cd228107bfa79/src/edu/stanford/nlp/ie/crf/CRFClassifier.java#L464-L465
https://github.com/stanfordnlp/CoreNLP/blob/63c4a2cd1976847afb6eae9e154cd228107bfa79/src/edu/stanford/nlp/ie/crf/CRFClassifier.java#L2085-L2086
we can trivially add a guard ` && k < featureValList.size()`. Also, the code already handles `null`, so when embeddings are disabled, we can omit building the array list of `null`s.

The javadoc of ""documentToDataAndLabels"" claims to be only used at test time, but it is also used at training time (in documentsToDataAndLabels, with plural documents)"
475,https://github.com/stanfordnlp/CoreNLP/issues/600,600,[],closed,2018-01-11 17:08:55+00:00,,paraphrasing techniques and datasets?,"Hello all, I'm not certain this is the best forum to pose this problem but I would appreciate any input.  I'm working on a project to do paraphrasing at a sentence level.  Essentially, given a sentence, I'd like to generate new sentences that carry the same meaning and semantics as the input sentence.  I've search around the internet for papers relating to the topic and I was wondering if anyone might be able to provide some insight on what the best techniques to try are, any available datasets, etc.  I'm open to deep learning and non-DL approaches.  If Stanford CoreNLP can handle such a task, could someone point me in the right direction?  and if not, any suggestions would be appreciated!"
476,https://github.com/stanfordnlp/CoreNLP/issues/601,601,[],closed,2018-01-12 03:22:01+00:00,,Is there anyone who can give us a clear document about how to train Truecaser model ?,"I'm using Truecaser model, and I am gonna using wikipedia data for training Truecaser model.
I searched on Google about this and it seems no one ever showed a successful experience of training a new Truecaser model.
So here, is there anyone who can give us a clear document about how to train Truecaser model ?
It should not be too much complicated, just some successful experience plz. "
477,https://github.com/stanfordnlp/CoreNLP/issues/602,602,"[{'id': 626016953, 'node_id': 'MDU6TGFiZWw2MjYwMTY5NTM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/analysis-bug', 'name': 'analysis-bug', 'color': 'f98685', 'default': False, 'description': None}]",closed,2018-01-13 14:25:08+00:00,,Manning: a touch of irony,"failed syntax parse on: ""Manning is a top NLP expert.""
(it's tagging Manning as VBG). 

=-=-=- 
Most syntax parsers are able to get this one right. I've noticed that CoreNLP has issues with proper names.  Not sure if you'd consider using one of the many 'names lists' available with a friendly license as new training tokens:
https://github.com/FinNLP/humannames/blob/master/list.txt 
"
478,https://github.com/stanfordnlp/CoreNLP/issues/603,603,"[{'id': 626016953, 'node_id': 'MDU6TGFiZWw2MjYwMTY5NTM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/analysis-bug', 'name': 'analysis-bug', 'color': 'f98685', 'default': False, 'description': None}]",closed,2018-01-15 22:17:23+00:00,,case sensitive lemma failure,"in some cases, the lemma function is failing based on upper case characters. Here's some examples:
Farms, Places, Saints, Aborted, Teachers

Note that the lower case version works fine. "
479,https://github.com/stanfordnlp/CoreNLP/issues/604,604,[],closed,2018-01-18 22:52:27+00:00,,Quote.Attribution Speakers Unknown ,"In the commit [21c9c0630e7d2e4d796aa40f5001ed97c30e2b3d](https://github.com/stanfordnlp/CoreNLP/commit/21c9c0630e7d2e4d796aa40f5001ed97c30e2b3d) , people, corefs and quotes are being correctly identified, but not being included in the quote annotation. What could be some of the potential causes? 

Edit:

My apologies @J38 , I should've given more context. I went through your commits and made sure to comply with the changes, so my first notion was to check if you had to break something intermediately. 

By the way, thanks for those updates and the issue response!

I ran the sentence and annotators that worked for you and received the response below. Tried on both my dev server and corenlp.run and got the same response. From what I understand, corenlp.run is using the standard documented setup and integrates passing builds, right?

Could it be an issue in the http request encoding?

 Response:

```javascript
""corefs"": {
    ""2"": [
      {
        ""id"": 0,
        ""text"": ""Joe Smith"",
        ""type"": ""PROPER"",
        ""number"": ""SINGULAR"",
        ""gender"": ""MALE"",
        ""animacy"": ""ANIMATE"",
        ""startIndex"": 1,
        ""endIndex"": 3,
        ""headIndex"": 2,
        ""sentNum"": 1,
        ""position"": [
          1,
          1
        ],
        ""isRepresentativeMention"": true
      },
      {
        ""id"": 4,
        ""text"": ""He"",
        ""type"": ""PRONOMINAL"",
        ""number"": ""SINGULAR"",
        ""gender"": ""MALE"",
        ""animacy"": ""ANIMATE"",
        ""startIndex"": 1,
        ""endIndex"": 2,
        ""headIndex"": 1,
        ""sentNum"": 2,
        ""position"": [
          2,
          3
        ],
        ""isRepresentativeMention"": false
      },
      {
        ""id"": 2,
        ""text"": ""I"",
        ""type"": ""PRONOMINAL"",
        ""number"": ""SINGULAR"",
        ""gender"": ""UNKNOWN"",
        ""animacy"": ""ANIMATE"",
        ""startIndex"": 4,
        ""endIndex"": 5,
        ""headIndex"": 4,
        ""sentNum"": 2,
        ""position"": [
          2,
          1
        ],
        ""isRepresentativeMention"": false
      }
    ]
  },
  ""quotes"": [
    {
      ""id"": 0,
      ""text"": ""\""I want pizza.\"""",
      ""beginIndex"": 40,
      ""endIndex"": 55,
      ""beginToken"": 9,
      ""endToken"": 14,
      ""beginSentence"": 1,
      ""endSentence"": 1,
      ""speaker"": ""Unknown""
    }
  ]
```

Request:

```javascript
REQUEST { url: 'http://corenlp.run',
  method: 'POST',
  form: 'Joe Smith decided to get lunch. He said ""I want pizza.""',
  qs: { properties: '{""annotators"":""tokenize,ssplit,pos,lemma,ner,depparse,coref,quote"",""pipelineLanguage"":""en""}' },
  json: true,
  agent: false,
  pool: { maxSockets: 5 },
  forever: true,
  timeout: 120000,
  callback: [Function: RP$callback],
  transform: undefined,
  simple: true,
  resolveWithFullResponse: false,
  transform2xxOnly: false }

REQUEST onRequestResponse http://corenlp.run/?properties=%7B%22annotators%22%3A%22tokenize%2Cssplit%2Cpos%2Clemma%2Cner%2Cdepparse%2Ccoref%2Cquote%22%2C%22pipelineLanguage%22%3A%22en%22%7D 200 { 'access-control-allow-credentials-header': '*',
  'access-control-allow-headers': '*',
  date: 'Fri, 19 Jan 2018 16:15:03 GMT',
  'access-control-allow-methods': 'GET,POST,PUT,DELETE,OPTIONS',
  'content-type': 'application/json;charset=UTF-8',
  'access-control-allow-origin': '*',
  'access-control-allow-credentials': 'true',
  'content-length': '7811' }

REQUEST response end http://corenlp.run/?properties=%7B%22annotators%22%3A%22tokenize%2Cssplit%2Cpos%2Clemma%2Cner%2Cdepparse%2Ccoref%2Cquote%22%2C%22pipelineLanguage%22%3A%22en%22%7D 200 { 'access-control-allow-credentials-header': '*',
  'access-control-allow-headers': '*',
  date: 'Fri, 19 Jan 2018 16:15:03 GMT',
  'access-control-allow-methods': 'GET,POST,PUT,DELETE,OPTIONS',
  'content-type': 'application/json;charset=UTF-8',
  'access-control-allow-origin': '*',
  'access-control-allow-credentials': 'true',
  'content-length': '7811' }
```"
480,https://github.com/stanfordnlp/CoreNLP/issues/605,605,[],open,2018-01-19 10:25:28+00:00,,Too optimistic about recognizing dates?,"In the following sentence from todays news,

> Mrs May praised the ""uniquely close relationship"" between the two nations.

Mrs ""May"" is considered to be `2017-05`."
481,https://github.com/stanfordnlp/CoreNLP/issues/606,606,"[{'id': 45387507, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNw==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/cantreproduce', 'name': 'cantreproduce', 'color': 'dddddd', 'default': False, 'description': None}]",closed,2018-01-19 12:12:26+00:00,,FileNotFound exception for TrainFileList,"`TrainFileList` flag option is not working in stanford-ner version 3.8

I have the files existing in the destination which is mentioned as a list of files in the `TrainFileList`, but I get the FileNotFound exception instead.


```
Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: java.io.FileNotFoundException: ['sample_2113.conll'] (No such file 
or directory)
        at edu.stanford.nlp.io.IOUtils.inputStreamFromFile(IOUtils.java:519)
        at edu.stanford.nlp.io.IOUtils.readerFromFile(IOUtils.java:560)
        at edu.stanford.nlp.objectbank.ReaderIteratorFactory$ReaderIterator.setNextObject(ReaderIteratorFactory.java:189)
        at edu.stanford.nlp.objectbank.ReaderIteratorFactory$ReaderIterator.<init>(ReaderIteratorFactory.java:161)
        at edu.stanford.nlp.objectbank.ResettableReaderIteratorFactory.iterator(ResettableReaderIteratorFactory.java:98)
        at edu.stanford.nlp.objectbank.ObjectBank$OBIterator.<init>(ObjectBank.java:414)
        at edu.stanford.nlp.objectbank.ObjectBank.iterator(ObjectBank.java:253)
        at edu.stanford.nlp.sequences.ObjectBankWrapper.iterator(ObjectBankWrapper.java:45)
        at edu.stanford.nlp.ie.crf.CRFClassifier.train(CRFClassifier.java:1585)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequenceClassifier.java:780)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequenceClassifier.java:751)
        at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3012)
Caused by: java.io.FileNotFoundException: ['sample_2113.conll'] (No such file or directory)
        at java.io.FileInputStream.open0(Native Method)
        at java.io.FileInputStream.open(FileInputStream.java:195)
        at java.io.FileInputStream.<init>(FileInputStream.java:138)
```
"
482,https://github.com/stanfordnlp/CoreNLP/issues/607,607,[],open,2018-01-22 06:59:56+00:00,,how to use initial model trained by bakeoff2005 pku corpus without other dictionary or post processing?,I  wanna only use pku model .
483,https://github.com/stanfordnlp/CoreNLP/issues/608,608,"[{'id': 103162424, 'node_id': 'MDU6TGFiZWwxMDMxNjI0MjQ=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/request', 'name': 'request', 'color': '94c5e9', 'default': False, 'description': None}]",closed,2018-01-22 08:49:12+00:00,,OutOfMemEx on seemingly innocuous data,"I've noticed that a fairly standard pipeline will throw a OOME on common ""Internet"" data. I've used 22+GB heaps and it still errors out on input less that 4k chars. Typical scenarios is which this occurs are repeated phrases (sentence fragments) or many braces. I have gone into the code and removed a lot of places where OOME are caught (catching these in a library is rarely a good idea, IMO).

The offending code is the large allocs in ExhaustivePCFGParser.createArrays():

...
iScore = new float[length][length + 1][];
...
	for (int start = 0; start < length; start++) {
		for (int end = start + 1; end <= length; end++) {
			try {
				iScore[start][end] = null;
				iScore[start][end] = new float[numStates]; // oome here
...

numStates=12977 seems to be a model dependent constant - in this case the stock model.
length=1007 in one case
The allocs for this 3-d array are only ~1.1GB and yet are causing a 22GB JVM to OOME.

Looking through the code I don't see any way to limit the allocs here. The regular mode of operation should probably handle most all text within 4GB. If it can't , it should throw maybe a validation exception of some sort.

The props I used are: ""tokenize, ssplit, pos, lemma, parse""
props.put(""parse.maxlen"", 512); // len in tokens; doesn't seem to work


Simple input enclosed in dquotes follows:
""I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg      I am a bad egg"""
484,https://github.com/stanfordnlp/CoreNLP/issues/609,609,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",open,2018-01-22 17:52:59+00:00,,CoreNLPServer's update function overwrites existing annotations,See https://github.com/stanfordnlp/python-stanford-corenlp/issues/8#issuecomment-359349937 for an example of this error.
485,https://github.com/stanfordnlp/CoreNLP/issues/610,610,[],closed,2018-01-22 19:29:51+00:00,,Verbs used as Adjectives not being supported,"""During the Dark Ages, cultivated superstition and cultivated ignorance combined with political chaos to stagnate and regress European culture.""

In this sentence, in CoreNLP 3.8, the word ""cultivated"" in both cases is tagged as VBN by the PartOfSpeechAnnotator. They should be tagged as JJ.

""Mikimoto cultured pearls are respected.""

In this sentence, in CoreNLP 3.8, the word ""cultured"" is tagged as VBN by the PartOfSpeechAnnotator. It should be tagged as JJ.

There seems to be an issue where past tense verbs being used as adjectives is unsupported."
486,https://github.com/stanfordnlp/CoreNLP/issues/611,611,[],closed,2018-01-25 09:51:42+00:00,,Stanford CoreNLP Truecase Annotator giving null TrueCaseTextAnnotation,"I am trying to case correct an all lowercase text using Stanford's trueCaseAnnotator so, if I have a statement like this: ""jack went to russia"", I want it to be ""**J**ack went to¬†**R**ussia""I've tried to make a pipeline as follows:
```
var props = new Properties();
// set pipeline annotators.
props.setProperty(""annotators"", ""tokenize, ssplit, truecase, ner"");
props.setProperty(""ner.useSUTime"", ""0"");
props.setProperty(""truecase.overwriteText"", ""true"");

Directory.SetCurrentDirectory(modelsPath);
pipeline = new StanfordCoreNLP(props);

// annotate text
var annotation = new Annotation(text);
pipeline.annotate(annotation);
```

and whenever I annotate a text using this pipeline configuration, I only get three annotations

1. TextAnnotation
2. TokenAnnotation
3. SentenceAnnotation

and there is no record for TrueCaseTextAnnotation which is what I need, instead it changes values of the tokens to be truecase only.

I used Stanford CoreNLP 3.8.0 and 3.7.0 (English models) with the [C# wrapper ](https://www.nuget.org/packages/Stanford.NLP.CoreNLP/)for the Stanford library and the problem still existing."
487,https://github.com/stanfordnlp/CoreNLP/issues/616,616,[],closed,2018-01-28 20:12:43+00:00,,Quote Attribution Speaker Data,"It seems like the name of quote.speaker doesn't resolve to the representative mention for it's corefs, at least for JSON output. It would also be really useful to populate entitylink data in quote as well. 

If these features haven't been implemented, would a custom annotator be the best way to accomplish this? "
488,https://github.com/stanfordnlp/CoreNLP/issues/617,617,[],closed,2018-01-29 13:59:54+00:00,,CoreNLP Server for Chinese coreference resolution returns empty,"I started the server by the command:
`java -mx8g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000`

WhenI submitted the following text: Â∞èÊòéÂêÉ‰∫Ü‰∏™ÂÜ∞Ê£íÔºåÂÆÉÂæàÁîú„ÄÇ
![qq 20180129214542](https://user-images.githubusercontent.com/5766062/35513460-1e773cec-053e-11e8-8538-d8e7586cbd00.png)

It returns like this:
![qq 20180129214844](https://user-images.githubusercontent.com/5766062/35513598-980a3bae-053e-11e8-90ce-83389aa1b758.png)

I have checked the Chrome Developer Tools:

data sent:
![qq 20180129214915](https://user-images.githubusercontent.com/5766062/35513664-c90300d8-053e-11e8-94b6-a5ab7abacc4f.png)

response:
![qq 20180129214947](https://user-images.githubusercontent.com/5766062/35513675-cef0d4e8-053e-11e8-96f7-1a33772b919d.png)

The corefs object is empty.

And the following command returns the same result.
`java -Xmx8g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-chinese.properties -port 9000 -timeout 15000`


Does CoreNLP Server support Chinese coreference resolution? If not now, will it be added in the future version?


"
489,https://github.com/stanfordnlp/CoreNLP/issues/618,618,[],closed,2018-01-31 06:09:59+00:00,,why I get this Error: Could not find or load main class edu.stanford.nlp.pipeline.StanfordCoreNLP,"I unzip the downloaded 3.8 core nlp full version from https://stanfordnlp.github.io/CoreNLP/download.html, and do the exact steps as Steps to setup from the official release.
I am running an Ubuntu 14.04, java 1.8.
The error pops up when running:
`java -mx3g edu.stanford.nlp.pipeline.StanfordCoreNLP -outputFormat json -file input.txt`"
490,https://github.com/stanfordnlp/CoreNLP/issues/619,619,[],closed,2018-02-02 17:46:29+00:00,,Tokenizer increases the number of lines in a file,"I have a text file that contains, let's say, 20,000 lines. When I use the following command the number of lines increases to like 20,100 and that's not what I want. I want to keep the number of lines to the same 20,000.
`java edu.stanford.nlp.process.PTBTokenizer -preserveLines -lowerCase input.txt`

What goes wrong?"
491,https://github.com/stanfordnlp/CoreNLP/issues/620,620,[],open,2018-02-04 16:32:22+00:00,,"Dependency: ""Dep"" tag","This is a feature request, not a bug. 
=-=-=-

The ""dep"" tag indicates that the parser was unable to determine a likely tag. Conceptually, it makes sense to have a label indicating ""I don't know"", however, practically it doesn't help. My request is to either:
1. Never return a Dep tag (always return a best guess)
2. Return the Dep tag, along with a best guess: (DEP:TAG-X)

"
492,https://github.com/stanfordnlp/CoreNLP/issues/621,621,[],closed,2018-02-06 06:59:36+00:00,,Difference b/w Model Jar Language English and English(KBP),"Hi Team,

We are trying to download the Stanford NLP Model jar from the below link for English language.
https://stanfordnlp.github.io/CoreNLP/download.html

But we found two jars of type English and English(KBP)
![image](https://user-images.githubusercontent.com/20786984/35845631-4a244118-0b39-11e8-9ecc-9ad75fa616ef.png)

What is the difference between these two and which one is the best. Please provide your suggestions and explanations.

Thank you in advance"
493,https://github.com/stanfordnlp/CoreNLP/issues/622,622,"[{'id': 626016953, 'node_id': 'MDU6TGFiZWw2MjYwMTY5NTM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/analysis-bug', 'name': 'analysis-bug', 'color': 'f98685', 'default': False, 'description': None}]",open,2018-02-06 22:30:11+00:00,,PTBTokenizer does not recognize filenames at the end of the input,"PTBTokenizer includes a rule of the form:

{FILENAME}/({SPACE}|[.?!,\""'<])

This is supposed to catch file names or path names in the input and return them as a single token. This unfortunately breaks when the file name is at the end, because the lookahead forces it to have a space or delimiter character afterwards.
As a result, the token is split on slashes and periods, which is incorrect.

(The use case here is to handle chatbot commands of the form ""open images/cat.png"". In our use case, punctuation is usually inconsistent and/or missing)"
494,https://github.com/stanfordnlp/CoreNLP/issues/623,623,[],closed,2018-02-08 19:16:39+00:00,,An Issue in importing StanfordCoreNLP library in an Android Studio project,"I am developing an Android application (I am a beginner). I want to use Stanford CoreNPL 3.8.0 library in my app to extract the part of speech, the lemma, the parser and so on from the user sentences.I have tried a simple java code in NetBeans by following this youtube tutorial https://www.youtube.com/watch?v=9IZsBmHpK3Y, and it is working perfectly.The jar files that I imported to the NetBeans project are: stanford-corenlp-3.8.0.jar and stanford-corenlp-3.8.0-models.jar.

And this is the java source code:

```
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

import java.util.List;
import java.util.Properties;

public class CoreNlpExample {

    public static void main(String[] args) {

        // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        // read some text in the text variable
        String text = ""What is the Weather in Bangalore right now?"";

        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);

        List<CoreMap> sentences = document.get(CoreAnnotations.SentencesAnnotation.class);

        for (CoreMap sentence : sentences) {
            // traversing the words in the current sentence
            // a CoreLabel is a CoreMap with additional token-specific methods
            for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
                // this is the text of the token
                String word = token.get(CoreAnnotations.TextAnnotation.class);
                // this is the POS tag of the token
                String pos = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);
                // this is the NER label of the token
                String ne = token.get(CoreAnnotations.NamedEntityTagAnnotation.class);

                System.out.println(String.format(""Print: word: [%s] pos: [%s] ne: [%s]"", word, pos, ne));
            }
        }
    }
}
```

I wanted to try the same code in Android Studio and display the result in a textview, but I am facing a problem with adding these external libraries in my Android Studio 3.0.1 project.

I have read on some websites that I need to reduce the size of the jar files, and I did that and made sure that the reduced jars are still working fine in the Netbeans project. But I am still facing problems in Android studio and this is the error that I am getting:

`java.lang.VerifyError: Rejecting class edu.stanford.nlp.pipeline.StanfordCoreNLP that attempts to sub-type erroneous class edu.stanford.nlp.pipeline.AnnotationPipeline (declaration of 'edu.stanford.nlp.pipeline.StanfordCoreNLP' appears in /data/app/com.example.fatimah.nlpapplication-bhlUJOCUwLhSbkWE7NBERA==/split_lib_dependencies_apk.apk)`

Any suggestions on how I can fix this and import Stanford library successfully?
"
495,https://github.com/stanfordnlp/CoreNLP/issues/624,624,[],closed,2018-02-09 09:08:17+00:00,,CoreNLP Server error -> Unknown annotator: mention,"Hello!

I'm trying to run the server for CoreNLP but when I start it using all annotators (no -anotators flag in the start command) I get this error:

```
ubuntu@ip:~/CoreNLP/stanford-corenlp-full-2018-01-31$ java -mx10g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000
[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - setting default constituency parser
[main] INFO CoreNLP - using SR parser: edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP -     Threads: 4
[main] INFO CoreNLP - Starting server...
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000
java.lang.IllegalArgumentException: Unknown annotator: mention
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.ensurePrerequisiteAnnotators(StanfordCoreNLP.java:352)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.getProperties(StanfordCoreNLPServer.java:423)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.access$200(StanfordCoreNLPServer.java:54)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:710)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
        at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
        at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
        at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
```

I have tried downloading http://nlp.stanford.edu/software/stanford-english-corenlp-models-current.jar as suggested in other similar issue #238 but I still get the same error. 

Thank you for your help!"
496,https://github.com/stanfordnlp/CoreNLP/issues/626,626,[],open,2018-02-15 15:20:21+00:00,,Parsing of 'tomorrow afternoon' depends on whether reference contains time of day,"When parsing a sentence containing 'tomorrow afternoon', I get different behaviour depending on whether my reference contains seconds (e.g. '2018-02-15T14:54:10') or not (e.g. '2018-02-15'). In the latter case, when the reference only contains a date, the output is '2018-02-16TAF' as expected. In the case where time is included in the reference, the output becomes the reference plus 1 day, (e.g. '2018-02-16T14:54:10'). Is this a bug or is there some ambiguity that I'm not picking up?

Similarly, if you parse a sentence with just 'tomorrow', if your reference contains no time you get tomorrow's date, but if your reference contains time you get your reference plus 1 day. This must be related.  

FYI: I'm using python-sutime, but I have the latest CoreNLP."
497,https://github.com/stanfordnlp/CoreNLP/issues/627,627,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",open,2018-02-16 09:50:58+00:00,,version and model information in output of server,"Stanford coreNLP improves from version to version - would it be possible to include the information about the version and the models used for training in the output xml (and json) files? The tagsets used for some languages change from version to version and confusion could be avoided if the relevant information would be included in each output file - at a very small cost!
thank you!
"
498,https://github.com/stanfordnlp/CoreNLP/issues/629,629,[],closed,2018-02-23 14:47:33+00:00,,Quote annotator failing to initialise (3.9.0),"Hello

I am trying to use the quote annotator with CoreNLPServer.
I am using the following annotators
`'tokenize, ssplit, pos, lemma, ner, entitymentions, coref, depparse, quote'`

The server throws an IllegalArgumentException

`java.lang.IllegalArgumentException: annotator ""quote"" requires annotation ""CorefChainAnnotation"". The usual requirements for this annotator are: tokenize,ssplit,pos,lemma,ner`

Here is the stdout from the server:
```
[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - setting default constituency parser
[main] INFO CoreNLP - using SR parser: edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP -     Threads: 2
[main] INFO CoreNLP - Starting server...
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000
[pool-1-thread-1] INFO CoreNLP - [/__.__.___.___:_____] API call w/annotators tokenize,ssplit,pos,lemma,ner,entitymentions,depparse,quote,coref
... 
[some text here]
...
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[pool-1-thread-1] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.8 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [3.3 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [2.9 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [1.1 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
[pool-1-thread-1] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator ner.fine.regexner: Read 580729 unique entries out of 581888 from edu/stanford/nlp/models/kbp/regexner_caseless.tab, 0 TokensRegex patterns.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator ner.fine.regexner: Read 4857 unique entries out of 4868 from edu/stanford/nlp/models/kbp/regexner_cased.tab, 0 TokensRegex patterns.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator ner.fine.regexner: Read 585586 unique entries from 2 files
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator entitymentions
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse
[pool-1-thread-1] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... 
[pool-1-thread-1] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 20.732 (s)
[pool-1-thread-1] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [32.1 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator quote
[pool-1-thread-1] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... 
[pool-1-thread-1] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 17.613 (s)
[pool-1-thread-1] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [25.6 sec].
java.lang.IllegalArgumentException: annotator ""quote"" requires annotation ""CorefChainAnnotation"". The usual requirements for this annotator are: tokenize,ssplit,pos,lemma,ner
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:504)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:201)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:194)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:181)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.mkStanfordCoreNLP(StanfordCoreNLPServer.java:344)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.access$700(StanfordCoreNLPServer.java:54)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:750)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
	at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
	at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
	at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
```

Here is an example of a wget that produces the same error
```
wget --post-data 'The quick brown fox jumped over the lazy dog.' '___.___.___.__:9000/?properties={""annotators"":""tokenize,ssplit,pos,lemma,ner,entitymentions,coref,depparse,quote"",""outputFormat"":""json""}' -O -
```

**Notes:**
- I am using release 3.9.0
- The same request **appears to work with 3.8.0**
- I have both the shift-reduce parser (`stanford-english-corenlp-2017-06-09-models.jar`) and kbp models ( `stanford-english-kbp-corenlp-2018-01-31-models.jar`) in my classpath.

"
499,https://github.com/stanfordnlp/CoreNLP/issues/630,630,[],closed,2018-02-24 01:20:27+00:00,,Cannot disable pos annotator in web server when selecting PCFG model.,"Cannot use the English PCFG model for both POS tagging and constituency tree parsing when running core NLP web services, as the ""pos"" annotator is always inserted through

getProperties -> ensurePrerequisiteAnnotators -> DEFAULT_REQUIREMENTS.

More detail:

In server properties we specified

```
annotators = tokenize, ssplit, parse, lemma, ner, depparse

tokenize.class = PTBTokenizer
tokenize.language = en

parse.model = edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz
parse.binaryTrees = false
```

When sending request, the annotators properties are set to ""tokenize,ssplit,parse"", as in URL

```
http://0.0.0.0:9000/?properties=%7B%22annotators%22:%22tokenize%2Cssplit%2Cparse%22%2C%22inputFormat%22:%22serialized%22%2C%22inputSerializer%22:%22edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer%22%2C%22outputFormat%22:%22serialized%22%2C%22serializer%22:%22edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer%22%7D
```

However, server side always adds the pos annotator.

```
[pool-1-thread-2] INFO CoreNLP - [/127.0.0.1:63248] API call w/annotators tokenize,ssplit,pos,parse 
```"
500,https://github.com/stanfordnlp/CoreNLP/issues/631,631,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2018-02-24 23:01:58+00:00,,Nullpointer exception on sentences,"I'm getting a NullPointerException when trying to parse with openie ""They and have taken. However, in comparison with two other characters, the titles of these people look like Little spittles ..."" through the NLP server:

```
nlp.annotate(""They and have taken. However, in comparison with two other characters, the titles of these people look like Little spittles ..."", properties={""openie.resolve_coref"":""true"",""annotators"":""tokenize,ssplit,pos,lemma,depparse,natlog,coref,openie"",
                                 ""timeout"":""300000"",""outputFormat"": ""json"",""triple.strict"":""true"", ""splitter.disable"":""true"",""openie.triple.all_nominals"":""true""})
```
"
501,https://github.com/stanfordnlp/CoreNLP/issues/632,632,[],open,2018-02-26 22:26:26+00:00,,Reducing the time for loading Stanford CoreNLP dependency parser model in an Android Studio project,"I am developing an android application that makes use of Stanford CoreNLP pipeline with the properties:  ""tokenize, ssplit, pos, lemma"", and the  Stanford DependencyParser. I have managed to use them successfully in my app but the issue is that the parser takes too long to load the model. To be more specific, it takes too long to load english_SD.gz model in these lines of code:

```
String modelPath = ""edu/stanford/nlp/models/parser/nndep/english_SD.gz"";
DependencyParser parser = DependencyParser.loadFromModelFile(modelPath);
```

The time is about 2 to 3 minutes which is kind of unacceptable, is there any way to reduce this time or overcome this issue without using server-client architecture, or changing the type of model that I am using in the meanwhile? "
502,https://github.com/stanfordnlp/CoreNLP/issues/633,633,[],closed,2018-02-28 13:34:01+00:00,,Call edu.stanford.nlp.pipeline.Annotation with new Properties,"I'm running several annotations at once like `tokenize,ssplit,pos,parse,sentiment,ner,regexner,entitymentions`, that I setup when initializing a new `edu.stanford.nlp.pipeline.StanfordCoreNLP` instance and passing the `Properties` object.

I would like to run a different set of Properties in my process without reloading the `StanfordCoreNLP`, passing a different configuration for the tokenizer/ssplit like `""ssplit.newlineIsSentenceBreak"": ""two""` instead of `""ssplit.newlineIsSentenceBreak"": ""never""`, etc.

So that I could call the `StanfordCoreNLP.annotate` after setting up the new `Properties` without reload everything. Is that possibile or shall I have to clone this different pipeline (same annotators), but initializing with a different set of Properties?"
503,https://github.com/stanfordnlp/CoreNLP/issues/634,634,[],closed,2018-03-01 20:58:25+00:00,,HTTP 400 Errors on CoreNLP server,"I've been seeing http status 400 errors when providing certain
input files to the CoreNLP server:

  https://stanfordnlp.github.io/CoreNLP/corenlp-server.html

Is this a known issue?  

Is there a way to configure the server to generate diagnostics
for the failure?

"
504,https://github.com/stanfordnlp/CoreNLP/issues/635,635,[],closed,2018-03-03 06:56:46+00:00,,Cannot load English models properly (v3.9.1),"Two issues I noticed:

1. It's not possible to use `stanford-english-corenlp-2018-02-27-models.jar` by itself (without the KBP ""add-on"") since by default the properties file seems to reference files from the KBP jar. This is using default configuration (no special properties set), and the annotator pipeline `tokenize,ssplit,pos,lemma,ner`
2. Even when adding both English model JAR files (""regular"" and KBP) to the classpath, it seems the annotator pipeline fails to be built. Error is 

> Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: Couldn't read TokensRegexNER from edu/stanford/nlp/models/kbp/regexner_caseless.tab
> Caused by: java.io.IOException: Unable to open ""edu/stanford/nlp/models/kbp/regexner_caseless.tab"" as class path, filename or URL

Looking inside `stanford-english-kbp-corenlp-2018-02-27-models.jar` I see that indeed there is no file at `edu/stanford/nlp/models/kbp/regexner_caseless.tab`. Instead, they're all at `edu/stanford/nlp/models/kbp/english/regexner_caseless.tab`  (note the additional **english** path segment)

I tried setting the property (in Scala)
```
val props = new Properties()
props.put(""regexner.mapping"", ""ignorecase=true,validpospattern=^(NN|JJ).*,edu/stanford/nlp/models/kbp/english/regexner_caseless.tab;edu/stanford/nlp/models/kbp/english/regexner_cased.tab"")
props.put(""annotators"", ""tokenize,ssplit,pos,lemma,ner"")
props.put(""threads"", ""8"")
val nlp = new StanfordCoreNLP(props)
```
but that also didn't work (same error).

Am I missing something, or is this a bug?

Thank you."
505,https://github.com/stanfordnlp/CoreNLP/issues/636,636,[],closed,2018-03-03 21:17:17+00:00,,CORENLP error edu.stanford.nlp.time.TimeExpressionExtractorImpl,"Hi, I'm getting the above error.

I've this code from the coreNLP page.

`import edu.stanford.nlp.scenegraph.SceneGraph;
import edu.stanford.nlp.scenegraph.RuleBasedParser;
public class TestSceneGraph{


	public static String text = ""Joe Smith was born in California. "" +
			""In 2017, he went to Paris, France in the summer. "" +
			""His flight left at 3:00pm on July 10th, 2017. "" +
			""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
			""He sent a postcard to his sister Jane Smith. "" +
			""After hearing about Joe's trip, Jane decided she might go to France one day."";

	public static void main(String[] args) {
		    // set up pipeline properties
	        RuleBasedParser parser = new RuleBasedParser();
		    SceneGraph sg = parser.parse(text);
		    //printing the scene graph in JSON form;
		    System.out.println(sg.toString());    
		    
	}

}`

When I run this, I get the following error.

`[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.3 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.5 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.8 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator ner.fine.regexner: Read 580641 unique entries out of 581790 from edu/stanford/nlp/models/kbp/regexner_caseless.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator ner.fine.regexner: Read 4857 unique entries out of 4868 from edu/stanford/nlp/models/kbp/regexner_cased.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator ner.fine.regexner: Read 585498 unique entries from 2 files
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.7 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].
[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
Exception in thread ""main"" edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl
	at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:38)
	at edu.stanford.nlp.time.TimeExpressionExtractorFactory.create(TimeExpressionExtractorFactory.java:60)
	at edu.stanford.nlp.time.TimeExpressionExtractorFactory.createExtractor(TimeExpressionExtractorFactory.java:43)
	at edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.<init>(NumberSequenceClassifier.java:86)
	at edu.stanford.nlp.ie.NERClassifierCombiner.<init>(NERClassifierCombiner.java:135)
	at edu.stanford.nlp.pipeline.NERCombinerAnnotator.<init>(NERCombinerAnnotator.java:131)
	at edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(AnnotatorImplementations.java:68)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$44(StanfordCoreNLP.java:546)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$69(StanfordCoreNLP.java:625)
	at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)
	at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
	at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:495)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:201)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:194)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:181)
	at edu.stanford.nlp.scenegraph.AbstractSceneGraphParser.initPipeline(AbstractSceneGraphParser.java:33)
	at edu.stanford.nlp.scenegraph.AbstractSceneGraphParser.parse(AbstractSceneGraphParser.java:42)
	at TestSceneGraph.main(TestSceneGraph.java:29)
Caused by: edu.stanford.nlp.util.MetaClass$ClassCreationException: MetaClass couldn't create public edu.stanford.nlp.time.TimeExpressionExtractorImpl(java.lang.String,java.util.Properties) with args [sutime, {}]
	at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:237)
	at edu.stanford.nlp.util.MetaClass.createInstance(MetaClass.java:382)
	at edu.stanford.nlp.util.ReflectionLoading.loadByReflection(ReflectionLoading.java:36)
	... 18 more
Caused by: java.lang.reflect.InvocationTargetException
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:488)
	at edu.stanford.nlp.util.MetaClass$ClassFactory.createInstance(MetaClass.java:233)
	... 20 more
Caused by: java.lang.NoClassDefFoundError: javax/xml/bind/JAXBException
	at de.jollyday.util.CalendarUtil.<init>(CalendarUtil.java:42)
	at de.jollyday.HolidayManager.<init>(HolidayManager.java:66)
	at de.jollyday.impl.DefaultHolidayManager.<init>(DefaultHolidayManager.java:46)
	at edu.stanford.nlp.time.JollyDayHolidays$MyXMLManager.<init>(JollyDayHolidays.java:148)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:488)
	at java.base/java.lang.Class.newInstance(Class.java:558)
	at de.jollyday.caching.HolidayManagerValueHandler.instantiateManagerImpl(HolidayManagerValueHandler.java:60)
	at de.jollyday.caching.HolidayManagerValueHandler.createValue(HolidayManagerValueHandler.java:41)
	at de.jollyday.caching.HolidayManagerValueHandler.createValue(HolidayManagerValueHandler.java:13)
	at de.jollyday.util.Cache.get(Cache.java:51)
	at de.jollyday.HolidayManager.createManager(HolidayManager.java:168)
	at de.jollyday.HolidayManager.getInstance(HolidayManager.java:148)
	at edu.stanford.nlp.time.JollyDayHolidays.init(JollyDayHolidays.java:57)
	at edu.stanford.nlp.time.Options.<init>(Options.java:119)
	at edu.stanford.nlp.time.TimeExpressionExtractorImpl.init(TimeExpressionExtractorImpl.java:44)
	at edu.stanford.nlp.time.TimeExpressionExtractorImpl.<init>(TimeExpressionExtractorImpl.java:39)
	... 25 more
Caused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:582)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:185)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:496)
	... 44 more
`


I'm also using Java 1.8"
506,https://github.com/stanfordnlp/CoreNLP/issues/637,637,[],closed,2018-03-06 10:47:12+00:00,,"WARN  e.s.nlp.pipeline.ParserAnnotator - Parsing of sentence failed, possibly because of out of memory.","After upgrading to CoreNLP 3.9.1 I get a WARN, that causes then an `uncaughtException` apparently raised by `ParserAnnotator`:

```
11:42:01.590 [Thread-1] WARN  e.s.nlp.pipeline.ParserAnnotator - Parsing of sentence failed, possibly because of out of memory.  Will ignore and continue: Skin covered in ego Get to talkin ' like ya involved , like a rebound Got no end game , got no reason , got to stay down It 's the way that you making me feel Like nobody ever loved me like you do , you do You kinda feeling like you 're tryna get away from me If you do , I wo n't move I just cry for no reason , I just pray for no reason I give thanks for the days , for the hours And another way , another life breathin ' I did it all 'cause it feel good I would n't do it at all if it feel bad Better live your life , we 're runnin ' out of time
11:42:01.590 [Thread-2] WARN  e.s.nlp.pipeline.ParserAnnotator - Parsing of sentence failed, possibly because of out of memory.  Will ignore and continue: Skin covered in ego Get to talkin ' like ya involved , like a rebound Got no end game , got no reason , got to stay down It 's the way that you making me feel Like nobody ever loved me like you do , you do You kinda feeling like you 're tryna get away from me If you do , I wo n't move I just cry for no reason , I just pray for no reason I give thanks for the days , for the hours And another way , another life breathin ' I did it all 'cause it feel good I would n't do it at all if it feel bad Better live your life , we 're runnin ' out of time
```

My configuration for `en` was almost standard and it is

```json
{
""en"" : {

                ""annotators"" : ""tokenize,ssplit,pos,lemma,ner,parse,sentiment"",
                
                ""tokenize.language"" : ""en"",
                ""tokenize.whitespace"": false,
                ""tokenize.keepeol"": false,

                ""ner.applyFineGrained"" : true,
                ""ner.buildEntityMentions"" : true,

                ""ssplit.isOneSentence"": false,
                ""ssplit.newlineIsSentenceBreak"": ""two""
            }
}
```

Partial core dump following:

```
---------------  T H R E A D  ---------------

Current thread (0x000000010505d800):  VMThread [stack: 0x000070000f578000,0x000070000f678000] [id=14851]

siginfo: si_signo: 11 (SIGSEGV), si_code: 1 (SEGV_MAPERR), si_addr: 0x0000000000000018

Registers:
RAX=0x00000000000000a8, RBX=0x000070000f6776d8, RCX=0x0000002cfcc499fa, RDX=0x0000000000000000
RSP=0x000070000f6775d0, RBP=0x000070000f6775d0, RSI=0x000070000f6775f8, RDI=0x0000000109c78f70
R8 =0x0000000000000165, R9 =0x00000000001e6901, R10=0x0000000113115a48, R11=0xffffffff00000000
R12=0x0000000000000000, R13=0x00000001337247c0, R14=0x00000001049e4ef0, R15=0x000070000f677750
RIP=0x00000001095fea0b, EFLAGS=0x0000000000010202, ERR=0x0000000000000004
  TRAPNO=0x000000000000000e

Top of Stack: (sp=0x000070000f6775d0)
0x000070000f6775d0:   000070000f677620 00000001096064cc
0x000070000f6775e0:   000070000f677600 fffffffffffffffe
0x000070000f6775f0:   00000001049e4ef0 0000002cfcc499fa
0x000070000f677600:   0000000000000000 00000001097f349f
0x000070000f677610:   00000001049e4ef0 0000000109c78fe0
0x000070000f677620:   000070000f6777d0 000000010984bcc3
0x000070000f677630:   0000000000000000 0000000000000000
0x000070000f677640:   0000000000000000 74aa8b3b4fce001d
0x000070000f677650:   000070000f677798 000070000f6779f0
0x000070000f677660:   000070000d552620 000070000d552624
0x000070000f677670:   000070000f6776b0 00007fffc78fa9d3
0x000070000f677680:   000070000f677798 000000010505d800
0x000070000f677690:   0000000000000080 000000000000000c
0x000070000f6776a0:   000070000d552630 000070000d552634
0x000070000f6776b0:   000070000f6776d0 00000001096d0cad
0x000070000f6776c0:   000070000f6776d0 0000000109811edb
0x000070000f6776d0:   000070000f677601 000000010999bf9f
0x000070000f6776e0:   000070000d550100 0000000109c78f70
0x000070000f6776f0:   0000002c3d42b596 0000000000000000
0x000070000f677700:   0000000000000005 0000000000000011
0x000070000f677710:   0000000000000017 00000001131140a0
0x000070000f677720:   000070000d552650 0000000109c5d810
0x000070000f677730:   0000000113113480 0000000109c5d830
0x000070000f677740:   000070000f677700 0000000113113480
0x000070000f677750:   0000000109c5f5d0 0000000000000008
0x000070000f677760:   0000000113112b20 0000000000000008
0x000070000f677770:   000000010999bf91 0000000000000100
0x000070000f677780:   0000000109c78f70 0000002c3d424579
0x000070000f677790:   0000000000000000 0000000109811edb
0x000070000f6777a0:   000070000f6777b0 00000001049dcf00
0x000070000f6777b0:   000000000000000c 0000000000003a00
0x000070000f6777c0:   0000000113113480 000070000f677901 

Instructions: (pc=0x00000001095fea0b)
0x00000001095fe9eb:   ff 55 48 89 e5 8b 47 44 ff c8 89 47 44 48 63 c0
0x00000001095fe9fb:   48 63 44 87 30 48 6b c0 38 48 8b 0e 48 8b 57 28
0x00000001095fea0b:   48 8b 52 18 48 89 4c 02 28 48 8b 4e 08 48 89 4c
0x00000001095fea1b:   02 30 48 89 d6 48 01 c6 48 83 c7 28 5d e9 f3 fe 

Register to memory mapping:

RAX=0x00000000000000a8 is an unknown value
RBX=0x000070000f6776d8 is an unknown value
RCX=0x0000002cfcc499fa is an unknown value
RDX=0x0000000000000000 is an unknown value
RSP=0x000070000f6775d0 is an unknown value
RBP=0x000070000f6775d0 is an unknown value
RSI=0x000070000f6775f8 is an unknown value
RDI=0x0000000109c78f70: _ZN17PSParallelCompact9_gc_timerE+0 in /Library/Java/JavaVirtualMachines/jdk1.8.0_121.jdk/Contents/Home/jre/lib/server/libjvm.dylib at 0x0000000109388000
R8 =0x0000000000000165 is an unknown value
R9 =0x00000000001e6901 is an unknown value
R10=0x0000000113115a48 is an unknown value
R11=0xffffffff00000000 is an unknown value
R12=0x0000000000000000 is an unknown value
R13=0x00000001337247c0 is an unknown value
R14=0x00000001049e4ef0 is an unknown value
R15=0x000070000f677750 is an unknown value


Stack: [0x000070000f578000,0x000070000f678000],  sp=0x000070000f6775d0,  free space=1021k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
V  [libjvm.dylib+0x276a0b]
V  [libjvm.dylib+0x27e4cc]
V  [libjvm.dylib+0x4c3cc3]
V  [libjvm.dylib+0x4c536b]
V  [libjvm.dylib+0x4c998c]
V  [libjvm.dylib+0x49c955]
V  [libjvm.dylib+0x5b1d04]
V  [libjvm.dylib+0x5b84c5]
V  [libjvm.dylib+0x5b6b31]
V  [libjvm.dylib+0x5b6f7e]
V  [libjvm.dylib+0x5b689d]
V  [libjvm.dylib+0x48a5b2]
C  [libsystem_pthread.dylib+0x393b]  _pthread_body+0xb4
C  [libsystem_pthread.dylib+0x3887]  _pthread_body+0x0
C  [libsystem_pthread.dylib+0x308d]  thread_start+0xd

VM_Operation (0x000070000dd558d8): ParallelGCFailedAllocation, mode: safepoint, requested by thread 0x00000001345bb800
```"
507,https://github.com/stanfordnlp/CoreNLP/issues/638,638,[],closed,2018-03-06 10:54:52+00:00,,Unknown property: |sutime.language|,"I get this INFO, that seems to be more a warning, not sure if this causes anything, but I didn't had it in CoreNLP 3.8.0, reporting then:

```
11:51:23.863 [main] INFO  e.s.nlp.sequences.SeqClassifierFlags - sutime.language=spanish
11:51:23.863 [main] INFO  e.s.nlp.sequences.SeqClassifierFlags - Unknown property: |sutime.language|
11:51:24.184 [main] INFO  e.s.nlp.sequences.SeqClassifierFlags - Unknown property: |sutime.language|
11:51:24.371 [main] INFO  e.s.n.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/spanish.ancora.distsim.s512.crf.ser.gz ... done [0.5 sec].
11:51:24.371 [main] INFO  e.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
11:51:24.372 [main] INFO  e.s.n.t.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/spanish.sutime.txt
```

my configuration for spanish is the default one where is used:

```
# sutime
sutime.language = spanish
```

i.e. the default configuration for spanish [here](https://github.com/stanfordnlp/CoreNLP/blob/d8bccdd2724f06dafea03f561299833c67244688/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-spanish.properties)."
508,https://github.com/stanfordnlp/CoreNLP/issues/639,639,[],open,2018-03-06 16:14:11+00:00,,How to use -processedData,"Hi,

I want to use parameter -processedData to load model:

java -cp stanford-ner.jar  edu.stanford.nlp.ie.crf.CRFClassifier -trainFileList question_crf_file.txt-low -serializeTo question_crf_model -prop crf_config.prop -loadProcessedData question_crf_model2

 but it failed with error:
...
Time to convert docs to feature indices: 0.8 seconds
numClasses: 21 [0=O,1=NULL,2=B-Order,3=B-Limit,4=B-GroupBy,5=B-OrderBy,6=I-OrderBy,7=B-Field,8=B-FilterValue,9=I-GroupBy,10=I-Field,11=B-ComparisonWith,12=B-Aggregation,13=I-Aggregation,14=B-FilterField,15=I-FilterField,16=B-FilterOp,17=I-FilterOp,18=i-field,19=i-orderby,20=i-filterfield]
numDocuments: 3000
numDatums: 26801
numFeatures: 8598
Time to convert docs to data/labels: 0.5 seconds
Loading processed data from serialized file ... done. Got 2 datums.
Exception in thread ""main"" java.lang.NullPointerException
	at edu.stanford.nlp.ie.crf.CRFClassifier.train(CRFClassifier.java:1707)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequenceClassifier.java:785)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequenceClassifier.java:756)
	at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:2993)

I check and modify the code a lot but still cannot make it work.
And I found nowhere the method saveProcessedData is used in the whole project ...

Can anyone help to explain a little bit about the ""-processedData""? (As no doc is about it)
Thanks !

Cheers,
Jackie"
509,https://github.com/stanfordnlp/CoreNLP/issues/640,640,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",closed,2018-03-06 18:18:14+00:00,,outdated libraries. any plans to patch them?,"Looking over lib folder I see very old libraries. for example

log4j-1.2.16.jar	Put CoreNLP on GitHub	5 years ago
lucene-analyzers-common-4.10.3.jar	Compile with new requirements system	2 years ago

any plans to update them to take advantage of bug fixes, security patches and new capabilities?"
510,https://github.com/stanfordnlp/CoreNLP/issues/642,642,[],closed,2018-03-08 07:58:34+00:00,,Can train based on last result ?,"Hi,

We have a very big corpus and our customer may add new corpus very frequently, but everytime the train will take long time. So I have a question: Can we do incremental training or say only train new added corpus  then merge with last result ? or any other way to enhance the training performance is appreciated.

Thanks a lot !

"
511,https://github.com/stanfordnlp/CoreNLP/issues/643,643,[],closed,2018-03-12 12:36:29+00:00,,Indicate model version & corenlp version in online demo corenlp.run,"Thank you very much for http://corenlp.run/ - I frequently use it to check how Stanford processes some particular sentence during development and debugging.

However, the web site does not indicate which CoreNLP version, and even more important, which model it uses. For example, I cannot tell if it is already updated to 3.9.x.

Please include the version number and model in the web site output. Thank you."
512,https://github.com/stanfordnlp/CoreNLP/issues/644,644,[],open,2018-03-12 23:26:10+00:00,,Retrieve Probabilities of Named Entities Labels,"printProbs can be used to print the probabilities that each token is a particular type of named entity. Would it be possible to build out functionality to instead return a HashMap of sorts, perhaps mapping a token to another map of label to probability? "
513,https://github.com/stanfordnlp/CoreNLP/issues/645,645,[],closed,2018-03-13 02:30:53+00:00,,"The size of JAR is too big, and I only use one of the tools. Is there any way to simplify the JAR?","The jar packages are too heavy. I only use the lemmatization tools. Is there any way to make the jar smaller if I only need one of the tools?

Thanks."
514,https://github.com/stanfordnlp/CoreNLP/issues/647,647,"[{'id': 45387508, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}]",closed,2018-03-14 19:49:41+00:00,,Custom Pipeline,"Hello,

I really like CoreNLP, thanks for the good tool.

I'm wondering if customizing the pipeline is possible.
For example, I want to call only the CoreNLP's POS tagger component with a tokenized string list, 
or I want to call only the dependency parser component using my own POS tags.

FYI, Spacy supports a customized pipeline as described in [here](https://spacy.io/usage/processing-pipelines).

Hope I could find a clue of this feature.
Thank you very much!"
515,https://github.com/stanfordnlp/CoreNLP/issues/648,648,[],closed,2018-03-15 14:32:55+00:00,,java.lang.NoSuchMethodError: de.jollyday.Holiday.getDate()Lorg/joda/time/LocalDate,"I'm getting a runtime exception when running the CoreNLP command-line software. It failed with a runtime exception after successfully analyzing more than 2000 other phrases. The phrase I'm analyzing is this:

```
Memorial Day 2014: Banks, post office, Walmart open or closed Monday, May 26?
```

```
kb in ~/workspace/standford-corenlp/stanford-corenlp-full-2018-02-27         
$ java --add-modules java.se.ee -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input02.txt -outputFormat json                                                                                                                                 
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize                                                                         
[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.                                        
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit                                                                           
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos 
[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.7 sec].                                                                                                                        
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma                                                                            
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner 
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].                                                                                                                                           
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].                                                                                                                                           
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.6 sec].                                                                                                                                         
[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.                                                                                                                          
[main] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt                                                            
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator ner.fine.regexner: Read 580641 unique entries out of 581790 from edu/stanford/nlp/models/kbp/regexner_caseless.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator ner.fine.regexner: Read 4857 unique entries out of 4868 from edu/stanford/nlp/models/kbp/regexner_cased.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator ner.fine.regexner: Read 585498 unique entries from 2 files
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.5 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator dcoref
[main] INFO edu.stanford.nlp.pipeline.CorefMentionAnnotator - Using mention detector type: dependency

Processing file /home/kb/workspace/standford-corenlp/stanford-corenlp-full-2018-02-27/input02.txt ... writing to /home/kb/workspace/standford-corenlp/stanford-corenlp-full-2018-02-27/input02.txt.json
Exception in thread ""main"" java.lang.NoSuchMethodError: de.jollyday.Holiday.getDate()Lorg/joda/time/LocalDate;
        at edu.stanford.nlp.time.JollyDayHolidays$JollyHoliday.resolveWithYear(JollyDayHolidays.java:211)
        at edu.stanford.nlp.time.JollyDayHolidays$JollyHoliday.resolve(JollyDayHolidays.java:223)
        at edu.stanford.nlp.time.JollyDayHolidays$JollyHoliday.intersect(JollyDayHolidays.java:197)
        at edu.stanford.nlp.time.SUTime$Time.intersect(SUTime.java:1568)
        at edu.stanford.nlp.time.SUTime$TemporalOp$7.apply(SUTime.java:1124)
        at edu.stanford.nlp.time.SUTime$TemporalOp.apply(SUTime.java:1355)
        at edu.stanford.nlp.time.SUTime$TemporalOp.apply(SUTime.java:1360)
        at edu.stanford.nlp.time.GenericTimeExpressionPatterns$5.apply(GenericTimeExpressionPatterns.java:357)
        at edu.stanford.nlp.ling.tokensregex.types.Expressions$FunctionCallExpression.evaluate(Expressions.java:929)
        at edu.stanford.nlp.ling.tokensregex.SequenceMatchRules$SequenceMatchResultExtractor.apply(SequenceMatchRules.java:748)
        at edu.stanford.nlp.ling.tokensregex.SequenceMatchRules$SequenceMatchResultExtractor.apply(SequenceMatchRules.java:725)
        at edu.stanford.nlp.ling.tokensregex.SequenceMatchRules$SequencePatternExtractRule.apply(SequenceMatchRules.java:993)
        at edu.stanford.nlp.ling.tokensregex.SequenceMatchRules$SequencePatternExtractRule.apply(SequenceMatchRules.java:945)
        at edu.stanford.nlp.ling.tokensregex.SequenceMatchRules$CoreMapFunctionApplier.apply(SequenceMatchRules.java:1153)
        at edu.stanford.nlp.ling.tokensregex.SequenceMatchRules$CoreMapFunctionApplier.apply(SequenceMatchRules.java:1134)
        at edu.stanford.nlp.ling.tokensregex.MatchedExpression$SingleAnnotationExtractor.annotate(MatchedExpression.java:116)
        at edu.stanford.nlp.ling.tokensregex.MatchedExpression$SingleAnnotationExtractor.annotate(MatchedExpression.java:108)
        at edu.stanford.nlp.ling.tokensregex.MatchedExpression.extractAnnotation(MatchedExpression.java:236)
        at edu.stanford.nlp.ling.tokensregex.MatchedExpression.extractAnnotation(MatchedExpression.java:226)
        at edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor.annotateExpressions(CoreMapExpressionExtractor.java:542)
        at edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor.applyCompositeRule(CoreMapExpressionExtractor.java:432)
        at edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor.extractExpressions(CoreMapExpressionExtractor.java:495)
        at edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressions(TimeExpressionExtractorImpl.java:198)
        at edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressions(TimeExpressionExtractorImpl.java:184)
        at edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressionCoreMaps(TimeExpressionExtractorImpl.java:115)
        at edu.stanford.nlp.time.TimeExpressionExtractorImpl.extractTimeExpressionCoreMaps(TimeExpressionExtractorImpl.java:105)
        at edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.runSUTime(NumberSequenceClassifier.java:345)
        at edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.classifyWithSUTime(NumberSequenceClassifier.java:143)
        at edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.classifyWithGlobalInformation(NumberSequenceClassifier.java:106)
        at edu.stanford.nlp.ie.NERClassifierCombiner.recognizeNumberSequences(NERClassifierCombiner.java:368)
        at edu.stanford.nlp.ie.NERClassifierCombiner.classifyWithGlobalInformation(NERClassifierCombiner.java:311)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifySentenceWithGlobalInformation(AbstractSequenceClassifier.java:343)
        at edu.stanford.nlp.pipeline.NERCombinerAnnotator.doOneSentence(NERCombinerAnnotator.java:290)
        at edu.stanford.nlp.pipeline.SentenceAnnotator.annotate(SentenceAnnotator.java:102)
        at edu.stanford.nlp.pipeline.NERCombinerAnnotator.annotate(NERCombinerAnnotator.java:253)
        at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:660)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:670)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1265)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1010)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1365)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1427)
```

My system is Ubuntu 17.10 with openjdk-8."
516,https://github.com/stanfordnlp/CoreNLP/issues/649,649,[],open,2018-03-20 12:35:21+00:00,,Standford UI identifying 2016 as DATE but programatically calling MentionsAnnotation 2016 is identified as NUMBER,"Hi Team,

Considering the below sentence as an example
""Polytec Asset Holdings Limited Annual Report **2016** REPORT OF THE DIRECTORS HUMAN RESOURCES As at 31 December 2016, the total number of employees of the Group was about 370 (2015: 370)""

when tested through UI 2016 is identified as DATE which is correct.

![image](https://user-images.githubusercontent.com/20786984/37654535-7b1dfe12-2c68-11e8-9265-03f054e31737.png)

However when passed the same sentence to Annotation and fetching the mentions using MentionsAnnotation, **2016** is identified as **NUMBER which not right**. Below is the code used following the screenshot of mentions captured

 Annotation text = new Annotation(sentence);
            _pipeline.annotate(text);
            List mentions = (List)text.get(java.lang.Class.forName(""edu.stanford.nlp.ling.CoreAnnotations$MentionsAnnotation""));


![image](https://user-images.githubusercontent.com/20786984/37654697-08916004-2c69-11e8-8cb1-f5d9ad19d5df.png)

Could you please guide us on where exactly the issue is and how to fix it. 
Thank you in advance."
517,https://github.com/stanfordnlp/CoreNLP/issues/650,650,[],closed,2018-03-22 15:32:24+00:00,,Question: Can the CoreNLP Classifier do multi-label classification?,I've been reading the classifier docs and source code trying to figure out how to do it from the command line but I've had no luck. I see there is a KNNClassifierFactory class but I don't know how to call it or how to specify what the columns for classes are. Is there any way to do multi-label classification currently?
518,https://github.com/stanfordnlp/CoreNLP/issues/653,653,[],closed,2018-03-23 11:29:45+00:00,,quote.attribution annotation protobuf serialization (CoreNLPServer). ,"Currently, I can only get quote attribution annotations through the CoreNLPServer in json output format.
Are there any plans to add protobuf support? 

Thanks"
519,https://github.com/stanfordnlp/CoreNLP/issues/655,655,[],closed,2018-03-25 13:18:03+00:00,,The stanford-english-corenlp-2018-02-27-models.jar file from Stanford domain has invalid property file,"It has  a file called: StanfordCoreNLP.properties
Which contains keys with parser.model instead of parse.model

"
520,https://github.com/stanfordnlp/CoreNLP/issues/656,656,[],closed,2018-03-29 12:48:27+00:00,,Adverbs of Orientation,"Most of the adverbs of orientation are tagged incorrectly. Some are showing up as compound nouns, others as adjectives. 

[Adverbs of Orientation.xlsx](https://github.com/stanfordnlp/CoreNLP/files/1860108/Adverbs.of.Orientation.xlsx)
"
521,https://github.com/stanfordnlp/CoreNLP/issues/657,657,[],closed,2018-03-29 14:31:16+00:00,,NER Recognizing Amazon as a location,"Donald Trump has lashed out at Amazon.

Amazon .. location ?"
522,https://github.com/stanfordnlp/CoreNLP/issues/658,658,[],closed,2018-03-30 08:49:25+00:00,,RegexNERAnnotator | ValidPosPattern issue,"Hi,
I am using the corenlp pipeline for entities recognition.

Could you explain me a little bit what is the proper use of the ValidPosPattern option?

I noticed that using the default parameters ValidPosPattern is set to null.
By the way the mapping files are loaded in the following way:

`
""ignorecase=true,validpospattern=^(NN|JJ).*, edu/stanford/nlp/models/kbp/regexner_caseless.tab;""
   ""edu/stanford/nlp/models/kbp/regexner_cased.tab;""
`

At first glance I thought that the pattern ""^(NN|JJ).*"" would have applied only for the entities contained in the first file.
However, when the POS tags are checked [here](https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java#L419), it seems like the pattern is applied to all the entries without any distinction.

Thank you for your help
"
523,https://github.com/stanfordnlp/CoreNLP/issues/660,660,[],open,2018-04-02 09:13:51+00:00,,Skipping the same roles in AceEventMention,"In the class `edu.stanford.nlp.ie.machinereading.domains.ace.reader.AceEventMention`, the class is designed to have a hashmap named `mRolesToArguments`, and thus to skip the same roles when parsing ACE2005 dataset.

But in ACE2005, a part of arguments with the same roles is not unpopular.
For example, there are two Destination roles in the `event_mention ID=""AFP_ENG_20030616.0715-EV10-1""` of `English/nw/timex2norm/APF_ENG_20030616.0715.apf.xml`.

```xml
<event_mention_argument REFID=""AFP_ENG_20030616.0715-E22-32"" ROLE=""Destination"">
  <extent>
    <charseq START=""1201"" END=""1235"">the closet at the Byrds Creek house</charseq>
  </extent>
</event_mention_argument>
<event_mention_argument REFID=""AFP_ENG_20030616.0715-E18-52"" ROLE=""Destination"">
  <extent>
    <charseq START=""1269"" END=""1314"">the closet of her apartment in Richland
Center</charseq>
  </extent>
</event_mention_argument>
```

Now I have replaced the hashmap with a kind of pair tuple list in my own branch, wish to see similar bug-fix in the master branch."
524,https://github.com/stanfordnlp/CoreNLP/issues/661,661,[],closed,2018-04-02 09:56:01+00:00,,CTB9.0 to UD,Is there anyway to tansfer ctb9.0 to Universal Dependencies?
525,https://github.com/stanfordnlp/CoreNLP/issues/662,662,[],closed,2018-04-03 19:40:41+00:00,,Clarification on licensing (GPLv3 vs. GPLv2),"Hi,

The licensing section of CoreNLP's documentation states, ""in general Stanford NLP code is GPL v2+, but CoreNLP uses several Apache-licensed libraries, and so the composite is v3+"".  Does this mean the CoreNLP source and its models are licensed under the GPLv2, but that you must claim the GPLv3 license because some of its transitive dependencies are GPLv3 licensed?  If that's the case, if the GPLv3 licensed dependencies are removed or changed to non-GPLv3 versions, would that mean the transitive dependency closure (including CoreNLP) would then be GPLv2?

Also, what is the difference between the standalone CoreNLP distributions and its main distribution?  I found this issue stating they differ in licensing:

https://github.com/stanfordnlp/CoreNLP/issues/220"
526,https://github.com/stanfordnlp/CoreNLP/issues/663,663,[],closed,2018-04-05 14:35:57+00:00,,Unable to start CoreNLP server,"Getting the below error when I run this code 'java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000'

I changed to the path where I placed the CoreNLP folder to load the code, libraries, and model jars that the CoreNLP uses: java -cp ""/Users/stanford-corenlp-full-2018-01-31/*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -file inputFile

I also created the configuration file: java -cp ""*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -props sampleProps.properties

Please let me know what next steps I should take to fix this.

[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - setting default constituency parser
[main] INFO CoreNLP - warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP - using: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz instead
[main] INFO CoreNLP - to use shift reduce parser download English models jar from:
[main] INFO CoreNLP - http://stanfordnlp.github.io/CoreNLP/download.html
[main] INFO CoreNLP -     Threads: 8
[main] INFO CoreNLP - Starting server...
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:414)
	at sun.nio.ch.Net.bind(Net.java:406)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at sun.net.httpserver.ServerImpl.<init>(ServerImpl.java:100)
	at sun.net.httpserver.HttpServerImpl.<init>(HttpServerImpl.java:50)
	at sun.net.httpserver.DefaultHttpServerProvider.createHttpServer(DefaultHttpServerProvider.java:35)
	at com.sun.net.httpserver.HttpServer.create(HttpServer.java:130)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.run(StanfordCoreNLPServer.java:1297)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.main(StanfordCoreNLPServer.java:1389)
[Thread-0] INFO CoreNLP - CoreNLP Server is shutting down.



"
527,https://github.com/stanfordnlp/CoreNLP/issues/664,664,[],closed,2018-04-05 21:50:30+00:00,,IndexOutOfBoundsException on empty `CoreDocument`s,"If a document is empty (i.e. because a previous Annotator removed some tokens), and thus, the list of sentences is empty, `wrapAnnotations` in `CoreDocument` fails:
```
Exception in thread ""main"" java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at java.util.ArrayList.rangeCheck(ArrayList.java:657)
	at java.util.ArrayList.get(ArrayList.java:433)
	at edu.stanford.nlp.pipeline.CoreDocument.wrapAnnotations(CoreDocument.java:37)
```
Example code:
```
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.*;

import java.util.Properties;

public class UnclosedSentence {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,cleanxml,ssplit"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        CoreDocument doc = new CoreDocument(""<random> <xml/> </stuff>"");
        pipeline.annotate(doc.annotation());
        System.out.println(doc.annotation().get(CoreAnnotations.SentencesAnnotation.class)); // annotation works
        doc.wrapAnnotations(); // fails with IndexOutOfBoundsException: Index: 0, Size: 0
    }
}
```"
528,https://github.com/stanfordnlp/CoreNLP/issues/666,666,[],closed,2018-04-06 01:50:41+00:00,,Compound-splitter ,"Hi,

I wonder if the CoreNLP has a model to split the compounds words like ""HappyNewYear"" to ""Happy New Year""."
529,https://github.com/stanfordnlp/CoreNLP/issues/667,667,[],open,2018-04-06 11:46:10+00:00,,complex capturing group,"I am trying to extract skills from plain text using tokensRegex and the capturing groups techniques.

for example when i have  the sentence ""teamwork, verbal communication, listening and phone skills."".
I want to extract the info using the following tokensRegex ""( [tag:NN | tag:NNS ]+ "","")*  ( [tag:NN | tag:NNS ]+ [tag:CC])?  ( [tag:NN | tag:NNS ])* skills""

whenever I try to put multible quantifiers inside a group (like + and then * on different levels of the grouping) I get ""NullPointerException"".
I tried simpler tokensregex like ""( [tag:NN | tag:NNS ]+ "","")*   ([tag:NN])"" but it didn't work.
if I try ""( [tag:NN | tag:NNS ]+ "","")*"" alone  it works fine. even with its multible quantifiers

I guiess the problem happens when putting multible capturing gropus and each with different quantifiers.
by the way when i try non-capturing group it works fine. but i need the groups to extract the specific data."
530,https://github.com/stanfordnlp/CoreNLP/issues/668,668,[],closed,2018-04-08 02:30:41+00:00,,Can CoreNLP Server receive already tokenized inputs?,"Because I want to use my own trained tokenizer, I want to know whether there is a way to inputs tokenized sentence to the CoreNLP Server Annotators, such as NER, POS, dependency parser...

I have searched the issues and googled it, but I can't get a solution.

Thank you"
531,https://github.com/stanfordnlp/CoreNLP/issues/669,669,[],closed,2018-04-09 22:21:12+00:00,,"Default NER models and code keep state, resulting in ordering effects","We run NER across a directory of files and observe that the results are different across runs.  This happens most often when files are processed in parallel, but it can happen in serial cases as well if the data is reordered.

According to english.all.3class.distsim.prop and two other files packaged with the models, wordShape=chris2useLC or dan2useLC.  This means they use algorithms for word shaping that consult knownLCWords, a record of known lowercase words.  The problem is that the record is updated as text is processed by code in ObjectBankWrapper, namely knownLCWords.add(word);, and there doesn't appear to be a way to reset it other than to reconstruct the processor.

What happens is that we see a capitalized word like Belt in a sentence.  It is not in the initial knownLCWords and gets a shape something like Xxxx which for our sentence eventually translates to a category O.  Some later text then has belt, which gets added to knownLCWords.  If the sentence with Belt is run again (or it is run for the first time after seeing belt), the shape is something like Xxxxk which eventually results in LOCATION.  So, if our files or sentences come in ""Belt belt"" order, we get one answer and if they are ""belt Belt"" we get a different answer for the same sentence.

I have not found code that will allow me to reset knownLCWords between runs, despite there being various reinit() methods.  knownLCWords is designed to be thread-safe (see below), so reserving it for one text and resetting it for the next is not desirable anyway.  flags.useShapeStrings might be useful, but I think that's set in the serialized models.  It seems like the best way to achieve repeatable results is to change the wordShape settings to chris2 and dan2 without the useLC.  This may imply a rebuild of the models.

This issue is also described at https://github.com/clulab/eidos/issues/261.

Although it may be a feature to have the WordShapeClassifier learn new lowercase words, for our application it's closer to a bug.  I hope someone can comment on any plans related to the issue.  Thanks.


  /** Different threads can add or query knownLCWords at the same time,
   *  so we need a concurrent data structure.  Created in reinit().
   */
  protected MaxSizeConcurrentHashSet<String> knownLCWords; // = null;"
532,https://github.com/stanfordnlp/CoreNLP/issues/670,670,[],closed,2018-04-10 14:26:27+00:00,,German parsing without dependency relations,"When I use stanford parser to analyse German, the only two labeled dependency relations are root and punct, so how can i get further analysis of dependency relations?"
533,https://github.com/stanfordnlp/CoreNLP/issues/671,671,[],closed,2018-04-10 14:56:56+00:00,,Sentence Splitting Issues,"Sentence splitting is failing on enumerated lists. 

Test cases: 
""Bob ate three things: 1. a pizza, 2. a pie and 3. a cookie.""
""Bob ate three things: (1). a pizza, (2). a pie and (3). a cookie."""
534,https://github.com/stanfordnlp/CoreNLP/issues/672,672,[],open,2018-04-10 15:53:49+00:00,,Broken Pipe and Concurrent Timeout Exceptions from CoreNLP Server,"Hi guys,

I am receiving several errors that is continually appearing every few minutes and I am not sure why, can someone please tell me what these following errors maybe caused by?

Mainly two errors, one is a IOException broken pipe (the most common) and another one is a timeout error from Concurrent execution. Screenshot attached.

<img width=""938"" alt=""screen shot 2018-04-10 at 16 32 16"" src=""https://user-images.githubusercontent.com/3438620/38568068-a3b42f4a-3cdf-11e8-8a97-3810d6ae1fa3.png"">

<img width=""930"" alt=""screen shot 2018-04-10 at 16 37 03"" src=""https://user-images.githubusercontent.com/3438620/38568073-a7f197d2-3cdf-11e8-93a4-3ca15810ba5b.png"">
"
535,https://github.com/stanfordnlp/CoreNLP/issues/673,673,[],closed,2018-04-10 19:57:25+00:00,,Prepend URI prefix,"Hello, any way to run the server in a different URI context other than / ? Say /corenlp or something configurable so I can attach it to a shared ELB on AWS?"
536,https://github.com/stanfordnlp/CoreNLP/issues/674,674,[],open,2018-04-12 01:46:48+00:00,,Please add some examples on how to create custom SUTime rules,"I am trying to add rules for:
- Year to date
- Quarter to date
- Month to date
- Week to date

The documentation wasn't straight forward. I've spent hours digging on how to do this.

Kindly add some documentation."
537,https://github.com/stanfordnlp/CoreNLP/issues/675,675,[],closed,2018-04-13 15:00:54+00:00,,"Interrogative ""may""","Sample:
""May Susan go to lunch?""

When ""may"" is used as an interrogative, it is consistently tagged incorrectly from a part of speech perspective. 



"
538,https://github.com/stanfordnlp/CoreNLP/issues/676,676,[],closed,2018-04-16 03:11:34+00:00,,coref without pronoun,"I want to use coref annotator to find out entities in a cluster. But it seems that the coref annotator will skip those entity which isn't referred by a pronoun.

For example, 
sentence1 ""David is a student and he study in Pittsburgh"", 
the annotator will give coref chain of [""David"", 'he""]

sentence2 ""David is a student""
the annotator will give no coref chain.

I suppose this should be a necessary features, and what might be the best way to do this?

(Actually my task is to find out all entities, and count their frequency in a document. Therefore I need to group entities name into some cluster, and resolve the pronoun as well) "
539,https://github.com/stanfordnlp/CoreNLP/issues/677,677,[],closed,2018-04-16 09:10:19+00:00,,Question: Progress in NER CFR Reference and support for AllenNLP GRU-NER,"According to the [docs](https://nlp.stanford.edu/software/CRF-NER.html) the reference paper of the current CRF tagger was

> Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-370. http://nlp.stanford.edu/~manning/papers/gibbscrf3.pdf

In more recent papers new implementations came out like the one depicted in the AllenNLP framework, described in the paper [Semi-supervised sequence tagging with bidirectional language models](https://www.semanticscholar.org/paper/Semi-supervised-sequence-tagging-with-bidirectional-Peters-Ammar/73e59cb556351961d1bdd4ab68cbbefc5662a9fc) that achieves state-of-the-art results in terms of F1-score accuracy for supervised tagging scores (+1% improvement to the current reference score), and by the way it is using a Gated Recurrent Unit (GRU) character encoder and CRF tagger at the end of the tagger model. A pre-trained model is provided as well [here](https://s3-us-west-2.amazonaws.com/allennlp/models/ner-model-2018.02.12.tar.gz). My questions are: 1) which is the current version of the ner module in the CoreNLP we are using and which is the current reference accuracy? Is it possibile to support the AlleNLP pre-trained ner model in the ner pipeline?
Thanks a lot.
"
540,https://github.com/stanfordnlp/CoreNLP/issues/678,678,[],closed,2018-04-17 16:40:17+00:00,,Issue in getting the relation.,"hi 
  I try to get the relation from the RelationAnnatator but null is getting returned every time.I don't know the reason why it's not working I go through the codes but I can't get any idea.Please, can anyone help me on this issue?


java.lang.NullPointerException
	at edu.stanford.nlp.ie.machinereading.MachineReading.assignSyntacticHeadToEntities(MachineReading.java:650)
	at edu.stanford.nlp.ie.machinereading.MachineReading.annotate(MachineReading.java:596)
	at edu.stanford.nlp.ie.machinereading.MachineReading.annotate(MachineReading.java:567)
	at edu.stanford.nlp.pipeline.RelationExtractorAnnotator.annotate(RelationExtractorAnnotator.java:55)
	at com.nlp.main.RelationFinder.main(RelationFinder.java:31)


This is the exception i got."
541,https://github.com/stanfordnlp/CoreNLP/issues/679,679,[],closed,2018-04-19 09:03:32+00:00,,Can anyone suggest the austen.prop documentation page,"Hi! I am using NER for finding out Education Entities(College, Score, Year, Department, Degree), with default Austen.prop property I am not getting good accuracy. So I want to know about property in details"
542,https://github.com/stanfordnlp/CoreNLP/issues/680,680,[],open,2018-04-19 23:24:26+00:00,,POS Tagging Errors,The statistical model for part of speech tagging is not perfect due to both the limitations of the algorithm and training data.  We should use this thread to catalog errors that users identify.
543,https://github.com/stanfordnlp/CoreNLP/issues/681,681,[],closed,2018-04-22 06:33:59+00:00,,Problem with splitHyphenated,"In the document, I found the splitHyphenated for tokenizer options with the description: Whether or not to tokenize segments of hyphenated words separately (""school"" ""-"" ""aged"", ""frog"" ""-"" ""lipped"")...Default is currently false, which maintains old treebank tokenizer behavior. (This default will likely change in a future release.)

I've tried setting this options to True but I completly get the wrong tree. For example, with the noun phrase ""Chemical-induced disease"", I expect the disease being the main Noun.
Something like this
![image](https://user-images.githubusercontent.com/10630842/39092166-4b8767f0-4631-11e8-9edb-91024e1e9297.png)
Actually, I got a tree with the Chemical is subject and disease is object of a sentence. The whole meaning was changed.

Is there any way to solve this problem?
Thanks!"
544,https://github.com/stanfordnlp/CoreNLP/issues/682,682,[],closed,2018-04-23 12:19:57+00:00,,CoreNLP server cannot handle french unicode characters,"CoreNLP Server won't recognise unicode characters.
I start the server like this:
`java -mx2g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-french.properties -port 9000 -timeout 15000`

Then I send a request to it from python like this:

`params = (('properties', '{""annotators"":""tokenize,ssplit,pos,lemma,depparse"",""outputFormat"":""conllu""}'),)`

`data = ""Il a √©t√© cr√©√© quand?""`

`response = requests.post('http://localhost:9000/', params=params, data=data)`

`print(response.text)`

The output is then:

1	Il	il	_	PRON	_	4	nsubj	_	_
2	a	a	_	AUX	_	4	aux	_	_
3	t	t	_	X	_	4	compound	_	_
4	cr	cr	_	X	_	0	root	_	_
5	quand	quand	_	ADV	_	4	advmod	_	_
6	?	?	_	PUNCT	_	4	punct	_	_

So the '√©'s are missing. How can I fix it? It should be able to do it, because if I just run stanford CoreNLP  (from python) on a text file, then it works correctly. I use for this:

`[""java"", ""-classpath"", stanford_main_path, ""-Xmx2g"", ""edu.stanford.nlp.pipeline.StanfordCoreNLP"", ""-ssplit.eolonly"", ""-annotators"", ""tokenize,ssplit,pos,lemma,depparse"", ""-file"", input_path + input_name, ""-props"", ""StanfordCoreNLP-french.properties"", ""-outputDirectory"", output_directory, ""-outputFormat"", ""conllu""]`"
545,https://github.com/stanfordnlp/CoreNLP/issues/683,683,[],closed,2018-04-23 18:47:44+00:00,,Parser for Multilingual support - Danish,"Hi Team,,

Thanks for the support!!
I am trying to use danish for my NLP project but unable to find the parser for the same.3
I could also see the multilingual support for french, german etc but not Danish.
Is there a way to achieve danish parsing or do I need to add any library for it?

Please do the needful it's really appreciatable
Thanks
Sudhir
(CBS-Denmark)"
546,https://github.com/stanfordnlp/CoreNLP/issues/684,684,[],closed,2018-04-24 10:12:04+00:00,,Sentiment Tree as JSON Object,"Recently in the **Sentiment** module output a _SentimentTree_ output has been added:

```javascript
{
                ""index"": 4,
                ""sentimentValue"": ""2"",
                ""sentiment"": ""Neutral"",
                ""sentimentDistribution"": [
                  0.02319716964425,
                  0.24382036048496,
                  0.67127763735208,
                  0.05488071716532,
                  0.0068241153534
                ],
                ""sentimentTree"": ""(ROOT|sentiment=2|prob=0.671 (NP|sentiment=2|prob=0.998 this)\n  (@S|sentiment=2|prob=0.743 (ADVP|sentiment=2|prob=0.997 here)\n    (VP|sentiment=2|prob=0.749 (VBZ|sentiment=2|prob=0.994 's)\n      (SBAR|sentiment=2|prob=0.583 (WHNP|sentiment=2|prob=0.994 what)\n        (S|sentiment=2|prob=0.569 (NP|sentiment=2|prob=0.995 you)\n          (VP|sentiment=2|prob=0.648 (VBP|sentiment=2|prob=0.992 call)\n            (NP|sentiment=2|prob=0.432 (DT|sentiment=2|prob=0.990 a) (JJ|sentiment=1|prob=0.336 flip))))))))"",
                ""tokens"":[]
```

This output has been widely discussed here https://github.com/stanfordnlp/CoreNLP/issues/465 by @J38 and others. The output is it ok to be used _within_ Java CoreNLP through a `TreeFactory` module impl like the `LabeledScoredTreeFactory`, by the way it cannot be used by a visual representation like the D3 dataviz in the demo:

<img width=""963"" alt=""schermata 2018-04-24 alle 12 05 57"" src=""https://user-images.githubusercontent.com/163333/39180701-e15f4c0c-47b7-11e8-8e24-3a1edce7e7b2.png"">

This data visualization is build using the good D3.js and a JSON structure of the tree:

```javascript
drawTrees( d3.select( ""div.trees"" ), data.trees, params );
```

where the `data.trees` have a struct like

```javascript
{
	""version"": 2,
	""build"": ""2018-04-24 02:35:39.775761"",
	""trees"": [{
		""child0"": {
			""child0"": {
				""index"": 3,
				""leaf"": true,
				""rating"": 13.0,
				""text"": ""This"",
				""scoreDistr"": [0.0001, 0.0007, 0.9985, 0.0006, 0.0001],
				""tokens"": 1,
				""depth"": 1,
				""numChildren"": 0,
				""pixels"": 26
			},
			""index"": 2,
			""leaf"": false,
			""rating"": 13.0,
			""text"": ""This movie"",
			""scoreDistr"": [0.0002, 0.0032, 0.9908, 0.0056, 0.0002],
			""tokens"": 3,
			""depth"": 2,
			""numChildren"": 2,
			""pixels"": 63
		},
		""index"": 1,
		""leaf"": false,
		""rating"": 8.0,
		""text"": ""This movie does n't care about cleverness , wit or any other kind of intelligent humor ."",
		""scoreDistr"": [0.1685, 0.7187, 0.0903, 0.0157, 0.0068],
		""tokens"": 33,
		""depth"": 10,
		""numChildren"": 2,
		""pixels"": 493
	}]
}
```

(this is a simplified version with just one root node and one child - see the whole version [here](https://pastebin.com/yvvmikLZ)

How to translate the current string tree representation in the `sentimentTree` to JSON struct represented in the Sentiment live demo?"
547,https://github.com/stanfordnlp/CoreNLP/issues/685,685,[],closed,2018-04-24 10:28:56+00:00,,Sentiment Treebank Annotation Tool,"The live demo has a annotation tool of the Sentiment Tree. I have tried to annotate from this nifty UI that let to annotate the sentiment classes by each tree node:

<img width=""982"" alt=""schermata 2018-04-24 alle 12 26 50"" src=""https://user-images.githubusercontent.com/163333/39181721-d2f0a352-47ba-11e8-85f8-0eb6eb27817a.png"">

Is this tool still working i.e. the submitted annotation are used to re-train the model?
Also, currently in the CoreNLP pipeline demo there is a live demo, but the RNTN demo plus annotator it is missing. Why not provide this demo as well in the CoreNLP demo web app? This would help to improve the annotator as well as the annotated dataset for the Treebank Sentiment.
"
548,https://github.com/stanfordnlp/CoreNLP/issues/686,686,[],closed,2018-04-26 05:51:27+00:00,,[sutime] no value extracted for input with token 'first',"I gave a try to sutime via online demo
[http://nlp.stanford.edu:8080/sutime/process](http://nlp.stanford.edu:8080/sutime/process)

seems that the demo has problem with input containing token '**first**', 


- output for 'first week in jan':

| text        | value           | timex tag  |
| ------------- |:-------------:| -----:|
| first week in jan|  |\<TIMEX3 alt_value=""XXXX-01 INTERSECT P1W-#1"" anchorTimeID=""t2"" temporalFunction=""true"" tid=""t1"" type=""DATE"" valueFromFunction=""tf0""\>first week in jan\</TIMEX3\>|

replacing '**first**' with  '**second**' leads to correct result,

- output for 'second week in jan':

| text        | value           | timex tag  |
| ------------- |:-------------:| -----:|
| second week in jan| 2018-W02 | \<TIMEX3 tid=""t1"" type=""DATE"" value=""2018-W02""\>second week in jan\</TIMEX3\> |

is there any way to fix this?"
549,https://github.com/stanfordnlp/CoreNLP/issues/687,687,[],closed,2018-04-27 19:48:03+00:00,,How to correlate between parser input string and output tree/tokens,"Sometimes an input string can have for example multiple spaces, or multiple sentences, or other characters that are being removed by the parser, and sometimes the parser returns a forest (list of trees, each representing a sentence).

I want to be able to correlate exactly from an input segment (start/end character), to the corresponding token in the output forest. How can I easily do it? I mean, the parser knows what characters were removed/trimmed, so it would be useful to add an API parameter (for the CoreNLP server), that instead of just returning the tokens, and their POS, also the start/end character in the corresponding input sentence."
550,https://github.com/stanfordnlp/CoreNLP/issues/688,688,[],closed,2018-04-29 13:47:45+00:00,,About parsing adv. to its original word,"What if we want to parse words like ""sadly"", ""desperately"" or ""deleting"" into their original verbs/nouns/adj word? Like ""sadly"" -> ""sad"", ""deleting"" -> ""delete"", ""desperately"" -> ""desperate""..
Concurrently adv words were marked as RB or RBR after the PoS and lemma;
What should we do? ISorry maybe its my carelessness, but I can not find the option to solve this question;"
551,https://github.com/stanfordnlp/CoreNLP/issues/689,689,[],closed,2018-04-29 21:58:26+00:00,,Typed Dependencies Output.,"I've created a simple demo program that takes a sentence as an input and produces the Stanford universal dependencies as output, I've used the [C# wrapper ](https://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html)for StanfordCoreNLP which basically just calls java code. The code is as follows:
```
static StanfordCoreNLP pipeline;
static void Main(string[] args) {
    string modelsPath = @""stanford-corenlp-3.9.1-models"";
    var props = new Properties();
    
    // set pipeline annotators.
    props.setProperty(""annotators"", ""tokenize, ssplit,  pos, lemma, parse, depparse"");
    props.setProperty(""ner.useSUTime"", ""0"");
    props.setProperty(""depparse.model"", ""edu/stanford/nlp/models/parser/nndep/english_UD.gz"");
    props.setProperty(""parse.nthreads"", ""4"");
    Directory.SetCurrentDirectory(modelsPath);
    pipeline = new StanfordCoreNLP(props);

    // annotate text
    Console.WriteLine(""FINISHED LOADING MODELS."");

    while (true) {
        text = Console.ReadLine();
        var annotation = new Annotation(text);
        pipeline.annotate(annotation);
        var sentences = annotation.get(new CoreAnnotations.SentencesAnnotation().getClass()) as ArrayList;
        foreach (CoreMap sentence in sentences) {
            SemanticGraph graph = (SemanticGraph)sentence.get(new EnhancedPlusPlusDependenciesAnnotation().getClass());
            var typedDependencies = graph.typedDependencies() as ArrayList;
            foreach (var td in typedDependencies) Console.WriteLine(td.ToString());
         }
     }
 }
```
When I add the sentence 

_The hotel has pulled in millions of dollars._

 as input, the output is:

> root(ROOT-0, pulled-4)
> det(hotel-2, The-1)
> nsubj(pulled-4, hotel-2)
> aux(pulled-4, has-3)
> nmod(pulled-4, dollars-8)
> punct(pulled-4, .-9)
> case(millions-6, in-5)
> mwe(millions-6, of-7)
> det:qmod(dollars-8, millions-6)


My question is: shouldn't the relation ""case(millions-6, in-5)"" have a governor of ""dollars-8"" instead of ""millions-6"" as ""millions-6"" is considered only a determiner here for the token ""dollars-8"" according to the relation ""det:qmod(dollars-8, millions-6)""?

I am using StanfordCoreNLP 3.9.1"
552,https://github.com/stanfordnlp/CoreNLP/issues/690,690,[],open,2018-05-01 07:00:19+00:00,,Using QNMinimizer intead of OWLQNMinimizer ,"The `l1reg` option in CRFClassifier and ColumnDataClassifier uses OWLQNMinimizer by default, but it is not distributed in the public release. I posted this query in the mailing group and Prof. Manning said that it's possible to use QNMinimizer to get the regularization functionality. 

I was able to modify the code in ColumnDataClassifier and get `l1reg` working. Is there any reasons for not making this change in the official release. "
553,https://github.com/stanfordnlp/CoreNLP/issues/691,691,[],closed,2018-05-02 00:01:44+00:00,,Link inaccessible to the demo,The link to the demo at http://nlp.stanford.edu:8080/sentiment/rntnDemo.html is not opening up. Can it be fixed?
554,https://github.com/stanfordnlp/CoreNLP/issues/692,692,[],closed,2018-05-02 17:19:55+00:00,,"Error while loading a tagger model, while reading models from path","Hello!

I am using Stanford-NLP 3.8.0 for my project in work
I was reading a lot questions about my problem, no in stackoverflow and any other sites, but i still didnt find the solution, and there no any situation like my in all of the places, where i was looking for, so i create this question

In my work i need to use Stanford NLP in a web application whithout dependencies of Stanford-parse and Stanford-models, so the solution like [here](https://stackoverflow.com/questions/43677813/program-for-tagger-and-sentiment-analysis-in-stanford-nlp) not for me. Why whithout two this dependencies? Cause they weigh too much. In my project i can only load an Standord-Core-Nlp dependency, and thats all.

The problem is next.

I have got two models. The first is ""russian-ud-pos.tagger"" from MANASLU8 project of students of ITMO Univercity, you can download it [here](https://drive.google.com/drive/folders/0B4TmAgcGLMriMG96cFZSSWhWcEU). The second is a Stanford CRF model english.all.3class.distsim.crf.ser.gz, it is a standart model, that you can download [here](https://nlp.stanford.edu/software/CRF-NER.html)

So i got this two files, and i got two codes, almost identical
The pom for both codes is the same:
edu.stanford.nlp stanford-corenlp 3.8.0

And thats all (yes, my pom is whithout parser, models and etc, only the Stanford-Core)

1) The first code work well. Here i put two my files in src/main/resources
And the code is next

        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        props.setProperty(""pos.model"", ""russian-ud-pos.tagger"");
        props.setProperty(""ner.model"", ""english.all.3class.distsim.crf.ser.gz"");
        props.setProperty(""ner.useSUTime"", ""false"");
        props.setProperty(""ner.applyNumericClassifiers"", ""false"");
        props.setProperty(""sutime.includeRange"", ""false"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

As you can see, i dont use any paths in properties object, just the name of files

When i get start my application, it show me next stackTrace:

    19:17:55.979 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
    19:17:55.994 [main] INFO  e.s.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.
    19:17:55.994 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
    19:17:55.994 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
    19:17:56.790 [main] INFO  e.s.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from russian-ud-pos.tagger ... done [0.8 sec].
    19:17:56.790 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
    19:17:56.790 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
    19:18:00.737 [main] INFO  e.s.n.ie.AbstractSequenceClassifier - Loading classifier from english.all.3class.distsim.crf.ser.gz ... done [3.9 sec].
    19:18:01.002 [main] INFO  e.s.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from russian-ud-pos.tagger ... done [0.3 sec].

And then it work successfully

2) Here is the second code
        
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
        props.setProperty(""pos.model"", ""file:D:\\russian-ud-pos.tagger"");
        props.setProperty(""ner.model"", ""file:D:\\english.all.3class.distsim.crf.ser.gz"");
        props.setProperty(""ner.useSUTime"", ""false"");
        props.setProperty(""ner.applyNumericClassifiers"", ""false"");
        props.setProperty(""sutime.includeRange"", ""false"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

As you can see, i give the way to file to properties, like
(""file:D:\\english.all.3class.distsim.crf.ser.gz"") but not the (""english.all.3class.distsim.crf.ser.gz""),
Also i delete files from src/main/resources,
And when i start my code, stacktrace is folowing:

    19:25:16.109 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
    19:25:16.109 [main] INFO  e.s.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.
    19:25:16.125 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
    19:25:16.125 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
    19:25:16.936 [main] INFO  e.s.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from file:D:\russian-ud-pos.tagger ... done [0.8 sec].
    19:25:16.936 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
    19:25:16.936 [main] INFO  e.s.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
    19:25:21.257 [main] INFO  e.s.n.ie.AbstractSequenceClassifier - Loading classifier from file:D:\english.all.3class.distsim.crf.ser.gz ... done [4.2 sec].

    edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file)

    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:791)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.<init>(MaxentTagger.java:312)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.<init>(MaxentTagger.java:265)
    at stanfordapplication.StanfordApplication.start(StanfordApplication.java:49)
    at Test1.stanfordStringReader(Test1.java:56)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
    at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
    at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
    at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
    Caused by: java.io.IOException: Unable to open ""russian-ud-pos.tagger"" as class path, filename or URL
    at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:480)
    at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:789)
    ... 26 more

    Process finished with exit code -1

Here, as you can see, Stanford was ""Loading POS tagger from file:D:\russian-ud-pos.tagger"" and ""Loading classifier from file:D:\english.all.3class.distsim.crf.ser.gz"", but didnt load ""Loading POS tagger from russian-ud-pos.tagger"", like in the first code


I was trying to use other versions of Stanford-Core-NLP (3.9.1 and less).
I was watching the code of Sanford NLP and debug it, and still i cant understand the reasons.
Also, i was trying to put files in dick C, and in dick D, (i was thinking about the administrator rights ) and start IDEA with administration rigths.
And put a  path without ""file"", like this ""D:\english.all.3class.distsim.crf.ser.gz"".
Alse i was trying to aply some flags to properties, like (""ner.useSUTime"", ""false"") or (""ner.applyNumericClassifiers"", ""false"") etc.

It sounds strange, cause Stanford load first two files, (as i understand)
why it cant read the last?

Maybe Stanford cant read it, or remember, or read it more then one time

Anybody, help me please, i am trying to solve this problem about a week!"
555,https://github.com/stanfordnlp/CoreNLP/issues/693,693,[],closed,2018-05-03 03:10:40+00:00,,Exceptions while using Java client send Chinese to coreNLP Server ,"I set up a CoreNLP Server on my computer to Handle Chinese NLP.

> D:\repository\edu\stanford\nlp\stanford-corenlp\3.9.1>java -Xmx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-chinese.properties -port 9000 -timeout 15000
--- StanfordCoreNLPServer#main() called ---
setting default constituency parser
using SR parser: edu/stanford/nlp/models/srparser/englishSR.ser.gz
    Threads: 8
Starting server...
StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000

when i use Chrome(http post) to call coreNLP api ,It works.:

> [/0:0:0:0:0:0:0:1:64386] API call w/annotators tokenize,ssplit,pos,depparse,lemma,natlog,ner,openieÂæÖÊú∫Êó∂Èó¥Èïø
[/0:0:0:0:0:0:0:1:64423] API call w/annotators tokenize,ssplit,pos,depparse,lemma,natlog,ner,openie
ËøêË°åÈÄüÂ∫¶Âø´
[/0:0:0:0:0:0:0:1:64447] API call w/annotators tokenize,ssplit,pos,depparse,lemma,natlog,ner,openie
ÁîµÊ±†ÂÆπÈáèÂ§ß
[/0:0:0:0:0:0:0:1:64447] API call w/annotators tokenize,ssplit,pos,depparse,lemma,natlog,ner,openie
ÊúçÂä°ÊÄÅÂ∫¶ÂæàÂ•Ω
[/0:0:0:0:0:0:0:1:51261] API call w/annotators tokenize,ssplit,pos,depparse,lemma,natlog,ner,openie
‰ªäÂ§©Â§©Ê∞îÁúü‰∏çÈîô

But when I use Java Clinet to Call api ,it Throws Exception on Server.
Client Code(official example):

> Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLPClient pipeline = new StanfordCoreNLPClient(props, ""http://localhost"", 9000, 2)  ;
        String text = ""‰ªäÂ§©Â§©Ê∞îÁúüÂ•Ω""; // Add your text here!
        Annotation document = new Annotation(text);
        pipeline.annotate(document);

Client is holding on,because it recieve nothing probably.
Server ExceptionsÔºö
>  API call w/annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref
Exception in thread ""pool-1-thread-8"" java.lang.NoClassDefFoundError: edu/stanford/nlp/pipeline/CoreNLPProtos$Document
        at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.read(ProtobufAnnotationSerializer.java:193)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.getDocument(StanfordCoreNLPServer.java:325)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.access$500(StanfordCoreNLPServer.java:50)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:831)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Unknown Source)
        at sun.net.httpserver.AuthFilter.doFilter(Unknown Source)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Unknown Source)
        at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(Unknown Source)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Unknown Source)
        at sun.net.httpserver.ServerImpl$Exchange.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)

I am guessing I somehow miss Specify Parameters?

 When I first call API by using java Client, Exception traces on client:

> TOP ELEMENT: java.lang.ClassLoader.defineClass1(Native Method)
[Thread-1] ERROR edu.stanford.nlp.pipeline.StanfordCoreNLPClient - java.lang.NoClassDefFoundError: com/google/protobuf/GeneratedMessageV3$ExtendableMessageOrBuilder
  java.lang.ClassLoader.defineClass1(Native Method)
  java.lang.ClassLoader.defineClass(ClassLoader.java:763)
  java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
  java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
  java.net.URLClassLoader.access$100(URLClassLoader.java:73)
  java.net.URLClassLoader$1.run(URLClassLoader.java:368)
  java.net.URLClassLoader$1.run(URLClassLoader.java:362)
  java.security.AccessController.doPrivileged(Native Method)
  java.net.URLClassLoader.findClass(URLClassLoader.java:361)
  java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)
  java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  java.lang.ClassLoader.defineClass1(Native Method)
  java.lang.ClassLoader.defineClass(ClassLoader.java:763)
  java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
  java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
  java.net.URLClassLoader.access$100(URLClassLoader.java:73)
  java.net.URLClassLoader$1.run(URLClassLoader.java:368)
  java.net.URLClassLoader$1.run(URLClassLoader.java:362)
  java.security.AccessController.doPrivileged(Native Method)
  java.net.URLClassLoader.findClass(URLClassLoader.java:361)
  java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)
  java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProtoBuilder(ProtobufAnnotationSerializer.java:611)
  edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProto(ProtobufAnnotationSerializer.java:579)
  edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.write(ProtobufAnnotationSerializer.java:184)
  edu.stanford.nlp.pipeline.StanfordCoreNLPClient.lambda$null$482(StanfordCoreNLPClient.java:437)
  java.lang.Thread.run(Thread.java:748)
Caused by: class java.lang.ClassNotFoundException: com.google.protobuf.GeneratedMessageV3$ExtendableMessageOrBuilder
  java.net.URLClassLoader.findClass(URLClassLoader.java:381)
  java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)
  java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  java.lang.ClassLoader.defineClass1(Native Method)
  ...28 more
[Thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[Thread-1] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.

"
556,https://github.com/stanfordnlp/CoreNLP/issues/694,694,[],open,2018-05-04 03:16:57+00:00,,Dependency Parser Errors,"This is an open thread for dependency parse errors.

https://github.com/stanfordnlp/CoreNLP/issues/569"
557,https://github.com/stanfordnlp/CoreNLP/issues/695,695,[],closed,2018-05-05 16:24:31+00:00,,How to get the percentage of Data Processed,"How can I the percentage of Data Processed using the Library in Java?
"
558,https://github.com/stanfordnlp/CoreNLP/issues/696,696,[],closed,2018-05-06 14:55:15+00:00,,Meaning of '?' in annotator dependencies,"What is the meaning of '?' beside `regexner` and `sentiment` in annotator dependency list

![screenshot from 2018-05-06 10-51-53](https://user-images.githubusercontent.com/25760501/39674579-f0804c3c-511b-11e8-86ed-32ef50a6c01a.png)

Taken from the following website : 
https://stanfordnlp.github.io/CoreNLP/dependencies.html"
559,https://github.com/stanfordnlp/CoreNLP/issues/697,697,[],closed,2018-05-07 10:13:10+00:00,,How to get train data ,"Like  /u/nlp/data/chinese-dictionaries/plain/ne_wikipedia-utf8.txt
where I can get nlp data"
560,https://github.com/stanfordnlp/CoreNLP/issues/698,698,[],open,2018-05-08 03:36:16+00:00,,[sutime] no value extracted for input with token 'first',"c.f. [old closed issue](https://github.com/stanfordnlp/CoreNLP/issues/686)

Sorry to reopen the issue.

I checked the new demo url seems that the 'Named Entity Recognition' result is exactly the same as the out of date demos.

'first week of jan' gives 'XXXX-01 INTERSECT P1W-#1'
'second week of jan' gives '2018-W02'
I am quite new to TimeX and the output format like '2018-W02' is appreciated. If it is difficult to fix, is there any api which could convert ‚ÄòXXXX-01 INTERSECT P1W-#1‚Äô to '2018-W01'?"
561,https://github.com/stanfordnlp/CoreNLP/issues/699,699,[],closed,2018-05-09 18:24:41+00:00,,error: Program type already present,"i tried to use the library, and i got this error:

""Program type already present: edu.stanford.nlp.fsm.FastExactAutomatonMinimizer
Message{kind=ERROR, text=Program type already present: edu.stanford.nlp.fsm.FastExactAutomatonMinimizer, sources=[Unknown source file], tool name=Optional.of(D8)}""


![dd](https://user-images.githubusercontent.com/815634/39832060-e1c8f896-53ce-11e8-9528-28e7b0dacc9e.jpg)


please help me, i spent the whole day trying of making things work ü•á but it didnt worked out :)

this is the code from android studio:

package il.co.cafa.nlp;

import android.os.Bundle;
import android.support.v7.app.AppCompatActivity;

import java.util.List;
import java.util.Properties;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class MainActivity extends AppCompatActivity {

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);


        // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        // read some text in the text variable
        String text = ""What is the Weather in Bangalore right now?"";

        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);

        // run all Annotators on this text
        pipeline.annotate(document);

        List<CoreMap> sentences = document.get(CoreAnnotations.SentencesAnnotation.class);

        for (CoreMap sentence : sentences) {
            // traversing the words in the current sentence
            // a CoreLabel is a CoreMap with additional token-specific methods
            for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
                // this is the text of the token
                String word = token.get(CoreAnnotations.TextAnnotation.class);
                // this is the POS tag of the token
                String pos = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);
                // this is the NER label of the token
                String ne = token.get(CoreAnnotations.NamedEntityTagAnnotation.class);

                System.out.println(String.format(""Print: word: [%s] pos: [%s] ne: [%s]"", word, pos, ne));
            }
        }


    }
}



this is my gradle:

apply plugin: 'com.android.application'

android {
    compileSdkVersion 27
    defaultConfig {
        applicationId ""il.co.cafa.nlp""
        minSdkVersion 26
        targetSdkVersion 27
        versionCode 1
        versionName ""1.0""
        testInstrumentationRunner ""android.support.test.runner.AndroidJUnitRunner""
    }
    buildTypes {
        release {
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
        }
    }
}

dependencies {
    implementation fileTree(include: ['*.jar'], dir: 'libs')
    implementation 'com.android.support:appcompat-v7:27.1.1'
    implementation 'com.android.support.constraint:constraint-layout:1.1.0'
    testImplementation 'junit:junit:4.12'
    androidTestImplementation 'com.android.support.test:runner:1.0.2'
    androidTestImplementation 'com.android.support.test.espresso:espresso-core:3.0.2'
    implementation 'edu.stanford.nlp:stanford-parser:3.9.1'
    implementation 'edu.stanford.nlp:stanford-corenlp:3.9.1'
    implementation files('C:/Users/user/AndroidStudioProjects/NLP2/lib/slf4j-api.jar')
}


"
562,https://github.com/stanfordnlp/CoreNLP/issues/700,700,[],open,2018-05-10 08:16:44+00:00,,Correlating between parser input string and output tokens and their character positions.,"I tried 2 input strings. The only difference is in the leading space.

When I parse 'The boy' - I get for ""the"" the positions (0,3) as it supposed to do.
When I parse ' The boy' - I get for ""the"" the positions (0,3), which is wrong (I added a leading space).

It feels like the whitespaces are trimmed at the beginning of the text? Is this a bug, or intended? What's the right scheme to fully correlate between input string and output tokens?"
563,https://github.com/stanfordnlp/CoreNLP/issues/701,701,[],closed,2018-05-10 09:00:29+00:00,,Coreference and Dependency Parsing with JAR files w/o running a server,"Can I use 'corefs'and 'depparse' without starting a server but using JAR files like ""from nltk.tag.stanford import StanfordPOSTagger"" for POS tagging?"
564,https://github.com/stanfordnlp/CoreNLP/issues/702,702,[],closed,2018-05-11 08:06:37+00:00,,Illegal char <:> at index 44: \edu\stanford\nlp\models\kbp\tokensregex\per:siblings.rules,"i got this error in android studio while trying to run the CoreNLP, i imported the jars (maybe not?)

someone has a clue? i spent almost 2 days trying to import this library for using it :)
any kind of help is blessing 

[this is a screenshot of android studio](https://i.stack.imgur.com/jsNxH.jpg)"
565,https://github.com/stanfordnlp/CoreNLP/issues/704,704,[],closed,2018-05-15 10:15:48+00:00,,JSONDecodeError: Expecting value with CoreNLP 3.9.0 and 3.9.1 NOT with 3.7.0,"```
DEBUG:urllib3.connectionpool:http://localhost:9000 ""POST /?properties=%7B%27annotators%27%3A+%27coref%27%2C+%27pinelineLanguage%27%3A+%27en%27%7D HTTP/1.1"" 500 57
Traceback (most recent call last):

  File ""<ipython-input-9-1877aa015047>"", line 104, in <module>
    keywords = extract_phrases(df.iloc[:,1].tolist())

  File ""<ipython-input-9-1877aa015047>"", line 51, in extract_phrases
    result = json.loads(nlp.annotate(doc[i].replace('\n', ' ').replace('\r', ' '), properties= {'annotators': 'coref', 'pinelineLanguage': 'en'}))

  File ""D:\Anaconda3\envs\traceability\lib\json\__init__.py"", line 319, in loads
    return _default_decoder.decode(s)

  File ""D:\Anaconda3\envs\traceability\lib\json\decoder.py"", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())

  File ""D:\Anaconda3\envs\traceability\lib\json\decoder.py"", line 357, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None

JSONDecodeError: Expecting value
```"
566,https://github.com/stanfordnlp/CoreNLP/issues/705,705,[],closed,2018-05-15 20:51:11+00:00,,dependency parser assigned a governor to the root of a sentence,"When I tested the dependency parser for the sentence: Anyone else 's mom and dad do that thing where they call each other mom and dad , with the following command: 
 
      `wget --post-data ""Anyone else 's mom and dad do that thing where they call each other mom and dad"" 'localhost:9000/?properties={""annotators"":""tokenize,ssplit,pos,depparse"",""outputFormat"":""json""}' -O - `

it gave me a json like this: 
`{""sentences"":[{""index"":0,""basicDependencies"":[{""dep"":""ROOT"",""governor"":0,""governorGloss"":""ROOT"",""dependent"":1,""dependentGloss"":""Anyone""},{""dep"":""nmod:poss"",""governor"":4,""governorGloss"":""mom"",""dependent"":2,""dependentGloss"":""else""},{""dep"":""case"",""governor"":2,""governorGloss"":""else"",""dependent"":3,""dependentGloss"":""'s""},{""dep"":""nsubj"",""governor"":7,""governorGloss"":""do"",""dependent"":4,""dependentGloss"":""mom""},{""dep"":""cc"",""governor"":4,""governorGloss"":""mom"",""dependent"":5,""dependentGloss"":""and""},{""dep"":""conj"",""governor"":4,""governorGloss"":""mom"",""dependent"":6,""dependentGloss"":""dad""},{""dep"":""acl:relcl"",""governor"":1,""governorGloss"":""Anyone"",""dependent"":7,""dependentGloss"":""do""},{""dep"":""det"",""governor"":9,""governorGloss"":""thing"",""dependent"":8,""dependentGloss"":""that""},{""dep"":""dobj"",""governor"":7,""governorGloss"":""do"",""dependent"":9,""dependentGloss"":""thing""},{""dep"":""advmod"",""governor"":12,""governorGloss"":""call"",""dependent"":10,""dependentGloss"":""where""},{""dep"":""nsubj"",""governor"":12,""governorGloss"":""call"",""dependent"":11,""dependentGloss"":""they""},{""dep"":""acl:relcl"",""governor"":9,""governorGloss"":""thing"",""dependent"":12,""dependentGloss"":""call""},{""dep"":""det"",""governor"":15,""governorGloss"":""mom"",""dependent"":13,""dependentGloss"":""each""},{""dep"":""amod"",""governor"":15,""governorGloss"":""mom"",""dependent"":14,""dependentGloss"":""other""},{""dep"":""dobj"",""governor"":12,""governorGloss"":""call"",""dependent"":15,""dependentGloss"":""mom""},{""dep"":""cc"",""governor"":15,""governorGloss"":""mom"",""dependent"":16,""dependentGloss"":""and""},{""dep"":""conj"",""governor"":15,""governorGloss"":""mom"",""dependent"":17,""dependentGloss"":""dad""}],""enhancedDependencies"":[{""dep"":""ROOT"",""governor"":0,""governorGloss"":""ROOT"",""dependent"":1,""dependentGloss"":""Anyone""},{""dep"":""det"",""governor"":9,""governorGloss"":""thing"",""dependent"":1,""dependentGloss"":""Anyone""},{""dep"":""nmod:poss"",""governor"":4,""governorGloss"":""mom"",""dependent"":2,""dependentGloss"":""else""},{""dep"":""case"",""governor"":2,""governorGloss"":""else"",""dependent"":3,""dependentGloss"":""'s""},{""dep"":""nsubj"",""governor"":7,""governorGloss"":""do"",""dependent"":4,""dependentGloss"":""mom""},{""dep"":""cc"",""governor"":4,""governorGloss"":""mom"",""dependent"":5,""dependentGloss"":""and""},{""dep"":""conj:and"",""governor"":4,""governorGloss"":""mom"",""dependent"":6,""dependentGloss"":""dad""},{""dep"":""nsubj"",""governor"":7,""governorGloss"":""do"",""dependent"":6,""dependentGloss"":""dad""},{""dep"":""acl:relcl"",""governor"":1,""governorGloss"":""Anyone"",""dependent"":7,""dependentGloss"":""do""},{""dep"":""ref"",""governor"":1,""governorGloss"":""Anyone"",""dependent"":8,""dependentGloss"":""that""},{""dep"":""dobj"",""governor"":7,""governorGloss"":""do"",""dependent"":9,""dependentGloss"":""thing""},{""dep"":""advmod"",""governor"":12,""governorGloss"":""call"",""dependent"":10,""dependentGloss"":""where""},{""dep"":""nsubj"",""governor"":12,""governorGloss"":""call"",""dependent"":11,""dependentGloss"":""they""},{""dep"":""acl:relcl"",""governor"":9,""governorGloss"":""thing"",""dependent"":12,""dependentGloss"":""call""},{""dep"":""det"",""governor"":15,""governorGloss"":""mom"",""dependent"":13,""dependentGloss"":""each""},{""dep"":""amod"",""governor"":15,""governorGloss"":""mom"",""dependent"":14,""dependentGloss"":""other""},{""dep"":""dobj"",""governor"":12,""governorGloss"":""call"",""dependent"":15,""dependentGloss"":""mom""},{""dep"":""cc"",""governor"":15,""governorGloss"":""mom"",""dependent"":16,""dependentGloss"":""and""},{""dep"":""dobj"",""governor"":12,""governorGloss"":""call"",""dependent"":17,""dependentGloss"":""dad""},{""dep"":""conj:and"",""governor"":15,""governorGloss"":""mom"",""dependent"":17,""dependentGloss"":""dad""}],""enhancedPlusPlusDependencies"":[{""dep"":""ROOT"",""governor"":0,""governorGloss"":""ROOT"",""dependent"":1,""dependentGloss"":""Anyone""},{""dep"":""det"",""governor"":9,""governorGloss"":""thing"",""dependent"":1,""dependentGloss"":""Anyone""},{""dep"":""nmod:poss"",""governor"":4,""governorGloss"":""mom"",""dependent"":2,""dependentGloss"":""else""},{""dep"":""case"",""governor"":2,""governorGloss"":""else"",""dependent"":3,""dependentGloss"":""'s""},{""dep"":""nsubj"",""governor"":7,""governorGloss"":""do"",""dependent"":4,""dependentGloss"":""mom""},{""dep"":""cc"",""governor"":4,""governorGloss"":""mom"",""dependent"":5,""dependentGloss"":""and""},{""dep"":""conj:and"",""governor"":4,""governorGloss"":""mom"",""dependent"":6,""dependentGloss"":""dad""},{""dep"":""nsubj"",""governor"":7,""governorGloss"":""do"",""dependent"":6,""dependentGloss"":""dad""},{""dep"":""acl:relcl"",""governor"":1,""governorGloss"":""Anyone"",""dependent"":7,""dependentGloss"":""do""},{""dep"":""ref"",""governor"":1,""governorGloss"":""Anyone"",""dependent"":8,""dependentGloss"":""that""},{""dep"":""dobj"",""governor"":7,""governorGloss"":""do"",""dependent"":9,""dependentGloss"":""thing""},{""dep"":""advmod"",""governor"":12,""governorGloss"":""call"",""dependent"":10,""dependentGloss"":""where""},{""dep"":""nsubj"",""governor"":12,""governorGloss"":""call"",""dependent"":11,""dependentGloss"":""they""},{""dep"":""acl:relcl"",""governor"":9,""governorGloss"":""thing"",""dependent"":12,""dependentGloss"":""call""},{""dep"":""det"",""governor"":15,""governorGloss"":""mom"",""dependent"":13,""dependentGloss"":""each""},{""dep"":""amod"",""governor"":15,""governorGloss"":""mom"",""dependent"":14,""dependentGloss"":""other""},{""dep"":""dobj"",""governor"":12,""governorGloss"":""call"",""dependent"":15,""dependentGloss"":""mom""},{""dep"":""cc"",""governor"":15,""governorGloss"":""mom"",""dependent"":16,""dependentGloss"":""and""},{""dep"":""dobj"",""governor"":12,""governorGloss"":""call"",""dependent"":17,""dependentGloss"":""dad""},{""dep"":""conj:and"",""governor"":15,""governorGloss"":""mom"",""dependent"":17,""dependentGloss"":""dad""}],""tokens"":[{""index"":1,""word"":""Anyone"",""originalText"":""Anyone"",""characterOffsetBegin"":0,""characterOffsetEnd"":6,""pos"":""NN"",""before"":"""",""after"":"" ""},{""index"":2,""word"":""else"",""originalText"":""else"",""characterOffsetBegin"":7,""characterOffsetEnd"":11,""pos"":""RB"",""before"":"" "",""after"":"" ""},{""index"":3,""word"":""'s"",""originalText"":""'s"",""characterOffsetBegin"":12,""characterOffsetEnd"":14,""pos"":""POS"",""before"":"" "",""after"":"" ""},{""index"":4,""word"":""mom"",""originalText"":""mom"",""characterOffsetBegin"":15,""characterOffsetEnd"":18,""pos"":""NN"",""before"":"" "",""after"":"" ""},{""index"":5,""word"":""and"",""originalText"":""and"",""characterOffsetBegin"":19,""characterOffsetEnd"":22,""pos"":""CC"",""before"":"" "",""after"":"" ""},{""index"":6,""word"":""dad"",""originalText"":""dad"",""characterOffsetBegin"":23,""characterOffsetEnd"":26,""pos"":""NN"",""before"":"" "",""after"":"" ""},{""index"":7,""word"":""do"",""originalText"":""do"",""characterOffsetBegin"":27,""characterOffsetEnd"":29,""pos"":""VBP"",""before"":"" "",""after"":"" ""},{""index"":8,""word"":""that"",""originalText"":""that"",""characterOffsetBegin"":30,""characterOffsetEnd"":34,""pos"":""DT"",""before"":"" "",""after"":"" ""},{""index"":9,""word"":""thing"",""originalText"":""thing"",""characterOffsetBegin"":35,""characterOffsetEnd"":40,""pos"":""NN"",""before"":"" "",""after"":"" ""},{""index"":10,""word"":""where"",""originalText"":""where"",""characterOffsetBegin"":41,""characterOffsetEnd"":46,""pos"":""WRB"",""before"":"" "",""after"":"" ""},{""index"":11,""word"":""they"",""originalText"":""they"",""characterOffsetBegin"":47,""characterOffsetEnd"":51,""pos"":""PRP"",""before"":"" "",""after"":"" ""},{""index"":12,""word"":""call"",""originalText"":""call"",""characterOffsetBegin"":52,""characterOffsetEnd"":56,""pos"":""VBP"",""before"":"" "",""after"":"" ""},{""index"":13,""word"":""each"",""originalText"":""each"",""characterOffsetBegin"":57,""characterOffsetEnd"":61,""pos"":""DT"",""before"":"" "",""after"":"" ""},{""index"":14,""word"":""other"",""originalText"":""other"",""characterOffsetBegin"":62,""characterOffsetEnd"":67,""pos"":""JJ"",""before"":"" "",""after"":"" ""},{""index"":15,""word"":""mom"",""originalText"":""mom"",""characterOffsetBegin"":68,""characterOffsetEnd"":71,""pos"":""NN"",""before"":"" "",""after"":"" ""},{""index"":16,""word"":""and"",""originalText"":""and"",""characterOffsetBegin"":72,""characterOffsetEnd"":75,""pos"":""CC"",""before"":"" "",""after"":"" ""},{""index"":17,""word"":""dad"",""originalText"":""dad"",""characterOffsetBegin"":76,""characterOffsetEnd"":79,""pos"":""NN"",""before"":"" "",""after"":""""}]}]}`

The enhanced dependencies treated ""anyone"" as the root but also assigned it a governor ""thing"": 
`{""dep"":""ROOT"",""governor"":0,""governorGloss"":""ROOT"",""dependent"":1,""dependentGloss"":""Anyone""},{""dep"":""det"",""governor"":9,""governorGloss"":""thing"",""dependent"":1,""dependentGloss"":""Anyone""},`

The same happened to the enhanced plus-plus dependencies. I wonder if this is a valid output. "
567,https://github.com/stanfordnlp/CoreNLP/issues/706,706,[],closed,2018-05-16 16:46:55+00:00,,[URGENT] Parser Online Demo Not Working ,"Hi, we are using the online demos (http://nlp.stanford.edu:8080/parser/index.jsp) to look at the parse trees of sentences for a final project that is due tomorrow. But the website is down from around noon yesterday. There were some downs before, but never for this long period of time. Would you mind bring the online demos back up? Thank you so much!"
568,https://github.com/stanfordnlp/CoreNLP/issues/707,707,[],open,2018-05-25 11:18:03+00:00,,How to generate the custom relation extractor model in stanford core nlp.,"Hi All,
I am a beginner to the stanford core nlp.
Can someone help me in generating the custom relation extractor model (.corp file) to train the relation extractor.

I have gone through the following links: https://nlp.stanford.edu/software/relationExtractor.html and https://stanfordnlp.github.io/CoreNLP/relation.html and found some information regarding, how to train the custom relation extractor in stanford core nlp. "
569,https://github.com/stanfordnlp/CoreNLP/issues/708,708,[],open,2018-05-26 15:34:40+00:00,,How to get the sentence that NE positions (characterOffsets) refer to?,"**Situation**

1. I feed sentences into a pipeline of (ssplit, tokenise, pos, lemma, ner).
1. These sentences in their original form contain linebreaks, multiple empty spaces in a row and all the other mess you expect in raw data.
1. The detected named entities now contain the characterOffsetBegin and characterOffsetEnd.

**Problem**
The problem is that these offsets do not correspond to my original sentence but to some intermediate sentence after the tokenisation step where some whitespace has been removed (by Stanford CoreNLP).

**What I have tried**

- I have tried to reconstruct the sentence in various ways, i.a. 
  - by chaining all tokens back together again with their before and after characters in between. But I never get a full match and many NE positions do not match.

**Question**
How do I reconstruct the full sentence that the NE positions refer to?

I work in Python using pycorenlp."
570,https://github.com/stanfordnlp/CoreNLP/issues/710,710,[],closed,2018-05-30 07:27:24+00:00,,Make annotators to ignore some tokens,"Hi everyone,

I'm working with a CoreNLP pipeline and at a certain stage I'm selecting some tokens to be ignored by further components of the pipeline.
Consider this sentence:

> I have a big cat

I would like to mark the token ""big"" in order to be ignored by successive annotators.
Instead of completely removing the token I would prefer to simply mark it, since this would allow to reconstruct the original sentence in an easier way.

Do you have any idea about how to do that?

Thank you"
571,https://github.com/stanfordnlp/CoreNLP/issues/711,711,[],closed,2018-05-31 07:16:03+00:00,,corenlp ner prediction takes a long time,"sentence cost statistics
Total: 363630, min: 14ms, max: 60135ms, Average: 148.62655446470313ms, P90: 119ms, P95: 158ms, P99: 445ms, 0.1%above: 13807ms,0.01%above: 14364ms, 0.001%above: 14672ms

sentence length statistics
Total: 363630, min: 1, max: 605, Average: 15.197348953606689, P90: 46, P95: 48, P99: 94, 0.1%above: 205, 0.01%above: 253, 0.001%above: 510
when the sentence is long, prediction takes a long time,how to reduce it"
572,https://github.com/stanfordnlp/CoreNLP/issues/712,712,[],closed,2018-06-01 05:35:01+00:00,,"how does the basic authentication with ""username"" and ""password"" params when starting up the server work?","https://stanfordnlp.github.io/CoreNLP/corenlp-server.html
From the docs here, it is mentioned that you can flag username and password params when running the command to start the coreNLP server for basic authentication. However, there is not much else written on how to use it. Could someone guide me on how it works and how to provide the username and pw when making a request so that the authentication works?"
573,https://github.com/stanfordnlp/CoreNLP/issues/713,713,[],open,2018-06-04 06:40:10+00:00,,where I can find chinese dictionaries train data,"this:
String inputDicts = ""/u/nlp/data/chinese-dictionaries/plain/ne_wikipedia-utf8.txt,/u/nlp/data/chinese-dictionaries/plain/newsexplorer_entities_utf8.txt,/u/nlp/data/chinese-dictionaries/plain/Ch-name-list-utf8.txt,/u/nlp/data/chinese-dictionaries/plain/wikilex-20070908-zh-en.txt,/u/nlp/data/chinese-dictionaries/plain/adso-1.25-050405-monolingual-clean.
"
574,https://github.com/stanfordnlp/CoreNLP/issues/714,714,[],closed,2018-06-04 10:54:28+00:00,,CoreNLP - How to speed up NER?,"Hi guys,

I need tips to improve the speed of NER as I am constant getting timeouts even for small pieces of text, 
look at this log excerpts:

```
[pool-1-thread-2] INFO CoreNLP - [/172.17.0.1:41852] API call w/annotators tokenize,ssplit,pos,lemma,ner
Nicki Minaj has confirmed rumours that she's dating Eminem, after answering a fan's question on Instagram on Thursday. The Chun-Li rap beauty, 35, took to the image-sharing app to promote a new single she's featured on ‚Äî YG's Big Bank, alongside 2 Chainz and Big Sean ‚Äî when one of her followers boldly enquired: 'You dating Eminem???'Trinidadian-born Nicki, who was most recently romantically linked to 44-year-old rap veteran Nas, simply wrote back: 'Yes.'Making sweet music together? Nicki Minaj, left, confirmed that she's dating fellow rapper Eminem, right, during an exchange with a fan on Instagram on Thursday Interestingly, she mentions Eminem during her verse on Big Bank, rapping: 'Uh oh/Back again/Back to back Maybach, stack the M's/Told 'em I met Slim Shady, bag the M/Once he go black, he'll be back again.'MailOnline has contacted representatives for Nicki Minaj and Eminem for comment
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [3.9 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.6 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [2.1 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
[pool-1-thread-1] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator ner.fine.regexner: Read 580641 unique entries out of 581790 from edu/stanford/nlp/models/kbp/regexner_caseless.tab, 0 TokensRegex patterns.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator ner.fine.regexner: Read 4857 unique entries out of 4868 from edu/stanford/nlp/models/kbp/regexner_cased.tab, 0 TokensRegex patterns.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator ner.fine.regexner: Read 585498 unique entries from 2 files
java.util.concurrent.TimeoutException
```

Here is the run command:

```
java -cp ""*"" \
         -mx4g edu.stanford.nlp.pipeline.StanfordCoreNLPServer \
         -port $PORT -timeout 15000 -annotators ""tokenize,ssplit,pos,lemma"" \
         -preload ""tokenize,ssplit,pos,lemma"" \
         -parse.model ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"" \
         -ner.model edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz \
         -ner.useSUTime false \
         -ner.applyNumericClassifiers \
         -threads 2
```

Update:

Just to add some more context, I am particularly interested in just People, Location and Organisation and I can increase memory beyond 4gb if required
"
575,https://github.com/stanfordnlp/CoreNLP/issues/715,715,[],closed,2018-06-05 15:02:22+00:00,,Pos tagger model from 3.9.1 models jar file not recognized,"When using maven, either from the command line or from within eclipse, I get the

> Caused by: java.io.IOException: Unable to open ""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"" as class path, filename or URL

error.  In my pom.xml file, I have

  	<dependency>
  		<groupId>edu.stanford.nlp</groupId>
  		<artifactId>stanford-corenlp</artifactId>
  		<version>3.9.1</version>
  	</dependency>
    <dependency>
        <groupId>edu.stanford.nlp</groupId>
        <artifactId>stanford-corenlp</artifactId>
        <version>3.9.1</version>
        <classifier>models</classifier>
    </dependency>

When viewing through the Maven Dependencies in eclipse, when I expand the model, I see that all the other models have the package structure edu.stanford.nlp.models.xxx and the appropriate models are underneath them.  However, there is a pos-tagger folder underneath the package edu.stanford.nlp.models and then another folder underneath that for english-left3words.  I don't know if this important or not, but I do know that the classic pipeline is not picking out the default pos model even with ""models"" as a maven dependency."
576,https://github.com/stanfordnlp/CoreNLP/issues/716,716,[],closed,2018-06-07 11:12:34+00:00,,Tokens with empty text classified as NUMBERS,"Hi,

I am working with the NLP pipeline.
I noticed that if I set the text of a token to an empty string, then it is treated in a very strange way by successive annotators.

I'm working on English text.
I am basically doing the following:

```
token.setWord("""");
token.setValue("""");
```

Then the POS annotator sets the POS to CD and the NER annotator sets it as a NUMBER.

Do you have any idea why ?

Best regards"
577,https://github.com/stanfordnlp/CoreNLP/issues/717,717,[],closed,2018-06-07 18:15:25+00:00,,How can I change the output of dependency parser of stanford corenlp into conllU format?,"I want to train my Classifier with CONLLU format, however, Stanford corenlp dependency parser produces a tree structure. I need to have dependency parser out in the following format:
**The	DET	DT	det
Mayan	PROPN	NNP	compound
Empire	PROPN	NNP	nsubj
grew	VERB	VBD	ROOT
from	ADP	IN	prep
about	ADP	IN	prep
the	DET	DT	det
year	NOUN	NN	pobj
400	NUM	CD	nummod
to	ADP	IN	quantmod
900	NUM	CD	advcl
.	PUNCT	.	punct**
I need stanford corenlp api that changes the result of dependency parser. "
578,https://github.com/stanfordnlp/CoreNLP/issues/718,718,[],open,2018-06-08 03:53:04+00:00,,Incorrectly tagged label as part of money,"I used core NLP to process ""I want to buy a $1000 television"".

In the labels in `CoreAnnotations.TokensAnnotation.class`, the label representing ""television"" was identified as ""NUMBER"" with `CoreAnnotations.NormalizedNamedEntityTagAnnotation.class` value of ""$1000"" and the `CoreAnnotations.EntityMentionIndexAnnotation.class` value was set to 0 (which is the value of the label for ""$1000"").

I am using version 3.9.1 and created the `StanfordCoreNLP` instance as follows:
```
Properties props = new Properties();
props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner"");
props.put(""sutime.language"", ""british"");
StanfordCoreNLP nlp = new StanfordCoreNLP(props);
```

I believe it is a bug.

I tried ""I want to buy a $1000 TV"" and it correctly labelled ""TV"" not being part of ""$1000"".

I also found similar misbehaviour in ""I want to buy a $50 green jacket"" which core NLP claims ""green"" is ""MONEY"" with `CoreAnnotations.NormalizedNamedEntityTagAnnotation.class` value of ""$50"" and entity mention index also set to the entity mention index of ""$50"".

If it is not a bug but my mistake, please let me know.  Thanks."
579,https://github.com/stanfordnlp/CoreNLP/issues/719,719,"[{'id': 45387507, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNw==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/cantreproduce', 'name': 'cantreproduce', 'color': 'dddddd', 'default': False, 'description': None}]",closed,2018-06-08 11:44:54+00:00,,StackOverflowError in SemanticGraph object,"I am running the neural dependency parser from CoreNLP 3.8.0 large-scale by using Apache Spark. The CoreNLP pipeline I am using for parsing is this:

```
public static StanfordCoreNLP StanfordDepNNParser(){
    Properties props = new Properties();

    props.put(""language"", ""english"");
    props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, depparse"");
    props.put(""depparse.model"", ""edu/stanford/nlp/models/parser/nndep/english_SD.gz"");
    props.put(""parse.originalDependencies"", true);

    return new StanfordCoreNLP(props);
}
```

However, after a while, I get the following error:

```
18/06/08 08:16:40 WARN scheduler.TaskSetManager: Lost task 136.0 in stage 0.0 (TID 140, server.com): java.lang.StackOverflowError
    at java.util.HashMap.putVal(HashMap.java:630)
    at java.util.HashMap.put(HashMap.java:611)
    at java.util.HashSet.add(HashSet.java:219)
    at edu.stanford.nlp.semgraph.SemanticGraph.toCompactStringHelper(SemanticGraph.java:1526)
    at edu.stanford.nlp.semgraph.SemanticGraph.toCompactStringHelper(SemanticGraph.java:1541)
    at edu.stanford.nlp.semgraph.SemanticGraph.toCompactStringHelper(SemanticGraph.java:1541)
    at edu.stanford.nlp.semgraph.SemanticGraph.toCompactStringHelper(SemanticGraph.java:1541)
    at edu.stanford.nlp.semgraph.SemanticGraph.toCompactStringHelper(SemanticGraph.java:1541)
    at edu.stanford.nlp.semgraph.SemanticGraph.toCompactStringHelper(SemanticGraph.java:1541)
    at edu.stanford.nlp.semgraph.SemanticGraph.toCompactStringHelper(SemanticGraph.java:1541)
    at edu.stanford.nlp.semgraph.SemanticGraph.toCompactStringHelper(SemanticGraph.java:1541)
    at edu.stanford.nlp.semgraph.SemanticGraph.toCompactStringHelper(SemanticGraph.java:1541)
    at edu.stanford.nlp.semgraph.SemanticGraph.toCompactStringHelper(SemanticGraph.java:1541)
    at edu.stanford.nlp.semgraph.SemanticGraph.toCompactStringHelper(SemanticGraph.java:1541)
    at edu.stanford.nlp.semgraph.SemanticGraph.toCompactStringHelper(SemanticGraph.java:1541)
    . . . .
```

Unfortunately, I cannot locate the exact sentence (or document) that triggers this exception. Is it possible to make some sort of check upfront that would just skip this sentence?"
580,https://github.com/stanfordnlp/CoreNLP/issues/720,720,[],closed,2018-06-08 22:38:47+00:00,,Documentation: QuoteAttributionAnnotator not listed with other annotators,"I was looking for a tool to find quote attributions in text.  I scoured https://stanfordnlp.github.io/CoreNLP/ and neither the sidebar nor the [annotators list](https://stanfordnlp.github.io/CoreNLP/annotators.html) mentioned the QuoteAttributionAnnotator.  I eventually stumbled upon https://stanfordnlp.github.io/CoreNLP/quoteattribution.html thanks to Google search.

Is this documentation omission intentional, or simply an oversight?"
581,https://github.com/stanfordnlp/CoreNLP/issues/721,721,[],closed,2018-06-13 02:22:24+00:00,,java.io.IOException: Unable to open <path>,"I am trying to lemmatize text but I keep running into this issue even though I am not using pos-tagging:

`Caused by: `java.io.IOException: Unable to open` ""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger"" as class path, filename or URL`

I checked to see if I downloaded the model jar, which I did. I also downloaded the most recent package as well. Also, for some reason Intellij isn't importing the model jar. IntelliJ recognizes all the other jars except for the model jar. I am not sure how to fix this
"
582,https://github.com/stanfordnlp/CoreNLP/issues/722,722,[],closed,2018-06-13 05:58:45+00:00,,-filelist has inconsistent naming,"From the command line, the parameters are outputFormat, outputDirectory, replaceExtension and so on, but fileList is not recognized (while filelist is).

This is inconsistent and mildly confusing, and it would be a minor improvement to accept both versions (so that you don't break existing scripts)"
583,https://github.com/stanfordnlp/CoreNLP/issues/723,723,[],open,2018-06-13 20:21:47+00:00,,Head finder does not recognize head correctly,"With texts such as ""Boys & Girls"", CoreNLP detects Boys as head but not Girls. Shouldn't they be treated similarly? "
584,https://github.com/stanfordnlp/CoreNLP/issues/724,724,[],open,2018-06-13 20:28:51+00:00,,POS tagger assigns wrong tags to Currency symbols,"I am using CoreNLP POS tagger to identify POS tags of tokens, but it gives me wrong tags for currency signs. For example, for ‚Ç¨ in this text, ""Items under 50‚Ç¨"", it says POS = CD (cardinal number), or in this text, ""‚Ç¨100 and $300"", for ‚Ç¨ it says POS = NN. "
585,https://github.com/stanfordnlp/CoreNLP/issues/725,725,[],open,2018-06-17 18:00:11+00:00,,throwing NPE sutime,"I'm using 3.9.1 version and it is throwing me an error after this line
`Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.`

Here is stacktrace
```
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.solr.core.SolrResourceLoader.newInstance(SolrResourceLoader.java:638)
	... 22 more
Caused by: java.lang.RuntimeException: Error initializing binder 1
	at edu.stanford.nlp.time.Options.<init>(Options.java:121)
	at edu.stanford.nlp.time.TimeExpressionExtractorImpl.init(TimeExpressionExtractorImpl.java:44)
	at edu.stanford.nlp.time.TimeExpressionExtractorImpl.<init>(TimeExpressionExtractorImpl.java:39)
	at edu.stanford.nlp.time.TimeAnnotator.<init>(TimeAnnotator.java:189)
	at edu.stanford.nlp.time.TimeAnnotator.<init>(TimeAnnotator.java:185)
	at com.prasnottar.solr.StanfordFactory.<init>(StanfordFactory.java:29)
	... 27 more
Caused by: java.lang.NullPointerException
	at de.jollyday.configuration.impl.DefaultConfigurationProvider.getProperties(DefaultConfigurationProvider.java:59)
	at de.jollyday.configuration.ConfigurationProviderManager.addInternalConfigurationProviderProperies(ConfigurationProviderManager.java:63)
	at de.jollyday.configuration.ConfigurationProviderManager.mergeConfigurationProperties(ConfigurationProviderManager.java:57)
	at de.jollyday.HolidayManager.createManager(HolidayManager.java:163)
	at de.jollyday.HolidayManager.getInstance(HolidayManager.java:148)
	at edu.stanford.nlp.time.JollyDayHolidays.init(JollyDayHolidays.java:57)
	at edu.stanford.nlp.time.Options.<init>(Options.java:119)
	... 32 more

I don't know what seem to be the problem, when i do normal test to run following line of code it is working as expected.

`       Properties properties = new Properties();
        properties.setProperty(""annotators"",""tokenize, cleanxml,ssplit,pos,lemma,ner"");
        properties.setProperty(""ner.useSUTime"", ""false"");
        this.pipeline = new StanfordCoreNLP(properties);
        this.pipeline.addAnnotator(new TimeAnnotator(""sutime"", properties));`

When I used along with Apache Solr 7.3.1 app, It is giving me above stacktrace. I have loaded all the required jar."
586,https://github.com/stanfordnlp/CoreNLP/issues/728,728,[],open,2018-06-20 12:06:53+00:00,,Sutime identifies period incorrectly,"Question: June 2018 gross

Output (from http://nlp.stanford.edu:8080/sutime/process) :

june 2018 gross |   2162-06 |	<TIMEX3 tid=""t1"" type=""DATE"" value=""2162-06"">june 2018 gross</TIMEX3>
"
587,https://github.com/stanfordnlp/CoreNLP/issues/729,729,[],closed,2018-06-20 14:54:26+00:00,,Typo on website,"The header for your website has a typo in the header:

\<meta name=""twitter:description"" content=""High-performance human language analysis tools. Widely used, **aavailable** open source; written in Java.""\>"
588,https://github.com/stanfordnlp/CoreNLP/issues/730,730,[],closed,2018-06-21 13:08:53+00:00,,Issue with ProtobufAnnotationSerializer,"Hi, 

I tried to use `ProtobufAnnotationSerializer` to serialize an `Annotation` object  as follow:
```
String text = ""Stanford University is located in California. It is a great university, founded in 1891."";
 Annotation document = new Annotation(text);
 Properties props = new Properties();
 props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse,depparse"");    
 StanfordCoreNLP pip = new StanfordCoreNLP(props);
 pip.annotate(document);
 
ProtobufAnnotationSerializer serializer = new ProtobufAnnotationSerializer();
FileOutputStream fileOut = new FileOutputStream(""path/to/anno.ser"");
ObjectOutputStream out = new ObjectOutputStream(fileOut);
serializer.write(document, out);

```
This bug came out:
```
Exception in thread ""main"" java.lang.VerifyError: Bad type on operand stack
Exception Details:
  Location:
    com/google/protobuf/GeneratedMessageV3$ExtendableMessage.getExtension(Lcom/google/protobuf/Extension;I)Ljava/lang/Object; @3: invokevirtual
  Reason:
    Type 'com/google/protobuf/Extension' (current frame, stack[1]) is not assignable to 'com/google/protobuf/ExtensionLite'
  Current Frame:
    bci: @3
    flags: { }
    locals: { 'com/google/protobuf/GeneratedMessageV3$ExtendableMessage', 'com/google/protobuf/Extension', integer }
    stack: { 'com/google/protobuf/GeneratedMessageV3$ExtendableMessage', 'com/google/protobuf/Extension', integer }
  Bytecode:
    0x0000000: 2a2b 1cb6 0024 b0                      

	at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProtoBuilder(ProtobufAnnotationSerializer.java:611)
	at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProto(ProtobufAnnotationSerializer.java:579)
	at edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.write(ProtobufAnnotationSerializer.java:184)
at xxxxxxxxxxxxxx.main(xxxxxxx.java:303)  \\ line: serializer.write(document, out);

```
I think that there is an inconsistency between CoreNLP ProtobufAnnotationSerializer and protobuf package. I am using version 3.9.1 downloaded directly from [CoreNLP home page](https://stanfordnlp.github.io/CoreNLP/download.html), and I even tried some alternative solutions but none of them work. I tried:

- versions 3.9 3.8
- download the package and its dependencies directly from maven
- download and build (with ant) the source code on github.

The error occurs with other languages (i tested with French) and even when calling the server."
589,https://github.com/stanfordnlp/CoreNLP/issues/731,731,[],open,2018-06-24 11:29:18+00:00,,Training Format,"Hi,

I want to create NER sample using my own data set. I could not see the `entitymentions` sample data format anywhere. Could you please advise the format of `entitymentions` to train?

So far, I could create data model for NER using the following details.
[https://stanfordnlp.github.io/CoreNLP/ner.html](url)

Thanks in advance."
590,https://github.com/stanfordnlp/CoreNLP/issues/732,732,[],open,2018-06-24 22:41:44+00:00,,java.lang.NullPointerException when adding certain dcoref.sievePasses flags,"`-dcoref.sievePasses MarkRole,DiscourseMatch,ExactStringMatch,RelaxedExactStringMatch,PreciseConstructs,StrictHeadMatch1,StrictHeadMatch2,StrictHeadMatch3,StrictHeadMatch4,RelaxedHeadMatch,PronounMatch` plus certain other flags results in 

```
done.
Adding true-case annotation...
Exception in thread ""main"" java.lang.NullPointerException
	at edu.stanford.nlp.dcoref.ScorerMUC.calculateRecall(ScorerMUC.java:23)
	at edu.stanford.nlp.dcoref.CorefScorer.calculateScore(CorefScorer.java:58)
	at edu.stanford.nlp.dcoref.SieveCoreferenceSystem.coreference(SieveCoreferenceSystem.java:987)
	at edu.stanford.nlp.dcoref.SieveCoreferenceSystem.corefReturnHybridOutput(SieveCoreferenceSystem.java:830)
	at edu.stanford.nlp.pipeline.DeterministicCorefAnnotator.annotate(DeterministicCorefAnnotator.java:160)
	at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:660)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:670)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1265)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1010)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1382)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1427)
```

version 3.9.1"
591,https://github.com/stanfordnlp/CoreNLP/issues/733,733,[],open,2018-06-24 22:45:35+00:00,,Allow multiple output-flags ,"Would it be less waste to allow for multiple output formats in one command-line string and re-use internal data before parse-outputting:

- space separated (if surrounded by quotes)
- comma-separated 

 e.g. 
-outputFormat 'xml,tsv,json'"
592,https://github.com/stanfordnlp/CoreNLP/issues/734,734,[],closed,2018-06-25 07:52:35+00:00,,"What does it mean - ""Updated for compatibility""","Hi!

I am looking for explanation of the release notes of Stanford NLP

If i come to this page (https://github.com/stanfordnlp/CoreNLP/blob/master/doc/lexparser/README.txt) and read about chenges, so in version 3.8.0 i found - ""Updated for compatibility"".

But if i come here (https://stanfordnlp.github.io/CoreNLP/history.html), i see ""Web service annotator, discussion forum handling, new French and Spanish UD POS models, emoji support"", and i understand what does it mean. But i dont see dependency of this text with the ""Updated for compatibility"". 

Maybe, i am not good in English enoth, or you can add more information to explain the frase ""Updated for compatibility"".  

Please answer me as fast? as you can, this information is very important for me!"
593,https://github.com/stanfordnlp/CoreNLP/issues/735,735,[],open,2018-06-25 20:41:55+00:00,,crossValidator,Does the crossValidator work?  I don't see any tests.  If it does could you provide and example.  Thanks!
594,https://github.com/stanfordnlp/CoreNLP/issues/737,737,[],closed,2018-07-01 08:45:28+00:00,,"getting Exception in thread ""main"" java.io.IOException: Unable to open ""-file"" as class path, filename or URL","when i run the next command : 
java  -cp ""*"" edu.stanford.nlp.scenegraph.RuleBasedParser -file input.txt

i get the next error 

Exception in thread ""main"" java.io.IOException: Unable to open ""-file"" as class path, filename or URL
	at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:480)
	at edu.stanford.nlp.io.IOUtils.readerFromString(IOUtils.java:617)
	at edu.stanford.nlp.scenegraph.RuleBasedParser.main(RuleBasedParser.java:266)"
595,https://github.com/stanfordnlp/CoreNLP/issues/738,738,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",open,2018-07-05 03:52:29+00:00,,There is no way to ignore xml tags when tagging,"Dear all,

I'm doing some POS tagging on lots of files containing XML tags. So far everything gets tagged, even the XML tags, which is not what I want. It seems that there is no option to tell the tagger to ignore those XML tags. Is there a way to implement this in the future? 
Thank you."
596,https://github.com/stanfordnlp/CoreNLP/issues/739,739,[],open,2018-07-05 16:13:47+00:00,,Correct path (remove gazetteers) for DEFAULT_KBP_TOKENSREGEX_DIR in DefaultPaths class,"When building the jar from GitHub head following the instructions in https://stanfordnlp.github.io/CoreNLP/download.html, the resulting code fails to load the NER models because of the extra ""gazetteers"" in 

public static final String DEFAULT_KBP_REGEXNER_CASELESS = ""edu/stanford/nlp/models/kbp/english/gazetteers/regexner_caseless.tab"";

Steps:

1. I built the javanlp-core.jar from HEAD

ant jar

2. I downloaded all model files and added them to the CLASSPATH, but it still would not help

export CLASSPATH=""$CLASSPATH:/pathto/corenlp/javanlp-core.jar:/pathto/corenlp/stanford-corenlp-3.9.1-models.jar:/pathto/corenlp/stanford-corenlp-3.9.1-models-english.jar:/pathto/corenlp/stanford-corenlp-3.9.1-models-english-kbp.jar"";

3. Only when I downloaded the stanford-corenlp-3.9.1.jar and replaced javanlp-core.jar with it, was I able to successfully run

java -mx3g edu.stanford.nlp.pipeline.StanfordCoreNLP -outputFormat json -file input.txt

(actually, I had to increase memory to 5g from 3g - 3g is not enough; you might want to change these instructions as well)
"
597,https://github.com/stanfordnlp/CoreNLP/issues/740,740,[],closed,2018-07-06 02:36:40+00:00,,About chinese CoreNLP modles ,"I use chinese CoreNLP modles to make a NER of adress. But i can't understand what do GPE and facility mean????
When i test some Chinese adresses,i got the result below:
![image](https://user-images.githubusercontent.com/40876756/42356610-0d4c1ce6-8106-11e8-88e1-9dedfec0e98d.png)
Can someone help me with this issue? THANK YOU VERY MUCH!

je suis entrain de faire une reconnaissance de l'adress NER .je voulais savoir les sens des labels GPE et facility dans le module chinois CoreNlp. merci beaucoup!

ÂêåÂøó‰ª¨ÔºåÊúâÊ≤°Êúâ‰∫∫Áî®‰∏≠ÊñáÁöÑCORENLPÊ®°ÂûãÂÅöNERÔºüÊàëÂèëÁé∞ÂÆÉËØÜÂà´Âá∫Êù•ÁöÑÂú∞ÂùÄÂæàÂ∞ëÊâìÊ†áÊòØLOCATIONÁöÑÔºåÂ§ßÈÉ®ÂàÜÊòØGPEÂíåFACILITYÔºåÊúâÊ≤°Êúâ‰∫∫Áü•ÈÅìËøô‰∏§‰∏™Ê†áËØÜÊòØ‰ªÄ‰πàÔºüÔºüÔºüË∞¢Ë∞¢‰∫ÜÔºÅ"
598,https://github.com/stanfordnlp/CoreNLP/issues/741,741,[],closed,2018-07-06 20:10:43+00:00,,Could not load main class Error,"Command:
java -Xmx5g -cp C:\Users\harsh\CoreNLP\stanford-corenlp-full-2018-02-27\stanford-corenlp-3.9.1.jar:stanford-english-corenlp-models-current.jar:* edu.stanford.nlp.pipeline.StanfordCoreNLP -props C:\Users\harsh\CoreNLP\src\edu\stanford\nlp\coref\properties\deterministic-english.properties -file C:\Users\harsh\PycharmProjects\untitled3\to_openIE.txt


Get the following error:
Error: Could not find or load main class edu.stanford.nlp.pipeline.StanfordCoreNLP

Any help?"
599,https://github.com/stanfordnlp/CoreNLP/issues/742,742,[],closed,2018-07-07 17:40:18+00:00,,Steps to generate a new sentiment model file,"Can someone please confirm if the following is the correct sequence of steps to generate a new sentiment model (eg sentiment.ser.gz)?
Specifically questions on.

- How do you break each sentence into phrases in step 4?
- Why do we need SOStr.txt in step 6?  I know it's an input to ReadSentimentDataset.java for generating the three tree files train.txt, dev.txt, and test.txt.  But is that the only reason why this file is required?
- How do we generate STree.txt in step 7?  Seriously this is a mystery.
- Is it true that both methods 1 & 2 under step 8 can be used to generate the tree files?
- For step 8 method 1, how do we know we need to use 'good', 'good day', and 'a good day'?  What about 'is not'?  What do the numeric labels mean?

## Preparation

1. Find a bag of comments such as original_rt_snippets.txt which contains 10,605 comments.
2. For each comment in the original_rt_snippets.txt file that has multiple sentences, break up the comment to have each of its sentence on its own line.  Applying this process on the 10,605 comments in original_rt_snippets.txt creates datasetSentences.txt which has 11,855 sentences.
    This should be easy by using the ssplit annotator.  Here is an example of how to do this interactively.
    ```text
    C:\Users\cheun\.m2\repository\edu\stanford\nlp\stanford-corenlp\3.9.1>java -cp ""*;C:\Users\cheun\.m2\repository\org\ejml\*"" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit
    Adding annotator tokenize
    No tokenizer type provided. Defaulting to PTBTokenizer.
    Adding annotator ssplit

    Entering interactive shell. Type q RETURN or EOF to quit.
    NLP> are you sure?  really?

    Sentence #1 (4 tokens):
    are you sure?

    Tokens:
    [Text=are CharacterOffsetBegin=0 CharacterOffsetEnd=3]
    [Text=you CharacterOffsetBegin=4 CharacterOffsetEnd=7]
    [Text=sure CharacterOffsetBegin=8 CharacterOffsetEnd=12]
    [Text=? CharacterOffsetBegin=12 CharacterOffsetEnd=13]

    Sentence #2 (2 tokens):
    really?

    Tokens:
    [Text=really CharacterOffsetBegin=15 CharacterOffsetEnd=21]
    [Text=? CharacterOffsetBegin=21 CharacterOffsetEnd=22]
    NLP>
    ```
3. Each line in datasetSentences.txt is assigned into one of three different lists, which are 1 - train, 2 - test, and 3 - dev. datasetSplit.txt is a map datasetSentences.txt to a number representing which of these three list it is assigned to.
4. Each sentence is then broken down into phrases.  Doing this on the 11,855 sentences in datasetSentences.txt we have 239,232 phrases, each represented by a line in dictionary.txt.
5. Have someone manually assign a sentiment to each of the 239,232 phrases to create sentiment_labels.txt.
6. Run the Stanford Tokenizer on datasetSentences.txt to generate SOStr.txt.  Not sure why we need this file as it appears to be simply delimiting the words in a sentence with the '|' character.
    We can see the example in step 2 above how the tokens are generated.
7. Create a STree.txt.  Not sure how this is done and what the numbers mean.
8. Create the train.txt, test.txt, and dev.txt tree files based on sentences categorized during step 3 above.  These are two ways of doing this.

    ##### Method 1 - BuildBinarizedDataset.java class

    Example of how to do this is as follow.

    - Create an input file containing the original sentences from datasetSentences.txt.
    - For each of these lines we will list out all its parts.
    - Assign all these with a numeric label.  Have absolutely no idea what the numeric label represents.

    So we have something like this as an input file.
    ```text
    1 Today is not a good day.
    3 good
    3 good day
    3 a good day
    ```

    Running the input file with BuildBinarizedDataset creates a tree for the sentence.  Example of how to do this is as follow.
    ```text
    C:\Users\cheun\.m2\repository\edu\stanford\nlp\stanford-corenlp\3.9.1>java -cp ""stanford-corenlp-3.9.1.jar;stanford-corenlp-3.9.1-models.jar;C:\Users\cheun\.m2\repository\org\ejml\*"" -Xmx5g edu.stanford.nlp.sentiment.BuildBinarizedDataset -input treebanksource.txt
    Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.7 sec].
    (3 (3 Today) (3 (3 (5 (3 is) (3 not)) (30 (3 a) (3 (1 good) (3 day)))) (3 .)))
    ```

    ##### Method 2 - ReadSentimentDataset.java

    Using dictionary.txt, sentiment_labels.txt, SOStr.txt, STree.txt, and datasetSplit.txt, ReadSentimentDataset.java can generate the three tree files train.txt, dev.txt, and test.txt.

    ```text
    java edu.stanford.nlp.sentiment.ReadSentimentDataset -dictionary stanfordSentimentTreebank/dictionary.txt -sentiment stanfordSentimentTreebank/sentiment_labels.txt -tokens stanfordSentimentTreebank/SOStr.txt -parse stanfordSentimentTreebank/STree.txt -split stanfordSentimentTreebank/datasetSplit.txt -train train.txt -dev dev.txt -test test.txt
    ```



## Generate Model
After having prepping all the data above we can now generate a model with the following, creating the new model.ser.gz file.

```text
java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath dev.txt -train -model model.ser.gz
```"
600,https://github.com/stanfordnlp/CoreNLP/issues/743,743,[],closed,2018-07-12 08:07:18+00:00,,Duplicate dependency edges in Enhanced++,"When using plain text output format, the `depparse` annotator (`nndep/english_UD.gz`) seem to generate some duplicate dependency edges, like below:

```
Dependency Parse (enhanced plus plus dependencies):
root(ROOT-0, published-3)
nsubjpass(published-3, UNIQLOCK-1)
nsubjpass(published-3''', UNIQLOCK-1)
auxpass(published-3, was-2)
conj:and(published-3, published-3''')
...
```

As you can notice, some numbers are followed by a triple quotation mark, like line 4 `nsubjpass(published-3''', UNIQLOCK-1)`, and it is just a duplicate to line 3. Some of the triple-quoted edges are not duplicates, like the last row, but this format looks weird to me.

What does the triple quotation mark mean? Why are there duplicates?

----

### Version

CoreNLP: `stanford-corenlp-full-2018-02-27`
Java: jdk 10

### Steps to reproduce

1. Start CoreNLP in interactive mode:
```
java --add-modules java.se.ee -mx6g -cp ""./*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -port 9000 -timeout 15000 -preload tokenize,ssplit,pos,lemma,ner,parse,depparse
```

2. Paste the following input:

```
UNIQLOCK was published about a year before , and among geeks and people who are fashion concious , it is known as very fashionable and techinical screensaver .
```

3. You should see the following output:

```
Sentence #1 (28 tokens):
UNIQLOCK was published about a year before , and among geeks and people who are fashion concious , it is known as very fashionable and techinical screensaver .

Tokens:
[Text=UNIQLOCK CharacterOffsetBegin=0 CharacterOffsetEnd=8 PartOfSpeech=NNP Lemma=UNIQLOCK NamedEntityTag=LOCATION]
[Text=was CharacterOffsetBegin=9 CharacterOffsetEnd=12 PartOfSpeech=VBD Lemma=be NamedEntityTag=O]
[Text=published CharacterOffsetBegin=13 CharacterOffsetEnd=22 PartOfSpeech=VBN Lemma=publish NamedEntityTag=O]
[Text=about CharacterOffsetBegin=23 CharacterOffsetEnd=28 PartOfSpeech=IN Lemma=about NamedEntityTag=DURATION NormalizedNamedEntityTag=P1Y Timex=<TIMEX3 tid=""t1"" type=""DURATION"" value=""P1Y"">about a year</TIMEX3>]
[Text=a CharacterOffsetBegin=29 CharacterOffsetEnd=30 PartOfSpeech=DT Lemma=a NamedEntityTag=DURATION NormalizedNamedEntityTag=P1Y Timex=<TIMEX3 tid=""t1"" type=""DURATION"" value=""P1Y"">about a year</TIMEX3>]
[Text=year CharacterOffsetBegin=31 CharacterOffsetEnd=35 PartOfSpeech=NN Lemma=year NamedEntityTag=DURATION NormalizedNamedEntityTag=P1Y Timex=<TIMEX3 tid=""t1"" type=""DURATION"" value=""P1Y"">about a year</TIMEX3>]
[Text=before CharacterOffsetBegin=36 CharacterOffsetEnd=42 PartOfSpeech=IN Lemma=before NamedEntityTag=O]
[Text=, CharacterOffsetBegin=43 CharacterOffsetEnd=44 PartOfSpeech=, Lemma=, NamedEntityTag=O]
[Text=and CharacterOffsetBegin=45 CharacterOffsetEnd=48 PartOfSpeech=CC Lemma=and NamedEntityTag=O]
[Text=among CharacterOffsetBegin=49 CharacterOffsetEnd=54 PartOfSpeech=IN Lemma=among NamedEntityTag=O]
[Text=geeks CharacterOffsetBegin=55 CharacterOffsetEnd=60 PartOfSpeech=NNS Lemma=geek NamedEntityTag=O]
[Text=and CharacterOffsetBegin=61 CharacterOffsetEnd=64 PartOfSpeech=CC Lemma=and NamedEntityTag=O]
[Text=people CharacterOffsetBegin=65 CharacterOffsetEnd=71 PartOfSpeech=NNS Lemma=people NamedEntityTag=O]
[Text=who CharacterOffsetBegin=72 CharacterOffsetEnd=75 PartOfSpeech=WP Lemma=who NamedEntityTag=O]
[Text=are CharacterOffsetBegin=76 CharacterOffsetEnd=79 PartOfSpeech=VBP Lemma=be NamedEntityTag=O]
[Text=fashion CharacterOffsetBegin=80 CharacterOffsetEnd=87 PartOfSpeech=NN Lemma=fashion NamedEntityTag=O]
[Text=concious CharacterOffsetBegin=88 CharacterOffsetEnd=96 PartOfSpeech=JJ Lemma=concious NamedEntityTag=O]
[Text=, CharacterOffsetBegin=97 CharacterOffsetEnd=98 PartOfSpeech=, Lemma=, NamedEntityTag=O]
[Text=it CharacterOffsetBegin=99 CharacterOffsetEnd=101 PartOfSpeech=PRP Lemma=it NamedEntityTag=O]
[Text=is CharacterOffsetBegin=102 CharacterOffsetEnd=104 PartOfSpeech=VBZ Lemma=be NamedEntityTag=O]
[Text=known CharacterOffsetBegin=105 CharacterOffsetEnd=110 PartOfSpeech=VBN Lemma=know NamedEntityTag=O]
[Text=as CharacterOffsetBegin=111 CharacterOffsetEnd=113 PartOfSpeech=IN Lemma=as NamedEntityTag=O]
[Text=very CharacterOffsetBegin=114 CharacterOffsetEnd=118 PartOfSpeech=RB Lemma=very NamedEntityTag=O]
[Text=fashionable CharacterOffsetBegin=119 CharacterOffsetEnd=130 PartOfSpeech=JJ Lemma=fashionable NamedEntityTag=O]
[Text=and CharacterOffsetBegin=131 CharacterOffsetEnd=134 PartOfSpeech=CC Lemma=and NamedEntityTag=O]
[Text=techinical CharacterOffsetBegin=135 CharacterOffsetEnd=145 PartOfSpeech=JJ Lemma=techinical NamedEntityTag=O]
[Text=screensaver CharacterOffsetBegin=146 CharacterOffsetEnd=157 PartOfSpeech=NN Lemma=screensaver NamedEntityTag=O]
[Text=. CharacterOffsetBegin=158 CharacterOffsetEnd=159 PartOfSpeech=. Lemma=. NamedEntityTag=O]

Dependency Parse (enhanced plus plus dependencies):
root(ROOT-0, published-3)
nsubjpass(published-3, UNIQLOCK-1)
nsubjpass(published-3''', UNIQLOCK-1)
auxpass(published-3, was-2)
conj:and(published-3, published-3''')
advmod(year-6, about-4)
det(year-6, a-5)
nmod:before(published-3, year-6)
case(year-6, before-7)
punct(year-6, ,-8)
cc(published-3, and-9)
case(geeks-11, among-10)
nmod:among(published-3''', geeks-11)
nsubj(concious-17, geeks-11)
cc(geeks-11, and-12)
nmod:among(published-3''', people-13)
conj:and(geeks-11, people-13)
nsubj(concious-17, people-13)
ref(geeks-11, who-14)
cop(concious-17, are-15)
nmod:npmod(concious-17, fashion-16)
acl:relcl(geeks-11, concious-17)
punct(published-3, ,-18)
nsubjpass(known-21, it-19)
auxpass(known-21, is-20)
parataxis(published-3, known-21)
case(fashionable-24, as-22)
advmod(fashionable-24, very-23)
nmod:as(known-21, fashionable-24)
cc(fashionable-24, and-25)
nmod:as(known-21, techinical-26)
conj:and(fashionable-24, techinical-26)
dep(fashionable-24, screensaver-27)
punct(published-3, .-28)

Extracted the following NER entity mentions:
UNIQLOCK	LOCATION
about a year	DURATION

Coreference set:
	(1,19,[19,20]) -> (1,1,[1,2]), that is: ""it"" -> ""UNIQLOCK""
```"
601,https://github.com/stanfordnlp/CoreNLP/issues/744,744,[],closed,2018-07-14 01:42:06+00:00,,"Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded - while training NER ","Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.util.Arrays.copyOfRange(Arrays.java:3664)
	at java.lang.String.<init>(String.java:207)
	at java.lang.String.substring(String.java:1969)
	at java.lang.String.split(String.java:2353)
	at java.lang.String.split(String.java:2422)
	at edu.stanford.nlp.sequences.ColumnDocumentReaderAndWriter$ColumnDocParser.apply(ColumnDocumentReaderAndWriter.java:85)
	at edu.stanford.nlp.sequences.ColumnDocumentReaderAndWriter$ColumnDocParser.apply(ColumnDocumentReaderAndWriter.java:64)
	at edu.stanford.nlp.objectbank.DelimitRegExIterator.parseString(DelimitRegExIterator.java:71)
	at edu.stanford.nlp.objectbank.DelimitRegExIterator.setNext(DelimitRegExIterator.java:67)
	at edu.stanford.nlp.objectbank.DelimitRegExIterator.<init>(DelimitRegExIterator.java:63)
	at edu.stanford.nlp.objectbank.DelimitRegExIterator$DelimitRegExIteratorFactory.getIterator(DelimitRegExIterator.java:128)
	at edu.stanford.nlp.sequences.ColumnDocumentReaderAndWriter.getIterator(ColumnDocumentReaderAndWriter.java:58)
	at edu.stanford.nlp.objectbank.ObjectBank$OBIterator.setNextObjectHelper(ObjectBank.java:435)
	at edu.stanford.nlp.objectbank.ObjectBank$OBIterator.setNextObject(ObjectBank.java:419)
	at edu.stanford.nlp.objectbank.ObjectBank$OBIterator.next(ObjectBank.java:459)
	at edu.stanford.nlp.sequences.ObjectBankWrapper$WrappedIterator.primeNextHelper(ObjectBankWrapper.java:61)
	at edu.stanford.nlp.sequences.ObjectBankWrapper$WrappedIterator.hasNext(ObjectBankWrapper.java:71)
	at edu.stanford.nlp.ie.crf.CRFClassifier.train(CRFClassifier.java:1579)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequenceClassifier.java:785)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequenceClassifier.java:756)
	at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:2991)
"
602,https://github.com/stanfordnlp/CoreNLP/issues/745,745,[],open,2018-07-15 10:39:30+00:00,,cant run file to parse scene graph from commanda line,"I'm running the next command:
java wiki_00 -mx2g -cp ""*"" edu.stanford.nlp.scenegraph.RuleBasedParser
and getting the next error
Error: Could not find or load main class wiki_00
"
603,https://github.com/stanfordnlp/CoreNLP/issues/746,746,[],closed,2018-07-19 01:19:58+00:00,,RegexNER Annotator: Whitespace inside capture groups,"Hello,

I am having some trouble with a regex expression that I want to match as a named-entity:

```regex
([A-Z][a-z]+ )+(Dog|Cat)
```
Here is some sample text

> The little Brown Dog went into the forest. John had an old Egyptian Cat, and its fur was patchy.

Using the Regexr.com debugging tool, it works perfectly (sorry if the link is not persistent):

[https://regexr.com/3slph](https://regexr.com/3slph)

## In CoreNLP

Here is my regexner.mapping file
```
([A-Z][a-z]+ )+(Dog|Cat)     ANIMAL   ORGANIZATION,PERSON,LOCATION,MISC,NUMBER,MONEY,PERCENT,DATE,TIME        1.0
```

When I load the annotator and try the sentence in the debugging interface, I get this error:
```java
java.util.regex.PatternSyntaxException: Unclosed group near index 12 ([A-Z][a-z]+ ^
```

Here is the console print for the CoreNLP server:

```
[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - setting default constituency parser
[main] INFO CoreNLP - warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP - using: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz instead
[main] INFO CoreNLP - to use shift reduce parser download English models jar from:
[main] INFO CoreNLP - http://stanfordnlp.github.io/CoreNLP/download.html
[main] INFO CoreNLP -     Threads: 32
[main] INFO CoreNLP - Starting server...
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000
[pool-1-thread-1] ERROR CoreNLP - Failure to load language specific properties: StanfordCoreNLP.properties for en
[pool-1-thread-1] INFO CoreNLP - [/0:0:0:0:0:0:0:1:57510] API call w/annotators tokenize,ssplit,pos,lemma,ner,regexner
The little Brown Dog went into the forest. John had an old Egyptian Cat, and its fur was patchy.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[pool-1-thread-1] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.6 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
[pool-1-thread-1] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator ner.fine.regexner: Read 580729 unique entries out of 581888 from edu/stanford/nlp/models/kbp/regexner_caseless.tab, 0 TokensRegex patterns.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator ner.fine.regexner: Read 4857 unique entries out of 4868 from edu/stanford/nlp/models/kbp/regexner_cased.tab, 0 TokensRegex patterns.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator ner.fine.regexner: Read 585586 unique entries from 2 files
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator regexner
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator regexner: Read 1 unique entries out of 1 from strat_rules.txt, 0 TokensRegex patterns.
java.util.regex.PatternSyntaxException: Unclosed group near index 12
([A-Z][a-z]+
            ^
        at java.util.regex.Pattern.error(Pattern.java:1957)
        at java.util.regex.Pattern.accept(Pattern.java:1815)
        at java.util.regex.Pattern.group0(Pattern.java:2910)
        at java.util.regex.Pattern.sequence(Pattern.java:2053)
        at java.util.regex.Pattern.expr(Pattern.java:1998)
        at java.util.regex.Pattern.compile(Pattern.java:1698)
        at java.util.regex.Pattern.<init>(Pattern.java:1351)
        at java.util.regex.Pattern.compile(Pattern.java:1054)
        at edu.stanford.nlp.ling.tokensregex.ComplexNodePattern$StringAnnotationRegexPattern.<init>(ComplexNodePattern.java:254)
        at edu.stanford.nlp.ling.tokensregex.ComplexNodePattern.newStringRegexPattern(ComplexNodePattern.java:53)
        at edu.stanford.nlp.ling.tokensregex.CoreMapNodePattern.valueOf(CoreMapNodePattern.java:45)
        at edu.stanford.nlp.pipeline.TokensRegexNERAnnotator.createPatternMatcher(TokensRegexNERAnnotator.java:355)
        at edu.stanford.nlp.pipeline.TokensRegexNERAnnotator.<init>(TokensRegexNERAnnotator.java:295)
        at edu.stanford.nlp.pipeline.AnnotatorImplementations.tokensRegexNER(AnnotatorImplementations.java:85)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$7(StanfordCoreNLP.java:548)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$30(StanfordCoreNLP.java:625)
        at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)
        at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
        at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:495)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:201)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:194)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:181)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.mkStanfordCoreNLP(StanfordCoreNLPServer.java:345)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.access$700(StanfordCoreNLPServer.java:54)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:751)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
        at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
        at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
        at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
        at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
```

## Swap whitespace ` `for regex special whitespace character `\s`

I also tried this pattern
```
([A-Z][a-z]+\s)+(Dog|Cat)     ANIMAL   ORGANIZATION,PERSON,LOCATION,MISC,NUMBER,MONEY,PERCENT,DATE,TIME        1.0
```

With this, there are no errors, however my pattern does not get matched.

Any suggestions? What did I do wrong?

Best regards,
Bj√∏rn Tore Kopperud
"
604,https://github.com/stanfordnlp/CoreNLP/issues/747,747,[],closed,2018-07-19 21:02:59+00:00,,Identical class name warning when using stanford-corenlp and stanford parser,"We are using both stanford-corenlp and stanford-parser in the same program.  Does one contain the other?  If not, they both contain identical classes in the util directory sow we see tons of these types warnings 

2018-07-19 17:51:49.396:WARN:oeja.AnnotationParser:qtp1161082381-27: edu.stanford.nlp.fsm.FastExactAutomatonMinimizer$Block scanned from multiple locations: jar:file:///tmp/jetty-0.0.0.0-9999-root.war-_-any-8948454423037923483.dir/webapp/WEB-INF/lib/stanford-parser-3.8.0.jar!/edu/stanford/nlp/fsm/FastExactAutomatonMinimizer$Block.class, jar:file:///tmp/jetty-0.0.0.0-9999-root.war-_-any-8948454423037923483.dir/webapp/WEB-INF/lib/stanford-corenlp-3.8.0.jar!/edu/stanford/nlp/fsm/FastExactAutomatonMinimizer$Block.class

Notice that one is called from stanford-corenlp-3.8.0.jar and the other from stanford-parser-3.8.0.jar, but otherwise from the same package edu/stanford/nlp/fsm .  Should the stanford util package be a separate maven package?"
605,https://github.com/stanfordnlp/CoreNLP/issues/750,750,[],closed,2018-07-31 23:49:20+00:00,,In case you do not want the CoreNLP server to use all your computer's processors on Linux,"This is information only and should be closed.

The following causes the CoreNLP java server to use only processors 0 and 1. (0x03 is processors 1 and 2 by the way taskset counts.) The links below show how to select which and how many processors.

taskset 0x03 java ...

How to set CPU affinity to a process
https://askubuntu.com/questions/102258/how-to-set-cpu-affinity-to-a-process

Linux Setting processor affinity for a certain task or process
https://www.cyberciti.biz/tips/setting-processor-affinity-certain-task-or-process.html"
606,https://github.com/stanfordnlp/CoreNLP/issues/751,751,[],closed,2018-08-06 09:29:36+00:00,,Why CoreNLP server 3.9.1 is much slower than 3.8.0?,"HiÔºå

Recently I am trying to use the server mode of CoreNLP 3.9.1, The annotation contains ssplit, tokenize, pos, lemma and ner. The speed seems much slower than 3.8.0. 
Do you know why is that 
thanksÔºå

Peter"
607,https://github.com/stanfordnlp/CoreNLP/issues/752,752,[],closed,2018-08-08 08:41:42+00:00,,Dependency parser created a weird self-dependency,"I am currently using coreNLP for dependency parsing. When I use the function on the following sentence, a word ""work"" have a dependency on itself on enhanced dependency. I tried it on `corenlp.run` and the issue also shows up.

The sentence:

`Micron does not disclose production capacity of the new phase of the Fab 10, but says that the new plant will require it to hire additional personnel to work at the fab and in the supply chain.`

The issue can be reproduced through  [corenlp.run](http://corenlp.run/), just run this sentence on [corenlp.run](http://corenlp.run/) and the issued will shown in ""Enhanced++ Dependencies:""(The `work` word).

"
608,https://github.com/stanfordnlp/CoreNLP/issues/753,753,[],closed,2018-08-13 04:31:22+00:00,,How to reload a custom entity model in CoreNLP Server?,"I have trained a Custom Entity Model using Core NLP and I am able to load it into the CoreNLP Server and get entities using the model. However after retraining the model and replacing the old model file with the new file, I am not getting results as per the new training without restarting the server. Seems CoreNLP server is using the same model that was first loaded into it.  Restarting the server after each training is not possible in our business use case. Is there a way to reload a model into CoreNLP server without restarting the server?
"
609,https://github.com/stanfordnlp/CoreNLP/issues/754,754,[],closed,2018-08-15 17:09:54+00:00,,CleanXmlAnnotator.process throws NullPointerException,"`CleanXmlAnnotator.process(tokens)` invokes `process(null /* annotation */, tokens)` at https://github.com/stanfordnlp/CoreNLP/blob/bfc7f66/src/edu/stanford/nlp/pipeline/CleanXmlAnnotator.java#L382. However, `process(null, tokens)` calls `annotation.set` without null checking 
 at https://github.com/stanfordnlp/CoreNLP/blob/bfc7f66/src/edu/stanford/nlp/pipeline/CleanXmlAnnotator.java#L535. This triggers an NPE.
"
610,https://github.com/stanfordnlp/CoreNLP/issues/755,755,[],closed,2018-08-16 17:57:36+00:00,,Is there a way to provide pre-tokenized sentences while still expect CoreNLP to do PTB-style conversion?,"For the lexicalalized parser, I can specify both `-tokenized` and `-escaper edu.stanford.nlp.process.PTBEscapingProcessor`. But for the CoreNLP server, if I specify `'tokenize.whitespace': 'true'`, then it looks like `'tokenize.options': 'ptb3Escaping=true'` is ignored. (I am using the [official Python wrapper](https://github.com/stanfordnlp/python-stanford-corenlp))"
611,https://github.com/stanfordnlp/CoreNLP/issues/756,756,[],closed,2018-08-16 23:33:35+00:00,,Is truecase annotator in a sense an informal prereq to the dependency parser?,"It looks like if I have all-caps in a sentence, the dependency parser has very bad performance."
612,https://github.com/stanfordnlp/CoreNLP/issues/757,757,[],open,2018-08-20 20:33:57+00:00,,ngram argument while running ./corenlp.sh,Is there a way that I can specify ngrams through an argument for the pipeline while running the corenlp.sh script through command line?
613,https://github.com/stanfordnlp/CoreNLP/issues/758,758,[],closed,2018-08-21 17:14:38+00:00,,POS multiple punctuation issues,"I have some issues with the sentences with the ""!!!"" or ""???"" at the end. CoreNLP POS produces a bit illogical results for them:

For example, for `Hello, sir!!!` I'm receiving the latest POS tag for one `!!!` token  as: 

> NN (Noun, singular or mass)

I was expecting something like `.`,`!` or SYM token (or tokens) like Stanford NLP do with one `!`).

To compare behaviour, I also have tested this example with Google NLP and received (more logical) 3 PUNCT tokens (instead one token).

Does anybody know any tricks or settings to change this behaviour?

I'm using Stanford NLP v3.9.1 and my settings are:

```
      ""language"", ""english"", 
      ""tokenize.class"", ""PTBTokenizer"",
      ""tokenize.language"", ""en"",
      ""pos.class"", ""edu.stanford.nlp.tagger.maxent.MaxentTagger"",
      ""pos.model"", ""edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger""
```
"
614,https://github.com/stanfordnlp/CoreNLP/issues/759,759,[],closed,2018-08-22 23:18:26+00:00,,invertible=true not working with XML output,"When `invertible=true` is used with JSON output, the three additional fields `originalText`, `before`, and `after` do show up. However, when the output format is changed to XML, these three fields are lost.

Using 2018-02-27 version of CoreNLP, with official [python-stanford-corenlp](https://github.com/stanfordnlp/python-stanford-corenlp) wrapper."
615,https://github.com/stanfordnlp/CoreNLP/issues/760,760,[],closed,2018-08-24 00:52:54+00:00,,Using too much RAM,"I'm trying to run the example code using IntelliJ.

Here is a link to the code I run.
https://stanfordnlp.github.io/CoreNLP/api.html

I give IntelliJ 7GB of memory by setting -Xmx7g -Xms256m, but I still received below error.

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/Users/vincent/.ivy2/cache/org.slf4j/slf4j-log4j12/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/Users/vincent/Desktop/stanford-corenlp-full-2018-02-27/slf4j-simple.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
log4j:WARN No appenders could be found for logger (edu.stanford.nlp.pipeline.StanfordCoreNLP).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
	at edu.stanford.nlp.parser.nndep.Classifier.initGradientHistories(Classifier.java:606)
	at edu.stanford.nlp.parser.nndep.Classifier.<init>(Classifier.java:140)
	at edu.stanford.nlp.parser.nndep.Classifier.<init>(Classifier.java:115)
	at edu.stanford.nlp.parser.nndep.DependencyParser.loadModelFile(DependencyParser.java:624)
	at edu.stanford.nlp.parser.nndep.DependencyParser.loadFromModelFile(DependencyParser.java:499)
	at edu.stanford.nlp.pipeline.DependencyParseAnnotator.<init>(DependencyParseAnnotator.java:57)
	at edu.stanford.nlp.pipeline.AnnotatorImplementations.dependencies(AnnotatorImplementations.java:240)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$57(StanfordCoreNLP.java:559)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$29/1418621776.apply(Unknown Source)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$69(StanfordCoreNLP.java:625)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$38/1007603019.get(Unknown Source)
	at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)
	at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
	at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:495)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:201)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:194)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:181)
	at com.chatmeter.grimore.scripts.BasicPipelineExample.main(BasicPipelineExample.java:29)

Has anyone encountered a similar problem using IntelliJ before?
"
616,https://github.com/stanfordnlp/CoreNLP/issues/761,761,[],closed,2018-08-25 03:41:59+00:00,,"Would like to get all the labels for NER, thanks~","I know there are 3 class:	Location, Person, Organization
4 class:	Location, Person, Organization, Misc
7 class:	Location, Person, Organization, Money, Percent, Date, Time

While I would like to know all the labels that will show up in the 3 class NER.
(PS. I got these in my test.)
PPS. I am dealing with Chinese text, thanks a lot.
facility
organization
person
title
number
ordinal
ideology
date
location
state or province
country
city
GPE
MISC
DEMONYM
cause of death"
617,https://github.com/stanfordnlp/CoreNLP/issues/762,762,[],closed,2018-09-02 04:44:02+00:00,,nlp.stanford.edu/js/brat/style-vis.css timeout,"Hi,

I want to use the web version of CoreNLP server. But when I access [localhost:9000](localhost:9000), I got [https://nlp.stanford.edu/js/brat/client/lib/head.load.min.js](https://nlp.stanford.edu/js/brat/client/lib/head.load.min.js) and [https://nlp.stanford.edu/js/brat/style-vis.css](https://nlp.stanford.edu/js/brat/style-vis.css) timeout from chrome console, which makes the web version cannot work.

I guess the main cause is that [https://nlp.stanford.edu](https://nlp.stanford.edu) cannot be connected now. Any idea when it will be fixed? Thanks!"
618,https://github.com/stanfordnlp/CoreNLP/issues/763,763,[],closed,2018-09-02 18:35:09+00:00,,Download CoreNLP 3.9.1 not working,The link on this page https://stanfordnlp.github.io/CoreNLP/#download does not complete connection. This is likely a minor issue.
619,https://github.com/stanfordnlp/CoreNLP/issues/764,764,[],closed,2018-09-03 07:53:19+00:00,,Can't download anything,"Is the server up?

```
http://nlp.stanford.edu/software/stanford-ner-2015-12-09.zip
```

Thanks"
620,https://github.com/stanfordnlp/CoreNLP/issues/765,765,[],closed,2018-09-03 15:03:13+00:00,,Java Docs down,I can't get a connection to your JavaDocs. And the download is also not processing.
621,https://github.com/stanfordnlp/CoreNLP/issues/766,766,[],closed,2018-09-05 14:08:19+00:00,,Co-reference tagger wrongly tags unrelated terms,"Sample Statement

> Alex Mathews loves Martha. She loves Jake. She hates Alex.

The corefs response from API returns the json:
```
{
    ""4"": [
    {
        ""id"": 1,
        ""text"": ""Martha"",
        ""type"": ""PROPER"",
        ""number"": ""SINGULAR"",
        ""gender"": ""FEMALE"",
        ""animacy"": ""ANIMATE"",
        ""startIndex"": 4,
        ""endIndex"": 5,
        ""headIndex"": 4,
        ""sentNum"": 1,
        ""position"": [1, 2],
        ""isRepresentativeMention"": true
    },
    {
        ""id"": 3,
        ""text"": ""She"",
        ""type"": ""PRONOMINAL"",
        ""number"": ""SINGULAR"",
        ""gender"": ""FEMALE"",
        ""animacy"": ""ANIMATE"",
        ""startIndex"": 1,
        ""endIndex"": 2,
        ""headIndex"": 1,
        ""sentNum"": 2,
        ""position"": [2, 2],
        ""isRepresentativeMention"": false
    },
    {
        ""id"": 5,
        ""text"": ""She"",
        ""type"": ""PRONOMINAL"",
        ""number"": ""SINGULAR"",
        ""gender"": ""FEMALE"",
        ""animacy"": ""ANIMATE"",
        ""startIndex"": 1,
        ""endIndex"": 2,
        ""headIndex"": 1,
        ""sentNum"": 3,
        ""position"": [3, 2],
        ""isRepresentativeMention"": false
    },
    {
        ""id"": 4,
        ""text"": ""Alex."",
        ""type"": ""PROPER"",
        ""number"": ""SINGULAR"",
        ""gender"": ""UNKNOWN"",
        ""animacy"": ""ANIMATE"",
        ""startIndex"": 3,
        ""endIndex"": 4,
        ""headIndex"": 3,
        ""sentNum"": 3,
        ""position"": [3, 1],
        ""isRepresentativeMention"": false
    }]
}
```

basically, it is tagging a separate entity as another entity and fails to identify original entity as the coref"
622,https://github.com/stanfordnlp/CoreNLP/issues/767,767,"[{'id': 626016953, 'node_id': 'MDU6TGFiZWw2MjYwMTY5NTM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/analysis-bug', 'name': 'analysis-bug', 'color': 'f98685', 'default': False, 'description': None}]",open,2018-09-06 15:15:39+00:00,,Corenlp recognizes ???/ *** as number,"If any sentence has ????? or **** core\nlp tokenizes it and identifies it as Number, which should not happen."
623,https://github.com/stanfordnlp/CoreNLP/issues/768,768,[],closed,2018-09-06 22:08:15+00:00,,Visual output of http://corenlp.run/ does not match core NLP constituency parser output,"Hi,

I am using the output of constituency parser of Stanford (binarizedTree) and I get that from the [official python wrapper of corenlp](https://github.com/stanfordnlp/python-stanford-corenlp). I noticed that there are some minor differences between the visualization I have using [corenlp.run](http://corenlp.run/) and parser output and just want to figure out which one is correct. 

For example, if we parse the following piece of text:

> ""I bought it because I read a good review."" 

In visualization, we only have 4 noun phrases (NP) while in the actual output of the parser there are 5 NPs including: [""I"", ""it"", ""I"", ""a good review"", ""good review""]

Again, just want to make sure which one is correct. It is important to mention that I use the visualization to get a better understanding of the structure of my sample sentences. And, based on the structure and assuming that it is the same as the real output of the parser, I go and write my codes. As a result, it is important to me to make sure that these two are exactly the same."
624,https://github.com/stanfordnlp/CoreNLP/issues/769,769,[],closed,2018-09-19 17:18:39+00:00,,Is it possible to ignore certain phrases while doing tokenization?,"Say I have some text and it contains some phrases that would normally get broken down while doing tokenization. 
For example: ""Windows 10"" -> 
Windows
10

Is it possible to pass in some options to ignore the phrase ""Windows 10"" while doing tokenization?
If not, will it be implemented in the future? Any alternatives? "
625,https://github.com/stanfordnlp/CoreNLP/issues/771,771,[],closed,2018-09-25 14:26:21+00:00,,Readiness endpoint throws an exception,"Hi!
It seems like the `/ready` endpoints throws an exception every time it is queried:
```
java.lang.IllegalArgumentException: Unknown annotator: mention
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.ensurePrerequisiteAnnotators(StanfordCoreNLP.java:352)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.getProperties(StanfordCoreNLPServer.java:423)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.access$200(StanfordCoreNLPServer.java:54)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:710)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
	at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
	at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
	at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
```

We start our server with:
`java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 8080 -timeout 15000`
it is a bit annoying because our service discovery ping the service every 30s and it pollutes the logs.
Not sure what can be done to silence this exception.

Thanks!  "
626,https://github.com/stanfordnlp/CoreNLP/issues/772,772,[],closed,2018-09-25 16:27:57+00:00,,How to retrain the Standford Constituency parser on Yahoo Questions data,"Hi,
I am dealing with a dataset which is primarily questions. I want to retrain the existing English Stanford Constituency parser on Questions from English Web Treebank.

How do I organize this data and what commands should I run to do this? 

Also since I already know that my sentence is a question can I force the parser to predict (SBARQ at the head of the sentence? This should increase the accuracy of the overall parse. 

Thanks for the help"
627,https://github.com/stanfordnlp/CoreNLP/issues/773,773,[],closed,2018-09-26 22:00:47+00:00,,download page has outdated link,"https://stanfordnlp.github.io/CoreNLP/download.html references

wget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-01-31.zip

for example, instead of 

http://nlp.stanford.edu/software/stanford-corenlp-full-2018-02-27.zip"
628,https://github.com/stanfordnlp/CoreNLP/issues/774,774,"[{'id': 45387508, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}]",closed,2018-09-27 05:12:24+00:00,,Change directory for output file,"How can I specify a particular directory to create my output of Coref in ?
I am executing the following command:
java -Xmx5g -cp C:\Users\harsh\CoreNLP\stanford-corenlp-full-2018-02-27\stanford-corenlp-3.9.1.jar;C:\Users\harsh\CoreNLP\stanford-corenlp-full-2018-02-27\stanford-english-corenlp-models-current.jar;* edu.stanford.nlp.pipeline.StanfordCoreNLP -props C:\Users\harsh\CoreNLP\src\edu\stanford\nlp\coref\properties\deterministic-english.properties -file C:\Users\harsh\public_mm\public_mm\metamap\transformed_openie_output.txt
"
629,https://github.com/stanfordnlp/CoreNLP/issues/775,775,[],closed,2018-10-06 12:43:10+00:00,,Correct Version Number for POM (3.9.1-->3.9.2),"Hey, it appears that the version number of the regular [pom.xml](https://github.com/stanfordnlp/CoreNLP/blob/master/pom.xml#L5) file is `3.9.1` rather than `3.9.2` as shown in the [Gradle](https://github.com/stanfordnlp/CoreNLP/blob/master/build.gradle#L14) and the [Pom Java 11](https://github.com/stanfordnlp/CoreNLP/blob/master/pom-java-11.xml). 

Furthermore [this](https://github.com/stanfordnlp/CoreNLP/blame/master/README.md#L41) line in the Readme needs updating to at least version `3.9.1` as `mvn package` lists version as of current `3.9.1` (or just add some insert version number here comment)."
630,https://github.com/stanfordnlp/CoreNLP/issues/776,776,[],closed,2018-10-11 08:50:26+00:00,,TokensRegexRetokenizeDemo throw NullPointerException,"When I run TokensRegexRetokenizeDemo.java demo, I found a NullPointerException at edu.stanford.nlp.pipeline.WordsToSentencesAnnotator.annotate(WordsToSentencesAnnotator.java:317).the corenlp version is 3.9.1

any help is appreciated!
"
631,https://github.com/stanfordnlp/CoreNLP/issues/777,777,[],closed,2018-10-12 09:39:31+00:00,,PTB trees to UD conversion: error in applying NER tags,"Hello,

I try to convert the Penn constituency treebank to UD format using the current release of CoreNLP. I use the following command:

`java -cp stanford-corenlp-3.9.2.jar -mx1g edu.stanford.nlp.trees.ud.UniversalDependenciesConverter -treeFile ptb-treebank.txt > ud-treeabank.txt`

The conversion process is completed. However, during the run it displays the following error, stating that NER tags were not applied:

```
java.io.IOException: Unable to open ""edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz"" as class path, filename or URL
	at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:481)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1503)
	at edu.stanford.nlp.ie.crf.CRFClassifier.getClassifier(CRFClassifier.java:2926)
	at edu.stanford.nlp.ie.ClassifierCombiner.loadClassifierFromPath(ClassifierCombiner.java:286)
	at edu.stanford.nlp.ie.ClassifierCombiner.loadClassifiers(ClassifierCombiner.java:270)
	at edu.stanford.nlp.ie.ClassifierCombiner.<init>(ClassifierCombiner.java:142)
	at edu.stanford.nlp.ie.NERClassifierCombiner.<init>(NERClassifierCombiner.java:130)
	at edu.stanford.nlp.ie.NERClassifierCombiner.createNERClassifierCombiner(NERClassifierCombiner.java:275)
	at edu.stanford.nlp.ie.NERClassifierCombiner.createNERClassifierCombiner(NERClassifierCombiner.java:214)
	at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at edu.stanford.nlp.trees.ud.UniversalDependenciesConverter.setupNERTagger(UniversalDependenciesConverter.java:153)
	at edu.stanford.nlp.trees.ud.UniversalDependenciesConverter.addNERTags(UniversalDependenciesConverter.java:183)
	at edu.stanford.nlp.trees.ud.UniversalDependenciesConverter.convertTreeToBasic(UniversalDependenciesConverter.java:70)
	at edu.stanford.nlp.trees.ud.UniversalDependenciesConverter.access$000(UniversalDependenciesConverter.java:28)
	at edu.stanford.nlp.trees.ud.UniversalDependenciesConverter$TreeToSemanticGraphIterator.next(UniversalDependenciesConverter.java:101)
	at edu.stanford.nlp.trees.ud.UniversalDependenciesConverter$TreeToSemanticGraphIterator.next(UniversalDependenciesConverter.java:83)
	at edu.stanford.nlp.trees.ud.UniversalDependenciesConverter.main(UniversalDependenciesConverter.java:238)
Error setting up NERClassifierCombiner!  Not applying NER tags!
Error running NERClassifierCombiner on Tree!  Not applying NER tags!
```

The same error appears when using other versions of CoreNLP. What causes this error? Is the use of NER crucial for a valid conversion process?

Thanks!
"
632,https://github.com/stanfordnlp/CoreNLP/issues/779,779,[],closed,2018-10-15 07:58:28+00:00,,Stanford nlp.ner() giving: No JSON object could be decoded,"I downloaded stanford on local and trying to parse emails. But whenever i'm passing documents/text to **stanfordcorenlp.ner()** function then it's returning **No JSON object could be decoded** exception.

And sometimes it's work fine for same text/emails.

Getting Exception for following code:
   
 `
    try:

       classified_text = nlp.ner(text)

    except Exception, e:

       print ""Exception is : "", e`

I didn't get why it's not working and returning **No JSON object could be decoded** most of the time and sometime it's giving result for same text."
633,https://github.com/stanfordnlp/CoreNLP/issues/780,780,"[{'id': 626016953, 'node_id': 'MDU6TGFiZWw2MjYwMTY5NTM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/analysis-bug', 'name': 'analysis-bug', 'color': 'f98685', 'default': False, 'description': None}]",closed,2018-10-16 21:49:07+00:00,,Tokenizer incorrectly assign single quote to apostrophe,"When dealing with partial segment, such as [He'll], [She'll], the tokenizer treats ['] as opening single quote. Interestingly, if we write [He'll, She'll], then the first one is treated as [will], while the second one is treated as opening single quote.

![image](https://user-images.githubusercontent.com/6566533/47049576-8c874980-d152-11e8-85d5-714a7e075c33.png)
"
634,https://github.com/stanfordnlp/CoreNLP/issues/781,781,[],closed,2018-10-23 16:54:25+00:00,,Disable NER Fine Grained in CoreNLP Server,"I'm accessing the CoreNLPServer from a python script which works excellent, however I believe the new ""fine-grained"" NER is significantly slowing down parsing in v3.9.2.  I've seen references that say it can be turned off with the java code...
```
ner.applyFineGrained=false
ner.buildEntityMentions=false 
```

How can I pass these commands to the startup script for the server?  I'm currently using a startup command something like...
```
cmd = 'java -mx4g -cp %s/* edu.stanford.nlp.pipeline. StanfordCoreNLPServer ' % (config.core_nlp)
cmd += '--port %d --preload tokenize,ssplit,pos,lemma,ner,parse ' % (config.port)
```
Alternately I could pass a param in with requests, but again, I'm not sure of the format.  Currently I'm using...
```
reqdict = {'annotators': 'tokenize, ssplit, pos, lemma, ner', 'outputFormat': 'json'}
requests.post(self.server_url, params={'properties':reqdict}, data=text.encode(), headers{'Connection': 'close'})
```
Can fine-grained be disabled through the startup script or the client's requests?  Any hints on how to do this would be appreciated."
635,https://github.com/stanfordnlp/CoreNLP/issues/783,783,[],open,2018-10-25 10:49:00+00:00,,Issue with conversion of the month to the reference year,"How can we convert relative time expressions to the year of the document (reference date) using ""english.sutime.txt"" dictionary?
For instance, for a time expression such as ""January"", SUTime converts the expression to the closest date according to the month of reference date. For example, if the reference date is 2011/03/03, SUTime  converts ""January"" to 2011-01. However, if the reference date is  2011/10/03, ""January"" will be converted to 2012-01."
636,https://github.com/stanfordnlp/CoreNLP/issues/784,784,"[{'id': 1134499406, 'node_id': 'MDU6TGFiZWwxMTM0NDk5NDA2', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/wrapper-bug', 'name': 'wrapper-bug', 'color': 'd93f0b', 'default': False, 'description': 'A problem with a package that provides an API or interface to CoreNLP, but not CoreNLP itself'}]",closed,2018-10-26 13:33:51+00:00,,strange behavior of skipping characters in tokenization,"I found that the PTBLexer sometimes decides to remove characters in the input text, as a result the character offsets would not apply to the original text any more. I wonder why that is and how to avoid it. For example, if the input is ""%ACTL"", the tokenizer would remove ""%AC"", treating it as a single character, and parse only ""TL"". I tried resetting different ""untokenizable"" options (e.g., ""allKeep"", ""noneKeep"") but that doesn't seem to help. Could someone please help me with this? Thanks."
637,https://github.com/stanfordnlp/CoreNLP/issues/785,785,[],closed,2018-10-27 03:51:39+00:00,,stanford-corenlp-3.9.2.jar don't contain the class HybridCorefAnnotator,I download stanford-corenlp-3.9.2.jar and try to to run Chinese Coreference . But I don't find the class HybridCorefAnnotator and the [git repos](https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/HybridCorefAnnotator.java) contanins the class. 
638,https://github.com/stanfordnlp/CoreNLP/issues/786,786,[],closed,2018-10-29 16:25:01+00:00,,Issue with EntityMentionsAnnotator when using custom NER and Mention annotations,"Hi,
I want to draw your attention to a little bug that I discovered in the EntityMentionsAnnotator  class.  I was trying to avoid any overlap with existing NER tags and entity mentions. So, I created custom NER annotations which I used with tokensregex annotator. Then, I tried to set up the EntityMentionsAnnotator to work with my new NER tags. This is the configuration I used :

```
//The full pipeline

Properties props = new Properties();
props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, entitymentions, tokensregex, parse, depparse, coref, kbp"");
props.setProperty(""customAnnotatorClass.tokensregex"", ""edu.stanford.nlp.pipeline.TokensRegexAnnotator"");
props.setProperty(""tokensregex.rules"", ""skills.rules,pattern.rules"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
pipeline.annotate(document);
		
//the custom Entity Mentions annotator

Properties mprops = new Properties();
mprops.setProperty(""mymention.nerCoreAnnotation"", ""com.nlptools.pipeline.ie.MyCoreAnnotations$MyNamedEntityTagAnnotation"" ) ;
mprops.setProperty(""mymention.nerNormalizedCoreAnnotation"", ""com.nlptools.pipeline.ie.MyCoreAnnotations$MyNamedEntityTagAnnotation"" ) ;
mprops.setProperty(""mymention.mentionsCoreAnnotation"", ""com.nlptools.pipeline.ie.MyCoreAnnotations$MyMentionsAnnotation"") ;
EntityMentionsAnnotator mentionsAnnotator = new EntityMentionsAnnotator(""mymention"", mprops) ;
mentionsAnnotator.annotate(document);
```


I succeeded to get the list of the new customized entity mentions at the sentence level (by calling `sentence.get(MyCoreAnnotations.MyMentionsAnnotation.class)`), but the mentions did not appear at the document level (when calling `document.get(MyCoreAnnotations.MyMentionsAnnotation.class)`). 
I figured out that the problem came from line 319 at the EntityMentionsAnnotator.java (latest version at Github)
```
// build document wide entity mentions list
...
for (CoreMap entityMention : sentence.get(CoreAnnotations.MentionsAnnotation.class)) {
```
which I replaced by with 

`for (CoreMap entityMention : sentence.get(mentionsCoreAnnotationClass)) `

In fact, `mentionsCoreAnnotationClass` is  the class name of the entity mention annotation set up at the constructor level of the EntityMentionsAnnotator. Now, all works fine for me.  I‚Äôm not really very experienced with CoreNLP, but I hope  that my comment would somehow help improve the CoreNLP API. 
"
639,https://github.com/stanfordnlp/CoreNLP/issues/787,787,[],closed,2018-10-31 05:54:14+00:00,,Set up CoreNLPServer on Multiple Languages,"I have few questions on setting up CoreNLP server for multi-language support.

1. Can we load multiple language models on the same CoreNLPServer instance? 
     I am using the below command to start the server
     sudo su nlp -c ""/usr/bin/authbind --deep java -Djava.net.preferIPv4Stack=true -Djava.io.tmpdir=/home/ice-ner-dev/CoreNLP_NER/corenlp -cp ""$CLASSPATH"" -XX:+UseG1GC -XX:MaxGCPauseMillis=500 -XX:+UseStringDeduplication -XX:SoftRefLRUPolicyMSPerMB=100 -mx20g edu.stanford.nlp.pipeline.StanfordCoreNLPServer -preload ""tokenize,ssplit,pos,lemma,ner"" -port 9001 -timeout 75000 -serverProperties StanfordCoreNLP-spanish.properties""

     Using the above command, only the Spanish model gets loaded. If serverProperties are not specified, 
    only the English model gets loaded. Is there a way we can load all the models into the same server 
     instance?

2. For custom NER training, I am using the command below.
      java -cp ""$CLASSPATH""  edu.stanford.nlp.ie.crf.CRFClassifier -prop MedicalAssistant.prop
      How do we specify the language here? Don't we want the POS and other features to be used based 
      on the language?

3.  I loaded the spanish model during server startup and ran the train command above. Use the https://pypi.org/project/stanford-corenlp/ for getting the results from the CoreNLPServer. The entity was detected correctly but the POS seems incorrect for some words. eg: In the sentence ""Hola mi nombre es Rachel"", Hola was taken as a Proper Noun which is not correct. Please see the output below

Hola mi nombre es Rachel
{u'sentences': [{u'tokens': [{u'index': 1, u'word': u'Hola', u'lemma': u'hola', u'pos': u'PROPN', u'characterOffsetEnd': 4, u'characterOffsetBegin': 0, u'originalText': u'Hola', u'ner': u'O'}, {u'index': 2, u'word': u'mi', u'lemma': u'mi', u'pos': u'DET', u'characterOffsetEnd': 7, u'characterOffsetBegin': 5, u'originalText': u'mi', u'ner': u'O'}, {u'index': 3, u'word': u'nombre', u'lemma': u'nombre', u'pos': u'NOUN', u'characterOffsetEnd': 14, u'characterOffsetBegin': 8, u'originalText': u'nombre', u'ner': u'O'}, {u'index': 4, u'word': u'es', u'lemma': u'e', u'pos': u'VERB', u'characterOffsetEnd': 17, u'characterOffsetBegin': 15, u'originalText': u'es', u'ner': u'O'}, {u'index': 5, u'word': u'Rachel', u'lemma': u'rachel', u'pos': u'PROPN', u'characterOffsetEnd': 24, u'characterOffsetBegin': 18, u'originalText': u'Rachel', u'ner': u'O'}], u'index': 0, u'entitymentions': []}]}


"
640,https://github.com/stanfordnlp/CoreNLP/issues/788,788,[],closed,2018-10-31 17:57:28+00:00,,The dcoref link points to a nonexistent page on the annotator dependencies page,"On the annotator dependencies page located [here](https://stanfordnlp.github.io/CoreNLP/dependencies.html), the dcoref link points to a page which doesn't exist (causing a 404 error). If the proper annotator is coref and not dcoref (which I believe it is), I think it should point to https://stanfordnlp.github.io/CoreNLP/coref.html and not https://stanfordnlp.github.io/CoreNLP/dcoref.html. 

I couldn't find a repo for the website itself so I apologize if this is in the wrong spot!"
641,https://github.com/stanfordnlp/CoreNLP/issues/789,789,[],closed,2018-11-01 20:31:37+00:00,,Crash: openie.resolve_coref,"The following command-line invocation crashes OpenIE:

I want the system to resolve coreferences as I am trying to produce a simple training set of labeled data for an algorithm.

```(ml_python) Nicholass-MacBook-Pro-Work:crystal_ball nicholasroth$ java -mx16g -cp ""stanford-ner/*"" -Dopenie.resolve_coref=""true"" edu.stanford.nlp.naturalli.OpenIE relex_test_useful.txt -openie.resolve_coref
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.4 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse
[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... 
[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 7.446 (s)
[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [8.5 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.8 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].
[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
[main] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 580704 unique entries out of 581863 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_caseless.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 4869 unique entries out of 4869 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_cased.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 585573 unique entries from 2 files
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator mention
Exception in thread ""main"" java.lang.IllegalArgumentException: No annotator named mention
	at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:151)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:251)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:192)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:188)
	at edu.stanford.nlp.naturalli.OpenIE.main(OpenIE.java:735)```"
642,https://github.com/stanfordnlp/CoreNLP/issues/790,790,[],closed,2018-11-03 10:07:13+00:00,,Can't imported this in Android Studio.,"Hello guys,

I can't import this project in android studio, there are so many issues coming when I run the app in device.

@hanjiang-stanford : Please update this project to support latest android version 28.

Thanks."
643,https://github.com/stanfordnlp/CoreNLP/issues/791,791,[],closed,2018-11-08 07:27:27+00:00,,Coreference resolution: does not extract chains on some sentences,"Using the latest CoreNLP server and the neural models in the coreference resolution module, the following two sentences do not return any chain, besides I think they the coref resolution should ideally find one chain with two mentions:
```
Taxes on bill not revised after new financial year. Taxes applicable for last month to be recalculated.
```"
644,https://github.com/stanfordnlp/CoreNLP/issues/792,792,[],closed,2018-11-09 18:22:26+00:00,,Server crashes when using regexner,"# Crash description and steps to reproduce
When trying to run the CoreNLP server inside a docker container using the Dockerfile available in my PR to https://github.com/vineetvermait/corenlp-docker/pull/1, the server will crash whenever the regexner annotator is used (It is still similar to the recommend [frnkenstien/corenlp](https://hub.docker.com/r/frnkenstien/corenlp/), just with the latest release version). `docker build . -t corenlp && docker run --rm -p 9000:9000 corenlp`

A common crash report looks like this:
```
[pool-1-thread-1] INFO CoreNLP - [/172.17.0.1:52686] API call w/annotators tokenize,ssplit,pos,depparse,lemma,ner,coref,kbp
The quick brown fox jumps over the lazy dog.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[pool-1-thread-1] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.5 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse
[pool-1-thread-1] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... 
[pool-1-thread-1] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 13.546 (s)
[pool-1-thread-1] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [15.2 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [2.7 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.8 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.9 sec].
[pool-1-thread-1] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
[pool-1-thread-1] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 580704 unique entries out of 581863 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_caseless.tab, 0 TokensRegex patterns.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 4869 unique entries out of 4869 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_cased.tab, 0 TokensRegex patterns.
[pool-1-thread-1] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 585573 unique entries from 2 files
```


## Affected annotator selections

While this looks similar to #789, openie is working fine on the example sentence and the server is not crashing for this annotator.
The following annotator selections cause the server to use the regexner annotator and thus crash given the example sentence ""The quick brown fox jumps over the lazy dog."":
- named entities (regexner)
- coreference
- relations


## Using the latest version

I also used the following Dockerfile (`docker build -f Dockerfile.develop . -t corenlp:develop && docker --rm -p 9000:9000 corenlp:develop`) to run the latest version (this one takes a long time to build as it needs to download many things):
```Dockerfile
FROM openjdk:8-alpine

MAINTAINER Sebastian H√∂ffner <shoeffner@tzi.de>

ARG PORT=9000
ENV PORT=${PORT:-9000}
EXPOSE ${PORT}

RUN apk update \
    && apk add git maven \
    && git clone https://github.com/stanfordnlp/CoreNLP.git \
    && cd CoreNLP \
    && mvn package \
    && mv target/stanford-corenlp-*.jar .

RUN wget https://nlp.stanford.edu/software/stanford-corenlp-models-current.jar -P /CoreNLP \
    & wget https://nlp.stanford.edu/software/stanford-english-corenlp-models-current.jar -P /CoreNLP \
    & wget https://nlp.stanford.edu/software/stanford-english-kbp-corenlp-models-current.jar -P /CoreNLP

WORKDIR /CoreNLP

CMD java -cp '*' -Xmx8g edu.stanford.nlp.pipeline.StanfordCoreNLPServer
```

### Coreference & Relations

In this version, the same happens again for coreference and relations (log for relations):
```
--- StanfordCoreNLPServer#main() called ---
setting default constituency parser
warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz
using: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz instead
to use shift reduce parser download English models jar from:
http://stanfordnlp.github.io/CoreNLP/download.html
    Threads: 4
Starting server...
StanfordCoreNLPServer listening at /0.0.0.0:9000
[/172.17.0.1:52882] API call w/annotators tokenize,ssplit,pos,depparse,lemma,ner,coref,kbp
The quick brown fox jumps over the lazy dog.
Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.8 sec].
Adding annotator depparse
Loading depparse model: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... 
PreComputed 99996, Elapsed Time: 14.951 (s)
Initializing dependency parser ... done [16.8 sec].
Adding annotator lemma
Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [3.2 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.8 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [2.4 sec].
ner.fine.regexner: Read 580704 unique entries out of 581863 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_caseless.tab, 0 TokensRegex patterns.
ner.fine.regexner: Read 4869 unique entries out of 4869 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_cased.tab, 0 TokensRegex patterns.
ner.fine.regexner: Read 585573 unique entries from 2 files
```

### Named entities (regexner)

For the named entities (regexner) call `http://localhost:9000/?properties=%7B%22annotators%22%3A%20%22tokenize%2Cssplit%2Cregexner%22%2C%20%22date%22%3A%20%222018-11-09T19%3A15%3A55%22%7D&pipelineLanguage=en` the server returns a proper response in the latest version, but the client website does not seem to process it:
```json
{
  ""docDate"": ""2018-11-09T19:15:55"",
  ""sentences"": [
    {
      ""index"": 0,
      ""tokens"": [
        {
          ""index"": 1,
          ""word"": ""The"",
          ""originalText"": ""The"",
          ""characterOffsetBegin"": 0,
          ""characterOffsetEnd"": 3,
          ""before"": """",
          ""after"": "" ""
        },
        {
          ""index"": 2,
          ""word"": ""quick"",
          ""originalText"": ""quick"",
          ""characterOffsetBegin"": 4,
          ""characterOffsetEnd"": 9,
          ""before"": "" "",
          ""after"": "" ""
        },
        {
          ""index"": 3,
          ""word"": ""brown"",
          ""originalText"": ""brown"",
          ""characterOffsetBegin"": 10,
          ""characterOffsetEnd"": 15,
          ""before"": "" "",
          ""after"": "" ""
        },
        {
          ""index"": 4,
          ""word"": ""fox"",
          ""originalText"": ""fox"",
          ""characterOffsetBegin"": 16,
          ""characterOffsetEnd"": 19,
          ""before"": "" "",
          ""after"": "" ""
        },
        {
          ""index"": 5,
          ""word"": ""jumps"",
          ""originalText"": ""jumps"",
          ""characterOffsetBegin"": 20,
          ""characterOffsetEnd"": 25,
          ""before"": "" "",
          ""after"": "" ""
        },
        {
          ""index"": 6,
          ""word"": ""over"",
          ""originalText"": ""over"",
          ""characterOffsetBegin"": 26,
          ""characterOffsetEnd"": 30,
          ""before"": "" "",
          ""after"": "" ""
        },
        {
          ""index"": 7,
          ""word"": ""the"",
          ""originalText"": ""the"",
          ""characterOffsetBegin"": 31,
          ""characterOffsetEnd"": 34,
          ""before"": "" "",
          ""after"": "" ""
        },
        {
          ""index"": 8,
          ""word"": ""lazy"",
          ""originalText"": ""lazy"",
          ""characterOffsetBegin"": 35,
          ""characterOffsetEnd"": 39,
          ""before"": "" "",
          ""after"": "" ""
        },
        {
          ""index"": 9,
          ""word"": ""dog"",
          ""originalText"": ""dog"",
          ""characterOffsetBegin"": 40,
          ""characterOffsetEnd"": 43,
          ""before"": "" "",
          ""after"": """"
        },
        {
          ""index"": 10,
          ""word"": ""."",
          ""originalText"": ""."",
          ""characterOffsetBegin"": 43,
          ""characterOffsetEnd"": 44,
          ""before"": """",
          ""after"": """"
        }
      ]
    }
  ]
}

```

<img width=""1245"" alt=""image"" src=""https://user-images.githubusercontent.com/1836815/48280610-618fcd00-e454-11e8-80de-d09976608a0e.png"">


### Sentiment

For the sentiment annotator, I receive the following NoClassDefFoundError, but I might have made a mistake in setting up all dependencies properly. However, the server keeps running and doesn't time out on the client side ‚Äì¬†but a refresh fixes it:

```
--- StanfordCoreNLPServer#main() called ---
setting default constituency parser
warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz
using: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz instead
to use shift reduce parser download English models jar from:
http://stanfordnlp.github.io/CoreNLP/download.html
    Threads: 4
Starting server...
StanfordCoreNLPServer listening at /0.0.0.0:9000
[/172.17.0.1:52886] API call w/annotators tokenize,ssplit,pos,parse,sentiment
The quick brown fox jumps over the lazy dog.
Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.8 sec].
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.5 sec].
Adding annotator sentiment
Exception in thread ""pool-1-thread-1"" java.lang.NoClassDefFoundError: org/ejml/simple/SimpleBase
	at edu.stanford.nlp.pipeline.SentimentAnnotator.<init>(SentimentAnnotator.java:52)
	at edu.stanford.nlp.pipeline.AnnotatorImplementations.sentiment(AnnotatorImplementations.java:217)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$16(StanfordCoreNLP.java:534)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$30(StanfordCoreNLP.java:602)
	at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)
	at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
	at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:251)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:192)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:188)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.mkStanfordCoreNLP(StanfordCoreNLPServer.java:368)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.access$800(StanfordCoreNLPServer.java:50)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:855)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
	at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
	at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
	at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: org.ejml.simple.SimpleBase
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 22 more
```


### Typo for dependency parse

The server additionally crashes without any error indications when I supply ""The quick brown fox jumps over **th** lazy dog."" and try the dependency parse (it works fine without the typo ‚Äì¬†and I haven't tried this with the stable image or any other annotator):
```
[/172.17.0.1:52872] API call w/annotators tokenize,ssplit,pos,depparse
The quick brown fox jumps over th lazy dog.
Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Adding annotator depparse
Loading depparse model: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... 
```"
645,https://github.com/stanfordnlp/CoreNLP/issues/793,793,[],closed,2018-11-15 06:41:14+00:00,,Sentiment model training for different domain,"Hi
I tried following for generating model for my domain (product reviews) and it seems to be working (with a toy dataset). 

1. Generate sentiment-tree for train and dev data (using ""-output PENNTREES"" as mentioned at https://stackoverflow.com/a/42917906 )

2. The resulting tree needed some manual modifications because the ""unseen"" words were treated as neutral(2)
For example I had to modify below sentence: 
(2 (2 (2 Vendor) (2 (2 selling) (2 (2 defective) (2 products)))) (2 .))
By changing sentiment score for word ""defective"" (and its parents)
(1 (1 (2 Vendor) (1 (2 selling) (1 (1 defective) (2 products)))) (2 .))

3. Then I trained my model using the pruned train/dev files
java -mx8g edu.stanford.nlp.sentiment.SentimentTraining -numHid 25 -trainPath train.txt -devPath dev.txt -train -model my.model.ser.gz

4. Used my model for sentiment analysis
java -cp ""*"" -mx5g edu.stanford.nlp.sentiment.SentimentPipeline -stdin -output PENNTREES -sentimentModel my.model.ser.gz

I have some questions regarding training data:
1. Yeah, it does require some careful manual score replacement, but the standard approach is not understandable anyway  (https://github.com/stanfordnlp/CoreNLP/issues/742). So is this approach fine?
2. Should I train with just my domain specific input or will I be better off (will need to prepare smaller input) if I append my train/dev sentences to the supplied train/dev files
3. I notice there were some 'trivial' misses due to ""unseen"" words: like above example of ""defective"". If appending is fine, should I append a bunch of single-word 'sentences' (defective, sleek, classy etc) to get a easy improvement in accuracy. (just because it will be much easier/faster to prepare this training data) 

Kindly help
"
646,https://github.com/stanfordnlp/CoreNLP/issues/794,794,"[{'id': 45387508, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}]",closed,2018-11-15 08:46:17+00:00,,Clique in Stanford NER,"I am reading through the code for NER in CoreNLP and am unable to understand what a ""Clique"" is and why there are multiple feature extractors inside the NERFeatureFactory.getCliqueFeatures(). Can someone help me understand: 

**1. What is a clique?** 
**2. Why are there multiple helper methods to calculate the features for different types of cliques (cliqueX, cliquecp2c, etc.) ?** 

Thanks! 
"
647,https://github.com/stanfordnlp/CoreNLP/issues/796,796,[],open,2018-11-16 12:52:38+00:00,,How to use the neural coref model,"I was trying to use the coreference models as baselines. It went well with the 'dcoref' and 'statistical' coref model. But when I tried to use the 'neural' model, I received this bug:
```
[pool-1-thread-2] INFO CoreNLP - [/127.0.0.1:55494] API call w/annotators tokenize,ssplit,pos,lemma,ner,depparse,coref
Uh-huh . It happened that I was going to have lunch with a friend , um , at noon . And then , the friend first sent me an SMS , Uh-huh . saying he would come pick me up to go together .
[pool-1-thread-2] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[pool-1-thread-2] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[pool-1-thread-2] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[pool-1-thread-2] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.6 sec].
[pool-1-thread-2] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[pool-1-thread-2] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[pool-1-thread-2] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].
[pool-1-thread-2] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].
[pool-1-thread-2] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.6 sec].
[pool-1-thread-2] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
[pool-1-thread-2] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
[pool-1-thread-2] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 580704 unique entries out of 581863 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_caseless.tab, 0 TokensRegex patterns.
[pool-1-thread-2] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 4869 unique entries out of 4869 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_cased.tab, 0 TokensRegex patterns.
[pool-1-thread-2] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 585573 unique entries from 2 files
[pool-1-thread-2] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse
[pool-1-thread-2] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model: edu/stanford/nlp/models/parser/nndep/english_UD.gz ...
[pool-1-thread-2] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 8.211 (s)
[pool-1-thread-2] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [9.4 sec].
[pool-1-thread-2] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref
[pool-1-thread-2] INFO edu.stanford.nlp.coref.neural.NeuralCorefAlgorithm - Loading coref model edu/stanford/nlp/models/coref/neural/english-model-default.ser.gz ... done [0.4 sec].
[pool-1-thread-2] INFO edu.stanford.nlp.coref.neural.NeuralCorefAlgorithm - Loading coref embeddings edu/stanford/nlp/models/coref/neural/english-embeddings.ser.gz ... done [0.4 sec].
[pool-1-thread-2] INFO edu.stanford.nlp.pipeline.CorefMentionAnnotator - Error with building coref mention annotator!
[pool-1-thread-2] INFO edu.stanford.nlp.pipeline.CorefMentionAnnotator - java.lang.ClassNotFoundException: edu.stanford.nlp.hcoref.md.MentionDetectionClassifier
```
For your information, I am using the latest version (3.9.2) and I'm using the stanfordCoreNLP(https://github.com/Lynten/stanford-corenlp) to call the server.
```
nlp = StanfordCoreNLP(r'/Users/stanford-corenlp-full-2018-10-05/', quiet=False)
props = {'annotators': 'coref', 'coref.algorithm': 'neural', 'pipelineLanguage': 'en'}
```

Can anyone help? Thank you very much!"
648,https://github.com/stanfordnlp/CoreNLP/issues/797,797,[],open,2018-11-16 13:46:51+00:00,,Provide releases on GitHub?,"Hi!

I'm working on some Docker images involving CoreNLP which has unfortunately involved me having to download the latest release and the srparser models several times, with each attempt taking upwards of 40 minutes due to the speed of the main nlp.stanford.edu mirror.

In the interests of enabling people to download it faster (as well as reducing load on your servers!), one idea might be to put published releases on GitHub, using it as a secondary mirror? Just a thought!"
649,https://github.com/stanfordnlp/CoreNLP/issues/798,798,[],open,2018-11-16 15:40:32+00:00,,Invalid output when using ProtobufAnnotationSerializer,"I'm running CoreNLP server and I sent it this request:

`wget --post-data 'the quick brown fox jumped over the lazy dog' http://localhost:9000/?properties=%7B%22annotators%22%3A%22tokenize%2Cssplit%22%2C%22outputFormat%22%3A%22serialized%22%2C%22serializer%22%3A%22edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer%22%7D -O /tmp/output.bin`

The output is 455 bytes and looks like it might be correct at first glance:

```
$ xxd /tmp/output.bin 
00000000: c503 0a2c 7468 6520 7175 6963 6b20 6272  ...,the quick br
00000010: 6f77 6e20 666f 7820 6a75 6d70 6564 206f  own fox jumped o
00000020: 7665 7220 7468 6520 6c61 7a79 2064 6f67  ver the lazy dog
00000030: 128b 030a 240a 0374 6865 1a03 7468 652a  ....$..the..the*
00000040: 0032 0120 3a03 7468 6558 0060 0388 0100  .2. :.theX.`....
00000050: 9001 01a8 0100 b002 000a 2b0a 0571 7569  ..........+..qui
00000060: 636b 1a05 7175 6963 6b2a 0120 3201 203a  ck..quick*. 2. :
00000070: 0571 7569 636b 5804 6009 8801 0190 0102  .quickX.`.......
00000080: a801 00b0 0200 0a2b 0a05 6272 6f77 6e1a  .......+..brown.
00000090: 0562 726f 776e 2a01 2032 0120 3a05 6272  .brown*. 2. :.br
000000a0: 6f77 6e58 0a60 0f88 0102 9001 03a8 0100  ownX.`..........
000000b0: b002 000a 250a 0366 6f78 1a03 666f 782a  ....%..fox..fox*
000000c0: 0120 3201 203a 0366 6f78 5810 6013 8801  . 2. :.foxX.`...
000000d0: 0390 0104 a801 00b0 0200 0a2e 0a06 6a75  ..............ju
000000e0: 6d70 6564 1a06 6a75 6d70 6564 2a01 2032  mped..jumped*. 2
000000f0: 0120 3a06 6a75 6d70 6564 5814 601a 8801  . :.jumpedX.`...
00000100: 0490 0105 a801 00b0 0200 0a28 0a04 6f76  ...........(..ov
00000110: 6572 1a04 6f76 6572 2a01 2032 0120 3a04  er..over*. 2. :.
00000120: 6f76 6572 581b 601f 8801 0590 0106 a801  overX.`.........
00000130: 00b0 0200 0a25 0a03 7468 651a 0374 6865  .....%..the..the
00000140: 2a01 2032 0120 3a03 7468 6558 2060 2388  *. 2. :.theX `#.
00000150: 0106 9001 07a8 0100 b002 000a 280a 046c  ............(..l
00000160: 617a 791a 046c 617a 792a 0120 3201 203a  azy..lazy*. 2. :
00000170: 046c 617a 7958 2460 2888 0107 9001 08a8  .lazyX$`(.......
00000180: 0100 b002 000a 240a 0364 6f67 1a03 646f  ......$..dog..do
00000190: 672a 0120 3200 3a03 646f 6758 2960 2c88  g*. 2.:.dogX)`,.
000001a0: 0108 9001 09a8 0100 b002 0010 0018 0920  ............... 
000001b0: 0028 0030 2c98 0300 b003 0088 0400 5800  .(.0,.........X.
000001c0: 6800 7800 8001 00                        h.x....
```

However, the client I'm working on can't decode this as a `Document` message. It reports the error  ""proto: wrong wireType = 5 for field Sections"".

So I tried parsing the data with `protoc`, which also fals:
```
$ protoc --decode_raw < /tmp/output.bin
Failed to parse input.
```
It seems that the output is not valid protobuf data at all."
650,https://github.com/stanfordnlp/CoreNLP/issues/799,799,[],open,2018-11-16 17:12:23+00:00,,Using DepParse in conjunction with Quote in 3.9.x,"I'm using [poorna-kumar/gendermeme-core](https://www.github.com/poorna-kumar/gendermeme-core) in conjunction with CoreNLP and since upgrading to the latest version I am unable to make anything work due to the change to the Quote annotator.

The entirety of the annotator loadout I need is

```
['pos', 'lemma', 'ner', 'parse', 'depparse', 'dcoref', 'quote', 'openie']
```

(Which given that the first few get pulled in as deps can be simplified to `['depparse', 'dcoref', 'coref', 'openie', 'quote']`) ...but it seems the Quote and DepParse annotators are wholly incompatible.

I suppose I can downgrade to 3.8.x but I'd rather not if possible... Any suggestions? Thanks!"
651,https://github.com/stanfordnlp/CoreNLP/issues/800,800,[],closed,2018-11-27 14:06:37+00:00,,depparse hangs: Creating dependency tree...,"the latest version will not return from creating the dependency tree using the neural model for the following sentence:
```
And he was a key architect of Mr. Trump's populist 'America First' campaign agenda.
```"
652,https://github.com/stanfordnlp/CoreNLP/issues/801,801,[],closed,2018-11-30 03:18:07+00:00,,Requesting model for Hindi language,How can I develop a model for Hindi language for CoreNLP?
653,https://github.com/stanfordnlp/CoreNLP/issues/802,802,[],closed,2018-11-30 04:22:38+00:00,,Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.pipeline.StanfordCoreNLP,"I'm using Stanford NLP for sentiment analysis in my sbt project. I am getting the error when I run the project jar file on AWS EMR cluster, while it perfectly runs in the IntelliJ IDE."
654,https://github.com/stanfordnlp/CoreNLP/issues/803,803,[],closed,2018-11-30 17:33:34+00:00,,"Missing sanity check to prevent ""No roots in graph"" runtime exception","The following sentence is causing a runtime exception when generating universal dependencies.

**""(who provided me with above information).""** - from a real clinical note
  
Review of the existing class shows that the sanity check (or equivalent) exists in most methods:

    /* If the graph doesn't have a root (most likely because
     * a parsing error, we can't match Semgrexes, so do
     * nothing. */
    if (sg.getRoots().isEmpty())
      return;

Currently missing in demoteQuantificationalModifiers(SemanticGraph sg)

Will review methods and see where else the check should occur for that class."
655,https://github.com/stanfordnlp/CoreNLP/issues/804,804,[],open,2018-11-30 18:59:24+00:00,,OpenIE fails for sentence containing the word 'which',"When running OpenIE with a document contains the following sentence 'The first is a package for side-loading, followed by launcher and a tool that loads into process memory.' OpenIE crashes with

```
Caused by: java.lang.AssertionError
	at edu.stanford.nlp.naturalli.Util.cleanTree(Util.java:324)
	at edu.stanford.nlp.naturalli.OpenIE.annotateSentence(OpenIE.java:463)
	at edu.stanford.nlp.naturalli.OpenIE.lambda$annotate$2(OpenIE.java:547)
```
If we change the word 'which' to 'and' the error does not occur. 

We use the following settings and are running version 3.9.2:

props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,depparse,coref,natlog,openie"");
props.setProperty(""coref.algorithm"", ""statistical"");

Any idea how I can workaround this?

Regards,
Avon
"
656,https://github.com/stanfordnlp/CoreNLP/issues/806,806,[],closed,2018-12-03 10:49:06+00:00,,Stanford POS French Model: confusion between Homonym NOUN and VERB,"**The POS French Model not recognize Homonym verb noun**
Elle √©quipe la maison -> √©quipe is a verb but the POS tagger suggest NOUN
Elle danse tout le temps -> danse is a verb but the POS tagger suggest NOUN
elle est assembl√©e de cette fa√ßon -> assembl√©e is a verb but the POS tagger suggest NOUN
"
657,https://github.com/stanfordnlp/CoreNLP/issues/807,807,[],closed,2018-12-13 17:58:30+00:00,,Cannot get BinarizedTreeAnnotation,"Hi,
I cannot get BinarizedTreeAnnotation even with setting parameter in Stanford CoreNLP API:
""parse.binaryTrees"", ""true""!
Version: stanford-corenlp-full-2017-06-09
Any advice
Thanks for your help"
658,https://github.com/stanfordnlp/CoreNLP/issues/808,808,[],open,2018-12-15 19:18:23+00:00,,Inconsistent XML escaping of &lt; and &gt;,"In the public [NER demo](http://nlp.stanford.edu:8080/ner/process), I entered the text `& with""hi with 'my <<10 &lt; 10` to test the XML escaping (but it happens with other inputs too).

The result is strange. The `<` is being correctly escaped into `&lt` in the output, but then `&lt;` is left unescaped as `&lt;` instead of `&amp;lt;`:

<img width=""1440"" alt=""screen shot 2018-12-15 at 20 10 25"" src=""https://user-images.githubusercontent.com/610412/50046581-cff42a80-00a5-11e9-805b-49b80a26b221.png"">

This looks like a bug. There's no way to tell whether `&lt;` in the output should be unescaped to `<` or left as `&lt;`.

In the web demo, `'` and `""` are escaped into backticks (?), while when run via  normal API call, these seem to correctly escape into `&apos;` and `&quot;`. So no problem there.

I tried browsing the CoreNLP source to understand what's going on, but according to the [source code](https://github.com/stanfordnlp/CoreNLP/blob/eb43d5d9150de97f8061fa06b838f1d021586789/src/edu/stanford/nlp/util/XMLUtils.java#L937), it shouldn't be happening, all these entities should come out XML-escaped.

Can you point me to the actual code/logic for XML escaping the output? Especially for the normal API (not web demo) version. Thanks!"
659,https://github.com/stanfordnlp/CoreNLP/issues/809,809,[],closed,2018-12-16 23:40:33+00:00,,Set sentence ending tags on server,Is there a way to set the clean.sentenceendingtags property for the server?
660,https://github.com/stanfordnlp/CoreNLP/issues/811,811,[],open,2018-12-24 09:39:41+00:00,,"corefannotator ""coref"" requires annotation ""CategoryAnnotation"".","java.lang.IllegalArgumentException: annotator ""coref"" requires annotation ""CategoryAnnotation"". The usual requirements for this annotator are: tokenize,ssplit,pos,lemma,ner,depparse

when i try online chinese demo, it prints this error?"
661,https://github.com/stanfordnlp/CoreNLP/issues/812,812,[],open,2018-12-27 07:30:19+00:00,,Tokenization with UTF-8 literals and emoticons,"Hello,

I realize that while the tokenizer in the CoreNLP lib. (version 3.9.2) does support tokenizing on the emojis, there is no support for tokenizing on emoticons, or also for UTF-8 literals. Sharing few examples below:
Input 1: `""@Insanomania They do... Their mentality doesn't :(""` tokenizes as `[""@Insanomania"", ""They"", ""do"", ""..."", ""Their"", ""mentality"", ""does"", ""n't"", "":"" ,""(""]` while the expectation on the `:(` was to be treated as a single token.

Input 2: `""\xabNow that I can do.\xbb""` tokenizes as `[""\"", ""xabNow"", ""that"", ""I"", ""can"", ""do"", ""."", ""\"", ""xbb""]`"
662,https://github.com/stanfordnlp/CoreNLP/issues/813,813,[],closed,2018-12-27 09:48:44+00:00,,Issue with sentiment analyzer,"Hello,

I recently came across an interesting example where sentiment analysis on words containing negative words (like 'not', 'never') need not always indicate a negative sentiment. Example:
`""Sentiment analysis with VADER has never been this good.""` or `""This is not at all bad.""`
These should be tagged as a Positive sentiment. However, the default analyzer of the CoreNLP library marks them as Negative.

P.S. I've been using the web interface of CoreNLP (http://corenlp.run/), which is great for all the visualization that it provides :-)"
663,https://github.com/stanfordnlp/CoreNLP/issues/814,814,[],open,2018-12-27 20:02:12+00:00,,PTBTokenizer normalizeOtherBrackets option is changin CoreLabel.after() and CoreLabel.before(),"As per the StanfordCoreNLP [documentation for CoreLabel](https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/ling/CoreLabel.html), The functions **after()** and **before()** should return white space strings between the token and the next/previous tokens respectively.
However, they return an empty string always even if there are some white spaces when the tokenizer option **normalizeOtherBrackets** is equal to **false**."
664,https://github.com/stanfordnlp/CoreNLP/issues/815,815,[],closed,2019-01-02 12:11:45+00:00,,Difference between Stanford POS tagger and POS tagger from Stanford Parser,"What is the difference between Stanford POS tagger and POS tagger in Stanford Parser? I am getting different results from these two services for the same sentence. 

**Example:** 

**Fires** are the high-temperature phenomenon of relatively local extent.

Result from Pos tagger: 

The term **Fires** has captured as **VBZ**
![image](https://user-images.githubusercontent.com/39255595/50591258-a9f7c400-0eb4-11e9-80d6-3de0ae4233aa.png)

Result from Stanford parser POS tagger:

The term **Fires** has captured as **NNP**
![image](https://user-images.githubusercontent.com/39255595/50591365-44f09e00-0eb5-11e9-96d1-a8b133d933c2.png)








"
665,https://github.com/stanfordnlp/CoreNLP/issues/816,816,[],closed,2019-01-02 12:14:31+00:00,,I want to modify NER 3 class tagger to recognize some specific words to be PERSON entity. ,"I want to customize the CoreNLP NER functionality for my research experiment.   

What I want to add is some explcit NER rule to current NER module which will enable the module recognize several mentioning words like father, grandmother and etc. as <code>PERSON</code> entity. (I have a set of several word lists for this) 

Could you specify the part of the code I need to take a look at? Or is it impossible to do this because the NER taggers does not contain such a explicit rules checking for person names?"
666,https://github.com/stanfordnlp/CoreNLP/issues/817,817,[],open,2019-01-02 18:27:31+00:00,,How to get .json output to readable grapics,"Hello,

I'm new to this and am intrigued by the finished product on the website. I can get the program running to the point where it produces the .json files. However, I'm a bit stuck when it comes to creating a readable file such as what is shown on the website. Is there a trick to getting the .json to a graphic that shows dependencies?

Thanks!"
667,https://github.com/stanfordnlp/CoreNLP/issues/818,818,[],closed,2019-01-05 13:04:10+00:00,,/http://nlp.stanford.edu:8080/parser not uploading,"Hello,

I am trying to upload the /http://nlp.stanford.edu:8080/ website for a project I have to submit though it does not upload.

Is there an estimation for when it should work?

Thank you very much"
668,https://github.com/stanfordnlp/CoreNLP/issues/820,820,[],closed,2019-01-14 08:39:25+00:00,,Stop server prints to the console,"I tried adding the `-quiet` to the Java command line when running the StanfordCoreNLPServer.
I suppressed the prints of the text input, but I still get:
`[/127.0.0.1:60508] API call w/annotators tokenize,ssplit,pos,parse`
printout.
Prints may slow down and reduce performance. Is there way to stop these prints?"
669,https://github.com/stanfordnlp/CoreNLP/issues/821,821,[],open,2019-01-15 21:24:41+00:00,,Poor performance with Spanish parser?,"Not sure where to post this but I've found that the Spanish sentence parsers are struggling with basic sentences, usually involving recursion. I provide an example below. I'm using the default parser available [here](http://nlp.stanford.edu/software/stanford-spanish-corenlp-2018-10-05-models.jar).

```
# in java
java -Xmx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-spanish.properties -preload tokenize,ssplit,pos,ner -status_port 9003  -port 9003 -timeout 45000

# in python
from nltk.parse.corenlp import CoreNLPParser
parser = CoreNLPParser(url='http://localhost:9003', tagtype='ner')
test_sent = 'El n√∫mero telef√≥nico de Centro de Ayuda es el 123-456-7890.'
test_parse = next(parser.parse(parser.tokenize(test_sent)))
test_parse.pprint()
```

Output:
```
(ROOT
  (sentence
    (sn
      (grup.nom
        (DET El)
        (NOUN n√∫mero)
        (ADJ telef√≥nico)
        (ADP de)
        (PROPN Centro)
        (ADP de)
        (PROPN Ayuda)
        (VERB es)
        (DET el)
        (NOUN 123-456-7890)))
    (PUNCT .)))
```

In the provided tree I'm not sure why the entire sentence is being tagged as `grup.nom`. A similar problem happens with other sentences that are parsed as `interjeccio`."
670,https://github.com/stanfordnlp/CoreNLP/issues/822,822,[],closed,2019-01-17 16:27:47+00:00,,Is it possible to set Document Title without using XML input text?,"One of the core annotators DocTitleAnnotation appears to set the document title (if any) https://github.com/stanfordnlp/CoreNLP/blob/34873a01ddd56415e34d2f7338496e3d0c1d19a0/src/edu/stanford/nlp/pipeline/TextOutputter.java#L60

Is there anyway to set the document title via the API without having to use an XML document as the source of the document text?

I want to see if having the document title tagged as such makes any difference in annotator performance. In an ideal world annotators such as Truecase would apply different rules to a document title vs the document body. 

TIA"
671,https://github.com/stanfordnlp/CoreNLP/issues/823,823,[],closed,2019-01-22 12:45:46+00:00,,Is ther way to untokenize?,It is relatively easy to create some code that manually removes space in needed places but how it is plausible to uppercase names that can be center of the sentence? How about abbreviations that should be in uppercase like DL (deep learning)?
672,https://github.com/stanfordnlp/CoreNLP/issues/824,824,[],open,2019-01-23 16:13:20+00:00,,Wrong depedency structure while running corenlp locally.,"While generating dependency tree for below sentence , I am getting different dependencies structure with enhanced dependencies in comparison to the visualization I am getting  from [corenlp demo] (http://nlp.stanford.edu:8080/corenlp/process).  Seems like later one is the correct .  May I know, which version of corenlp, this demo website is using. 


**The board of directors of Accord new Group Limited announced that Dr. Shang Li has been re-designated as an executive director of the company, Dr. Jesse Zhixi Fang has been appointed as an independent non-executive director of the company, and Dr. Shang has resigned as a member of the remuneration committee of the company and Dr. Fang has been appointed as a member of the remuneration committee of the company.**


Thanks,
Saurabh"
673,https://github.com/stanfordnlp/CoreNLP/issues/825,825,[],open,2019-01-25 12:51:21+00:00,,the location effects the ner result? or the ner analysis has some bugs?,"ref the test result below, the named entity recognition result seems a little wired or some...

|text| ""‰∏≠ÂõΩ‰∏≠Â§ÆÂπøÊí≠ÁîµÂè∞‰∏≠Â§Æ‰∫∫Ê∞ëÂåó‰∫¨‰∏äÊµ∑ÂπøÂ∑ûÊ∑±Âú≥ÁæéÂõΩ‰∫∫Ê∞ë""|
|word|ner|
|‰∏≠ÂõΩ| ORGANIZATION|
|‰∏≠Â§Æ| ORGANIZATION|
|ÂπøÊí≠| ORGANIZATION|
|ÁîµÂè∞| ORGANIZATION|
|‰∏≠Â§Æ| FACILITY|
|‰∫∫Ê∞ë| FACILITY|
|Âåó‰∫¨| FACILITY|
|‰∏äÊµ∑| FACILITY|
|ÂπøÂ∑û| FACILITY|
|Ê∑±Âú≥| FACILITY|
|ÁæéÂõΩ| FACILITY|
|‰∫∫Ê∞ë| FACILITY|

|text: ""CHINA,USA,U.S.A.,Ireland,JAPAN,Japan,Japanese,California""|
|word|ner|
|china|ORGANIZATION|
|USA|O|
|U.S.A.|O|
|Ireland|O|
|JAPAN|O|
|Japan|O|
|Japanese|O|
|California|O|"
674,https://github.com/stanfordnlp/CoreNLP/issues/826,826,[],closed,2019-01-27 05:31:06+00:00,,NullPointerException with tokensregex,"I get the following error with tokensregex in 3.9.2 (also got same result in 3.8)

```
java.lang.NullPointerException
	at edu.stanford.nlp.pipeline.DeterministicCorefAnnotator.setNamedEntityTagGranularity(DeterministicCorefAnnotator.java:93)
	at edu.stanford.nlp.pipeline.DeterministicCorefAnnotator.annotate(DeterministicCorefAnnotator.java:101)
	at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:637)
at eu.fbk.dkm.pikes.tintop.AnnotationPipeline.parseAll(AnnotationPipeline.java:1081)
	at eu.fbk.dkm.pikes.tintop.AnnotationPipeline.parseAll(AnnotationPipeline.java:1048)
	at eu.fbk.dkm.pikes.tintop.AnnotationPipeline.parseFromString(AnnotationPipeline.java:1114)
	at eu.fbk.dkm.pikes.tintop.FolderOrchestrator$LocalTintopClient.run(FolderOrchestrator.java:115)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
```

I saw somewhere that it might be because CoreNLP is pointed to the wrong model file. Is there a way to check that? This CoreNLP is integrated into a larger program (Pikes), so it is hard for me to know what is pointing where. 

"
675,https://github.com/stanfordnlp/CoreNLP/issues/827,827,[],closed,2019-01-28 20:46:59+00:00,,Comma is a NER LOCATION?,"The `""ner""` annotator finds the following entities in `""Washington, DC is the capital of the USA.""`:
```
   'entitymentions': [{'docTokenBegin': 0,
     'docTokenEnd': 1,
     'tokenBegin': 0,
     'tokenEnd': 1,
     'text': 'Washington',
     'characterOffsetBegin': 0,
     'characterOffsetEnd': 10,
     'ner': 'STATE_OR_PROVINCE'},
    {'docTokenBegin': 1,
     'docTokenEnd': 2,
     'tokenBegin': 1,
     'tokenEnd': 2,
     'text': ',',
     'characterOffsetBegin': 10,
     'characterOffsetEnd': 11,
     'ner': 'LOCATION'},
    {'docTokenBegin': 2,
     'docTokenEnd': 3,
     'tokenBegin': 2,
     'tokenEnd': 3,
     'text': 'DC',
     'characterOffsetBegin': 12,
     'characterOffsetEnd': 14,
     'ner': 'STATE_OR_PROVINCE'},
    {'docTokenBegin': 8,
     'docTokenEnd': 9,
     'tokenBegin': 8,
     'tokenEnd': 9,
     'text': 'USA',
     'characterOffsetBegin': 37,
     'characterOffsetEnd': 40,
     'ner': 'COUNTRY'}],
```
Why is comma `"",""` identified as a `""LOCATION""`?"
676,https://github.com/stanfordnlp/CoreNLP/issues/828,828,[],open,2019-01-29 10:12:02+00:00,,ner.applyFineGrained and PERSON entity annotation,"When using `ner.applyFineGrained` set to `true` the NER annotator will get confused in some circumstances like in this phrase

```sh
George Washington went to Washington
```

in this case the term `George` will have any annotation i.e. a `O` value in the output:

```json
{
	""sentences"": [{
				""index"": 0,
				""text"": ""George Washington went to Washington"",
				""line"": 1,
				""sentimentValue"": ""1"",
				""tokens"": [{
						""index"": 1,
						""word"": ""George"",
						""characterOffsetBegin"": 0,
						""characterOffsetEnd"": 6,
						""before"": """",
						""after"": "" "",
						""pos"": ""NNP"",
						""ner"": ""O"",
						""lemma"": ""George""
					},
					{
						""index"": 2,
						""word"": ""Washington"",
						""characterOffsetBegin"": 7,
						""characterOffsetEnd"": 17,
						""before"": "" "",
						""after"": "" "",
						""pos"": ""NNP"",
						""ner"": ""STATE_OR_PROVINCE""
					},
					{
						""index"": 3,
						""word"": ""went"",
						""characterOffsetBegin"": 18,
						""characterOffsetEnd"": 22,
						""before"": "" "",
						""after"": "" "",
						""pos"": ""VBD"",
						""ner"": ""O""
					},
					{
						""index"": 4,
						""word"": ""to"",
						""characterOffsetBegin"": 23,
						""characterOffsetEnd"": 25,
						""before"": "" "",
						""after"": "" "",
						""pos"": ""TO"",
						""ner"": ""O""
					},
					{
						""index"": 5,
						""word"": ""Washington"",
						""characterOffsetBegin"": 26,
						""characterOffsetEnd"": 36,
						""before"": "" "",
						""after"": """",
						""pos"": ""NNP"",
						""ner"": ""STATE_OR_PROVINCE""
					}
				]
			}
```

While when set to `false`, the Annotator will correctly detect the NER `George`, so the output will look like

```json
{
	""sentences"": [{
		""index"": 0,
		""text"": ""George Washington went to Washington"",
		""line"": 1,
		""sentimentValue"": ""1"",
		""tokens"": [{
				""index"": 1,
				""word"": ""George"",
				""characterOffsetBegin"": 0,
				""characterOffsetEnd"": 6,
				""before"": """",
				""after"": "" "",
				""pos"": ""NNP"",
				""ner"": ""PERSON"",
				""lemma"": ""George"",
				""phoneme"": "" §…îÀà…π §"",
			},
			{
				""index"": 2,
				""word"": ""Washington"",
				""characterOffsetBegin"": 7,
				""characterOffsetEnd"": 17,
				""before"": "" "",
				""after"": "" "",
				""pos"": ""NNP"",
				""ner"": ""PERSON"",
				""lemma"": ""Washington"",
			},
			{
				""index"": 3,
				""word"": ""went"",
				""characterOffsetBegin"": 18,
				""characterOffsetEnd"": 22,
				""before"": "" "",
				""after"": "" "",
				""pos"": ""VBD"",
				""ner"": ""O"",
				""lemma"": ""go""
			},
			{
				""index"": 4,
				""word"": ""to"",
				""characterOffsetBegin"": 23,
				""characterOffsetEnd"": 25,
				""before"": "" "",
				""after"": "" "",
				""pos"": ""TO"",
				""ner"": ""O"",
				""lemma"": ""to""
			},
			{
				""index"": 5,
				""word"": ""Washington"",
				""characterOffsetBegin"": 26,
				""characterOffsetEnd"": 36,
				""before"": "" "",
				""after"": """",
				""pos"": ""NNP"",
				""ner"": ""LOCATION"",
				""lemma"": ""Washington""
			}
		]
	}]
}
```

Any reason for this behavior?"
677,https://github.com/stanfordnlp/CoreNLP/issues/829,829,[],closed,2019-02-03 14:51:02+00:00,,[tokensregex] how to define a regular expression for matching multiple tokens?,"How to write a regular expression for matching multiple tokens?
I tried `(?m){n,m} /abc/` as mentioned in the [document](https://nlp.stanford.edu/software/tokensregex.html#Usage), but it does not work.

```
{ ruleType: ""tokens"",
  pattern: ([word:(?2){2,2}/Our company/]),
  action: Annotate($0, ner, ""COMPANY""),
  result: ""COMPANY_RESULT"" }
```

The above rule causes an exception `edu.stanford.nlp.ling.tokensregex.parser.ParseException`."
678,https://github.com/stanfordnlp/CoreNLP/issues/830,830,[],closed,2019-02-08 20:44:00+00:00,,Running edu.stanford.nlp.pipeline.StanfordCoreNLPServer without depparse yields dependencies,"Running edu.stanford.nlp.pipeline.StanfordCoreNLPServer with tokenize,ssplit,pos,parse yields basicDependencies, enhancedDependencies, enhancedPlusPlusDependencies when I did not specify depparse.

However, when I do specify depparse, I  also get: basicDependencies, enhancedDependencies, enhancedPlusPlusDependencies, but with different results (different dependencies).

Should it be like that? If we don't specify the depparse annotation, what is the default dep parser we get?"
679,https://github.com/stanfordnlp/CoreNLP/issues/831,831,[],open,2019-02-09 20:55:39+00:00,,Intermittent PC lockups when running CoreNLP server,"On Ubuntu 18.04 I have been experiencing intermittent PC lockups (requiring a hard-reset) when running the CoreNLP server.  A few times they have occurred when starting the server and no other software is running (ie.. just startup, no software calling the server).  Other times the server has run fine for hours under very heavy workloads (ie.. 28-threads of a python client, each continuously sending  sentences to the server for parsing).  In the past, with earlier versions of CoreNLP, Java and Ubuntu, I haven't seen these issues and I'm looking for some ideas on what might be going on.  Unfortunately, no entries in /var/log/syslog point to the failure and there are no crash logs or cores left by the server that I have found.  When the crash happens the PC simply hangs for a few seconds and then reboots.

The command I'm using to run the server is...
```
CORENLP=""/home/bjascob/Libraries/StanfordNLP/stanford-corenlp-full-2018-10-05""
java -Xmx4g -cp ""$CORENLP/*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \
    --port 50001 \
    --preload tokenize,ssplit,pos,lemma,ner,parse \
    --ner.buildEntityMentions 0 \
    --ner.applyFineGrained 0 \
    --quiet
```
I'm using Oracle Java version 11.0.2 2018-10-16 LTS.  

With the exception of CoreNLP, my workstation has had no other issues.  It's been performing solidly for over a year and I've done simple things like checking the memory and turning off hyper-threading.  At this point I believe the issue in the this software package.

Are you aware of any issues related to lockups, in this configuration?  Any thoughts on what, in this code, could cause a complete and imeadiate lockup?  Any ideas on how I might track down the root cause?  I've been fighting with this for some time now and am looking for ideas on what to try next.

"
680,https://github.com/stanfordnlp/CoreNLP/issues/832,832,[],open,2019-02-10 01:39:08+00:00,,UniversalDependencyConverter does not generate head for a token in a parsing tree,"Hi!

I'm getting an unexpected output when I use the UniversalDependencyConverter module to generate the UD from a parsing tree. As you can see, **line 18** does not have a head in the below command output (it has a `_` instead). Do you know what could be causing this? 
```
$ CORENLP_HOME=""/home/gaguil20/tools/stanford-corenlp-full-2018-10-05""
$ java -cp ""$CORENLP_HOME/*"" \
       -mx5g edu.stanford.nlp.trees.ud.UniversalDependenciesConverter \
       -outputRepresentation enhanced++ \
       -treeFile parsing.txt

1	What	what	PRON	WP	_	3	nsubj	3:nsubj	_
2	's	be	VERB	VBZ	_	3	cop	3:cop	_
3	interesting	interesting	ADJ	JJ	_	14	dep	14:dep	_
4	,	,	PUNCT	,	_	14	punct	14:punct	_
5	I	I	PRON	PRP	_	6	nsubj	6:nsubj	_
6	mean	mean	VERB	VBP	_	18	parataxis	14:dep|18:parataxis	_
7	,	,	PUNCT	,	_	14	punct	14:punct	_
8	reading	read	VERB	VBG	_	14	dep	14:dep	_
9	the	the	DET	DT	_	10	det	10:det	_
10	article	article	NOUN	NN	_	8	dobj	8:dobj	_
11	,	,	PUNCT	,	_	14	punct	14:punct	_
12	basically	basically	ADV	RB	_	14	dep	14:dep	_
13	a	a	DET	DT	_	17	det:qmod	17:det:qmod	_
14	lot	lot	NOUN	NN	_	13	mwe	13:mwe	_
15	of	of	ADP	IN	_	13	mwe	13:mwe	_
16	the	the	DET	DT	_	17	det	17:det	_
17	celebrities	celebrity	NOUN	NNS	_	0	root	0:root	_
18	paid	pay	VERB	VBN	_	_	_	_	_
19	by	by	ADP	IN	_	20	case	20:case	_
20	him	he	PRON	PRP	_	18	nmod:by	18:nmod:by	_
21	to	to	PART	TO	_	22	mark	22:mark	_
22	attend	attend	VERB	VB	_	18	xcomp	18:xcomp	_
23	charity	charity	NOUN	NN	_	24	compound	24:compound	_
24	functions	function	NOUN	NNS	_	22	dobj	22:dobj	_
25	.	.	PUNCT	.	_	14	punct	14:punct	_
```

The `parsing.txt` file contains the following parsing tree (taken from the Broadcast News section of the OntoNotes 5.0):

```
(TOP
    (FRAG
        (SBAR
            (WHNP
                (WP What))
            (S
                (VP
                    (VBZ 's)
                    (ADJP 
                        (JJ interesting)))))
        (, ,)
        (PRN
            (S
                (NP
                    (PRP I))
                (VP
                    (VBP mean))))
        (, ,)
        (S
            (VP
                (VBG reading)
                (NP
                    (DT the)
                    (NN article))))
        (, ,)
        (ADVP
            (RB basically))
        (NP
            (NP
                (DT a)
                (NN lot))
            (PP
                (IN of)
                (NP
                    (DT the)
                    (NNS celebrities))))
        (VP
            (VBN paid)
            (PP
                (IN by)
                (NP
                    (PRP him)))
            (S
                (VP
                    (TO to)
                (VP
                    (VB attend)
                    (NP
                        (NN charity)
                        (NNS functions))))))
    (. .)))

```

I appreciate your help, and thank you for such a great tool!

Regards,
Gustavo

**[UPDATE]**: I'm running this command on Red Hat 7.6 and the OpenJDK version is 1.8  "
681,https://github.com/stanfordnlp/CoreNLP/issues/833,833,[],closed,2019-02-12 06:01:18+00:00,,CoreAnnotations.NumerizedTokensAnnotation,"I tried to annotate numbers in a sentence in below way. But, it didn't work.

https://github.com/stanfordnlp/CoreNLP/issues/199#issuecomment-223178447

Please check the below code:
```
props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"");
props.setProperty(""ner.useSUTime"", ""false"");
props.setProperty(""ner.applyFineGrained"", ""false"");
props.setProperty(""ner.buildEntityMentions"", ""true"");
pipeline = new StanfordCoreNLP(props);
Annotation annotation = new Annotation(""There have been 43 presidents."");
pipeline.annotate(annotation);
foreach (CoreMap sent in annotation.get(typeof(CoreAnnotations.SentencesAnnotation)) as ArrayList)
{
         foreach (CoreMap token in sent.get(typeof(CoreAnnotations.NumerizedTokensAnnotation)) as ArrayList)
             {
                   //Always sent.get(typeof(CoreAnnotations.NumerizedTokensAnnotation)) returning null
             }
}
```

sent.keySet()
[edu.stanford.nlp.util.ArrayCoreMap.1]: {[class edu.stanford.nlp.ling.CoreAnnotations$TextAnnotation, class edu.stanford.nlp.ling.CoreAnnotations$CharacterOffsetBeginAnnotation, class edu.stanford.nlp.ling.CoreAnnotations$CharacterOffsetEndAnnotation, class edu.stanford.nlp.ling.CoreAnnotations$TokensAnnotation, class edu.stanford.nlp.ling.CoreAnnotations$SentenceIndexAnnotation, class edu.stanford.nlp.ling.CoreAnnotations$TokenBeginAnnotation, class edu.stanford.nlp.ling.CoreAnnotations$TokenEndAnnotation, class edu.stanford.nlp.ling.CoreAnnotations$MentionsAnnotation]}

NumerizedTokensAnnotation is always returning null value.
Any changes required in my code to make it work.
"
682,https://github.com/stanfordnlp/CoreNLP/issues/834,834,[],closed,2019-02-13 07:55:39+00:00,,re-train new model throws java.lang.NumberFormatException,"I'm having problem with CoreNLP when feed my own data for re-train.
I get the following error:
Exception in thread ""main"" java.lang.NumberFormatException: For input string: ""Pouring""
        at java.lang.NumberFormatException.forInputString(Unknown Source)
        at java.lang.Integer.parseInt(Unknown Source)
        at java.lang.Integer.valueOf(Unknown Source)
        at edu.stanford.nlp.sentiment.SentimentUtils.attachLabels(SentimentUtils.java:37)
        at edu.stanford.nlp.sentiment.SentimentUtils.readTreesWithLabels(SentimentUtils.java:69)
        at edu.stanford.nlp.sentiment.SentimentUtils.readTreesWithGoldLabels(SentimentUtils.java:50)
        at edu.stanford.nlp.sentiment.SentimentTraining.main(SentimentTraining.java:184)

I found the sentences which contain ""Pouring"" and feed it one by one to the PENNTREE parser and everyone went fine to a bracket tree etc.
Anyway I did remove all sentences containing ""Pouring"", but then i get the same error, but for another strings.
My data set files are in ANSI format too.
Does anyone have similar problems ?"
683,https://github.com/stanfordnlp/CoreNLP/issues/835,835,[],open,2019-02-13 14:42:46+00:00,,SUTime does not identify invalid date,"Taking an example date of ""Feb 29, 2015"", the times values returned are 

`<TIMEX3 range=""(2019-02-01,2019-02-28,P1M)"" tid=""t1"" type=""DATE"" value=""2019-02"">Feb</TIMEX3>`

`<TIMEX3 range=""(2015-01-01,2015-12-31,P1Y)"" tid=""t2"" type=""DATE"" value=""2015"">2015</TIMEX3>`

Env: Using the stanford-corenlp version 3.7.0 in Java
We do need the includeRanges property to be set to true and use the standard unmodified english.sutime.txt and refs.sutime.txt files

Is there a parameter/configuration that just return ""invalid date"". ?"
684,https://github.com/stanfordnlp/CoreNLP/issues/836,836,[],closed,2019-02-16 15:26:45+00:00,,"POS verb error for the sequence NN,PRP,IN","This affected two sentences out of a body of 473.


Sentence: You must find pertinent objects, factor them into classes at the right granularity, define class interfaces and inheritance hierarchies, and establish key relationships among them.

You	PRP
must	MD
find	VB
pertinent	JJ
objects	NNS
,	,
**factor	NN (perhaps VB instead)
them	PRP
into	IN**
classes	NNS
at	IN
the	DT
right	JJ
granularity	NN
,	,
define	VB
class	NN
interfaces	NNS
and	CC
inheritance	NN
hierarchies	NNS
,	,
and	CC
establish	VB
key	JJ
relationships	NNS
among	IN
them	PRP
.	.

Sentence: Chain-of-Responsibility handles requests by forwarding them from one object to another along a chain of objects.

Chain-of-Responsibility	NN
handles	VBZ
requests	NNS
by	IN
**forwarding	NN (perhaps VB? instead)
them	PRP
from	IN**
one	CD
object	NN
to	TO
another	DT
along	IN
a	DT
chain	NN
of	IN
objects	NNS
.	.
"
685,https://github.com/stanfordnlp/CoreNLP/issues/837,837,[],closed,2019-02-17 06:25:06+00:00,,I'm running edu.stanford.nlp.sentiment.SentimentPipeline and getting below error,"[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.5 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator sentiment
Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: java.lang.ClassNotFoundException: edu.stanford.nlp.rnn.SimpleTensor
	at edu.stanford.nlp.sentiment.SentimentModel.loadSerialized(SentimentModel.java:633)
	at edu.stanford.nlp.pipeline.SentimentAnnotator.<init>(SentimentAnnotator.java:52)
	at edu.stanford.nlp.pipeline.AnnotatorImplementations.sentiment(AnnotatorImplementations.java:217)"
686,https://github.com/stanfordnlp/CoreNLP/issues/838,838,[],closed,2019-02-25 11:49:10+00:00,,How to use edu.stanford.nlp.parser.nndep.DependencyParser to parse raw text line by line ,"I use the command ""java -Xmx8g  edu.stanford.nlp.parser.nndep.DependencyParser     -model edu/stanford/nlp/models/parser/nndep/english_UD.gz     -textFile input.txt -outFile output.txt "" to parse my input.txt one line for one sentence, but filed. There are 250000 sentences in my input.txt, but the result of command is ""Parsed 24994 sentences in 136.33 seconds (183.34 sents/sec).""

How to use edu.stanford.nlp.parser.nndep.DependencyParser to parse raw text line by line ?
 I use the edu.stanford.nlp.parser.nndep.DependencyParser in the Stanford CoreNLP 3.9.2, the model is edu/stanford/nlp/models/parser/nndep/english_UD.gz 
Thanks."
687,https://github.com/stanfordnlp/CoreNLP/issues/839,839,[],open,2019-02-26 16:10:08+00:00,,QuoteAttributionAnnotator doesn't use provided CHARACTERS_FILE ,"I tried providing my own character list so that QuoteAttributionAnnotator could use it as instructed. On turning verbose log on, it prints that it received the path, but it doesn't use it. Also, if I actually provide a non existent file, it doesn't crash.

I think that the if condition in the code below is wrong, and that it should check if I provided a CHARACTERS_FILE as characterMap will always be null at that point; at least according to my understanding and IntelliJ's intellisense.

https://github.com/stanfordnlp/CoreNLP/blob/f83b3243e070431a9724411175d732564c5e978a/src/edu/stanford/nlp/pipeline/QuoteAttributionAnnotator.java#L183"
688,https://github.com/stanfordnlp/CoreNLP/issues/840,840,[],open,2019-02-27 11:19:38+00:00,,Provide example use case for ssplit.boundaryFollowersRegex,"The description in the document is a bit unclear. A sample use case would be very useful.
CoreNLP version: 3.9.2"
689,https://github.com/stanfordnlp/CoreNLP/issues/841,841,[],closed,2019-02-27 11:36:14+00:00,,constituency parse,"Dear All
I would like to ask you how to get access to the constituency parse, the same as the one displayed by http://corenlp.run/ webpage. I am using stanford-corenlp-full-2018-10-05 and I dont get always the same.
Where can I find this constituency parse?
Many thanks
valery"
690,https://github.com/stanfordnlp/CoreNLP/issues/843,843,[],open,2019-03-01 16:05:41+00:00,,About ParserAnnotator KBestTreesAnnotation,"I don't know how to use NLTK to get kbest constituency parse of sentences, Is anyone know? Thank you very much!"
691,https://github.com/stanfordnlp/CoreNLP/issues/844,844,[],closed,2019-03-05 22:19:57+00:00,,Precise commit hash release mapping?,"Does version x.y.z correspond to a precise commit? I see there are commits that specifically update release notes, but is there anything more precise?

Also, are releases ever changed or do you always increment version as a rule?

Thanks."
692,https://github.com/stanfordnlp/CoreNLP/issues/845,845,[],closed,2019-03-07 03:49:26+00:00,,simpleBase local class incompatible,"When I load the sentiment annotation into my project and init the StanfordCoreNLP object, the following exception always happens

`Invocation of init method failed; nested exception is edu.stanford.nlp.io.RuntimeIOException: java.io.InvalidClassException: org.ejml.simple.SimpleBase; local class incompatible: stream classdesc serialVersionUID = 7560584869544985034, local class serialVersionUID = -4908174115141247692`

The coreNLP version I am using is 3.9.2, which need to involve the ejml at version 0.30 (the same with the coreNLP dependency of v3.9.2), but I tried several versions of ejml, the only version which doesn't trigger this exception is v0.23, while the issue ""version conflict"" will happens...

It's a little confuse me that which version of ejml I need to involve here? or, does the coreNLP 3.9.2 a stable version?"
693,https://github.com/stanfordnlp/CoreNLP/issues/846,846,[],open,2019-03-07 11:41:24+00:00,,AttributeError: 'list' object has no attribute 'values',"Hi,

I'm using giveme5w1h library that utilise the corenlp library, then this error occurred. I had opened an issue with the giveme5w1h Github page, [giveme5w1h issue#32](https://github.com/fhamborg/Giveme5W1H/issues/32), the author @fhamborg advised that there may be an issue with the corenlp.

```
During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 601, in urlopen
    chunked=chunked)
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/usr/lib/python3.6/http/client.py"", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""/usr/lib/python3.6/http/client.py"", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""/usr/lib/python3.6/http/client.py"", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""/usr/lib/python3.6/http/client.py"", line 1026, in _send_output
    self.send(msg)
  File ""/usr/lib/python3.6/http/client.py"", line 964, in send
    self.connect()
  File ""/usr/lib/python3/dist-packages/urllib3/connection.py"", line 166, in connect
    conn = self._new_conn()
  File ""/usr/lib/python3/dist-packages/urllib3/connection.py"", line 150, in _new_conn
    self, ""Failed to establish a new connection: %s"" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7ff3af333c18>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/requests/adapters.py"", line 449, in send
    timeout=timeout
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/usr/lib/python3/dist-packages/urllib3/util/retry.py"", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=9000): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff3af333c18>: Failed to establish a new connection: [Errno 111] Connection refused',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/pycorenlp/corenlp.py"", line 19, in annotate
    requests.get(self.server_url)
  File ""/usr/local/lib/python3.6/dist-packages/requests/api.py"", line 75, in get
    return request('get', url, params=params, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/requests/api.py"", line 60, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/requests/sessions.py"", line 533, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/requests/sessions.py"", line 646, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/requests/adapters.py"", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=9000): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff3af333c18>: Failed to establish a new connection: [Errno 111] Connection refused',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/flask/app.py"", line 2292, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python3.6/dist-packages/flask/app.py"", line 1815, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python3.6/dist-packages/flask/app.py"", line 1718, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python3.6/dist-packages/flask/_compat.py"", line 35, in reraise
    raise value
  File ""/usr/local/lib/python3.6/dist-packages/flask/app.py"", line 1813, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python3.6/dist-packages/flask/app.py"", line 1799, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python3.6/dist-packages/Giveme5W1H/examples/extracting/server.py"", line 101, in extract
    extractor.parse(document)
  File ""/usr/local/lib/python3.6/dist-packages/Giveme5W1H/extractor/extractor.py"", line 104, in parse
    self.preprocess(doc)
  File ""/usr/local/lib/python3.6/dist-packages/Giveme5W1H/extractor/extractor.py"", line 87, in preprocess
    self.preprocessor.preprocess(doc)
  File ""/usr/local/lib/python3.6/dist-packages/Giveme5W1H/extractor/preprocessors/preprocessor_core_nlp.py"", line 112, in preprocess
    annotation = self.cnlp.annotate(document.get_full_text(), actual_config)
  File ""/usr/local/lib/python3.6/dist-packages/pycorenlp/corenlp.py"", line 21, in annotate
    raise Exception('Check whether you have started the CoreNLP server e.g.\n'
Exception: Check whether you have started the CoreNLP server e.g.
$ cd stanford-corenlp-full-2015-12-09/ 
$ java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer
192.168.121.149 - - [05/Mar/2019 17:35:48] ""POST /extract HTTP/1.1"" 500 -
edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/usr/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/usr/local/lib/python3.6/dist-packages/Giveme5W1H/extractor/extractor.py"", line 20, in run
    extractor.process(document)
  File ""/usr/local/lib/python3.6/dist-packages/Giveme5W1H/extractor/extractors/abs_extractor.py"", line 41, in process
    self._evaluate_candidates(document)
  File ""/usr/local/lib/python3.6/dist-packages/Giveme5W1H/extractor/extractors/action_extractor.py"", line 104, in _evaluate_candidates
    if any(doc_coref.values()):
AttributeError: 'list' object has no attribute 'values'
```

OS: Ubuntu 18.04
Python Version 3.6.7
Giveme5W1H Version 1.0.13
Stanford CoreNLP Version 2017-06-09"
694,https://github.com/stanfordnlp/CoreNLP/issues/847,847,[],open,2019-03-12 13:02:10+00:00,,Can I only output basicdependencies or only enhancedDependencies using command line?,"If I use the command line to generate the results, can I only keep the enhancedDependencies in the final json file?"
695,https://github.com/stanfordnlp/CoreNLP/issues/848,848,[],open,2019-03-13 03:44:13+00:00,,a memory leak case? ,"I was running an nlp server for most ssplit and some time sentiment analyze service in CentOS7, 64G with xmx&xms 60g

as I need to afford both Chinese end English analysis at the same time, the config as follow:
`
E:
annotators=tokenize,ssplit,pos,lemma,ner,parse,coref,sentiment
tokenize.language=en
C:
annotators=tokenize, ssplit, pos, lemma, ner, parse, coref, sentiment
`
and models:
`
<dependency>
			<groupId>edu.stanford.nlp</groupId>
			<artifactId>stanford-corenlp</artifactId>
			<version>${stanfordNLP.version}</version>
		</dependency>
		<dependency>
			<groupId>edu.stanford.nlp</groupId>
			<artifactId>stanford-corenlp</artifactId>
			<version>${stanfordNLP.version}</version>
			<classifier>models</classifier>
		</dependency>
		<dependency>
			<groupId>edu.stanford.nlp</groupId>
			<artifactId>stanford-corenlp</artifactId>
			<version>${stanfordNLP.version}</version>
			<classifier>models-chinese</classifier>
		</dependency>
		<dependency>
			<groupId>edu.stanford.nlp</groupId>
			<artifactId>stanford-corenlp</artifactId>
			<version>${stanfordNLP.version}</version>
			<classifier>models-english</classifier>
		</dependency>
`

And when the server was running with several requests hitting it, it seems the memory nlp server took will be linear increasing and never release till it die.

when the users increased up to 10/s, numbers of connection time out happens, and the memory this service took will be increasing into  up to 62G which the total memory all applications can take in this machine is about 62.5G (because of some minor tasks and log agents were running as well)

the exceptions were looping at the same time:
`org.apache.catalina.connector.ClientAbortException: java.io.IOException: Broken pipe
	at org.apache.catalina.connector.OutputBuffer.realWriteBytes(OutputBuffer.java:333)
	at org.apache.catalina.connector.OutputBuffer.flushByteBuffer(OutputBuffer.java:758)
	at org.apache.catalina.connector.OutputBuffer.append(OutputBuffer.java:663)
	at org.apache.catalina.connector.OutputBuffer.writeBytes(OutputBuffer.java:368)
	at org.apache.catalina.connector.OutputBuffer.write(OutputBuffer.java:346)
	at org.apache.catalina.connector.CoyoteOutputStream.write(CoyoteOutputStream.java:96)
	at com.fasterxml.jackson.core.json.UTF8JsonGenerator._flushBuffer(UTF8JsonGenerator.java:2085)
	at com.fasterxml.jackson.core.json.UTF8JsonGenerator.flush(UTF8JsonGenerator.java:1097)
	at com.fasterxml.jackson.databind.ObjectWriter.writeValue(ObjectWriter.java:915)
...
Caused by: java.io.IOException: Broken pipe
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:65)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
	at org.apache.tomcat.util.net.NioChannel.write(NioChannel.java:134)
	at org.apache.tomcat.util.net.NioBlockingSelector.write(NioBlockingSelector.java:101)
	at org.apache.tomcat.util.net.NioSelectorPool.write(NioSelectorPool.java:157)
	at org.apache.tomcat.util.net.NioEndpoint$NioSocketWrapper.doWrite(NioEndpoint.java:1225)
	at org.apache.tomcat.util.net.SocketWrapperBase.doWrite(SocketWrapperBase.java:743)
	at org.apache.tomcat.util.net.SocketWrapperBase.writeBlocking(SocketWrapperBase.java:513)
	at org.apache.tomcat.util.net.SocketWrapperBase.write(SocketWrapperBase.java:451)
	at org.apache.coyote.http11.Http11OutputBuffer$SocketOutputBuffer.doWrite(Http11OutputBuffer.java:530)
	at org.apache.coyote.http11.filters.ChunkedOutputFilter.doWrite(ChunkedOutputFilter.java:110)
	at org.apache.coyote.http11.Http11OutputBuffer.doWrite(Http11OutputBuffer.java:189)
	at org.apache.coyote.Response.doWrite(Response.java:599)
	at org.apache.catalina.connector.OutputBuffer.realWriteBytes(OutputBuffer.java:328)
	... 64 common frames omitted
`"
696,https://github.com/stanfordnlp/CoreNLP/issues/849,849,[],closed,2019-03-15 16:34:35+00:00,,3.9.2 canonical commit,"I want to create a fork off 3.9.2. The code should match the exact version 3.9.2 that went into the jar files on mavencentral. How can I get the correct commit-ID?
"
697,https://github.com/stanfordnlp/CoreNLP/issues/850,850,[],closed,2019-03-16 05:54:05+00:00,,Âú®google colaboratoryÂêØÂä®NLPserverÂ§±Ë¥•Ôºåaddress already in use,"!java -mx4g -cp ""code/DataProcessor/stanford-corenlp-full-2018-10-05/*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer 
[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - setting default constituency parser
[main] INFO CoreNLP - warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP - using: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz instead
[main] INFO CoreNLP - to use shift reduce parser download English models jar from:
[main] INFO CoreNLP - http://stanfordnlp.github.io/CoreNLP/download.html
[main] INFO CoreNLP -     Threads: 2
[main] INFO CoreNLP - Starting server...
java.net.BindException: Address already in use
	at java.base/sun.nio.ch.Net.bind0(Native Method)
	at java.base/sun.nio.ch.Net.bind(Net.java:433)
	at java.base/sun.nio.ch.Net.bind(Net.java:425)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:225)
	at java.base/sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at jdk.httpserver/sun.net.httpserver.ServerImpl.<init>(ServerImpl.java:101)
	at jdk.httpserver/sun.net.httpserver.HttpServerImpl.<init>(HttpServerImpl.java:50)
	at jdk.httpserver/sun.net.httpserver.DefaultHttpServerProvider.createHttpServer(DefaultHttpServerProvider.java:35)
	at jdk.httpserver/com.sun.net.httpserver.HttpServer.create(HttpServer.java:137)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.run(StanfordCoreNLPServer.java:1427)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.main(StanfordCoreNLPServer.java:1523)
[Thread-0] INFO CoreNLP - CoreNLP Server is shutting down."
698,https://github.com/stanfordnlp/CoreNLP/issues/851,851,[],closed,2019-03-20 04:30:44+00:00,,Disable ner.applyFineGrained on v3.9.2,"I was extracting NERs from [this article](https://www.rediff.com/news/special/parrikar-tamed-death-and-lived-like-a-warrior-king/20190319.htm). If ```ner.applyFineGrained``` set to ```false```, all tokens will be tagged as ""O"".

When I keep the ```ner.applyFineGrained``` as default (true), the processing cost too much time. Details as follow:

- MediumLengthArticle: 4.5 sec. for 356 tokens at **78.4 tokens/sec**.

- LongArticle: 8.4 sec. for 1683 tokens at **200.3 tokens/sec**.

That is way too slow (as [this website](https://stanfordnlp.github.io/CoreNLP/memory-time.html#parse) says it should be 10,000 tokens/sec)

I'm not sure if this is because the ```ner.applyFineGrained``` params. 
When I set ```-ner.applyFineGrained false```. Although It is much faster  0.6 sec. for 1683 tokens at **3027.0 tokens/sec.**, all tokens are tagged as ""O"".

cmd line as follow. Is there anything wrong?
```java -cp ""./*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner -ner.combinationMode HIGH_RECALL -ner.applyFineGrained false -ner.nthreads 4 -file longArticleSample.txt -outputFormat json```

"
699,https://github.com/stanfordnlp/CoreNLP/issues/852,852,[],open,2019-03-20 13:13:26+00:00,,Visualization broken in web demo,"http://nlp.stanford.edu:8080/corenlp/process

yields

<img width=""761"" alt=""Screen Shot 2019-03-20 at 9 11 09 AM"" src=""https://user-images.githubusercontent.com/1284441/54686715-50388a80-4af0-11e9-87aa-5d1989d0ad74.png"">

Thanks!"
700,https://github.com/stanfordnlp/CoreNLP/issues/853,853,[],open,2019-03-21 14:07:39+00:00,,Difference between english_UD to english_SD [Dependency Parser],"What is the difference in terms of quality between the english_UD and english_SD? I see different dependency graph, not only in terms of the labels on the arrows, but also in terms of the arrows themselves. 

Is there a difference in quality? Were the models trained on different training set size, or the same training set but with different tagging?"
701,https://github.com/stanfordnlp/CoreNLP/issues/854,854,"[{'id': 103162424, 'node_id': 'MDU6TGFiZWwxMDMxNjI0MjQ=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/request', 'name': 'request', 'color': '94c5e9', 'default': False, 'description': None}]",closed,2019-03-26 11:52:03+00:00,,"Sentence spliting doesn't work, if there is no whitespace after dot","I have text like this

> Dog loves cat.Cat loves mouse. Mouse hates everybody.

When I'm trying to split it into sentences, I got 2 sentences instead of 3.

> Dog loves cat.Cat loves mouse.
> 
> Mouse hates everybody.

My code is
```
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,coref"");
        props.setProperty(""ssplit.boundaryTokenRegex"", ""\\.|[!?]+"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
       pipeline.annotate(doc);
        List<CoreMap> sentences = doc.get(CoreAnnotations.SentencesAnnotation.class);
```

Also I tried to use PTBTokenizer, to split text to tokens, but again, it thinks that **cat.Cat** is a single word.

```
        PTBTokenizer ptbTokenizer = new PTBTokenizer(
                new FileReader(classLoader.getResource(""simplifiedParagraphs.txt"").getFile())
                ,new WordTokenFactory()
                ,""untokenizable=allKeep,tokenizeNLs=true,ptb3Escaping=true,strictTreebank3=true,unicodeEllipsis=true"");
        List<String> strings = ptbTokenizer.tokenize();
```

Which type of annotator should I use, to get 3 sentences as the output?
"
702,https://github.com/stanfordnlp/CoreNLP/issues/855,855,[],closed,2019-03-28 09:15:31+00:00,,Custom model and entity creation through API,"Microsoft LUIS API allow us to: 
1)Create custom intent
2)Create custom entities
3)Train models using set of utterances
4)Group intent and entities inside single application
5)Train and publish application

Can we do same with Stanford NLP?

If yes,  please mention API or classes for this. Stanford NLP documents are mostly focused on using existing model and entities."
703,https://github.com/stanfordnlp/CoreNLP/issues/856,856,[],open,2019-03-28 12:05:17+00:00,,UniversalDependenciesConverter IndexOutOfBoundsException,"I'm trying to convert English WSJ Penn treebank dataset to CoNLLu format with Stanford CoreNLP toolkit.

I downloaded CoreNLP from website, and I ran bellow command.

`java -mx1g -cp ""*"" edu.stanford.nlp.trees.ud.UniversalDependenciesConverter -treeFile treebank > treebank.conllu`

However, the error occured during conversion.

```
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.0 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.6 sec].
[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
[main] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
Exception in thread ""main"" java.lang.IndexOutOfBoundsException: Indexed access out of bounds: 6, 4
	at java.util.Objects.outOfBounds(java.base@9-internal/Objects.java:365)
	at java.util.Objects.checkIndex(java.base@9-internal/Objects.java:436)
	at java.util.Objects.checkIndex(java.base@9-internal/Objects.java:388)
	at java.util.ArrayList.get(java.base@9-internal/ArrayList.java:435)
	at edu.stanford.nlp.trees.ud.UniversalDependenciesConverter.main(UniversalDependenciesConverter.java:242)
```"
704,https://github.com/stanfordnlp/CoreNLP/issues/857,857,[],closed,2019-03-29 12:58:17+00:00,,Annotater too Slow,"I have the following properties for my pipeline:

Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,sentiment"");
        props.setProperty(""parse.model"", ""edu/stanford/nlp/models/srparser/englishSR.ser.gz"");
        props.setProperty(""ner.model"", ""edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz"");
        props.setProperty(""ner.applyNumericClassifiers"", ""false"");
        props.setProperty(""ner.includeRange"", ""false"");
        props.setProperty(""sutime.markTimeRanges"", ""false"");
        props.setProperty(""ner.combinationMode"",""NORMAL"");
        props.setProperty(""ner.applyFineGrained"",""false"");
. 

As far as I can tell I followed most of the advice on https://stanfordnlp.github.io/CoreNLP/memory-time.html yet I am still at ~260 ms per sentence. Any tuning advice to make the annotation process faster?"
705,https://github.com/stanfordnlp/CoreNLP/issues/858,858,[],closed,2019-03-31 19:09:33+00:00,,Server for file https://nlp.stanford.edu/js/brat/style-vis.css is down,"I am running the CoreNLP (Java) locally, and when trying to access the visual parser at
http://localhost:9001
It stopped working, because the files
https://nlp.stanford.edu/js/brat/style-vis.css
https://nlp.stanford.edu/js/brat/client/lib/head.load.min.js
are not served anymore."
706,https://github.com/stanfordnlp/CoreNLP/issues/859,859,[],closed,2019-04-01 02:55:21+00:00,,Run time error,"Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.2.1:exec (default-cli) on project stanford-corenlp: Command execution failed. Process exited with an error: 1 (Exit value: 1) -> [Help 1]

To see the full stack trace of the errors, re-run Maven with the -e switch.
Re-run Maven using the -X switch to enable full debug logging.

For more information about the errors and possible solutions, please read the following articles:"
707,https://github.com/stanfordnlp/CoreNLP/issues/860,860,[],open,2019-04-01 03:39:31+00:00,,update regexner mapping file in memory?,"As an online application, can i add new item in regexner mapping file without restart the server?"
708,https://github.com/stanfordnlp/CoreNLP/issues/861,861,[],closed,2019-04-01 15:30:45+00:00,,Issue in loading ParserAnnotator in Android,"I have a working version that runs properly in desktop Java but throws a NullPointerExeception when porting it to Android. I cannot use the webservice because the application should run offline.

This is how I added CoreNLP to gradle
``` 
    implementation('edu.stanford.nlp:stanford-corenlp:3.9.2') {
        exclude group: 'org.apache.lucene'
        exclude group: 'com.sun.xml.bind'
    }
    implementation('edu.stanford.nlp:stanford-corenlp:3.9.2:models') {
        exclude group: 'org.apache.lucene'
        exclude group: 'com.sun.xml.bind'
    }
```
Other info that might be usefull: minSdkVersion = 22, errors occur on Oneplus 6t (Api 28), pixel2 (Api 28), ...

This is how I build the pipeline
```
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,parse,pos,lemma"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
```

But building this pipeline throws following error:
```
2019-04-01 17:10:02.972 27200-27242/com.aurora.aurora E/AndroidRuntime: FATAL EXCEPTION: RxComputationThreadPool-3
    Process: com.aurora.aurora, PID: 27200
    io.reactivex.exceptions.OnErrorNotImplementedException: Attempt to invoke interface method 'int java.util.List.size()' on a null object reference
        at io.reactivex.internal.functions.Functions$OnErrorMissingConsumer.accept(Functions.java:704)
        at io.reactivex.internal.functions.Functions$OnErrorMissingConsumer.accept(Functions.java:701)
        at io.reactivex.internal.observers.LambdaObserver.onError(LambdaObserver.java:77)
        at io.reactivex.internal.observers.LambdaObserver.onNext(LambdaObserver.java:67)
        at io.reactivex.internal.operators.observable.ObservableMap$MapObserver.onNext(ObservableMap.java:64)
        at io.reactivex.internal.operators.observable.ObservableFilter$FilterObserver.onNext(ObservableFilter.java:52)
        at io.reactivex.internal.operators.observable.ObservableObserveOn$ObserveOnObserver.drainNormal(ObservableObserveOn.java:200)
        at io.reactivex.internal.operators.observable.ObservableObserveOn$ObserveOnObserver.run(ObservableObserveOn.java:252)
        at io.reactivex.internal.schedulers.ScheduledRunnable.run(ScheduledRunnable.java:66)
        at io.reactivex.internal.schedulers.ScheduledRunnable.call(ScheduledRunnable.java:57)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:301)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
        at java.lang.Thread.run(Thread.java:764)
     Caused by: java.lang.NullPointerException: Attempt to invoke interface method 'int java.util.List.size()' on a null object reference
        at edu.stanford.nlp.util.HashIndex.size(HashIndex.java:94)
        at edu.stanford.nlp.parser.lexparser.BinaryGrammar.init(BinaryGrammar.java:228)
        at edu.stanford.nlp.parser.lexparser.BinaryGrammar.readObject(BinaryGrammar.java:215)
        at java.lang.reflect.Method.invoke(Native Method)
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1067)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1902)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1803)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2002)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1926)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1803)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
        at edu.stanford.nlp.io.IOUtils.readObjectFromURLOrClasspathOrFileSystem(IOUtils.java:309)
        at edu.stanford.nlp.parser.common.ParserGrammar.loadModel(ParserGrammar.java:184)
        at edu.stanford.nlp.pipeline.ParserAnnotator.loadModel(ParserAnnotator.java:219)
        at edu.stanford.nlp.pipeline.ParserAnnotator.<init>(ParserAnnotator.java:121)
        at edu.stanford.nlp.pipeline.AnnotatorImplementations.parse(AnnotatorImplementations.java:113)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$11(StanfordCoreNLP.java:529)
        at edu.stanford.nlp.pipeline.-$$Lambda$StanfordCoreNLP$IYXSs0FaR-yYo5NN51RbYarn0xQ.apply(Unknown Source:4)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$30(StanfordCoreNLP.java:602)
        at edu.stanford.nlp.pipeline.-$$Lambda$StanfordCoreNLP$bGaSmS8dhCkVuNcmkJjJlAI05AU.get(Unknown Source:6)
        at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)
        at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
        at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:251)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:192)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:188)
2019-04-01 17:10:02.972 27200-27242/com.aurora.aurora E/AndroidRuntime:     at com.aurora.internalservice.internalnlp.InternalNLP.buildPipeline(InternalNLP.java:51)
        at com.aurora.internalservice.internalnlp.InternalNLP.<init>(InternalNLP.java:30)
        at com.aurora.kernel.PluginInternalServiceCommunicator.processFileWithInternalProcessor(PluginInternalServiceCommunicator.java:107)
        at com.aurora.kernel.PluginInternalServiceCommunicator.lambda$new$0(PluginInternalServiceCommunicator.java:40)
        at com.aurora.kernel.-$$Lambda$PluginInternalServiceCommunicator$6apnywtdZcS_QxLe7FHbnfTTEWI.accept(Unknown Source:4)
        at io.reactivex.internal.observers.LambdaObserver.onNext(LambdaObserver.java:63)
        	... 11 more
```

"
709,https://github.com/stanfordnlp/CoreNLP/issues/862,862,[],open,2019-04-02 09:01:35+00:00,,Questions about Chinese Coreference resolution,"1. In src/edu/stanford/nlp,  there are two folders, /coref  and  /dcoref, what are the differences between them? 
2. In src/edu/stanford/nlp/coref, there are src/edu/stanford/nlp/coref/hybrid,  src/edu/stanford/nlp/coref/statistical  and   src/edu/stanford/nlp/coref/neural, does it means the three ways in the homepage of Coreference Resolution to do coreference resolution? What is the difference between src/edu/stanford/nlp/coref/hybrid and the src/edu/stanford/nlp/dcoref ?
3. In src/edu/stanford/nlp/coref/properties, I did not find statistical properties in Chinese. There are only deterministic and neural properties in Chinese. Does this mean statistical algorithm has not been applied in Chinese?
4. I have put my own properties file into src/edu/nlp/pipeline and then used ant to build. Then if I use the command in Java:
String[] args = new String[] {""-props"", ""myself.properties"" };	
Properties props = StringUtils.argsToProperties(args);

It will get an error:   argsToProperties could not read properties file: myself.properties
But if I change those into
String[] args = new String[] {""-props"", ""edu/stanford/nlp/pipeline/StanfordCoreNLP-chinese.properties"" };	
It will be fine. 
Why does it happens? StanfordCoreNLP-chinese.properties and myself.properties are both properties files in src/edu/nlp/pipeline.
"
710,https://github.com/stanfordnlp/CoreNLP/issues/863,863,"[{'id': 45387504, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/bug', 'name': 'bug', 'color': 'fc2929', 'default': True, 'description': None}]",closed,2019-04-02 21:54:09+00:00,,Not using statistical NER model causing NPE,"        props.setProperty(""ner.model"", """");



java.lang.NullPointerException
	at edu.stanford.nlp.pipeline.EntityMentionsAnnotator.determineEntityMentionConfidences(EntityMentionsAnnotator.java:236)
	at edu.stanford.nlp.pipeline.EntityMentionsAnnotator.annotate(EntityMentionsAnnotator.java:333)
	at edu.stanford.nlp.pipeline.NERCombinerAnnotator.annotate(NERCombinerAnnotator.java:353)
	at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:637)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:629)"
711,https://github.com/stanfordnlp/CoreNLP/issues/864,864,[],closed,2019-04-04 17:32:10+00:00,,Dependency Parse - Tree annotation always null,"Using the Russian CoreNLP we're trying to get a dependency tree (from 'depparse') to analyze, but the call to CoreMap.get(TreeCoreAnnotation.TreeAnnotation.class) always returns a null tree. Here's a code snippet:
```
public boolean	apply(AbstractDocument docToParse){
		assert docToParse != null;
		int attempts = 0; 
		try
		{
			initializeState(docToParse);

			Annotation docAnnotation = new Annotation(workingDoc.getText());
			pipeline.annotate(docAnnotation);

			List<CoreMap> sentences = docAnnotation.get(CoreAnnotations.SentencesAnnotation.class);
			for (CoreMap itr : sentences)
			{
				if (itr.size() > 0)
				{
					Tree tree = itr.get(TreeCoreAnnotations.TreeAnnotation.class);
					//'tree' is always null here
```

We've been able to get a valid tree from English and German, which use the constituency parse, not dependency parse ('parse', not 'depparse'). How do we get the dependency tree?"
712,https://github.com/stanfordnlp/CoreNLP/issues/866,866,[],open,2019-04-09 17:27:25+00:00,,RegexNer gazetteer sources,"Is there a description of or a report on the creation of the gazetteer files (regexner_cased and regexner_caseless)? I'm interested in the sources of the entities and which kinds of rules were used to create e.g. different spellings. That may shed light on some of the errors or strange entries, such as ""train: CRIMINAL_CHARGE"" or ""river: TITLE""."
713,https://github.com/stanfordnlp/CoreNLP/issues/867,867,[],closed,2019-04-16 14:58:28+00:00,,Download server is broken,"https://stanfordnlp.github.io/CoreNLP/download.html

wget http://nlp.stanford.edu/software/stanford-corenlp-models-current.jar

can't get to the server for hours, went offline mid download.

Thanks"
714,https://github.com/stanfordnlp/CoreNLP/issues/868,868,[],open,2019-04-16 16:02:12+00:00,,Server for file https://nlp.stanford.edu/js/brat/style-vis.css is down again,"The file is can't be downloaded, please help."
715,https://github.com/stanfordnlp/CoreNLP/issues/869,869,[],open,2019-04-24 19:37:05+00:00,,Coreference in CoNLL output,"I'm trying to run the dcoref system on a plain text file and want to get the output in CoNLL 2012 format.

I've tried several things:
```
$ ./corenlp.sh -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref \
    -file /tmp/example.txt \
    -coref.conllOutputPath /tmp/example.conll
```
However, this option is ignored, and I get XML output.
```
$ ./corenlp.sh -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref \
    -file /tmp/example.txt -outputFormat conll \
    -output.columns doctitle,section,idx,word,lemma,pos,ner,headidx,deprel,link
```
This option is honored, but ""link"" does not give coreference information, and I don't see what other column I should use.

There are instructions on running the system on CoNLL 2011 data and evaluating on it, but for this use case, I don't have annotated data."
716,https://github.com/stanfordnlp/CoreNLP/issues/870,870,[],closed,2019-04-26 09:39:00+00:00,,Sentence split,"Sentence spliting fails on named entity. The following sentence is parsed as one with `Apple Inc.` correctly recognized as named entity.

> Shares of Apple Inc. fell 1.2% to lead Dow decliners after the company unveiled a suite of new products, including its long-awaited video streaming service.

However, the sentence is splited after `Meanwhile, Apple Inc.`
> Meanwhile, Apple Inc. Chief Executive Tim Cook got a 22% raise to $15.7 million in 2018 while the stock fell 6.8%, after getting a 47% raise in 2017 as the stock climbed 46%.

Reproducible on https://corenlp.run/ and with the following code snippet:
```
  val props = new Properties()
  props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"")

  val document1 = new Annotation(""Meanwhile, Apple Inc. Chief Executive Tim Cook got a 22% raise to $15.7 million in 2018 while the stock fell 6.8%, after getting a 47% raise in 2017 as the stock climbed 46%."")
  val document2 = new Annotation(""Shares of Apple Inc. fell 1.2% to lead Dow decliners after the company unveiled a suite of new products, including its long-awaited video streaming service."")
  val pipeline = new StanfordCoreNLP(props)
  pipeline.annotate(document1)
  pipeline.annotate(document2)
  println(document1.get(classOf[SentencesAnnotation]).asScala.length)
  println(document2.get(classOf[SentencesAnnotation]).asScala.length)
```"
717,https://github.com/stanfordnlp/CoreNLP/issues/873,873,[],open,2019-05-01 18:06:25+00:00,,Logging,"Hi all,

I've read here https://github.com/stanfordnlp/CoreNLP/issues/433 how to run CoreNLP server with slf4j logging, so when I start server with 
`java -Dorg.slf4j.simpleLogger.logFile=logfile -Dorg.slf4j.simpleLogger.showDateTime=true -Dorg.slf4j.simpleLogger.dateTimeFormat='yyyy-MM-dd HH:mm:ss:SSS' -Dorg.slf4j.simpleLogger.defaultLogLevel=debug -cp ""*"" -mx40m edu.stanford.nlp.pipeline.StanfordCoreNLPServer`

it indeed writes into logfile but not all we need. For instance we want to catch server related issues, like OOM or read-only FS etc.
If I mimic OOM by starting with little memory and passing large amount of text I see the following error in stdout but not in logfile 
`Exception in thread ""pool-1-thread-1"" java.lang.OutOfMemoryError: GC overhead limit exceeded
        at java.lang.StringBuilder.toString(StringBuilder.java:407)
`

What is missing in configuration to have all messages flow into specified logfile?

Thanks ahead,
Sergii"
718,https://github.com/stanfordnlp/CoreNLP/issues/874,874,[],open,2019-05-02 14:01:08+00:00,,Importing StanfordCoreNLP library in an Android Studio project - Reference #623,"How to do this
?


I have been able to overcome this issue and import Stanford CoreNLP library successfully by doing three steps:

**reducing the size of stanford-corenlp-3.8.0-models.jar.**

making the minimum SDK = 25 in build.gradle :
minSdkVersion 25

including the following line in gradle.properties:
android.enableD8=true"
719,https://github.com/stanfordnlp/CoreNLP/issues/876,876,[],closed,2019-05-11 12:20:45+00:00,,Stanford NLP NER when executed on android runs out of heap memory,"I am using this pipeline

Tokenizer, splitter , pos, lemma , ner

I tried tweaking the pipeline by setting properties.

But eventually this pipeline runs out of heap memory on android.

What can be the solution for this , considering it has to be run offline.

Or what could be alternate approach for this."
720,https://github.com/stanfordnlp/CoreNLP/issues/877,877,[],closed,2019-05-12 11:54:03+00:00,,TokenSequenceParser ignoring tail of patterns mentioned in rules,"Following function in TokenSequenceParser class **ignores tail of patterns** defined in rules for **tokensregex**

private String getStringFromTokens(Token head, Token tail, boolean includeSpecial) {
      StringBuilder sb = new StringBuilder();
      for( Token p = head ; p != tail ; p = p.next ) {
        if (includeSpecial) {
          appendSpecialTokens( sb, p.specialToken );
        }
        sb.append( p.image );
      }
      return sb.toString();
 }

Eg:
**([{lemma:/([a-zA-Z]{2,}_)?[a-zA-Z]{2,}[0-9]{2,}/}])** 
gets converted to 
**([{lemma:/([a-zA-Z]{2,}_)?[a-zA-Z]{2,}[0-9]{2,}/}]** 
while reading and don't provide intended matches"
721,https://github.com/stanfordnlp/CoreNLP/issues/878,878,[],open,2019-05-15 16:07:29+00:00,,Feature Request: Show the probabilities / similiar for the various classes while Truecasing,Show the various Truecasing classes results so one can better select the biases to use in his case
722,https://github.com/stanfordnlp/CoreNLP/issues/879,879,[],closed,2019-05-16 09:21:29+00:00,,French Tokenizer: set splitContraction to false,"Hi!

I've switched my dependencies to the latest release that added the management of contractions  in French (which seems to be splitContractions=true by default)
```
public static final String FTB_OPTIONS = ""ptb3Ellipsis=true,normalizeParentheses=true,ptb3Dashes=false,"" +
      ""splitContractions=true,splitCompounds=true"";
```
But I now get an error stating that:
`edu.stanford.nlp.international.french.process.FrenchLexer: Invalid options key in constructor: splitContractions`

I've then tried not to use the default FTB_OPTIONS settings and inputting `splitContractions=false` but I quite predictably got the same error.
When I look at the setOptions method, I can see how it works for either one of the existing options from FrenchLexer (ptb3Escaping, untokenizable, _etc_.) OR splitCompounds but I can't find how/where the splitContractions option is parsed? Did I miss something?
"
723,https://github.com/stanfordnlp/CoreNLP/issues/880,880,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",closed,2019-05-17 16:08:35+00:00,,Make AnnotationUtils.get*Mentions Methods Static,"Is there a good reason why the methods `getEntityMentions`, `getRelationMentions`, and `getEventMentions` in the class `edu.stanford.nlp.ie.machinereading.structure.AnnotationUtils` are non-static?

It would be nice to have access to these methods and avoid the `CoreMap.get(MachineReadingAnnotations.RelationMentionsAnnotation.class)` syntax.
They are currently non-static methods in a class with a private constructor, which makes them totally unaccessible."
724,https://github.com/stanfordnlp/CoreNLP/issues/882,882,[],closed,2019-05-19 07:40:48+00:00,,ËØ∑ÈóÆSegDemo.javaÁ±ª‰∏≠ÂºïÁî®ÁöÑÊñá‰ª∂ctb.gzÊòØÂê¶Áº∫Â§±Ôºü,"Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: java.io.IOException: Unable to open ""data/ctb.gz"" as class path, filename or URL
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1518)
	at edu.stanford.nlp.wordseg.demo.SegDemo.main(SegDemo.java:46)
Caused by: java.io.IOException: Unable to open ""data/ctb.gz"" as class path, filename or URL
	at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:480)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1503)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1516)
	... 1 more

"
725,https://github.com/stanfordnlp/CoreNLP/issues/883,883,[],closed,2019-05-20 14:18:14+00:00,,Get positions of tokens in parsing tree,"Hi,
I use CoreNLP to get the parsing tree of a sentence. However, all the leaves are just plain text. I would like to also get the position of each leaf in the original sentence (by position, I mean, the position of the first and last character of the token in the original sentence).
How could I do it?

**Note:** I use the CoreNLP Python plugin."
726,https://github.com/stanfordnlp/CoreNLP/issues/884,884,[],open,2019-05-21 22:42:17+00:00,,Constituency parser's visualization does not work on https://corenlp.run,"When applying constituency parser, there is no visualization shown in the output. I've already tried the parser from corenlp.run on multiple platforms and browsers, but it seems like there's a problem on the server side."
727,https://github.com/stanfordnlp/CoreNLP/issues/885,885,[],closed,2019-05-24 11:36:42+00:00,,Unable to create pipeline with pos annotator,"Hi i m Using core-nlp 3.9.2 and while creating pipeline when given parts of speech(pos) annotator, while getting the annotator it takes forever for  (compute method) in Lazy.java
Kindly provide solution"
728,https://github.com/stanfordnlp/CoreNLP/issues/887,887,[],closed,2019-05-26 04:59:23+00:00,,Is it normal to have multiple roots in depparse result ?,"Hi CoreNLP team, I'm using stanford corenlp to depparse a sentence and get the output in conll format. The command i use is below:

> java -cp ""*"" -Xmx10g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,depparse -tokenize.whitespace -ssplit.eolonly -file tok_train.en -outputFormat conll

**Given sentence:** A school uniform down to the ankles , because it had to last for six years .
Output in Conll format:
![image](https://user-images.githubusercontent.com/45225905/58377542-9a852e80-7fad-11e9-900c-f4ed29ed33a8.png)

I have a question that is it normal to have multiple roots like this ?
The corenlp version is stanford-corenlp-full-2018-10-05
"
729,https://github.com/stanfordnlp/CoreNLP/issues/888,888,[],closed,2019-06-01 09:08:19+00:00,,NullPointerException at edu.stanford.nlp.pipeline.NERCombinerAnnotator.annotate,"Hi, i'm using stanford corenlp command line to get ner result of data file to conll format. The command i use is below:

>java -cp ""*"" -Xmx10g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner -tokenize.whitespace -ssplit.eolonly -file tok_train.en -outputFormat conll

And it's run a while until this error appeared:

> [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.0 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [2.1 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.0 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.9 sec].
[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
[main] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 580704 unique entries out of 581863 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_caseless.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 4869 unique entries out of 4869 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_cased.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 585573 unique entries from 2 files
Processing file /content/stanford-corenlp-full-2018-10-05/tok_train.en ... writing to /content/stanford-corenlp-full-2018-10-05/tok_train.en.conll
Unknown variable: WORKDAY
Unknown variable: WORKDAY
Unknown variable: WORKDAY
Error resolving NEXT THIS PT1S, using docDate=null
Exception in thread ""main"" java.lang.NullPointerException
	at edu.stanford.nlp.pipeline.NERCombinerAnnotator.annotate(NERCombinerAnnotator.java:322)
	at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:637)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:647)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1226)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1060)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1326)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1389)
/content



"
730,https://github.com/stanfordnlp/CoreNLP/issues/890,890,[],closed,2019-06-02 08:34:04+00:00,,java.io.IOException: The process cannot access the file because another process has locked a portion of the file,"[This post](https://github.com/stanfordnlp/CoreNLP/issues/399) has been closed even though the problem persists:
When building with maven on windows, some files are 'locked', and no .jar is being produced

Tests in error:
  testPutLocal(edu.stanford.nlp.util.FileBackedCacheTest): java.io.IOException: The process cannot access the file because another process has locked a portion of the file
  testCacheWritingToDisk(edu.stanford.nlp.util.FileBackedCacheTest): java.io.IOException: The process cannot access the file because another process has locked a portion of the file
  testCollision(edu.stanford.nlp.util.FileBackedCacheTest): java.io.IOException: The process cannot access the file because another process has locked a portion of the file
  testGetLocal(edu.stanford.nlp.util.FileBackedCacheTest): java.io.IOException: The process cannot access the file because another process has locked a portion of the file
  testMerge(edu.stanford.nlp.util.FileBackedCacheTest): java.io.IOException: The process cannot access the file because another process has locked a portion of the file
  testSize(edu.stanford.nlp.util.FileBackedCacheTest): java.io.IOException: The process cannot access the file because another process has locked a portion of the file
  testContainsFile(edu.stanford.nlp.util.FileBackedCacheTest): java.io.IOException: The process cannot access the file because another process has locked a portion of the file
  testPutFile(edu.stanford.nlp.util.FileBackedCacheTest): java.io.IOException: The process cannot access the file because another process has locked a portion of the file
  testGetFile(edu.stanford.nlp.util.FileBackedCacheTest): java.io.IOException: The process cannot access the file because another process has locked a portion of the file
  testContainsLocal(edu.stanford.nlp.util.FileBackedCacheTest): java.io.IOException: The process cannot access the file because another process has locked a portion of the file
  testMapValueGoodPattern(edu.stanford.nlp.util.FileBackedCacheTest): java.io.IOException: The process cannot access the file because another process has locked a portion of the file
  testContainsRemoveFromMemory(edu.stanford.nlp.util.FileBackedCacheTest): java.io.IOException: The process cannot access the file because another process has locked a portion of the file
  testIterator(edu.stanford.nlp.util.FileBackedCacheTest): java.io.IOException: The process cannot access the file because another process has locked a portion of the file
  testComprehension(edu.stanford.nlp.util.FileBackedCacheTest): java.io.IOException: The process cannot access the file because another process has locked a portion of the file

Tests run: 1166, Failures: 16, Errors: 14, Skipped: 1

"
731,https://github.com/stanfordnlp/CoreNLP/issues/891,891,[],closed,2019-06-04 07:27:59+00:00,,Exporting trees for Tregex,The Tregex CLI and GUI both require tree files. How can I produce these files from CLI or web interface?
732,https://github.com/stanfordnlp/CoreNLP/issues/892,892,[],open,2019-06-04 17:06:04+00:00,,SUTIME - Seasons return Northern Hemisphere dates,"""Where did you go last winter?""

""last winter"" returns 01-12-2018 to 01-03-2018 for a reference date of 05/06/2019.
This is correct for the Northern Hemisphere, but not correct for the Southern Hemisphere.

Is there a way to specify the Hemisphere?"
733,https://github.com/stanfordnlp/CoreNLP/issues/893,893,"[{'id': 103162424, 'node_id': 'MDU6TGFiZWwxMDMxNjI0MjQ=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/request', 'name': 'request', 'color': '94c5e9', 'default': False, 'description': None}]",closed,2019-06-07 06:55:06+00:00,,Websites not working.,"Any of these mentioned links are not working in Pakistan. I had used stanford web api in my final thesis work these were working perfectly fine but from few days these sites are not working.
http://nlp.stanford.edu:8080/corenlp/process
http://corenlp.run/
https://nlp.stanford.edu/.

Please help what to do.
Thank you 
Regards 
Nageen Naeem"
734,https://github.com/stanfordnlp/CoreNLP/issues/895,895,[],closed,2019-06-19 14:59:26+00:00,,Problem running CorefAnnotator on CoNLL files,"I am following the guide [here](https://stanfordnlp.github.io/CoreNLP/coref.html) and trying to evaluate the statistical/neural coreference annotator on CoNLL 2012 files. The [jar](http://nlp.stanford.edu/software/stanford-english-corenlp-2016-01-10-models.jar) linked in the page does not contain `neural/english-model-conll.ser.gz` and `statistical/*_conll.ser.gz`, but [this jar](http://nlp.stanford.edu/software/stanford-english-corenlp-2016-10-31-models.jar) does.

After figuring that out, when I try to run the annotator using the command given in the page, I get the following output:
```
‚îî‚îÄ$ java -Xmx6g -cp stanford-corenlp-3.7.0.jar:stanford-english-corenlp-models-3.7.0.jar:* edu.stanford.nlp.coref.CorefSystem -props edu/stanford/nlp/coref/properties/neural-english-conll.properties -coref.data ~/ann-coref/test.conll -coref.conllOutputPath ./logs -coref.scorer ~/conll-2012/scorer/v8.01/scorer.pl
[main] INFO edu.stanford.nlp.coref.neural.NeuralCorefAlgorithm - Loading coref model edu/stanford/nlp/models/coref/neural/english-model-conll.ser.gz ... done [0.5 sec].
[main] INFO edu.stanford.nlp.coref.neural.NeuralCorefAlgorithm - Loading coref embeddings edu/stanford/nlp/models/coref/neural/english-embeddings.ser.gz ... done [0.6 sec].
[main] INFO CoreNLP - Identification of Mentions: Recall: (0 / 0) 0%	Precision: (0 / 0) 0%	F1: 0%
[main] INFO CoreNLP - METRIC muc:Coreference: Recall: (0 / 0) 0%	Precision: (0 / 0) 0%	F1: 0%
METRIC bcub:Coreference: Recall: (0 / 0) 0%	Precision: (0 / 0) 0%	F1: 0%
METRIC ceafm:Coreference: Recall: (0 / 0) 0%	Precision: (0 / 0) 0%	F1: 0%
METRIC ceafe:Coreference: Recall: (0 / 0) 0%	Precision: (0 / 0) 0%	F1: 0%
METRIC blanc:Coreference links: Recall: (0 / 0) 0%	Precision: (0 / 0) 0%	F1: 0%
Non-coreference links: Recall: (0 / 0) 0%	Precision: (0 / 0) 0%	F1: 0%
BLANC: Recall: (0 / 1) 0%	Precision: (0 / 1) 0%	F1: 0%
[main] INFO CoreNLP - Final conll score ((muc+bcub+ceafe)/3) = 0
```
Looks like the model is not predicting anything and hence all the scores are 0.

Any idea what might be the issue here?"
735,https://github.com/stanfordnlp/CoreNLP/issues/896,896,[],closed,2019-06-22 16:40:01+00:00,,"NullPointerException thrown when trying to getDate from Timex object with ""this january"" as entity. ","""this january"" entity has Timex object 
`<TIMEX3 alt_value=""THIS XXXX-01"" anchorTimeID=""t0"" temporalFunction=""true"" tid=""t1"" type=""DATE"" valueFromFunction=""tf0"">this january</TIMEX3>`

on calling getDate on this object NullPointerException is thrown. 

However, when parsing the same entity on http://corenlp.run/, 
it parses the date correctly as ""2019-01"". "
736,https://github.com/stanfordnlp/CoreNLP/issues/897,897,[],closed,2019-06-25 08:00:59+00:00,,Question about Chinese sentences ssplit in the server mode,"Hi,

I run corenlp server using ""StanfordCoreNLP-chinese.properties"" as default -serverProperties. When I annotate Chinese text with stanfordcorenlp, the results of ssplit are different. 
run1.py with ""'pipelineLanguage': 'zh',""
run2.py without ""'pipelineLanguage': 'zh',"".

In website visualisation version, I got the same result with run1.py. (only one sentence)
![image](https://user-images.githubusercontent.com/5457307/60080055-bffa8900-9761-11e9-85aa-9be39bd213f5.png)
How can get the right result (two sentences) of sentence split in website?

Corenlp: stanford-corenlp-full-2018-10-05
Model: stanford-*-corenlp-2018-10-05-models.jar

run1.py
```
from stanfordcorenlp import StanfordCoreNLP
corenlp = StanfordCoreNLP(""http://127.0.0.1"", port=9000)
sentence = ""Âø´ÈÄüÁöÑÊ£ïËâ≤ÁãêÁã∏Ë∑≥Ëøá‰∫ÜÊáíÊÉ∞ÁöÑÁãó„ÄÇÂø´ÈÄüÁöÑÊ£ïËâ≤ÁãêÁã∏Ë∑≥Ëøá‰∫ÜÊáíÊÉ∞ÁöÑÁãó„ÄÇ""
props = {'annotators': 'ssplit',
         'pipelineLanguage': 'zh',
         'outputFormat': 'conll'}
data = corenlp.annotate(sentence, properties=props)
print(data)
```
Got
```
1	Âø´ÈÄü	_	_	_	_	_
2	ÁöÑ	_	_	_	_	_
3	Ê£ïËâ≤	_	_	_	_	_
4	ÁãêÁã∏	_	_	_	_	_
5	Ë∑≥Ëøá	_	_	_	_	_
6	‰∫Ü	_	_	_	_	_
7	ÊáíÊÉ∞	_	_	_	_	_
8	ÁöÑ	_	_	_	_	_
9	Áãó	_	_	_	_	_
10	„ÄÇ	_	_	_	_	_
11	Âø´ÈÄü	_	_	_	_	_
12	ÁöÑ	_	_	_	_	_
13	Ê£ïËâ≤	_	_	_	_	_
14	ÁãêÁã∏	_	_	_	_	_
15	Ë∑≥Ëøá	_	_	_	_	_
16	‰∫Ü	_	_	_	_	_
17	ÊáíÊÉ∞	_	_	_	_	_
18	ÁöÑ	_	_	_	_	_
19	Áãó	_	_	_	_	_
20	„ÄÇ	_	_	_	_	_
```

run2.py
```
from stanfordcorenlp import StanfordCoreNLP
corenlp = StanfordCoreNLP(""http://127.0.0.1"", port=9000)
sentence = ""Âø´ÈÄüÁöÑÊ£ïËâ≤ÁãêÁã∏Ë∑≥Ëøá‰∫ÜÊáíÊÉ∞ÁöÑÁãó„ÄÇÂø´ÈÄüÁöÑÊ£ïËâ≤ÁãêÁã∏Ë∑≥Ëøá‰∫ÜÊáíÊÉ∞ÁöÑÁãó„ÄÇ""
props = {'annotators': 'ssplit',
         # 'pipelineLanguage': 'zh',
         'outputFormat': 'conll'}
data = corenlp.annotate(sentence, properties=props)
print(data)
```
Got
```
1	Âø´ÈÄü	_	_	_	_	_
2	ÁöÑ	_	_	_	_	_
3	Ê£ïËâ≤	_	_	_	_	_
4	ÁãêÁã∏	_	_	_	_	_
5	Ë∑≥Ëøá	_	_	_	_	_
6	‰∫Ü	_	_	_	_	_
7	ÊáíÊÉ∞	_	_	_	_	_
8	ÁöÑ	_	_	_	_	_
9	Áãó	_	_	_	_	_
10	„ÄÇ	_	_	_	_	_

1	Âø´ÈÄü	_	_	_	_	_
2	ÁöÑ	_	_	_	_	_
3	Ê£ïËâ≤	_	_	_	_	_
4	ÁãêÁã∏	_	_	_	_	_
5	Ë∑≥Ëøá	_	_	_	_	_
6	‰∫Ü	_	_	_	_	_
7	ÊáíÊÉ∞	_	_	_	_	_
8	ÁöÑ	_	_	_	_	_
9	Áãó	_	_	_	_	_
10	„ÄÇ	_	_	_	_	_
```"
737,https://github.com/stanfordnlp/CoreNLP/issues/898,898,[],closed,2019-06-26 08:20:17+00:00,,"3 tests are failing: ""No extraction for sentence!""","Three tests fail for me, so I cannot package the current version: `testPassiveReflexive`, `testObamaPresidentOfRegressionFull`, and `testReflexive`. All three give the message `No extraction for sentence!`.

```bash
$ mvn clean package
[INFO] Scanning for projects...
[INFO]
[INFO] -----------------< edu.stanford.nlp:stanford-corenlp >------------------
[INFO] Building Stanford CoreNLP 3.9.2
[INFO] --------------------------------[ jar ]---------------------------------
[INFO]
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ stanford-corenlp ---
[INFO] Deleting /opt/CoreNLP/target
[INFO]
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ stanford-corenlp ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /opt/CoreNLP/src/main/resources
[INFO]
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ stanford-corenlp ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 1707 source files to /opt/CoreNLP/target/classes
[WARNING] /opt/CoreNLP/src/edu/stanford/nlp/coref/docreader/CoNLLDocumentReader.java: Some input files use or override a deprecated API.
[WARNING] /opt/CoreNLP/src/edu/stanford/nlp/coref/docreader/CoNLLDocumentReader.java: Recompile with -Xlint:deprecation for details.
[WARNING] /opt/CoreNLP/src/edu/stanford/nlp/util/logging/OutputHandler.java: Some input files use unchecked or unsafe operations.
[WARNING] /opt/CoreNLP/src/edu/stanford/nlp/util/logging/OutputHandler.java: Recompile with -Xlint:unchecked for details.
[INFO]
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ stanford-corenlp ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /opt/CoreNLP/src/test/resources
[INFO]
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ stanford-corenlp ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 173 source files to /opt/CoreNLP/target/test-classes
[WARNING] /opt/CoreNLP/test/src/edu/stanford/nlp/patterns/CreatePatternsTest.java: Some input files use or override a deprecated API.
[WARNING] /opt/CoreNLP/test/src/edu/stanford/nlp/patterns/CreatePatternsTest.java: Recompile with -Xlint:deprecation for details.
[WARNING] /opt/CoreNLP/test/src/edu/stanford/nlp/util/ComparatorsTest.java: Some input files use unchecked or unsafe operations.
[WARNING] /opt/CoreNLP/test/src/edu/stanford/nlp/util/ComparatorsTest.java: Recompile with -Xlint:unchecked for details.
[INFO]
[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ stanford-corenlp ---
[INFO] Surefire report directory: /opt/CoreNLP/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
...

Running edu.stanford.nlp.naturalli.RelationTripleSegmenterTest
Tests run: 66, Failures: 3, Errors: 0, Skipped: 0, Time elapsed: 0.811 sec <<< FAILURE!
testPassiveReflexive(edu.stanford.nlp.naturalli.RelationTripleSegmenterTest)  Time elapsed: 0.048 sec  <<< FAILURE!
junit.framework.AssertionFailedError: No extraction for sentence!
	at junit.framework.Assert.fail(Assert.java:57)
	at junit.framework.Assert.assertTrue(Assert.java:22)
	at junit.framework.TestCase.assertTrue(TestCase.java:192)
	at edu.stanford.nlp.naturalli.RelationTripleSegmenterTest.testPassiveReflexive(RelationTripleSegmenterTest.java:460)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at junit.framework.TestCase.runTest(TestCase.java:176)
	at junit.framework.TestCase.runBare(TestCase.java:141)
	at junit.framework.TestResult$1.protect(TestResult.java:122)
	at junit.framework.TestResult.runProtected(TestResult.java:142)
	at junit.framework.TestResult.run(TestResult.java:125)
	at junit.framework.TestCase.run(TestCase.java:129)
	at junit.framework.TestSuite.runTest(TestSuite.java:252)
	at junit.framework.TestSuite.run(TestSuite.java:247)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:86)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)

testObamaPresidentOfRegressionFull(edu.stanford.nlp.naturalli.RelationTripleSegmenterTest)  Time elapsed: 0.019 sec  <<< FAILURE!
junit.framework.AssertionFailedError: No extraction for sentence!
	at junit.framework.Assert.fail(Assert.java:57)
	at junit.framework.Assert.assertTrue(Assert.java:22)
	at junit.framework.TestCase.assertTrue(TestCase.java:192)
	at edu.stanford.nlp.naturalli.RelationTripleSegmenterTest.testObamaPresidentOfRegressionFull(RelationTripleSegmenterTest.java:661)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at junit.framework.TestCase.runTest(TestCase.java:176)
	at junit.framework.TestCase.runBare(TestCase.java:141)
	at junit.framework.TestResult$1.protect(TestResult.java:122)
	at junit.framework.TestResult.runProtected(TestResult.java:142)
	at junit.framework.TestResult.run(TestResult.java:125)
	at junit.framework.TestCase.run(TestCase.java:129)
	at junit.framework.TestSuite.runTest(TestSuite.java:252)
	at junit.framework.TestSuite.run(TestSuite.java:247)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:86)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)

testReflexive(edu.stanford.nlp.naturalli.RelationTripleSegmenterTest)  Time elapsed: 0.009 sec  <<< FAILURE!
junit.framework.AssertionFailedError: No extraction for sentence!
	at junit.framework.Assert.fail(Assert.java:57)
	at junit.framework.Assert.assertTrue(Assert.java:22)
	at junit.framework.TestCase.assertTrue(TestCase.java:192)
	at edu.stanford.nlp.naturalli.RelationTripleSegmenterTest.testReflexive(RelationTripleSegmenterTest.java:448)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at junit.framework.TestCase.runTest(TestCase.java:176)
	at junit.framework.TestCase.runBare(TestCase.java:141)
	at junit.framework.TestResult$1.protect(TestResult.java:122)
	at junit.framework.TestResult.runProtected(TestResult.java:142)
	at junit.framework.TestResult.run(TestResult.java:125)
	at junit.framework.TestCase.run(TestCase.java:129)
	at junit.framework.TestSuite.runTest(TestSuite.java:252)
	at junit.framework.TestSuite.run(TestSuite.java:247)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:86)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)

Running edu.stanford.nlp.naturalli.PolarityTest
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.017 sec
Running edu.stanford.nlp.naturalli.UtilTest
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.018 sec
Running edu.stanford.nlp.naturalli.NaturalLogicRelationTest
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec
Running edu.stanford.nlp.naturalli.OpenIETest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.064 sec
Running edu.stanford.nlp.parser.metrics.TreeSpanScoringTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.028 sec
Running edu.stanford.nlp.parser.shiftreduce.ShiftReduceUtilsTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.015 sec
Running edu.stanford.nlp.parser.shiftreduce.BinaryTransitionTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec
Running edu.stanford.nlp.parser.shiftreduce.ShiftTransitionTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec
Running edu.stanford.nlp.parser.shiftreduce.ShiftReduceParserTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.046 sec
Running edu.stanford.nlp.parser.shiftreduce.ReorderingOracleTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.032 sec
Running edu.stanford.nlp.parser.shiftreduce.OracleTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.03 sec
Running edu.stanford.nlp.parser.lexparser.SpanishUnknownWordModelTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.01 sec
Running edu.stanford.nlp.patterns.surface.PatternsForEachTokenTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec
Running edu.stanford.nlp.patterns.surface.SurfacePatternTest
pats size is 1
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.017 sec
Running edu.stanford.nlp.patterns.surface.AnnotatedTextReaderTest
[[TextAnnotation=I am going to be in Italy sometime , soon . TokensAnnotation=[I, am, going, to, be, in, Italy, sometime, ,, soon, .] DocIDAnnotation=0-0], [TextAnnotation=Specifically in Tuscany . TokensAnnotation=[Specifically, in, Tuscany, .] DocIDAnnotation=0-1]]
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.002 sec
Running edu.stanford.nlp.patterns.CreatePatternsTest
Size of othersemantic class variables is 0
graph is -> present/VBP (root)
  -> We/PRP (nsubj)
  -> paper/NN (obj)
    -> a/DT (det)
  -> applied/VBN (ccomp)
    -> that/IN (mark)
    -> focuses/NN (nsubj)
      -> graphs/NNS (nmod:on)
        -> semantic/JJ (amod)
    -> language/NN (nmod:to)

{sent1={0=[], 2=[], 4=[], 7=[], 11=[]}}
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.064 sec

Results :

Failed tests:   testPassiveReflexive(edu.stanford.nlp.naturalli.RelationTripleSegmenterTest): No extraction for sentence!
  testObamaPresidentOfRegressionFull(edu.stanford.nlp.naturalli.RelationTripleSegmenterTest): No extraction for sentence!
  testReflexive(edu.stanford.nlp.naturalli.RelationTripleSegmenterTest): No extraction for sentence!

Tests run: 1153, Failures: 3, Errors: 0, Skipped: 1

[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  02:23 min
[INFO] Finished at: 2019-06-26T02:14:12-06:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.12.4:test (default-test) on project stanford-corenlp: There are test failures.
[ERROR]
[ERROR] Please refer to /opt/CoreNLP/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
```"
738,https://github.com/stanfordnlp/CoreNLP/issues/899,899,[],closed,2019-06-29 04:00:57+00:00,,localhost ipv6,"I'm running CoreNLP server on cluster node. Log tells me the following:

```
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000
```

And I tried to use python3 requests to call the service but 

`http://127.0.0.1:9000/?properties={""annotators"": ""parse"", ""outputFormat"": ""json""}` 
and 
`http://localhost:9000/?properties={""annotators"": ""parse"", ""outputFormat"": ""json""}`
gave me `connection refused`

Any idea?

so [0:0:0:0:0:0:0:0] is an unspecified ipv6 address. Is there an option to specify the complete url (http://localhost:9000) instead of port only?"
739,https://github.com/stanfordnlp/CoreNLP/issues/900,900,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}]",closed,2019-06-30 19:37:44+00:00,,distinguish types of adjectives,"It would be great if we can use coreNLP to distinguish which semantic type of adjective / adverb it is. Here is an example list I use to do this manually :
```
       ""quantitative"": [""half"", ""some"", ""several"", ""much"", ""all"", ""many"", ""whole"", ""no"", ""enough"", ""little"", ""any"", ""sufficient"", ""none"", ""most"", ""few""],
        ""demonstrative"": [""this"", ""that"", ""these"", ""those""],
        ""possessive"": [""my"", ""his"", ""their"", ""your"", ""our"", ""mine"", ""his"", ""hers"", ""theirs"", ""yours"", ""ours""],
        ""interrogative"": [""which"", ""what"", ""whose""],
        ""distributive"": [""each"", ""every"", ""either"", ""neither"", ""any""],
        ""article"": [""a"", ""an"", ""the""],
```"
740,https://github.com/stanfordnlp/CoreNLP/issues/902,902,[],closed,2019-07-07 06:32:12+00:00,,about PermissionError when calling StanfordCoreNLP,"`StanfordCoreNLP(r'/usr/local/Cellar/stanford-corenlp/3.9.1/libexec')` threw PermissionError. And I have to run it with `sudo script.py` in shell. That is inconvenient. How could I run it without the sudo.

> Traceback (most recent call last):
>   File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/psutil/_psosx.py"", line 339, in wrapper
>     return fun(self, *args, **kwargs)
>   File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/psutil/_psosx.py"", line 528, in connections
>     rawlist = cext.proc_connections(self.pid, families, types)
> PermissionError: [Errno 1] Operation not permitted

in mac, python3"
741,https://github.com/stanfordnlp/CoreNLP/issues/903,903,"[{'id': 45387506, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNg==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/enhancement', 'name': 'enhancement', 'color': '84b6eb', 'default': True, 'description': None}, {'id': 609575435, 'node_id': 'MDU6TGFiZWw2MDk1NzU0MzU=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/help%20wanted', 'name': 'help wanted', 'color': 'da8000', 'default': True, 'description': None}, {'id': 738203548, 'node_id': 'MDU6TGFiZWw3MzgyMDM1NDg=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/coref', 'name': 'coref', 'color': 'c5def5', 'default': False, 'description': None}]",open,2019-07-10 02:01:57+00:00,,Implement coreference for Spanish,"Is there a plan to include the Coreference detection for Spanish models any time soon?

Thank you,"
742,https://github.com/stanfordnlp/CoreNLP/issues/904,904,"[{'id': 45387508, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}]",closed,2019-07-10 11:34:16+00:00,,CoreNLP training doesn't work,"Hello!
I am trying to train a Stanford CoreNLP model for the French medieval.
I am using this command.
![Capture du 2019-07-10 13-29-55](https://user-images.githubusercontent.com/43485702/60966006-8e63ef00-a317-11e9-8cf8-772d090ec8e6.png)

Unfortunately, I have this result and the CoreNLP doesn't create a new model...
![Capture du 2019-07-10 13-32-32](https://user-images.githubusercontent.com/43485702/60965906-5361bb80-a317-11e9-9806-f0370c18cadf.png)
![Capture du 2019-07-10 13-32-49](https://user-images.githubusercontent.com/43485702/60965907-53fa5200-a317-11e9-8f68-31e3f75032e3.png)
![Capture du 2019-07-10 13-33-06](https://user-images.githubusercontent.com/43485702/60965908-53fa5200-a317-11e9-9b6e-8944518300c0.png)
Could you help me to find the error? 
"
743,https://github.com/stanfordnlp/CoreNLP/issues/905,905,[],closed,2019-07-10 13:18:22+00:00,,Coreference failure,"I've been using the QuoteAttributor on news articles, and 26% of quotes returned have been assigned with speaker 'unknown'. I've checked out and re-run a sample of the failed quote, and I've pasted some of the problems I found below. I'm not sure whether this is a result of my misunderstanding, a technical problem at my end or an issue with the software. I've been running CoreNLP on .txt files (returning .json) from Terminal with the command:

java -cp ""*"" -Xmx10g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,depparse,coref,quote -file input.txt -outputFormat json

Any help would be greatly appreciated!





**Case 1: Descriptive clause 1**
_Including a descriptive clause after the name, in a case where there is a paragraph break within the quote, leads to failure of attribution (canonical speaker = ‚Äòhe‚Äô)._

**Failure**: Hugh Hill, a doctor, said: ""They need urgent treatment to fix the issue.

""If they don't get it they'll die,"" he added.

**Success**: Hugh Hill said: ""They need urgent treatment to fix the issue.

""If they don't get it they'll die,"" he added.


**Case 2: Descriptive clause 2**
_Same issue, but with a sentence break between name and quote start, instead of a paragraph break within the quote._

**Failure**: Last night Khalid Mahmood, a Labour MP, said he was worried. ""What he has done previously is bring the whole community into disrepute and what he is doing now, whether it is legal or not, will do the same,‚Äù he said.


**Success**: Last night Khalid Mahmood said he was worried. ""What he has done previously is bring the whole community into disrepute and what he is doing now, whether it is legal or not, will do the same,"" he said.


**Case 3: Forgetting first name**
_Returns speaker as ‚ÄòSmith‚Äô not ‚ÄòJohn Smith‚Äô._

John Smith was banned from any involvement in schools after the so-called Trojan horse scandal.

Mr Smith said: ""I'm a trainer by profession so I want to use my skill to benefit the parents in educating their children.""


**Case 4: Titles**
_Returns speaker as ‚Äòhis‚Äô, not ‚ÄòAlam‚Äô or ‚ÄòMr Alam‚Äô_

**Failure (speaker = ‚Äòhis‚Äô)**: Mr Alam confirmed that the seminars were the first to be held by his association since the scandal.

He said: ""I'm a trainer by profession so I want to use my skill to benefit the parents in educating their children.""

**Success (speaker = ‚ÄòTahir Alam‚Äô)**: Tahir Alam confirmed that the seminars were the first to be held by his association since the scandal.

He said: ""I'm a trainer by profession so I want to use my skill to benefit the parents in educating their children.""


**Case 5: Repeated use of pronoun**
_Second quote returns speaker ‚Äòunknown‚Äô_

Jack Letts has told how he wanted to be a suicide bomber.

""I know I was definitely an enemy of Britain,"" he told the BBC. ""If there was a battle, I'm ready"", he added.



"
744,https://github.com/stanfordnlp/CoreNLP/issues/906,906,[],closed,2019-07-10 14:53:32+00:00,,Less-than and greater-than signs in documentation wrongly escaped,"For instance noticeable here: 

https://github.com/stanfordnlp/CoreNLP/blob/0d4cfd4209feec7ddbda9eab3fa9c9791caa3e36/src/edu/stanford/nlp/semgraph/semgrex/SemgrexPattern.java#L103

Within the code snippets it seems the html entities are not parsed here: https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/semgrex/SemgrexPattern.html"
745,https://github.com/stanfordnlp/CoreNLP/issues/907,907,[],closed,2019-07-11 11:10:55+00:00,,ÊÉÖÊÑüÂàÜÊûêÊòØÊÄé‰πàÂÅöÁöÑ,ËØ∑ÈóÆÊÉÖÊÑüÂàÜÊûêÊòØÊÄé‰πàÂÅöÁöÑÁî®Ëøô‰∏™Â∑•ÂÖ∑
746,https://github.com/stanfordnlp/CoreNLP/issues/908,908,[],closed,2019-07-11 11:52:19+00:00,,ÂëΩÂêçÂÆû‰ΩìËØÜÂà´Ê†áÁ≠æÂØπÂ∫îÁöÑ‰∏≠ÊñáÂú®Âì™ÈáåÂèØ‰ª•ÊâæÂà∞ÂØπÂ∫îÔºü,ÂëΩÂêçÂÆû‰ΩìËØÜÂà´Ê†áÁ≠æÂØπÂ∫îÁöÑ‰∏≠ÊñáÂú®Âì™ÈáåÂèØ‰ª•ÊâæÂà∞ÂØπÂ∫îÔºü
747,https://github.com/stanfordnlp/CoreNLP/issues/909,909,[],closed,2019-07-11 13:06:04+00:00,,Is the source code for the corenlp.run website available?,"I am interested in making some modifications to the demo site (than runs on corenlp.run) to test certain things more efficiently, using my local corenlp server (for my own use) and of course possibly submit improvements if they are useful in general. Is the website's source control repository published online anywhere?"
748,https://github.com/stanfordnlp/CoreNLP/issues/910,910,[],closed,2019-07-12 09:55:50+00:00,,Protential bug with cached Annotators,"Good morning -

I think I have found a potential bug in the caching of annotators and/or the property sets used to construct them.

I am using CoreNLP version 3.9.2, and have a server implementation where multiple pipelines are created for different property sets. For some of these pipelines, I am using the relation annotator along with a custom training model that I have created for my domain.

What I find is that an instance of edu.stanford.nlp.pipeline.RelationExtractorAnnotator is constructed when when the relation annotator is requested, but that the Properties used to construct the instance are not the same as the ones specified for that current pipeline.

Here's some code to describe this more clearly:

`
    // perform an annotation without relation annotation
    Properties props = new Properties();
    props.setProperty(""annotators"",
      ""tokenize,ssplit,pos,lemma,ner,depparse,coref,natlog,openie,regexner"");

    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(new CoreDocument(text)); 

    .
    . // everything is annotated properly
    .

    // perform an annotation with relation annotation and with custom trained model
    props = new Properties();
    props.setProperty(""annotators"",
      ""tokenize,ssplit,pos,lemma,ner,depparse,coref,natlog,openie,regexner,parse,relation"");

    props.setProperty(""sup.relation.model"", ""/tmp/MyModel.ser"");
    props.setProperty(""sup.relation.verbose"", ""true"");

    pipeline = new StanfordCoreNLP(props);
    pipeline.annotate(new CoreDocument(text)); 
    .
    . // edu.stanford.nlp.pipeline.RelationExtractorAnnotator is being 
    . // constructed with the earlier property set,
    . // which does not have the ""sup.relation.*"" properties,
    . // and even has the earlier annotator list without ,parse,relation
    .
`

I verified that the incorrect properties are being used by inserting some trace code in a local (overridden) copy of RelationExtractorAnnotator to show the properties specified during construction, and to verify the calling stack - both confirm what I have described above. I also performed a similar check in another annotator being used (TokensRegexAnnotator) and I see the same issue (incorrect properties specified during construction).

I also traced AnnotationPool, particularly the ""signature"" check, and the problem does not seem to be in there.

Lastly, I was able to workaround this problem, albeit with a most unsatisfactory workaround, by invoking StanfordCoreNLP.clearAnnotatorPool() before each new pipeline.annotate(). As this call simply clears the GLOBAL_ANNOTATOR_CACHE, I assume that something is getting improperly cached there.

Can you suggest a more satisfactory workaround?

Thanks in advance for your help -

Dave

"
749,https://github.com/stanfordnlp/CoreNLP/issues/911,911,[],closed,2019-07-12 16:34:18+00:00,,[question] POS-tagger: why are the tag values constraint this way?,"https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/sequences/ExactBestSequenceFinder.java#L55

I assume, this constraint has been introduced for the sake of efficiency of the viterbi algorithm, but how does this effect the precision and why are the values not pruned with respect to their log-probability instead? I could not find any explaination in eighter of the two papers (Toutanova 2000/2003)"
750,https://github.com/stanfordnlp/CoreNLP/issues/912,912,[],open,2019-07-13 18:30:31+00:00,,Transfer learning,"I've been trying to implement transfer learning with MaxentTagger. The basic approach I've taken so far adds a new method that takes a `trainFile`, the existing model's configuration, and the output path for the new model. It loads the existing model into a new MaxentTagger instance with `readModelAndInit`, changes the `model` field to the new output path, and starts training the same as `trainAndSaveModel`. 

Expecting this to be enough seems increasingly naive. Early on, I was able to output and load a model trained in this way but ran into exceptions when running a parse. I've done a couple passes trying to merge `tags` and `dict` produced by loading the existing model and preparing the training run for the input dataset. If this is the correct approach I'm thinking it'd need to be done in some way for `MaxentTagger.lambda`, the data held by `TaggerFeatures` and `TaggerExperiments`, and a few other things. That may require access to a fair number of fields that are currently marked as private.

Is there any interest from your side on adding this as a feature? Should I even think this is possible to do? Any general advice?"
751,https://github.com/stanfordnlp/CoreNLP/issues/913,913,[],closed,2019-07-16 01:51:35+00:00,,"POS tagging error for ""strong #2"" ""clear #2"" ""solid #2"" etc.","Sentence involving the sequence JJ-#-CD sometime incorrectly assign "","" where it should be CD. 

For example the sentence:

```Yes, we -- I think that was some contract wins we expect to actually be a solid #2 in the market.```

Tags as:

```
solid = JJ
# = #
2 = ,
```

If I change the ""2"" to a ""3"". It's correct:

```
solid = JJ
# = #
3 = CD
```

This only happens with `#2` any other integer works find.

I've attached some example sentences demonstrating the effect.

[unknown-tag-hash2.txt](https://github.com/stanfordnlp/CoreNLP/files/3394809/unknown-tag-hash2.txt)
"
752,https://github.com/stanfordnlp/CoreNLP/issues/914,914,"[{'id': 626016953, 'node_id': 'MDU6TGFiZWw2MjYwMTY5NTM=', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/analysis-bug', 'name': 'analysis-bug', 'color': 'f98685', 'default': False, 'description': None}]",closed,2019-07-16 01:56:38+00:00,,"POS tagging Asian name ""Ye"" as pronoun","""Ye"" is a common family name in Asia. In the context of a name should be tagged NNP not as PRP. 

I attached some example sentences which show the effect.

[unknown-pronoun-ye.txt](https://github.com/stanfordnlp/CoreNLP/files/3394857/unknown-pronoun-ye.txt)
"
753,https://github.com/stanfordnlp/CoreNLP/issues/915,915,[],closed,2019-07-16 06:21:11+00:00,,Getting Tragex UI Error,"When I am trying to integrate Stanford NLP to android.I  am getting following issues
Caused by: java.lang.IllegalAccessException: no such method: edu.stanford.nlp.trees.tregex.gui.TregexGUI.lambda$createFileChooser$0(JFileChooser,ActionEvent)void/invokeStatic
Caused by: java.lang.VerifyError: Bad type on operand stack

.Please Anyone help me. Thanks in Advance"
754,https://github.com/stanfordnlp/CoreNLP/issues/916,916,"[{'id': 45387508, 'node_id': 'MDU6TGFiZWw0NTM4NzUwOA==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/question', 'name': 'question', 'color': 'cc317c', 'default': True, 'description': None}]",closed,2019-07-24 07:28:30+00:00,,How should I train my own NER model in Chinese?,"I want to recognize some proper noun from a series of text, **especially the company and organization.**
I have found an article about how to train a NER model in English on the official site.
Is it the same way to handle Chinese text?
Thanks for your reply~"
755,https://github.com/stanfordnlp/CoreNLP/issues/917,917,[],closed,2019-07-24 08:28:17+00:00,,NullPointerException in EntityMentionsAnnotator,"https://github.com/stanfordnlp/CoreNLP/blob/089ff022c5cb3df39a00965b30ef2e90a29c594e/src/edu/stanford/nlp/pipeline/EntityMentionsAnnotator.java#L312


Caused by: java.lang.NullPointerException
	at edu.stanford.nlp.pipeline.EntityMentionsAnnotator.annotate(EntityMentionsAnnotator.java:312)

entityMentionsLanguage is null"
756,https://github.com/stanfordnlp/CoreNLP/issues/918,918,[],closed,2019-07-25 03:27:54+00:00,,Something happened during my NER train process,"According to the official instruction[https://nlp.stanford.edu/software/crf-faq.html#deterministic](https://nlp.stanford.edu/software/crf-faq.html#deterministic)(How can I train my own NER model?)
, I use the provided file **jane-austen-emma-ch1.tsv and jane-austen-emma-ch2.tsv** to train and test.
Things go well, however, after 

> `java -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier ner-model.ser.gz -testFile jane-austen-emma-ch2.tsv`

Here comes an exception:

> `testFile=jane-austen-emma-ch2.tsv
> loadClassifier=ner-model.ser.gz
> Loading classifier from ner-model.ser.gz ... done [0.1 sec].
> Error on line 1: CHAPTER		O
> Exception in thread ""main"" java.lang.UnsupportedOperationException: Argument array lengths differ: [class edu.stanford.nlp.ling.CoreAnnotations$TextAnnotation, class edu.stanford.nlp.ling.CoreAnnotations$AnswerAnnotation] vs. [CHAPTER, , O]
> 	at edu.stanford.nlp.ling.CoreLabel.initFromStrings(CoreLabel.java:263)
> 	at edu.stanford.nlp.ling.CoreLabel.<init>(CoreLabel.java:150)
> 	at edu.stanford.nlp.sequences.ColumnDocumentReaderAndWriter$ColumnDocParser.apply(ColumnDocumentReaderAndWriter.java:91)
> 	at edu.stanford.nlp.sequences.ColumnDocumentReaderAndWriter$ColumnDocParser.apply(ColumnDocumentReaderAndWriter.java:64)
> 	at edu.stanford.nlp.objectbank.DelimitRegExIterator.parseString(DelimitRegExIterator.java:71)
> 	at edu.stanford.nlp.objectbank.DelimitRegExIterator.setNext(DelimitRegExIterator.java:67)
> 	at edu.stanford.nlp.objectbank.DelimitRegExIterator.<init>(DelimitRegExIterator.java:63)
> 	at edu.stanford.nlp.objectbank.DelimitRegExIterator$DelimitRegExIteratorFactory.getIterator(DelimitRegExIterator.java:128)
> 	at edu.stanford.nlp.sequences.ColumnDocumentReaderAndWriter.getIterator(ColumnDocumentReaderAndWriter.java:58)
> 	at edu.stanford.nlp.objectbank.ObjectBank$OBIterator.setNextObjectHelper(ObjectBank.java:435)
> 	at edu.stanford.nlp.objectbank.ObjectBank$OBIterator.setNextObject(ObjectBank.java:419)
> 	at edu.stanford.nlp.objectbank.ObjectBank$OBIterator.<init>(ObjectBank.java:412)
> 	at edu.stanford.nlp.objectbank.ObjectBank.iterator(ObjectBank.java:250)
> 	at edu.stanford.nlp.sequences.ObjectBankWrapper.iterator(ObjectBankWrapper.java:45)
> 	at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyAndWriteAnswers(AbstractSequenceClassifier.java:1189)
> 	at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyAndWriteAnswers(AbstractSequenceClassifier.java:1133)
> 	at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyAndWriteAnswers(AbstractSequenceClassifier.java:1087)
> 	at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3053)
> `

Indeed, the provided file **jane-austen-emma-ch2.tsv** has three colomn. If there is something wrong with it, how can I use it correctly?"
757,https://github.com/stanfordnlp/CoreNLP/issues/919,919,[],open,2019-07-25 16:02:19+00:00,,TrueCaseAnnotator returns uppercase text,"Pretty often I got uppercased results. In some case model works good, but in some worse. Is any chance to fix this?
Some example of bad cases: 
World's Smallest Flower Vase! -> WORLD 'S SMALLEST FLOWER VASE !
Swarna Chaturvedy likes. Plants and few clicks away to win his Free terrace garden! -> SWARNA chaturvedy likes . Plants and few clicks away to WIN HIS FREE TERRACE GARDEN !
Thanos! Wins Infinity Gauntlet Fortnite: Battle Royale LIVE -> Thanos ! Wins Infinity Gauntlet FORTNITE : Battle Royale Live
DIY Static Orbit Sander With Hard Disk -> DIY STATIC ORBIT SANDER WITH HARD DISK
COOL CHRISTMAS CARDS -> COOL CHRISTMAS CARDS
This futuristic 3D printer uses light to print -> This futuristic 3D PRINTER USES LIGHT TO PRINT
Maia zooming for dinner -> MAIA ZOOMING FOR DINNER
Cosmetic surgeons use lasers to remove moles -> COSMETIC SURGEONS USE LASERS TO REMOVE MOLES"
758,https://github.com/stanfordnlp/CoreNLP/issues/920,920,[],closed,2019-07-25 20:55:40+00:00,,Sentiment Pipeline Score,"I read on the Stanford NLP website that the sentiment scores ran from 1-25 (most negative to most positive), does anyone know how I can access these scores and print them along with the sentiment itself for a given dataset?"
759,https://github.com/stanfordnlp/CoreNLP/issues/921,921,[],closed,2019-07-25 21:45:01+00:00,,How to create and train model from dataset,"Hello, I'm trying to create a sentiment analysis app in java, to retrieve tweets and analize them. I've already made it so that it works with the default model. 

Now I want to go one step further and create my own model. I have searched tutorials but I still don't understand how to do it, in most cases these tutorials just use the default models.

In my case, I already have 3 XML files, ""training.xml"", ""development.xml"" and ""test.xml"", that have the following structure, with 4 possible sentiments (P, N, NEU, NONE):

```
<tweets>
   <tweet>
      (tweet info, username, id...)
      <content>tweet content</content>
      <sentiment>
         <polarity>
            <value>N</value>
         </polarity>
      </sentiment>
   </tweet>
   (...)
</tweets>
```

But at this point, I don't know what are the steps convert these XML to a training model. 

Thank you for your time. "
760,https://github.com/stanfordnlp/CoreNLP/issues/922,922,[],open,2019-07-26 09:28:49+00:00,,Regexner is invalid?,"I try to follow the steps given by 
[https://nlp.stanford.edu/software/regexner.html](https://nlp.stanford.edu/software/regexner.html)
However, I cannot successfully get the named entity  ""Bachelor of Arts"" tagged as ""DEGREE"".

![image](https://user-images.githubusercontent.com/25773429/61941770-b5364c80-afca-11e9-9847-f1336254e7c3.png)

How to deal with it?
Thanks for your reply~"
761,https://github.com/stanfordnlp/CoreNLP/issues/923,923,[],closed,2019-07-29 10:40:42+00:00,,StanfordCoreNLP 3.9.1 and StanfordCoreNLP 3.9.2,"Why is the server we set up Stanford CoreNLP 3.9.1, and the latest API is Stanford CoreNLP 3.9.2?"
762,https://github.com/stanfordnlp/CoreNLP/issues/924,924,[],closed,2019-07-30 10:14:31+00:00,,What are the EnglishGrammatical classes ?,"Hello,

I am using a software (clausIE) based on stanfordNLP, which use the edu.stanford.nlp.trees.EnglishGrammaticalRelations and
edu.stanford.nlp.trees.EnglishGrammaticalStructure classes. However, I would like to transpose this software in french, but I did not find any equivalent of those classes. What are those classes for and how could I transpose them in french, if it does not exist already ?

Moreover, in the stanford-parser-models.jar, what are the differences between the englishFactored and englishPCFG models ? Could the frenchFactored model be used as a substitute of the englishPCFG model ?

Finally, I am using stanfordNLP 2.0.4, is the doc or any resource accessible online ?

Thanks

@gatemezing"
763,https://github.com/stanfordnlp/CoreNLP/issues/925,925,[],closed,2019-08-01 01:32:19+00:00,,what is the NT represent for,"In Part-Of-Speech Tagger, what does NT mean? Is there a part of speech?"
764,https://github.com/stanfordnlp/CoreNLP/issues/926,926,[],closed,2019-08-02 08:20:28+00:00,,Lemmatization doesn't work on corenlp.run,"As i know, lemmatization will return the base form with different word forms. When i used corenlp.run to pos tag, lemmtize and ner, lemmatization seem doesn't work.
![image](https://user-images.githubusercontent.com/45225905/62355313-eff33400-b538-11e9-8898-c6a98d30fb2d.png)

I think it should return ""Thank , Tom"" instead of ""thanks .....""
"
765,https://github.com/stanfordnlp/CoreNLP/issues/927,927,[],closed,2019-08-02 20:38:01+00:00,,Commit hashes for older versions,The latest two versions currently have a release page for them associated here on Github. Does anyone have the commit hashes for older versions? I could not find it on the version history page of the project. My current interest is actually version 3.5.0.
766,https://github.com/stanfordnlp/CoreNLP/issues/928,928,[],closed,2019-08-04 06:42:34+00:00,,Is there a way to specify constituents as additional context to the parser?,"Parts of speech can be specified as additional context for tokens. Can constituents or dependencies be specified as well? For instance, the verb phrase ""see water on land"" could be given either of these two constituency trees, and I want the second one:

(VP (VB see)
   (NP (NN water))
   (PP (IN on)
      (NP (NN land))))

(VP (VB see)
   (NP (NN water)
      (PP (IN on)
         (NP (NN land)))))

In the second one, the two last tokens are forced to be inside the second token's constituent."
767,https://github.com/stanfordnlp/CoreNLP/issues/929,929,[],closed,2019-08-05 04:13:46+00:00,,‚Äòdocument is too long‚Äô in my service,"I use the default English model to build the service, the error message is ‚Äòdocument is too long‚Äô, but there is no problem using the shift reduce parser model. How can I fix the default English model."
768,https://github.com/stanfordnlp/CoreNLP/issues/930,930,[],closed,2019-08-06 02:46:59+00:00,,"SD for Chinese, dep label confused?","CoreNLP convert for dep parser annotation:
```
java -cp ""*"" -mx1g edu.stanford.nlp.trees.international.pennchinese.ChineseGrammaticalStructure -basic -keepPunct -conllx -language zh -treeFile cctv_0000.parse > cctv_0000.parse.dep.ud
java -cp ""*"" -mx1g edu.stanford.nlp.trees.international.pennchinese.ChineseGrammaticalStructure -basic -keepPunct -conllx -language zh-sd -treeFile cctv_0000.parse > cctv_0000.parse.dep.sd
```
there are some cycle in dep parser annotation as the head is wrong."
769,https://github.com/stanfordnlp/CoreNLP/issues/931,931,[],closed,2019-08-06 08:39:08+00:00,,import edu.stanford.nlp.process.WordTokenFactory;,"hi,

I am using parser to get dependency parser.
lib_dir = os.path.join(params['base_path'], 'lib')
classpath = ':'.join([
        lib_dir,
        os.path.join(lib_dir, 'stanford_parser/stanford_parser.jar'),
        os.path.join(lib_dir, 'stanford_parser/stanford_parser-3.9.2-models.jar')])
command = 'javac -cp $%s lib/*.java' % classpath
os.system(command)

I have download http://nlp.stanford.edu/software/stanford_parser-full-2018-10-17.zip and http://nlp.stanford.edu/software/stanford-postagger-2018-10-16.zip. I can not find the package ""edu.stanford.nlp.process"".
How to fix it?"
770,https://github.com/stanfordnlp/CoreNLP/issues/932,932,[],closed,2019-08-07 02:12:10+00:00,,How should I use KBP with Java API? ,"I found useful tools to get the relation underneath the texts on page [KBPAnnotator](https://stanfordnlp.github.io/CoreNLP/kbp.html#example-usage)
But there is no example of Java API, so I cannot directly get the result in the IDE console. 
Does this module support Java API?
Thanks for your reply~"
771,https://github.com/stanfordnlp/CoreNLP/issues/933,933,[],closed,2019-08-07 22:39:14+00:00,,Use the accuracy tested by CoreNLP on the tree nodes as the accuracy of Recursive NN model in 2013 paper,Hi! Can we use CoreNLP and its results as the performance of Recursive model proposed in your 2013 paper? Thx!
772,https://github.com/stanfordnlp/CoreNLP/issues/934,934,[],closed,2019-08-08 07:45:26+00:00,,Editing PTB Tree format data with editable tree,"I have my own data in PTB format like this. 

(1 (2 (2 German) (2 nurse)) (2 (2 (2 sentenced) (2 (2 to) (2 (3 life) (2 (2 in) (2 prison))))) (2 (2 for) (2 (2 murdering) (2 (2 85) (2 patients))))))

I created this data using Stanford sentiment tagger. But I want to improve the dataset accuracy by manual annotation.

I want this to display on a tree and edit the sentiment values. 

Is it possible to display the above sentences similar to http://nlp.stanford.edu:8080/sentiment/rntnDemo.html editable tree? Any reference for this is highly appreciated. "
773,https://github.com/stanfordnlp/CoreNLP/issues/935,935,[],closed,2019-08-19 03:17:45+00:00,,Let's POS tagged as possessive when preceded by a quote,"Probably a tagging issue in the POS training data. When `Let's` is preceded with a quote as in `""Let's`, the `'s` is tagged as possessive rather than a pronoun. See below.

```shell
Let's go fishing  
Let = VB  
's = PRP  
go = VB  
fishing = NN  
```

```shell
""Let's go fishing  
"" = ``  
Let = VB  
's = POS  
go = VB  
fishing = NN  
```"
774,https://github.com/stanfordnlp/CoreNLP/issues/936,936,[],open,2019-08-19 06:03:32+00:00,,How should I render a visualization html in the demo style(http://corenlp.run/)?,"I love the cool visualization of NER results from http://corenlp.run/
I want to change some part of it but do not know whether there are some API docs.
For example, just highlight some kinds of NER label.
Thanks for your reply~
 "
775,https://github.com/stanfordnlp/CoreNLP/issues/937,937,[],closed,2019-08-20 13:08:31+00:00,,Null Pointer Exception in CustomNER,"I'm working on upgrading from 3.7.0 to 3.9.2, but after upgrading, I've been receiving a Null Pointer Exception that's throwing an error. However, after debugging, I've identified this line https://github.com/stanfordnlp/CoreNLP/blob/eddd12c73a7d77ea723c44c3513dab350dc11175/src/edu/stanford/nlp/pipeline/EntityMentionsAnnotator.java#L330 as causing the null pointer. This is code decompiled from the CoreNLP jar files, but for some reason when I remove this line, the null pointer ceases to be an issue, although I'm not sure why the language check would cause a null pointer, but it's a blocker for now.

Any guidance on how to overcome this null pointer or why it could exist? Thanks!"
776,https://github.com/stanfordnlp/CoreNLP/issues/938,938,[],closed,2019-08-22 10:03:21+00:00,,Nondeterminism in parse annotator results,"I'm running into a problem, where `parse` annotator (with `englishRNN.ser.gz` model) doesn't always produce the same dependency graph for the same input. For instance, if I annotate a sentence ""X should not be 0."" ten times in a loop, each iteration can end with one of the following parses (the difference is marked in **bold**):

-> be/VB (root)
  -> X/NNP (nsubj)
  -> should/MD (aux)
  -> not/RB (neg)
  **-> 0/CD (xcomp)**
  -> ./. (punct)

-> be/VB (root)
  -> X/NNP (nsubj)
  -> should/MD (aux)
  -> not/RB (neg)
  **-> 0/CD (nmod:tmod)**
  -> ./. (punct)

[Here is the code to reproduce the issue](https://github.com/stanfordnlp/CoreNLP/files/3529531/Determinism.txt).


I looked into it, and it seems like it might be caused by a rounding error. From what I see in debugger, `parse` method of `RerankingParserQuery` class first gets a list of parses and then reranks them. In `bestKParses` both annotations are present, but the one with 0-xcomp is always ranked above one with 0-nmod:tmod (parses in question are 0 and 15):

![image](https://user-images.githubusercontent.com/54396013/63502724-0d774600-c4c6-11e9-97dc-4b1c6f2d3faa.png) 

Then, these parses are getting re-ranked by the following line in `rerank` method of `RerankingParserQuery`:

`score = op.baseParserWeight * score + rerankerQuery.score(scoredTree.object());`

The first term is the score contained in `bestKParses`, but since `score` factor is equal to zero in `englishRNN.ser.gz` model, the new score is effectively the score given by `rerankerQuery`.  The two annotations I'm concerned about seem to be getting the same score, resulting in a tie. It's not a problem, because `Collections.sort()` is stable, so the correct annotation ends up at the top and is getting returned as the resulting annotation by `getBestParse` method (see 0 and 3):

![image](https://user-images.githubusercontent.com/54396013/63503385-449a2700-c4c7-11e9-96fc-d172683c6af3.png)

So far so good, but if I repeat the same in a loop, there comes an iteration where scores in `bestKParses` are the same as before, but scores in `rerank` seem to exhibit something that looks like a rounding/truncation error at some deeper level in the execution stack. For instance, I would expect the parse at position 0 to get a score of 2.1485035452860854, but it has 2.148503545286086 instead, which pushes it to the top of the list and makes it the annotation that is actually returned from the pipeline:

![image](https://user-images.githubusercontent.com/54396013/63504650-e4f14b00-c4c9-11e9-9570-3125dcc4be95.png)

I'm running JDK 1.8 and Stanford CoreNLP 3.9.2.

Let me know if I can help with anything else to help to chase it up."
777,https://github.com/stanfordnlp/CoreNLP/issues/939,939,[],closed,2019-08-23 21:23:14+00:00,,How to run coreference resolution on custum CoNLL data,"I have checked the instructions on replicating the CoNLL 2012 results.  However I'm not trying to replicate the results.    
Instead, I have some `.conll` files, and I want to perform coreference resolution on these files. Because the POS, NER, Speaker etc. tags are manually revised in these files,  I wish the system can make use of these information.

I tried this command:   
`java -Xmx6g -cp stanford-corenlp-3.9.2.jar;stanford-corenlp-3.9.2-models.jar;* edu.stanford.nlp.coref.CorefSystem -props edu/stanford/nlp/coref/properties/neural-english-conll.properties -coref.data ./conll-files -coref.conllOutputPath ./results -coref.scorer ./scorers/scorer.bat`

 `./conll-files` is where the `.conll` files are stored

However, I received this output:  
```
[main] INFO edu.stanford.nlp.coref.neural.NeuralCorefAlgorithm - Loading coref model edu/stanford/nlp/models/coref/neural/english-model-conll.ser.gz ... done [0.4 sec].
[main] INFO edu.stanford.nlp.coref.neural.NeuralCorefAlgorithm - Loading coref embeddings edu/stanford/nlp/models/coref/neural/english-embeddings.ser.gz ... done [0.4 sec].
[main] INFO CoreNLP - Identification of Mentions: Recall: (0 / 0) 0%    Precision: (0 / 0) 0%   F1: 0%
Coreference: Recall: (0 / 0) 0% Precision: (0 / 0) 0%   F1: 0%
Coreference: Recall: (0 / 0) 0% Precision: (0 / 0) 0%   F1: 0%
Coreference: Recall: (0 / 0) 0% Precision: (0 / 0) 0%   F1: 0%
Coreference: Recall: (0 / 0) 0% Precision: (0 / 0) 0%   F1: 0%
[main] INFO CoreNLP - Final conll score ((muc+bcub+ceafe)/3) = 0
```

It seems like the system was not able to load the files?  Is there a way to make it work?  "
778,https://github.com/stanfordnlp/CoreNLP/issues/941,941,[],closed,2019-08-26 15:47:36+00:00,,Cannot disable pos annotator while running lexical parse,"## TLDR
CoreNLP server runs the `pos` option in the pipeline even when it is instructed not to (both while launching the server and while sending the POST request)

## Description
Originally submitted as a different issue on the parser-user mailing, list [here ](https://mailman.stanford.edu/pipermail/parser-user/2019-August/003545.html), Chris Manning suggest that CoreNLP be run without `pos` as an option.

The CoreNLP server was run with the following command
```
java -mx500m -Xms2048m  -Xmx2048m -cp ""./*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -annotators tokenize,ssplit,parse
```

With the server running, requests were made to the server via different frontends(NLTK, Web, StanfordCoreNLPCLient). All these just end up sending a POST request to ```http://localhost:9000/```. For example, StanfordCoreNLPClient was run as follows
```
java -mx500m -Xms2048m  -Xmx2048m -cp ""./*"" edu.stanford.nlp.pipeline.StanfordCoreNLPClient -host localhost -port 9000 -annotators tokenize,ssplit,parse
```
An input string was then given. 

The line in the server log on getting the POST request says
```
[pool-1-thread-1] INFO CoreNLP - [/127.0.0.1:51882] API call w/annotators tokenize,ssplit,pos,parse
```
How can the lexical parser option be used without the pos stage being run?

"
779,https://github.com/stanfordnlp/CoreNLP/issues/942,942,[],closed,2019-08-27 07:06:59+00:00,,tokenisation problem with underscore words in new release,"In new release of core-nlp it is splitting words on underscore as well, which was not the case with earlier version.
e.g.: Parent_in_law will be splitted as ""Parent"", ""_"", ""in"", ""_"",  ""Law"",  is there a way to avoid this.
It is also creating ambiguity with dependency parser since now parser is reading ""_"" as a separate token.
![image](https://user-images.githubusercontent.com/20730799/63833349-73fdd780-c990-11e9-8656-140ada6cca86.png)

"
780,https://github.com/stanfordnlp/CoreNLP/issues/943,943,[],closed,2019-08-28 18:23:32+00:00,,Stemmer class missing from Maven Central,"As per the instructions here for Maven: https://stanfordnlp.github.io/CoreNLP/download.html

I've added the 2 dependencies to my project for the core classes and the models. However, they do not appear to contain the Stemmer class (https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/process/Stemmer.java) so I can't perform stemming.

I've even downloaded these jars directly from Maven Central and validated that neither the core JAR nor the models JAR has this Stemmer class.

Why is this missing from the JAR? Should I be using a different class to perform stemming?"
781,https://github.com/stanfordnlp/CoreNLP/issues/944,944,[],closed,2019-08-29 14:23:01+00:00,,Running quote annotator directly,"Hi,

I'm planning on using a different software for coreference (HuggingFace's neuralcoref), but then CoreNLP for quote attribution. I'm trying to figure out what input the CoreNLP quote annotator needs (so that I can massage my HuggingFace output to meet those requirements).

Before, when I was using the CoreNLP coreference annotator, running this from Terminal would work well:

`java -cp ""*"" -Xmx10g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,custom.lemma,ner,depparse,coref,quote -file input.txt -outputFormat text`

To get an acceptable input file for to use on 'quote', I tried running the same code but without quote annotator, and exporting as xml:

`java -cp ""*"" -Xmx10g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,depparse,coref -file input.txt -outputFormat xml`

I then took the output file (input.txt.xml) and fed it into the quote annotator directly as follows:

`java -cp ""*"" -Xmx10g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators quote -file input.txt.xml -outputFormat text`

However, this gave me the following error:

`Exception in thread ""main"" java.lang.IllegalArgumentException: annotator ""quote"" requires annotation ""EntityMentionIndexAnnotation"". The usual requirements for this annotator are: tokenize,ssplit,pos,lemma,ner`

What is the correct kind of file to feed directly into the quote annotator?

Any help much appreciated - thanks!"
782,https://github.com/stanfordnlp/CoreNLP/issues/945,945,[],closed,2019-09-04 03:40:42+00:00,,Sometimes OpenIE does not work during Chinese processing,"the OpenIE module works so great in English that most sentence will get a result.
However, when it comes to Chinese, the OpenIE is ineffective which means no result is shown at all.
For example, here are the default sentence and result in English and Chinese:

> The quick brown fox jumped over the lazy dog.

![image](https://user-images.githubusercontent.com/25773429/64223010-c5fcab00-cf04-11e9-9375-e8ba5985c955.png)

> Âø´ÈÄüÁöÑÊ£ïËâ≤ÁãêÁã∏Ë∑≥Ëøá‰∫ÜÊáíÊÉ∞ÁöÑÁãó„ÄÇ

![image](https://user-images.githubusercontent.com/25773429/64223114-360b3100-cf05-11e9-8b0e-f840cd04e437.png)

The most strange part is that the Chinese text seems to get nothing.

BTW,
1.
For the sentence ""ÊúâÊâçÂçéÁöÑÈ≤ÅËøÖ‰ΩèÂú®‰∏äÊµ∑„ÄÇ"", we can get the result below:

![image](https://user-images.githubusercontent.com/25773429/64223311-001a7c80-cf06-11e9-8cad-93c1e5a5f4a4.png)

However, for the sentence ""ÊúâÊâçÂçéÁöÑÈ≤ÅËøÖ‰ΩèÂú®Êó•Êú¨‰ªôÂè∞„ÄÇ"" in the same sentence structure, we get an incomplete result losing the location ""Êó•Êú¨‰ªôÂè∞""„ÄÇ

![image](https://user-images.githubusercontent.com/25773429/64223433-9babed00-cf06-11e9-9aba-9f785bafc698.png)

2.
For some sentences with ‚Äú‰∫Ü‚Äù(aspect marker) as a word to express the past, such as ""ÊùéÂÖàÁîüË¥≠‰π∞**‰∫Ü**ÊâãÊú∫"", and other sentences with ""‰∏ÄÈÉ®""(classifier modifier) as a classifier phrase, such as ""ÊùéÂÖàÁîüË¥≠‰π∞**‰∏ÄÈÉ®**ÊâãÊú∫"", we get NOTHING.
![image](https://user-images.githubusercontent.com/25773429/64223881-16c1d300-cf08-11e9-9e1a-878f5ac659e7.png)

![image](https://user-images.githubusercontent.com/25773429/64223897-2214fe80-cf08-11e9-909d-fb443ad28f52.png)

Just simple sentence ""ÊùéÂÖàÁîüË¥≠‰π∞ÊâãÊú∫"" without any aspect marker and classifier modifier:
![image](https://user-images.githubusercontent.com/25773429/64223914-2e00c080-cf08-11e9-8f87-d293ec3a1195.png)

So how can I do for correct OpenIE results?


"
783,https://github.com/stanfordnlp/CoreNLP/issues/947,947,[],closed,2019-09-17 14:11:27+00:00,,Chinese POS Tagger Tags Text as URL,"I guess this is an error in the training data. 

When using the Chinese tagger - on text which includes English. The English phrase below get tokens tagged as URL.

I'm also surprised to see a URL tag - I can't find that tag documented anywhere.

```
I learned a lot about chemisty. The terrible
I = PN
learned = VV
a = CD
lot = NN
about = URL
chemisty. = URL
The = DT
terrible = NN
```"
784,https://github.com/stanfordnlp/CoreNLP/issues/948,948,[],closed,2019-09-19 10:20:55+00:00,,Treebank Corpus,"How to create tree corpus from a sentence?I have created a model using RNTN and the model trained by stanford sentiment treebank train dataset.I have to test with new sentence.Please help me.
Thank you"
785,https://github.com/stanfordnlp/CoreNLP/issues/949,949,[],closed,2019-09-23 07:18:39+00:00,,How to use only one port & my port is not enough?," i ues corenlp-server like [ java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000 ]  , i want to split and tokenize sentencesÔºåwhen i run my script i found that each sentence  will occupy a new portÔºåand then throw exception [ Failed to establish a new connection  OSError: [WinError 10048] ÈÄöÂ∏∏ÊØè‰∏™Â•óÊé•Â≠óÂú∞ÂùÄ(ÂçèËÆÆ/ÁΩëÁªúÂú∞ÂùÄ/Á´ØÂè£)Âè™ÂÖÅËÆ∏‰ΩøÁî®‰∏ÄÊ¨°„ÄÇ
] 

my  script

from stanza.nlp.corenlp import CoreNLPClient

client = None

def annotate(sent):
    global client
    if client is None:
        client = CoreNLPClient(default_annotators='ssplit,tokenize'.split(','))
    words = []
    for sent in client.annotate(sent).sentences:
        for tok in sent:
            words.append(tok.word)
    return words"
786,https://github.com/stanfordnlp/CoreNLP/issues/950,950,[],closed,2019-09-25 12:04:35+00:00,,entitymentions  doesn't return entity built using Regexner,"Hello all, I loaded my custom tags in the pipeline, but I can't get the entities with the new tags.
here is my code:
```
Properties props = new Properties();
props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, regexner"");
String rules = Test.class.getResource(""/rules.txt"").getPath();
props.put(""regexner.mapping"", rules);
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
CoreDocument document = new CoreDocument(text);
pipeline.annotate(document);
List<CoreEntityMention> entityMentions = document.entityMentions();
for(CoreEntityMention em: entityMentions){
    System.out.printf(""Entity: %s - Type: %s\n"", em.text(), em.entityType());
}
```
however, when I iterate the token and see the tags each token got, I can see the regexner is working, But most if not all of my custom tags are 2 or more words. What I want is when i call the `document.entityMentions()` I get everything, default tags and custom tags.

I need help please! "
787,https://github.com/stanfordnlp/CoreNLP/issues/951,951,[],closed,2019-10-03 13:09:06+00:00,,NullPointerException while setting Annotation in corenlp 3.9.2,"I am getting below error, i am using pom.xml with below two dependencies and getting the same error, i am in java 1.8. The error is coming at line : -pipeline.annotate(document)

Code: - 

```
String result="""";
        Properties props = new Properties();
        props.put(""ner.applyFineGrained"", ""0"");
        props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref,sentiment"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        // create an empty Annotation just with the given text
        Annotation document = new Annotation(text);
        // run all Annotators on this text
        **pipeline.annotate(document);**
```


ERROR: 
```
Exception in thread ""main"" java.lang.NullPointerException
at edu.stanford.nlp.pipeline.DeterministicCorefAnnotator.setNamedEntityTagGranularity(DeterministicCorefAnnotator.java:93)
at edu.stanford.nlp.pipeline.DeterministicCorefAnnotator.annotate(DeterministicCorefAnnotator.java:173)
at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:637)
at 
```

Maven:

```
<dependency>
		    <groupId>edu.stanford.nlp</groupId>
		    <artifactId>stanford-corenlp</artifactId>
		    <version>3.9.2</version>
		</dependency>
		
		<dependency>
		    <groupId>edu.stanford.nlp</groupId>
		    <artifactId>stanford-corenlp</artifactId>
		    <version>3.9.2</version>
		    <classifier>models</classifier>
		</dependency>
```


"
788,https://github.com/stanfordnlp/CoreNLP/issues/952,952,[],closed,2019-10-15 14:11:09+00:00,,PoS tagging: noun-verb confusion,"Hi,

When I run PoS tagging for the sentence ""Tom is an idiot."", ""idiot"" is tagged as VB, not NN.

Tom NNP
is VBZ
an DT
idiot VB
. .

If I replace ""idiot"" with another noun such as ""extrovert/angel/etc."", the tagging is correct, so the issue is not systematic.

I'm using version 3.9.2, with the models from 2018-10-05, and am using the english-left3words-distsim.tagger model."
789,https://github.com/stanfordnlp/CoreNLP/issues/953,953,[],closed,2019-10-15 15:07:48+00:00,,Latest Spanish model detects Countries as Brands,"Hey team,

I'm getting some unexpected behavior while trying to recognize countries in Spanish - could you please have a look? Details below:

Model downloaded from: http://nlp.stanford.edu/software/stanford-spanish-corenlp-models-current.jar
Model Last Updated: 2019-08-27
Using these dependencies: 
* `edu.stanford.nlp:stanford-corenlp:3.9.2:models-spanish`
* `edu.stanford.nlp:stanford-corenlp:3.9.2:models`
* `edu.stanford.nlp:stanford-corenlp:3.9.2`

Reproduce this error by running this snippet:
```
    Properties props = new Properties();
    props.setProperty(""annotators"", ""tokenize,ssplit,ner"");
    props.setProperty(""tokenize.language"", ""es"");
    props.setProperty(""ner.model"", ""edu/stanford/nlp/models/ner/spanish.ancora.distsim.s512.crf.ser.gz"");
    props.setProperty(""ner.applyNumericClassifiers"", ""false"");
    props.setProperty(""ner.useSUTime"", ""false"");
    props.setProperty(""ner.applyFineGrained"", ""false"");
    props.setProperty(""ner.language"", ""es"");
    StanfordCoreNLP nlp = new StanfordCoreNLP(props);

    CoreDocument document = new CoreDocument(""El texto contiene Alemania e Inglaterra"");
    nlp.annotate(document);
    document.entityMentions().stream().forEach(e -> System.out.println(e.toString() + "", "" + e.entityType()));
```

This will print out
```
Alemania, ORGANIZATION
Inglaterra, ORGANIZATION
```

Which is incorrect, because Germany and England should be COUNTRY, not ORGANIZATION

Warm regards."
790,https://github.com/stanfordnlp/CoreNLP/issues/954,954,[],closed,2019-10-17 16:05:44+00:00,,How to put multiple text files for regexner mapping?,"```
Properties props=new Properties();
props.put(""annotators"", ""tokenize, ssplit, pos, lemma, ner, regexner"");
props.put(""regexner.mapping"", ""C:\\Users\\This\\Desktop\\chemicals.txt"");
props.put(""regexner.mapping"", ""C:\\Users\\This\\Desktop\\diseases.txt"");
props.put(""regexner.mapping"", ""C:\\Users\\This\\Desktop\\protein.txt"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
.
.
.
```
this does not work..."
791,https://github.com/stanfordnlp/CoreNLP/issues/955,955,[],open,2019-10-18 16:51:36+00:00,,How to get print Entity Mentions in REGEXNER entity tag?,"```
for (CoreMap entityMention : sentence.get(CoreAnnotations.MentionsAnnotation.class)) {
System.out.print(entityMention.get(CoreAnnotations.NamedEntityTagAnnotation.class));
}
```
Only prints NAME, ORG, LOC
My regexner have other entity tag"
792,https://github.com/stanfordnlp/CoreNLP/issues/957,957,[],closed,2019-10-24 18:51:20+00:00,,Server Timed out waiting for service to come alive.,"Hi,

I am having a problem when running the example code from [here](https://stanfordnlp.github.io/stanfordnlp/corenlp_client.html) under ""usage"", which results in the following error (be_quiet=False):
```
---
input text

Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.
---
starting up Java Stanford CoreNLP Server...
Starting server with command: java -Xmx5G -cp /Users/georgestoica/Desktop/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-0a8f603e8bb043a1.props -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref
[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - setting default constituency parser
[main] INFO CoreNLP - warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP - using: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz instead
[main] INFO CoreNLP - to use shift reduce parser download English models jar from:
[main] INFO CoreNLP - http://stanfordnlp.github.io/CoreNLP/download.html
[main] INFO CoreNLP -     Threads: 5
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.4 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.4 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.4 sec].
[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
[main] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 580704 unique entries out of 581863 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_caseless.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 4869 unique entries out of 4869 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_cased.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 585573 unique entries from 2 files
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.2 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse
[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... 
[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 7.668 (s)
[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [13.1 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref
[main] INFO edu.stanford.nlp.coref.statistical.SimpleLinearClassifier - Loading coref model edu/stanford/nlp/models/coref/statistical/ranking_model.ser.gz ... done [1.8 sec].
[main] INFO edu.stanford.nlp.pipeline.CorefMentionAnnotator - Using mention detector type: dependency
[main] INFO CoreNLP - Starting server...
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9001
Traceback (most recent call last):
  File ""/Users/georgestoica/Library/Preferences/PyCharm2019.1/scratches/scratch_58.py"", line 19, in <module>
    ann = client.annotate(text)
  File ""/opt/anaconda3/lib/python3.7/site-packages/stanfordnlp/server/client.py"", line 398, in annotate
    r = self._request(text.encode('utf-8'), request_properties, **kwargs)
  File ""/opt/anaconda3/lib/python3.7/site-packages/stanfordnlp/server/client.py"", line 311, in _request
    self.ensure_alive()
  File ""/opt/anaconda3/lib/python3.7/site-packages/stanfordnlp/server/client.py"", line 137, in ensure_alive
    raise PermanentlyFailedException(""Timed out waiting for service to come alive."")
stanfordnlp.server.client.PermanentlyFailedException: Timed out waiting for service to come alive.
```
Note that the port is 9001 because I manually changed it after initially having the exact same issue as #663.

To install, I followed the instructions [here](https://stanfordnlp.github.io/CoreNLP/download.html) and [here](https://stanfordnlp.github.io/stanfordnlp/corenlp_client.html) and I'm on OSX 10.14.6 with anaconda3's python3.7.4. 

I've looked online and while there are similar issues, such as #663, this issue appears to be a bit different. Is there anything obvious that I'm missing or that might be causing this problem? 

Any advice or suggestions would be greatly appreciated! 
Thanks!"
793,https://github.com/stanfordnlp/CoreNLP/issues/958,958,[],open,2019-10-26 10:18:21+00:00,,Error occurs when adding annotator 'kbp' for Chinese language,"I'm using KBP for relation extraction in Chinese. There is currently a model for Chinese according to the official introduction.
I added kbp annotator into `StanfordCoreNLP-chinese.properties` . When I ran the client with python interface, the error below occurred:
```
Starting server with command: java -Xmx6G -cp %CORENLP_HOME%/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 300000 -threads 8 -maxCharLength 100000 -quiet False -serverProperties StanfordCoreNLP-chinese.properties -preload tokenize,ssplit,pos,lemma,ner,parse,coref,kbp
[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - setting default constituency parser
[main] INFO CoreNLP - warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP - using: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz instead
[main] INFO CoreNLP - to use shift reduce parser download English models jar from:
[main] INFO CoreNLP - http://stanfordnlp.github.io/CoreNLP/download.html
[main] INFO CoreNLP -     Threads: 8
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/segmenter/chinese/ctb.gz ... done [10.3 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger ... done [0.8 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz ... done [5.2 sec].
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 21238 unique entries out of 21249 from edu/stanford/nlp/models/kbp/chinese/gazetteers/cn_regexner_mapping.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/srparser/chineseSR.ser.gz ... done [19.0 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref
[main] INFO edu.stanford.nlp.pipeline.CorefMentionAnnotator - Using mention detector type: rule
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator kbp
[main] ERROR CoreNLP - Could not pre-load annotators in server; encountered exception:
java.util.regex.PatternSyntaxException: Unclosed character class near index 3
[""Èà•Êºñ
   ^
        at java.util.regex.Pattern.error(Unknown Source)
        at java.util.regex.Pattern.clazz(Unknown Source)
        at java.util.regex.Pattern.sequence(Unknown Source)
        at java.util.regex.Pattern.expr(Unknown Source)
        at java.util.regex.Pattern.compile(Unknown Source)
        at java.util.regex.Pattern.<init>(Unknown Source)
        at java.util.regex.Pattern.compile(Unknown Source)
        at edu.stanford.nlp.semgraph.semgrex.NodePattern.<init>(NodePattern.java:81)
        at edu.stanford.nlp.semgraph.semgrex.NodePattern.<init>(NodePattern.java:47)
        at edu.stanford.nlp.semgraph.semgrex.SemgrexParser.Description(SemgrexParser.java:543)
        at edu.stanford.nlp.semgraph.semgrex.SemgrexParser.Child(SemgrexParser.java:440)
        at edu.stanford.nlp.semgraph.semgrex.SemgrexParser.ModNode(SemgrexParser.java:415)
        at edu.stanford.nlp.semgraph.semgrex.SemgrexParser.Relation(SemgrexParser.java:329)
        at edu.stanford.nlp.semgraph.semgrex.SemgrexParser.RelChild(SemgrexParser.java:230)
        at edu.stanford.nlp.semgraph.semgrex.SemgrexParser.ModRelation(SemgrexParser.java:195)
        at edu.stanford.nlp.semgraph.semgrex.SemgrexParser.RelationConj(SemgrexParser.java:176)
        at edu.stanford.nlp.semgraph.semgrex.SemgrexParser.RelationDisj(SemgrexParser.java:123)
        at edu.stanford.nlp.semgraph.semgrex.SemgrexParser.SubNode(SemgrexParser.java:103)
        at edu.stanford.nlp.semgraph.semgrex.SemgrexParser.Root(SemgrexParser.java:34)
        at edu.stanford.nlp.semgraph.semgrex.SemgrexPattern.compile(SemgrexPattern.java:291)
        at edu.stanford.nlp.semgraph.semgrex.SemgrexBatchParser.parse(SemgrexBatchParser.java:57)
        at edu.stanford.nlp.semgraph.semgrex.SemgrexBatchParser.compileStream(SemgrexBatchParser.java:47)
        at edu.stanford.nlp.semgraph.semgrex.SemgrexBatchParser.compileStream(SemgrexBatchParser.java:39)
        at edu.stanford.nlp.ie.KBPSemgrexExtractor.<init>(KBPSemgrexExtractor.java:56)
        at edu.stanford.nlp.pipeline.KBPAnnotator.<init>(KBPAnnotator.java:115)
        at edu.stanford.nlp.pipeline.AnnotatorImplementations.kbp(AnnotatorImplementations.java:290)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$25(StanfordCoreNLP.java:543)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$30(StanfordCoreNLP.java:602)
        at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)
        at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
        at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:251)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:192)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:188)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer.main(StanfordCoreNLPServer.java:1505)
```
I have downloaded the model for Chinese and got an NER result.
Does anybody know the reason for this error?"
794,https://github.com/stanfordnlp/CoreNLP/issues/959,959,[],closed,2019-10-26 21:35:14+00:00,,Incorrect character positions for tokens,"Hi,

Taking the simple following example : 

```
text = ""Barack Obama is the president of the United States. He is born in Hawaii.""
with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner','depparse'], timeout=60000, memory='8G') as client:
    ann = client.annotate(text)
    for s in ann.sentence:
      for token in s.token:
        t = {
          'lemma': token.lemma, 
          'pos': token.pos, 
          'ner': token.ner, 
          'value': token.originalText, 
          'begin_charachter': token.beginChar, 
          'end_character': token.endChar
          }
        print(t)
```

Yields the first token as 

```
{
        ""lemma"": ""Barack"",
        ""pos"": ""NNP"",
        ""ner"": ""PERSON"",
        ""value"": ""Barack"",
        ""begin_charachter"": 0,
        ""end_character"": 6
      }
```

With a character starting from 0, I've hard to see a reason why the end character is `6`.

Similarly, taking the full stop token of the sentence, it shows a single character token having a different start and end character position.

```
{
        ""lemma"": ""."",
        ""pos"": ""."",
        ""ner"": ""O"",
        ""value"": ""."",
        ""begin_charachter"": 50,
        ""end_character"": 51
      }
```

Behaviour is same using java. "
795,https://github.com/stanfordnlp/CoreNLP/issues/960,960,[],closed,2019-10-27 11:38:22+00:00,,ClassifierDemo.java : Cannot find the file examples/cheeseDisease.train,"Hello,

I am trying to implement a code to train my own Model like the code in `CoreNLP/src/edu/stanford/nlp/classify/demo/ClassifierDemo.java`, but I cannot find the file located in `examples/cheeseDisease.train` in order to prepare my own train file."
796,https://github.com/stanfordnlp/CoreNLP/issues/961,961,[],open,2019-10-27 22:49:06+00:00,,Android: NER NullPointerException on some models,"I (somewhat) successfully integrated CoreNLP (3.9.2) in an Android app.
The following annotator configuration works just fine:
```Kotlin
props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma"")
```
But as soon as I add the NER annotator I start to get the following error: 
```
Caused by: java.lang.NullPointerException: Attempt to invoke interface method 'int java.util.List.size()' on a null object reference
        at edu.stanford.nlp.util.HashIndex.size(HashIndex.java:94)
        at edu.stanford.nlp.ie.crf.CRFClassifier.getCliqueTree(CRFClassifier.java:1499)
        at edu.stanford.nlp.ie.crf.CRFClassifier.getSequenceModel(CRFClassifier.java:1190)
        at edu.stanford.nlp.ie.crf.CRFClassifier.getSequenceModel(CRFClassifier.java:1186)
        at edu.stanford.nlp.ie.crf.CRFClassifier.classifyMaxEnt(CRFClassifier.java:1218)
        at edu.stanford.nlp.ie.crf.CRFClassifier.classify(CRFClassifier.java:1128)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifySentence(AbstractSequenceClassifier.java:299)
        at edu.stanford.nlp.ie.ClassifierCombiner.classify(ClassifierCombiner.java:476)
        at edu.stanford.nlp.ie.NERClassifierCombiner.classifyWithGlobalInformation(NERClassifierCombiner.java:269)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifySentenceWithGlobalInformation(AbstractSequenceClassifier.java:343)
        at edu.stanford.nlp.pipeline.NERCombinerAnnotator.doOneSentence(NERCombinerAnnotator.java:368)
        at edu.stanford.nlp.pipeline.SentenceAnnotator.annotate(SentenceAnnotator.java:102)
        at edu.stanford.nlp.pipeline.NERCombinerAnnotator.annotate(NERCombinerAnnotator.java:310)
        at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:637)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:629)
```
The code I use (Kotlin):
```Kotlin
val props = Properties()
props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner"")
pipeline = StanfordCoreNLP(props)
val document = CoreDocument(""Joe Smith is from Seattle."")
pipeline.annotate(document)
```

The error is very similar to the one described in [this issue](https://github.com/stanfordnlp/CoreNLP/issues/861) where the author tried to use the parser annotator. 

## Debugging

I debugged the stack trace and found that the error is caused by this line (on `classIndex.size()`) in [`CRFClassifier:1480`](https://github.com/stanfordnlp/CoreNLP/blob/eb43d5d9150de97f8061fa06b838f1d021586789/src/edu/stanford/nlp/ie/crf/CRFClassifier.java#L1480):

```java
return CRFCliqueTree.getCalibratedCliqueTree(data, labelIndices, classIndex.size(), 
  classIndex, flags.backgroundSymbol, getCliquePotentialFunctionForTest(), featureVal);
```
Meaning `classIndex` is null and was not initialized properly.

The `classIndex` property of `CRFClassifier` is initialized in the [`loadClassifier(ObjectInputStream ois, Properties props)`](https://github.com/stanfordnlp/CoreNLP/blob/eb43d5d9150de97f8061fa06b838f1d021586789/src/edu/stanford/nlp/ie/crf/CRFClassifier.java#L2570) method:

```java
public void loadClassifier(ObjectInputStream ois, Properties props) {
    Object o = ois.readObject();
    [...]
    classIndex = (Index<String>) ois.readObject();
``` 

I found out that the passed ObjectInputStream is effectively a stream on the file from a model path that is determined in the `NERCombinerAnnotator` constructor: 

```java
public NERCombinerAnnotator(Properties properties) throws IOException {
    List<String> models = new ArrayList<>();
    String modelNames = properties.getProperty(""ner.model"");
    if (modelNames == null) {
      modelNames = DefaultPaths.DEFAULT_NER_THREECLASS_MODEL + ',' + DefaultPaths.DEFAULT_NER_MUC_MODEL + ',' + DefaultPaths.DEFAULT_NER_CONLL_MODEL;
    }
    [...]
    String[] loadPaths = models.toArray(new String[models.size()]);
```

Those `loadPaths` are iterated in the `loadClassifiers` method in `ClassifierCombiner`:

```java
 private void loadClassifiers(Properties props, List<String> paths) throws IOException {
    baseClassifiers = new ArrayList<>();
    [...]
    for(String path: paths) {
      AbstractSequenceClassifier<IN> cls = loadClassifierFromPath(props, path);
      baseClassifiers.add(cls);
      [...]
    }
```

By adding a breakpoint to this method I found out that the first model path (`DefaultPaths.DEFAULT_NER_THREECLASS_MODEL`) in the first iteration of the for-loop is loaded without problems and the `classIndex` property is set correctly:

![Bildschirmfoto vom 2019-10-27 23-39-33](https://user-images.githubusercontent.com/19290349/67643066-2a3f5800-f913-11e9-93a5-d4a9c44b8b02.png)

But in the second iteration, when loading from `DefaultPaths.DEFAULT_NER_MUC_MODEL`, it fails:
 
![Bildschirmfoto vom 2019-10-27 23-43-18](https://user-images.githubusercontent.com/19290349/67643095-9457fd00-f913-11e9-94f0-6bf9e8d1d6c2.png)

## Workaround

My current workaround is to just set the ner model to only the threeclass and conll model:
```Kotlin
props.setProperty(""ner.model"", DefaultPaths.DEFAULT_NER_THREECLASS_MODEL + "","" + DefaultPaths.DEFAULT_NER_CONLL_MODEL)
``` 

But I actually don't what the consequences are if the MUC model is missing.

## Explanation

My theory is that the MUC model is especially large and thus can not be loaded into memory on a mobile device. Is that true? How big is the model in particular?
But when I monitor the memory consumption of the app I don't spot anything critical, the app stays under 512 MB before it crashes."
797,https://github.com/stanfordnlp/CoreNLP/issues/962,962,[],closed,2019-11-05 08:15:18+00:00,,NullPointerException on KBP models,"part of demo code from the website
```
import edu.stanford.nlp.coref.data.CorefChain;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ie.util.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.*;
import edu.stanford.nlp.trees.*;
import java.util.*;


public class BasicPipelineExample {

    public static String text = ""Joe Smith was born in California. "" +
            ""In 2017, he went to Paris, France in the summer. "" +
            ""His flight left at 3:00pm on July 10th, 2017. "" +
            ""After eating some escargot for the first time, Joe said, \""That was delicious!\"" "" +
            ""He sent a postcard to his sister Jane Smith. "" +
            ""After hearing about Joe's trip, Jane decided she might go to France one day."";

    public static void main(String[] args) {
        // set up pipeline properties
        Properties props = new Properties();
        // set the list of annotators to run
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,parse,depparse,kbp"");
        // set a property for an annotator, in this case the coref annotator is being set to use the neural algorithm
        props.setProperty(""coref.algorithm"", ""neural"");
        // build pipeline
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        // create a document object
        CoreDocument document = new CoreDocument(text);
        // annnotate the document
        pipeline.annotate(document);
        // examples

        // kbp relations found in fifth sentence
        List<RelationTriple> relations =
                document.sentences().get(4).relations();
        System.out.println(""Example: relation"");
        System.out.println(relations.get(0));
        System.out.println();
        
    }

}

```

maven
```
<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.mycompany.app</groupId>
    <artifactId>my-app</artifactId>
    <version>1.0-SNAPSHOT</version>
    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <configuration>
                    <source>8</source>
                    <target>8</target>
                </configuration>
            </plugin>
        </plugins>
    </build>

    <properties>
        <maven.compiler.source>1.7</maven.compiler.source>
        <maven.compiler.target>1.7</maven.compiler.target>
    </properties>

    <dependencies>
        <dependency>
            <groupId>edu.stanford.nlp</groupId>
            <artifactId>stanford-corenlp</artifactId>
            <version>3.9.2</version>
        </dependency>
        <dependency>
            <groupId>edu.stanford.nlp</groupId>
            <artifactId>stanford-corenlp</artifactId>
            <version>3.9.2</version>
            <classifier>models</classifier>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-simple</artifactId>
            <version>1.6.1</version>
        </dependency>
    </dependencies>

</project>
```

error msg:

```
0 [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
7 [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
10 [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
936 [main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.9 sec].
937 [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
938 [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
1537 [main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.6 sec].
1540 [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse
1594 [main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... 
12013 [main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 9.382 (s)
12013 [main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [10.4 sec].
12013 [main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator kbp
12703 [main] INFO edu.stanford.nlp.pipeline.KBPAnnotator - Loading KBP classifier from: edu/stanford/nlp/models/kbp/english/tac-re-lr.ser.gz
Exception in thread ""main"" java.lang.NullPointerException
	at java.base/java.util.ArrayList.addAll(ArrayList.java:701)
	at edu.stanford.nlp.pipeline.KBPAnnotator.annotate(KBPAnnotator.java:331)
	at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:637)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:629)
	at BasicPipelineExample.main(BasicPipelineExample.java:31)
```

Thank you

"
798,https://github.com/stanfordnlp/CoreNLP/issues/963,963,[],closed,2019-11-06 17:02:14+00:00,,CRAN/R CoreNLP fails to initialise quotes annotator,"Hi,

I've managed to get CoreNLP running in R using a custom properties file. The only problem is that when I initialise CoreNLP every annotator loads _except_ for quote, which is what I really need.

For reference, this is my properties file, saved in my working directory as StanfordCoreNLP.properties:
`annotators = tokenize,ssplit,pos,lemma,ner,depparse,coref,quote`

This is the code I've been using:
```
library(rJava)
library(coreNLP)
setwd(""~/Desktop/University/Thesis/Misc/Stanford/R/R project"")`
initCoreNLP(""~/Desktop/University/Thesis/Misc/Stanford/stanford-corenlp-full-2018-10-05"", type = ""english"", parameterFile = ""StanfordCoreNLP.properties"", mem = ""15g"")
```

What I get after running the last command is:
```
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Searching for resource: StanfordCoreNLP.properties ... found.
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos
[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.7 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.2 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.6 sec].
[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
[main] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 580704 unique entries out of 581863 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_caseless.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 4869 unique entries out of 4869 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_cased.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 585573 unique entries from 2 files
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse
[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... 
[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 8.79 (s)
[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [10.5 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref
[main] INFO edu.stanford.nlp.coref.statistical.SimpleLinearClassifier - Loading coref model edu/stanford/nlp/models/coref/statistical/ranking_model.ser.gz ... done [0.6 sec].
[main] INFO edu.stanford.nlp.pipeline.CorefMentionAnnotator - Using mention detector type: dependency
```

As you can see there's no error, it just stops. As a result the annotated file is returned without a section for quotes/speakers.

Any help would be much appreciated!

Thanks

"
799,https://github.com/stanfordnlp/CoreNLP/issues/964,964,[],closed,2019-11-06 18:58:32+00:00,,corenlp.run down,"Please restart.

Thanks,

Keith"
800,https://github.com/stanfordnlp/CoreNLP/issues/966,966,[],closed,2019-11-16 07:18:35+00:00,,NullPointerException when parsing sentences in Arabic,"Hi,

I was trying to parse the sentences in Arabic from OpenSubtitles, but got the following error (pos tagging works fine):
```
Exception in thread ""main"" java.lang.NullPointerException
        at edu.stanford.nlp.parser.lexparser.ExhaustiveDependencyParser.getBestScore(ExhaustiveDependencyParser.java:607)
        at edu.stanford.nlp.parser.lexparser.ExhaustiveDependencyParser.hasParse(ExhaustiveDependencyParser.java:602)
        at edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.getBestParse(LexicalizedParserQuery.java:398)
        at edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.getBestParse(LexicalizedParserQuery.java:375)
        at edu.stanford.nlp.pipeline.ParserAnnotator.doOneSentence(ParserAnnotator.java:328)
        at edu.stanford.nlp.pipeline.ParserAnnotator.doOneSentence(ParserAnnotator.java:254)
        at edu.stanford.nlp.pipeline.SentenceAnnotator.annotate(SentenceAnnotator.java:102)
        at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:637)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:647)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1226)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1060)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1343)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1389)
```

Following is the command I used

```
java -cp stanford-corenlp-3.9.2.jar:stanford-arabic-corenlp-2018-10-05-models.jar \
     -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP \
     -tokenize.language ar \
     -segment.model edu/stanford/nlp/models/segmenter/arabic/arabic-segmenter-atb+bn+arztrain.ser.gz \
     -pos.model edu/stanford/nlp/models/pos-tagger/arabic/arabic.tagger \
     -parse.model edu/stanford/nlp/models/lexparser/arabicFactored.ser.gz \
     -threads 1 \
     -annotators tokenize,ssplit,parse \
     -ssplit.eolonly \
     -filelist intpu_file_list.txt -outputFormat text \
     -outputDirectory parsed_ar
```

Did I do anything wrong? Any help would be appreciated!"
801,https://github.com/stanfordnlp/CoreNLP/issues/967,967,[],closed,2019-11-18 18:38:49+00:00,,Annotating a .csv,"Hi there,

Is there an easy way to annotate a .csv file using CoreNLP in the command line?

My .csv is two columns: 'article_id' and 'text'. I want to annotate the content of 'text' line by line, producing a list of quotes in each article (with the quote annotator), each with an 'id' value corresponding to the article the quote was in (i.e. the original 'id' column in the csv). 

I can't find anything online about annotating csv - all the examples seem to use .txt files.

Thanks very much for your help!"
802,https://github.com/stanfordnlp/CoreNLP/issues/968,968,[],closed,2019-11-19 13:44:22+00:00,,Sentence Splitter Fails with Arabic Input,"I'm trying to run the Sentence splitter and tokenizer on unstructured Arabic text. It seems to be failing to detect quite a few sentence boundaries, resulting in larger-than-a-sentence splits that ruin further parser operations.

The regex string in the properties file seems to be ignored, or not totally relied on; I'm not sure what I'm doing wrong. The only thing different in my properties file from the default one is the removal of `pos.models` and `parse.models` declarations, and their associated annotators.

All of the input is UTF-8.

My system:

```bash
$ java -version
openjdk version ""14-ea"" 2020-03-17
OpenJDK Runtime Environment (build 14-ea+18-Ubuntu-1)
OpenJDK 64-Bit Server VM (build 14-ea+18-Ubuntu-1, mixed mode, sharing)
```

I have the following jars in this directory:

```
protobuf.jar
stanford-arabic-corenlp-2018-10-05-models.jar
stanford-corenlp-3.9.2.jar
stanford-corenlp-3.9.2-javadoc.jar
stanford-corenlp-3.9.2-models.jar
stanford-corenlp-3.9.2-sources.jar
StanfordCoreNLP-arabic-noparse.properties
StanfordCoreNLP-arabic.properties
stanford-english-corenlp-2018-10-05-models.jar
stanford-english-kbp-corenlp-2018-10-05-models.jar
```

My command:

```bash
$ java -mx8g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLP \
  -props StanfordCoreNLP-arabic-noparse.properties \
  -file test.txt \
  -outputFormat json \
  -outputExtension .json
```

My properties file:
```
# Pipeline options
annotators = tokenize, ssplit

# segment
#customAnnotatorClass.segment = edu.stanford.nlp.pipeline.ArabicSegmenterAnnotator
tokenize.language = ar
segment.model = edu/stanford/nlp/models/segmenter/arabic/arabic-segmenter-atb+bn+arztrain.ser.gz

# sentence split
ssplit.boundaryTokenRegex = [.]|[!?]+|[!\u061F]+
```

My test input and output:

[test.txt](https://github.com/stanfordnlp/CoreNLP/files/3863977/test.txt)
[test.txt.json.txt](https://github.com/stanfordnlp/CoreNLP/files/3863980/test_parsed.txt)
"
803,https://github.com/stanfordnlp/CoreNLP/issues/969,969,[],closed,2019-11-19 15:00:05+00:00,,NullPointerException,"**Describe the bug**
I use CoreNLP 3.9.2 as a remote server with StanfordNLP 0.2.0.
I want to work on pre-tokenized inputs by specifying the following properties
```
properties = {
        'tokenize.whitespace': True,
        'tokenize.keepeol': True,
        'ssplit.eolonly': True
        }
```

I received the following NullPointer error:
Client-side
```
starting up Java Stanford CoreNLP Server...
Starting server with command: java -Xmx16G -cp /myhome/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-5074c5d23e934dbf.props -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref
Traceback (most recent call last):
  File ""/myhome/.local/lib/python3.6/site-packages/stanfordnlp/server/client.py"", line 330, in _request
    r.raise_for_status()
  File ""/myhome/github_repo/requests/requests/models.py"", line 840, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:9000/?properties=%7B%27outputFormat%27%3A+%27serialized%27%7D

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test.py"", line 24, in <module>
    ann = client.annotate(text)
  File ""/myhome/.local/lib/python3.6/site-packages/stanfordnlp/server/client.py"", line 398, in annotate
    r = self._request(text.encode('utf-8'), request_properties, **kwargs)
  File ""/myhome/.local/lib/python3.6/site-packages/stanfordnlp/server/client.py"", line 336, in _request
    raise AnnotationException(r.text)
stanfordnlp.server.client.AnnotationException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
```

Server-side
```
[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref
java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:205)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:870)
        at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77)
        at jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82)
        at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80)
        at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:692)
        at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77)
        at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:664)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.NullPointerException
        at edu.stanford.nlp.pipeline.NERCombinerAnnotator.annotate(NERCombinerAnnotator.java:322)
        at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
        at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:637)
        at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.lambda$handle$0(StanfordCoreNLPServer.java:857)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        ... 3 more
```



**To Reproduce**
Run the following example, which is modified from the document example by adding the properties and change the input texts with EOL.

```python
from stanfordnlp.server import CoreNLPClient

# example text
print('---')
print('input text')
print('')

text = ""Chris Manning is a nice person.\nChris wrote a simple sentence. He also gives oranges to people.\ntest it.""

print(text)

# set up the client
print('---')
print('starting up Java Stanford CoreNLP Server...')

# set up the client
properties = {
        'tokenize.whitespace': True,
        'tokenize.keepeol': True,
        'ssplit.eolonly': True
        }
with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'], timeout=30000, memory='16G', properties=properties) as client:
    # submit the request to the server
    ann = client.annotate(text)

    # get the first sentence
    sentence = ann.sentence[0]
    
    # get the constituency parse of the first sentence
    print('---')
    print('constituency parse of first sentence')
    constituency_parse = sentence.parseTree
    print(constituency_parse)

    # get the first subtree of the constituency parse
    print('---')
    print('first subtree of constituency parse')
    print(constituency_parse.child[0])

    # get the value of the first subtree
    print('---')
    print('value of first subtree of constituency parse')
    print(constituency_parse.child[0].value)

    # get the dependency parse of the first sentence
    print('---')
    print('dependency parse of first sentence')
    dependency_parse = sentence.basicDependencies
    print(dependency_parse)

    # get the first token of the first sentence
    print('---')
    print('first token of first sentence')
    token = sentence.token[0]
    print(token)

    # get the part-of-speech tag
    print('---')
    print('part of speech tag of token')
    token.pos
    print(token.pos)

    # get the named entity tag
    print('---')
    print('named entity tag of token')
    print(token.ner)

    # get an entity mention from the first sentence
    print('---')
    print('first entity mention in sentence')
    print(sentence.mentions[0])

    # access the coref chain
    print('---')
    print('coref chains for the example')
    print(ann.corefChain)

    # Use tokensregex patterns to find who wrote a sentence.
    pattern = '([ner: PERSON]+) /wrote/ /an?/ []{0,3} /sentence|article/'
    matches = client.tokensregex(text, pattern)
    # sentences contains a list with matches for each sentence.
    assert len(matches[""sentences""]) == 3
    # length tells you whether or not there are any matches in this
    assert matches[""sentences""][1][""length""] == 1
    # You can access matches like most regex groups.
    matches[""sentences""][1][""0""][""text""] == ""Chris wrote a simple sentence""
    matches[""sentences""][1][""0""][""1""][""text""] == ""Chris""

    # Use semgrex patterns to directly find who wrote what.
    pattern = '{word:wrote} >nsubj {}=subject >dobj {}=object'
    matches = client.semgrex(text, pattern)
    # sentences contains a list with matches for each sentence.
    assert len(matches[""sentences""]) == 3
    # length tells you whether or not there are any matches in this
    assert matches[""sentences""][1][""length""] == 1
    # You can access matches like most regex groups.
    matches[""sentences""][1][""0""][""text""] == ""wrote""
    matches[""sentences""][1][""0""][""$subject""][""text""] == ""Chris""
    matches[""sentences""][1][""0""][""$object""][""text""] == ""sentence""

```


**Expected behavior**
Serialized parsing results

**Environment (please complete the following information):**
 - OS: Ubuntu
 - Python version: 3.6.x
 - StanfordNLP version: 0.2.0
 - CoreNLP version: 3.9.2

**Additional context**
CoreNLP 3.9.1 doesn't have this issue."
804,https://github.com/stanfordnlp/CoreNLP/issues/970,970,[],closed,2019-11-25 09:33:14+00:00,,ClassifierDema.java - How to get the accuracy of each tested sentence programmatically,"I am using the `ColumnDataClassifier` and I followed the example in the class ClassifierDemo.java.
I need now to get the **accuracy** ( `P(clAnswer)` in logs) of my classifier on each test-sentence **programmatically** like in the following logs.

<img width=""1370"" alt=""Screenshot 2019-11-25 at 10 25 35"" src=""https://user-images.githubusercontent.com/37305687/69528502-9f339b00-0f6e-11ea-9609-453c01a85b00.png"">

It will be appropriate for me to get that **accuracy** ( `P(clAnswer)` in logs) of each sentence in my test file here in the `for loop` like in the bellow code:

```
cdc.trainClassifier(where + ""mdr.train"");
cdc.serializeClassifier(where + ""mdr.generated.model"");
System.out.println();
log.info(""Testing predictions of ColumnDataClassifier"");   
for (String line : ObjectBank.getLineIterator(where + ""mdr.test"", ""utf-8"")) {
          // Here is the place I would like to get that accuracy of each sentence like in logs. 
          Datum<String, String> d = cdc.makeDatumFromLine(line);
          System.out.printf(""%s  ==>  %s (%.4f)%n"", line, cdc.classOf(d), 
          cdc.scoresOf(d).getCount(cdc.classOf(d)));
}
```"
805,https://github.com/stanfordnlp/CoreNLP/issues/971,971,[],closed,2019-11-27 17:27:26+00:00,,Chinese sentence splitter fails on eolonly,"From a directory with the corenlp jars and the chinese models jar:

```java -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -props StanfordCoreNLP-chinese.properties -annotators tokenize,ssplit,pos,parse -ssplit.eolonly -outputFormat text -file chinese.txt ```

where chinese.txt looks like

```
Â§úÁ©∫‰∏≠ÊúÄ‰∫ÆÁöÑÊòü ËÉΩÂê¶ËÅΩÊ∏Ö
ÈÇ£‰ª∞ÊúõÁöÑ‰∫∫ ÂøÉÂ∫ïÁöÑÂ≠§Áç®ÂíåÊ≠éÊÅØ
```

Fails with the error

```Exception in thread ""main"" java.lang.IndexOutOfBoundsException: Index 25 out of bounds for length 25
	at java.base/jdk.internal.util.Preconditions.outOfBounds(Preconditions.java:64)
	at java.base/jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Preconditions.java:70)
	at java.base/jdk.internal.util.Preconditions.checkIndex(Preconditions.java:248)
	at java.base/java.util.Objects.checkIndex(Objects.java:372)
	at java.base/java.util.ArrayList.get(ArrayList.java:458)
	at edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator.advancePos(ChineseSegmenterAnnotator.java:285)
	at edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator.runSegmentation(ChineseSegmenterAnnotator.java:395)
	at edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator.doOneSentence(ChineseSegmenterAnnotator.java:133)
	at edu.stanford.nlp.pipeline.ChineseSegmenterAnnotator.annotate(ChineseSegmenterAnnotator.java:127)
	at edu.stanford.nlp.pipeline.TokenizerAnnotator.annotate(TokenizerAnnotator.java:336)
	at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:637)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:647)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1226)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1060)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1326)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1389)
```"
806,https://github.com/stanfordnlp/CoreNLP/issues/972,972,[],open,2019-11-28 17:58:24+00:00,,How to get UD-POS tags in conllu dependency output?,"I am trying to run coreNLP for parsing some documents. How to get UD-POS tags along with fine-grained tags in output? I tried -outputFormat conllu, but the UPOS column remained empty.
For example, I ran this command:
 ""java -cp '../stanford-corenlp-full-2018-10-05/*' edu.stanford.nlp.pipeline.StanfordCoreNLP -file './samples/BA_wiki_00_id4397286_sample.txt' -outputFormat conllu -outputDirectory './samples/' ""
I want  this:
**1	Antonius	Antonius	NOUN	NNP	_	2	compound	_	_**
that is **'NOUN NNP'** both tags, but this is what I got:

1	Antonius	Antonius	_	NNP	_	2	compound	_	_
2	Romanus	Romanus	_	NNP	_	12	nsubj	_	_
3	(	(	_	-LRB-	_	4	punct	_	_
4	fl.	fl.	_	VBP	_	12	nsubj	_	_
5	1400	1400	_	CD	_	4	dep	_	_
6	--	--	_	:	_	7	punct	_	_
7	1432	1432	_	CD	_	5	dep	_	_
8	)	)	_	-RRB-	_	7	punct	_	_
9	was	be	_	VBD	_	12	cop	_	_
10	an	a	_	DT	_	12	det	_	_
11	Italian	italian	_	JJ	_	12	amod	_	_
12	composer	composer	_	NN	_	0	root	_	_
13	of	of	_	IN	_	17	case	_	_
14	the	the	_	DT	_	17	det	_	_
15	early	early	_	JJ	_	17	amod	_	_
16	15th	15th	_	JJ	_	17	amod	_	_
17	century	century	_	NN	_	12	nmod	_	_
18	.	.	_	.	_	12	punct	_	_

Please tell command-line solutions."
807,https://github.com/stanfordnlp/CoreNLP/issues/974,974,[],closed,2019-12-05 06:52:24+00:00,,How we batch to train own model?,"I am trying to train a own ner, but I always got out of memory.
Therefore, I try use batch training. 
The following is my step and cmd.
- java -cp ""stanford-ner.jar"" -mx4g edu.stanford.nlp.ie.crf.CRFClassifier -prop train/prop.txt

But after my first batch training, I got the domain.gz. How I train second batch based on domain.gz.
Because my input of cmd is stanford-ner.jar.
"
808,https://github.com/stanfordnlp/CoreNLP/issues/975,975,[],closed,2019-12-08 15:48:10+00:00,,IndexOutofBoundsException while running the annotation pipeline in English text,"I am getting IndexOutofBoundsException while running the annotation pipeline. Following is the trace of the error that I am getting:

Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.8 sec].
Adding annotator lemma
Adding annotator ner
Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].
Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.7 sec].
ner.fine.regexner: Read 580705 unique entries out of 581864 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_caseless.tab, 0 TokensRegex patterns.
ner.fine.regexner: Read 4869 unique entries out of 4869 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_cased.tab, 0 TokensRegex patterns.
ner.fine.regexner: Read 585574 unique entries from 2 files
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.5 sec].

Processing file /Users/syau/Desktop/Fall2019/830-610-890/XMLParser/Pilot/RST-PDTB-parser/DPLP/data/input.txt ... writing to /Users/syau/Desktop/Fall2019/830-610-890/XMLParser/Pilot/RST-PDTB-parser/DPLP/CoreNLP/input.txt.out
Exception in thread ""main"" java.lang.IndexOutOfBoundsException: Index 9 out of bounds for length 9
	at java.base/jdk.internal.util.Preconditions.outOfBounds(Preconditions.java:64)
	at java.base/jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Preconditions.java:70)
	at java.base/jdk.internal.util.Preconditions.checkIndex(Preconditions.java:248)
	at java.base/java.util.Objects.checkIndex(Objects.java:372)
	at java.base/java.util.ArrayList.get(ArrayList.java:458)
	at edu.stanford.nlp.pipeline.NERCombinerAnnotator.transferNERAnnotationsToAnnotation(NERCombinerAnnotator.java:403)
	at edu.stanford.nlp.pipeline.NERCombinerAnnotator.annotate(NERCombinerAnnotator.java:495)
	at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:639)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:649)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1228)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1062)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1328)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1391)"
809,https://github.com/stanfordnlp/CoreNLP/issues/976,976,[],open,2019-12-09 08:53:00+00:00,,Stanford Classifier: What is the max value of the scores for each datum ?,"I am using the `ColumnDataClassifier.java  ` and for each of my test Datum and I get following scores like in the bellow screenshot:

<img width=""896"" alt=""Screenshot 2019-12-07 at 10 04 11"" src=""https://user-images.githubusercontent.com/37305687/70420758-b0999e80-1a68-11ea-85b3-6f78f9e02a11.png"">

**My issue** is: What is the **max value of the score** that can reach each test Datum ?
And it is possible to set this **max-score-value** to something like **10**. The purpose is to establish somme statistics on my **test Datum**."
810,https://github.com/stanfordnlp/CoreNLP/issues/977,977,[],closed,2019-12-11 02:01:41+00:00,,CoreNLP Visualizer (Constituency Parse) Does Not Match Code Output,"For the text: ""i dropped my phone and I want to do a swap"". In corenlp.run, a VP (Verb Phrase) of ""dropped my phone"" is present, but doing a constituency parse on my code, the VP is not.

Here's my annotations: ""tokenize, ssplit, parse, depparse, lemma"". I am using all the default models (no customizations)."
811,https://github.com/stanfordnlp/CoreNLP/issues/978,978,[],closed,2019-12-14 08:42:07+00:00,,Bad Sentence For English SR Parser,"This sentence generates a crash when trying to train the English SR Parser:

```
( (S (NP-SBJ (DT the) (NNPS knights) (NNP inn)) (VP (VBD was) (ADJP-PRD (ADJP (JJ small)) (ADJP (RB very) (JJ small)))) (. .)) (FRAG (PRN (S (NP-SBJ (PRP i)) (VP (VBP mean)))) (NP (CD 1) (NN room)) (PP (IN in) (NP (DT every) (NN room))) (. !)) (UCP (S (NP-SBJ (PRP it)) (VP (VBD was) (ADJP-PRD (JJ cozy)) (NP-ADV (DT a) (JJ little)))) (CC and) (NP (DT a) (JJ small) (NN tv)) (. .)) )
```"
812,https://github.com/stanfordnlp/CoreNLP/issues/979,979,[],open,2019-12-25 01:07:18+00:00,,Where is the NFLAnnotator.java,"Hi, I've seen NFLAnnotator.java in `src/edu/stanford/nlp/pipeline/package-info.java` but I can not find this file. Where the NFLAnnotator.java locates ?"
813,https://github.com/stanfordnlp/CoreNLP/issues/980,980,[],open,2020-01-08 07:18:36+00:00,,Python StanfordcorenlpServer client identify ::: NER ignorecase is not working,"**Started server with command:**
java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -annotators ""tokenize,ssplit,pos,lemma,parse,sentiment"" -port 9000 -timeout 30000

**Python Client:**
`
from collections import defaultdict
from stanfordcorenlp import StanfordCoreNLP
import json

class StanfordNLP:
    def __init__(self, host='http://localhost', port=9000):
        self.nlp = StanfordCoreNLP(host, port=port,
                                   timeout=30000)  # , quiet=False, logging_level=logging.DEBUG)
        self.props = {
            'annotators': 'tokenize, ssplit, pos, lemma, ner, parse, depparse, dcoref, relation, truecase',
            'pipelineLanguage': 'en',
            'truecase.overwriteText': 'true',
            'outputFormat': 'json'
        }If I provide the input text :
text = 'rajesh lives in hyderbad'

    def ner(self, sentence):
        return self.nlp.ner(sentence)

    def annotate(self, sentence):
        return json.loads(self.nlp.annotate(sentence, properties=self.props))

    @staticmethod
    def tokens_to_dict(_tokens):
        tokens = defaultdict(dict)
        for token in _tokens:
            tokens[int(token['index'])] = {
                'ner': token['ner']
            }
        return tokens

if __name__ == '__main__':
    sNLP = StanfordNLP()
    **text = 'Rajesh lives in Hyderabad'**
    print (""NER:"", `sNLP.ner(text))`

**_Expected Output:
NER: [('Rajesh', 'PERSON'), ('lives', 'O'), ('in', 'O'), ('Hyderabad', 'LOCATION')]_**

**_Actual Output:
NER: [('Rajesh', 'PERSON'), ('lives', 'O'), ('in', 'O'), ('Hyderabad', 'LOCATION')]_**

**If I provide the input text :
text = 'rajesh lives in hyderbad'**
**_Expected Output:
NER: [('rajesh', 'PERSON'), ('lives', 'O'), ('in', 'O'), ('hyderabad', 'LOCATION')]_**

**_Actual Output:
NER: [('rajesh', 'O'), ('lives', 'O'), ('in', 'O'), ('hyderabad', 'O')]_**

**I wish to solve this by using true-case annotation however my attempt didn't worked out**
"
814,https://github.com/stanfordnlp/CoreNLP/issues/981,981,[],open,2020-01-09 07:15:39+00:00,,"Question/Feature :: NER tag overwriting if its identified as other from one classifier but however identified correctly with next classifier, Is it possible with stanford core nlp???","I have a token which is recognized as others in some classifiers however the same token is identified as Person or Organization. In such a case the entity need to be identified whenever the first classifier detected as other.

example models used are 
ner.model = edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz,edu/stanford/nlp/models/ner/english.conll.4class.caseless.distsim.crf.ser.gz

Any Rule to be defined to overwrite the condition"
815,https://github.com/stanfordnlp/CoreNLP/issues/982,982,[],open,2020-01-10 01:07:10+00:00,,What's the 'speaker' attribute in the token output?,"I am testing CoreNLPClient with the sentence ""Apple is great"" with the following output:

Starting server with command: java -Xmx8G -cp /Users/congminmin/nlp/stnlp/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties StanfordCoreNLP.properties -preload [tokenize]
{'index': 1, 'word': 'apple', 'originalText': 'apple', 'lemma': 'apple', 'characterOffsetBegin': 0, 'characterOffsetEnd': 5, 'pos': 'NN', 'ner': 'O', 'speaker': 'PER0', 'before': '', 'after': ' '}
{'index': 2, 'word': 'is', 'originalText': 'is', 'lemma': 'be', 'characterOffsetBegin': 6, 'characterOffsetEnd': 8, 'pos': 'VBZ', 'ner': 'O', 'speaker': 'PER0', 'before': ' ', 'after': ' '}
{'index': 3, 'word': 'good', 'originalText': 'good', 'lemma': 'good', 'characterOffsetBegin': 9, 'characterOffsetEnd': 13, 'pos': 'JJ', 'ner': 'O', 'speaker': 'PER0', 'before': ' ', 'after': ''}

What are 'ner', 'speaker' in the output?"
816,https://github.com/stanfordnlp/CoreNLP/issues/983,983,[],closed,2020-01-11 20:34:12+00:00,,RegexNER overwrites CoreNLP NER tags,"**Update / Solution:**  it was a `$CLASSPATH` issue. Users reading this can skip most of the content below; here are the key comments.

https://github.com/stanfordnlp/CoreNLP/issues/983#issuecomment-573462561 [@J38 comment]
https://github.com/stanfordnlp/CoreNLP/issues/983#issuecomment-573536392  [@victoriastuart actual issue | solution]
https://github.com/stanfordnlp/CoreNLP/issues/983#issuecomment-573537834 [@victoriastuart comment re: appending `$CLASSPATH` to `~/.bashrc` or `~/.profile`]

---

I'm going to echo @DaveQuinn29 's concern [#910] that there is a cache and/or some other issue in CoreNLP.  Issues involved include:

* old results retained (""cached""?)
* in that regard, you can ""clear"" that old output by seqentially running (adding) the annotators (`tokenize | tokenize, ssplit | ...`)
* when `-regexner.mapping` is used in conjunction with `ner, regexner` annotators, the RegexNER mappings (in a TSV) are ignored
* when `-regexner.mapping` is used in conjunction with `regexner` annotator (`ner` is excluded), the RegexNER mappings (in a TSV) are applied
* there is no way to simultaneously tag with CoreNLP NER tags  and your own RegexNER tags

---

I had been trying to add custom NER tagging via a custom RegexNER file, like I used with the JAVA version a couple of years ago (more recently in Python via stanfordnlp).  I can't get RegexNER to work in Python, so I returned to the JAVA implementation of CoreNLP -- from the command line -- to troubleshoot )and work from there, if needed).

```bash
java -Xmx16g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLP \
-annotators 'tokenize,ssplit,pos,lemma,regexner' \
-regexner.mapping custom_entities.tsv \
-file input_sentence.txt \
-outputFormat text
```

However, once again I have not had much success in NER tagging with CoreNLP's trained models plus my custom RegexNER TSV file, formatted as described at https://nlp.stanford.edu/software/regexner.html

```
Victoria	PERSON	ORGANIZATION,CITY	2
Stuart	PERSON	ORGANIZATION,CITY	2
Victoria Stuart	PERSON		2
Yap	PRGE	PERSON	2
p53|p53-mediated	PRGE		2
...
```

I've tried various permutations of `'tokenize, ssplit, pos, lemma, ner, regexner` (always in that relative order) with `-regexner.mapping` | `-ner.additional.regexner.mapping` ... and I cannot simultaneously NER tag text with the default CoreNLP libraries plus my own custom NER tags.

* I expect the `ner,regexner` annotators combination with `-regexner.mapping` to co-tag with CoreNLP tags, superseded by custom NER tags.  Is that true, or am I mistaken?

* [I haven't looked at the `rules` approach, [https://stanfordnlp.github.io/CoreNLP/ner.html#regexner-rules-format](https://stanfordnlp.github.io/CoreNLP/ner.html#regexner-rules-format).
 
While I can get ***either*** NER (the default statistical model + ... fine NER rules added via the regexner annotator) ***or*** `-regexner.mapping` (with my custom tokens file) to work, it's always either one or the other.  And before I'm directed there (cough: @J38), I've certainly looked at the https://stanfordnlp.github.io/CoreNLP/ner.html page to which we are so often referred.

Furthermore, in evaluating various permutations of annotators, I've found upon stepwise additions adding the `lemma` annotator is particularly troublesome, immediately breaking RegexNER.   And, when I try to step back, old annotator settings are retained (cached?).  For example, I get lemmatization, a dependency parse, etc. in the output even if those annotators are not included in the annotators argument list.

It appears that whenever CoreNLP encounters an error, it silently loads the defaults, so that user-defined settings are ignored.


---

Similar issues / concerns have been raised elsewhere.

* https://stackoverflow.com/questions/32642008/ner-interfere-with-regexner [the accepted answer no longer works]
* https://stackoverflow.com/questions/37883035/having-both-ner-and-regexner-tags-in-stanfordcorenlpserver-output [the accepted answer likely never worked for anyone else, and the more recent CoreNLP 3.9.2 suggestion does not work in Python or JAVA]
* https://stackoverflow.com/questions/45806142/stanford-corenlp-regexner-unexpectedly-overwrites-ner-entities [see comments re: TokenRegexNERAnnotator]

---

If I slowly add annotators one at a time, I can (sort of: not consistently) ""reset"" CoreNLP:

```bash
java -Xmx16g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators 'tokenize,ssplit,pos,lemma,regexner' \
  -regexner.mapping custom_entities.tsv -file input_sentence.txt -outputFormat text; echo; cat input_sentence.txt.out; echo

  Adding annotator tokenize
  No tokenizer type provided. Defaulting to PTBTokenizer.
  Adding annotator ssplit
  Adding annotator pos
  Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.5 sec].
  Adding annotator lemma
  Adding annotator regexner
  TokensRegexNERAnnotator regexner: Read 13 unique entries out of 13 from custom_entities.tsv, 0 TokensRegex patterns.

  Processing file /mnt/Vancouver/apps/CoreNLP/target/input_sentence.txt ... writing to /mnt/Vancouver/apps/CoreNLP/target/input_sentence.txt.out
  Annotating file /mnt/Vancouver/apps/CoreNLP/target/input_sentence.txt ... done [0.2 sec].

  Annotation pipeline timing information:
  TokenizerAnnotator: 0.1 sec.
  WordsToSentencesAnnotator: 0.0 sec.
  POSTaggerAnnotator: 0.0 sec.
  MorphaAnnotator: 0.0 sec.
  TokensRegexNERAnnotator: 0.0 sec.
  TOTAL: 0.2 sec. for 9 tokens at 56.3 tokens/sec.
  Pipeline setup: 0.6 sec.
  Total time for StanfordCoreNLP pipeline: 0.8 sec.

  Document: ID=input_sentence.txt (1 sentences, 9 tokens)
  Sentence #1 (9 tokens):
  Victoria Stuart lives in Vancouver, British Columbia.
  [Text=Victoria CharacterOffsetBegin=0 CharacterOffsetEnd=8 PartOfSpeech=NNP Lemma=Victoria NamedEntityTag=PERSON]
  [Text=Stuart CharacterOffsetBegin=9 CharacterOffsetEnd=15 PartOfSpeech=NNP Lemma=Stuart NamedEntityTag=PERSON]
  [Text=lives CharacterOffsetBegin=16 CharacterOffsetEnd=21 PartOfSpeech=VBZ Lemma=live]
  [Text=in CharacterOffsetBegin=22 CharacterOffsetEnd=24 PartOfSpeech=IN Lemma=in]
  [Text=Vancouver CharacterOffsetBegin=25 CharacterOffsetEnd=34 PartOfSpeech=NNP Lemma=Vancouver]
  [Text=, CharacterOffsetBegin=34 CharacterOffsetEnd=35 PartOfSpeech=, Lemma=,]
  [Text=British CharacterOffsetBegin=36 CharacterOffsetEnd=43 PartOfSpeech=NNP Lemma=British]
  [Text=Columbia CharacterOffsetBegin=44 CharacterOffsetEnd=52 PartOfSpeech=NNP Lemma=Columbia]
  [Text=. CharacterOffsetBegin=52 CharacterOffsetEnd=53 PartOfSpeech=. Lemma=.]
```

Adding `ner` annotator breaks RegexNER tagging (above):

```bash
java -Xmx16g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators 'tokenize,ssplit,pos,lemma,ner,regexner' \
-regexner.mapping custom_entities.tsv -file input_sentence.txt -outputFormat text; echo; cat input_sentence.txt.out; echo

  Adding annotator tokenize
  No tokenizer type provided. Defaulting to PTBTokenizer.
  Adding annotator ssplit
  Adding annotator pos
  Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.5 sec].
  Adding annotator lemma
  Adding annotator ner
  Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.9 sec].
  Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].
  Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [1.3 sec].
  Adding annotator regexner
  TokensRegexNERAnnotator regexner: Read 13 unique entries out of 13 from custom_entities.tsv, 0 TokensRegex patterns.

  Processing file /mnt/Vancouver/apps/CoreNLP/target/input_sentence.txt ... writing to /mnt/Vancouver/apps/CoreNLP/target/input_sentence.txt.out
  Annotating file /mnt/Vancouver/apps/CoreNLP/target/input_sentence.txt ... done [0.1 sec].

  Annotation pipeline timing information:
  TokenizerAnnotator: 0.0 sec.
  WordsToSentencesAnnotator: 0.0 sec.
  POSTaggerAnnotator: 0.0 sec.
  MorphaAnnotator: 0.0 sec.
  NERCombinerAnnotator: 0.0 sec.
  TokensRegexNERAnnotator: 0.0 sec.
  TOTAL: 0.1 sec. for 9 tokens at 80.4 tokens/sec.
  Pipeline setup: 4.3 sec.
  Total time for StanfordCoreNLP pipeline: 4.5 sec.

  Document: ID=input_sentence.txt (1 sentences, 9 tokens)
  Sentence #1 (9 tokens):
  Victoria Stuart lives in Vancouver, British Columbia.
  [Text=Victoria CharacterOffsetBegin=0 CharacterOffsetEnd=8 PartOfSpeech=NNP Lemma=Victoria NamedEntityTag=ORGANIZATION]
  [Text=Stuart CharacterOffsetBegin=9 CharacterOffsetEnd=15 PartOfSpeech=NNP Lemma=Stuart NamedEntityTag=ORGANIZATION]
  [Text=lives CharacterOffsetBegin=16 CharacterOffsetEnd=21 PartOfSpeech=VBZ Lemma=live NamedEntityTag=O]
  [Text=in CharacterOffsetBegin=22 CharacterOffsetEnd=24 PartOfSpeech=IN Lemma=in NamedEntityTag=O]
  [Text=Vancouver CharacterOffsetBegin=25 CharacterOffsetEnd=34 PartOfSpeech=NNP Lemma=Vancouver NamedEntityTag=LOCATION]
  [Text=, CharacterOffsetBegin=34 CharacterOffsetEnd=35 PartOfSpeech=, Lemma=, NamedEntityTag=O]
  [Text=British CharacterOffsetBegin=36 CharacterOffsetEnd=43 PartOfSpeech=NNP Lemma=British NamedEntityTag=LOCATION]
  [Text=Columbia CharacterOffsetBegin=44 CharacterOffsetEnd=52 PartOfSpeech=NNP Lemma=Columbia NamedEntityTag=LOCATION]
  [Text=. CharacterOffsetBegin=52 CharacterOffsetEnd=53 PartOfSpeech=. Lemma=. NamedEntityTag=O]
```

""Cache"" (?) issue -- no classpath, etc. given yet outputs previous result (*obfuscating* debugging attempts, by the way):

```bash
java -Xmx16g -annotators 'tokenize,ssplit,pos,lemma,ner,regexner' -regexner.mapping custom_entities.tsv \
-file input_sentence.txt -outputFormat text; echo; cat input_sentence.txt.out; echo

Unrecognized option: -annotators
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

Document: ID=input_sentence.txt (1 sentences, 9 tokens)
Sentence #1 (9 tokens):
Victoria Stuart lives in Vancouver, British Columbia.
[Text=Victoria CharacterOffsetBegin=0 CharacterOffsetEnd=8 PartOfSpeech=NNP Lemma=Victoria NamedEntityTag=ORGANIZATION]
[Text=Stuart CharacterOffsetBegin=9 CharacterOffsetEnd=15 PartOfSpeech=NNP Lemma=Stuart NamedEntityTag=ORGANIZATION]
[Text=lives CharacterOffsetBegin=16 CharacterOffsetEnd=21 PartOfSpeech=VBZ Lemma=live NamedEntityTag=O]
[Text=in CharacterOffsetBegin=22 CharacterOffsetEnd=24 PartOfSpeech=IN Lemma=in NamedEntityTag=O]
[Text=Vancouver CharacterOffsetBegin=25 CharacterOffsetEnd=34 PartOfSpeech=NNP Lemma=Vancouver NamedEntityTag=LOCATION]
[Text=, CharacterOffsetBegin=34 CharacterOffsetEnd=35 PartOfSpeech=, Lemma=, NamedEntityTag=O]
[Text=British CharacterOffsetBegin=36 CharacterOffsetEnd=43 PartOfSpeech=NNP Lemma=British NamedEntityTag=LOCATION]
[Text=Columbia CharacterOffsetBegin=44 CharacterOffsetEnd=52 PartOfSpeech=NNP Lemma=Columbia NamedEntityTag=LOCATION]
[Text=. CharacterOffsetBegin=52 CharacterOffsetEnd=53 PartOfSpeech=. Lemma=. NamedEntityTag=O]

java -annotators 'tokenize,ssplit,pos,lemma,ner,regexner' -regexner.mapping custom_entities.tsv \
-file input_sentence.txt -outputFormat text; echo; cat input_sentence.txt.out; echo
Unrecognized option: -annotators
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

Document: ID=input_sentence.txt (1 sentences, 9 tokens)
Sentence #1 (9 tokens):
Victoria Stuart lives in Vancouver, British Columbia.
[Text=Victoria CharacterOffsetBegin=0 CharacterOffsetEnd=8 PartOfSpeech=NNP Lemma=Victoria NamedEntityTag=ORGANIZATION]
[Text=Stuart CharacterOffsetBegin=9 CharacterOffsetEnd=15 PartOfSpeech=NNP Lemma=Stuart NamedEntityTag=ORGANIZATION]
[Text=lives CharacterOffsetBegin=16 CharacterOffsetEnd=21 PartOfSpeech=VBZ Lemma=live NamedEntityTag=O]
[Text=in CharacterOffsetBegin=22 CharacterOffsetEnd=24 PartOfSpeech=IN Lemma=in NamedEntityTag=O]
[Text=Vancouver CharacterOffsetBegin=25 CharacterOffsetEnd=34 PartOfSpeech=NNP Lemma=Vancouver NamedEntityTag=LOCATION]
[Text=, CharacterOffsetBegin=34 CharacterOffsetEnd=35 PartOfSpeech=, Lemma=, NamedEntityTag=O]
[Text=British CharacterOffsetBegin=36 CharacterOffsetEnd=43 PartOfSpeech=NNP Lemma=British NamedEntityTag=LOCATION]
[Text=Columbia CharacterOffsetBegin=44 CharacterOffsetEnd=52 PartOfSpeech=NNP Lemma=Columbia NamedEntityTag=LOCATION]
[Text=. CharacterOffsetBegin=52 CharacterOffsetEnd=53 PartOfSpeech=. Lemma=. NamedEntityTag=O]

java 'tokenize,ssplit,pos,lemma,ner,regexner' -regexner.mapping custom_entities.tsv \
-file input_sentence.txt -outputFormat text; echo; cat input_sentence.txt.out; echo
Error: Could not find or load main class tokenize,ssplit,pos,lemma,ner,regexner

Document: ID=input_sentence.txt (1 sentences, 9 tokens)
Sentence #1 (9 tokens):
Victoria Stuart lives in Vancouver, British Columbia.
[Text=Victoria CharacterOffsetBegin=0 CharacterOffsetEnd=8 PartOfSpeech=NNP Lemma=Victoria NamedEntityTag=ORGANIZATION]
[Text=Stuart CharacterOffsetBegin=9 CharacterOffsetEnd=15 PartOfSpeech=NNP Lemma=Stuart NamedEntityTag=ORGANIZATION]
[Text=lives CharacterOffsetBegin=16 CharacterOffsetEnd=21 PartOfSpeech=VBZ Lemma=live NamedEntityTag=O]
[Text=in CharacterOffsetBegin=22 CharacterOffsetEnd=24 PartOfSpeech=IN Lemma=in NamedEntityTag=O]
[Text=Vancouver CharacterOffsetBegin=25 CharacterOffsetEnd=34 PartOfSpeech=NNP Lemma=Vancouver NamedEntityTag=LOCATION]
[Text=, CharacterOffsetBegin=34 CharacterOffsetEnd=35 PartOfSpeech=, Lemma=, NamedEntityTag=O]
[Text=British CharacterOffsetBegin=36 CharacterOffsetEnd=43 PartOfSpeech=NNP Lemma=British NamedEntityTag=LOCATION]
[Text=Columbia CharacterOffsetBegin=44 CharacterOffsetEnd=52 PartOfSpeech=NNP Lemma=Columbia NamedEntityTag=LOCATION]
[Text=. CharacterOffsetBegin=52 CharacterOffsetEnd=53 PartOfSpeech=. Lemma=. NamedEntityTag=O]
```"
817,https://github.com/stanfordnlp/CoreNLP/issues/984,984,[],closed,2020-01-14 08:18:43+00:00,,example.serialized.ncc.ncc.ser.gz not working  ::: JDK13 is Java version,"```
from nltk.tag import StanfordNERTagger

stanford_classifier  =  '/home/test/stanford-ner-2018-10-16/classifiers/example.serialized.ncc.ncc.ser.gz'
stanford_ner = '/home/test/stanford-ner-2018-10-16/stanford-ner.jar'
# Build NER tagger object
st = StanfordNERTagger(stanford_classifier, stanford_ner)

# A sample text for NER tagging
text = 'Ram works at Google' 
# Tag the sentence and print output
tagged = st.tag(str(text).split())
print(tagged)
```

Error reported:
/home/test/Desktop/myenv/bin/python3 /home/test/NlpSource-master/sss.py
Traceback (most recent call last):
  File ""/home/test/NlpSource-master/sss.py"", line 32, in <module>
    tagged = st.tag(str(text).split())
  File ""/home/test/Desktop/myenv/lib/python3.7/site-packages/nltk/tag/stanford.py"", line 93, in tag
    return sum(self.tag_sents([tokens]), [])
  File ""/home/test/Desktop/myenv/lib/python3.7/site-packages/nltk/tag/stanford.py"", line 116, in tag_sents
    cmd, classpath=self._stanford_jar, stdout=PIPE, stderr=PIPE
  File ""/home/test/Desktop/myenv/lib/python3.7/site-packages/nltk/internals.py"", line 146, in java
    raise OSError('Java command failed : ' + str(cmd))
OSError: Java command failed : ['/usr/bin/java', '-mx1000m', '-cp', '/home/test/stanford-ner-2018-10-16/stanford-ner.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', '/home/test/stanford-ner-2018-10-16/classifiers/example.serialized.ncc.ncc.ser.gz', '-textFile', '/tmp/tmph8k3mnol', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '""tokenizeNLs=false""', '-encoding', 'utf8']
Invoked on Tue Jan 14 13:52:00 IST 2020 with arguments: -loadClassifier /home/test/stanford-ner-2018-10-16/classifiers/example.serialized.ncc.ncc.ser.gz -textFile /tmp/tmph8k3mnol -outputFormat slashTags -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerOptions ""tokenizeNLs=false"" -encoding utf8
tokenizerOptions=""tokenizeNLs=false""
loadClassifier=/home/test/stanford-ner-2018-10-16/classifiers/example.serialized.ncc.ncc.ser.gz
encoding=utf8
outputFormat=slashTags
textFile=/tmp/tmph8k3mnol
tokenizerFactory=edu.stanford.nlp.process.WhitespaceTokenizer
Exception in thread ""main"" java.lang.RuntimeException: java.lang.ClassCastException: class java.util.Properties cannot be cast to class [Ledu.stanford.nlp.util.Index; (java.util.Properties is in module java.base of loader 'bootstrap'; [Ledu.stanford.nlp.util.Index; is in unnamed module of loader 'app')
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1520)
	at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:2993)
Caused by: java.lang.ClassCastException: class java.util.Properties cannot be cast to class [Ledu.stanford.nlp.util.Index; (java.util.Properties is in module java.base of loader 'bootstrap'; [Ledu.stanford.nlp.util.Index; is in unnamed module of loader 'app')
	at edu.stanford.nlp.ie.crf.CRFClassifier.loadClassifier(CRFClassifier.java:2600)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1473)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1505)
	at edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1516)
	... 1 more


Process finished with exit code 1"
818,https://github.com/stanfordnlp/CoreNLP/issues/985,985,[],closed,2020-01-16 07:13:00+00:00,,Are these latest Chines model significantly worse than the Stanford online parser?,"I tested the latest Chinese CoreNLP 3.92 version, and found the results are quite horrible. Here are few examples:

ÊàëÁöÑÊúãÂèãÔºöalways tags ""ÊàëÁöÑ"" as one NN token.
ÊàëÁöÑÁãóÂêÉËãπÊûúÔºö ‚ÄòÊàëÁöÑÁãó‚Äô tagged as one NN token.
‰ªñÁöÑÁãóÂêÉËãπÊûúÔºö'ÁãóÂêÉ' tagged as one NN token.
È´òË¥®ÈáèÂ∞±‰∏öÊàêÊó∂‰ª£: 'Â∞±‰∏ö' tagged as VV

When I compared them with the results from http://nlp.stanford.edu:8080/parser/index.jsp, surprisingly, the results of those examples are all right. Why is that? Are the models different? Is there a bug in the new 3.92 version model? "
819,https://github.com/stanfordnlp/CoreNLP/issues/986,986,[],open,2020-01-20 13:35:48+00:00,,How big is your truecase model?,"Hey there!

I've trained a truecase model for the german language on a dataset of 1 million sentences. The resulting model is quite big (80MB) and I am having memory issues including it into my annotating pipeline. But when I use the english truecase model there are no issues.

How big is your truecase model (edu/stanford/nlp/models/truecase/truecasing.fast.caseless.qn.ser.gz)?
And how does the annotator impact the memory consumption of the pipeline?"
820,https://github.com/stanfordnlp/CoreNLP/issues/987,987,[],closed,2020-01-22 00:24:36+00:00,,TimeoutException after running for a while,"I have the following error coming up in the console and then it eventually just kills itself.

````[pool-1-thread-2] INFO CoreNLP - [/xxx.xxx.xxx.xx:54536] API call w/annotators tokenize,ssplit,pos,depparse,lemma,ner
java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:205)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:870)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
	at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:72)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
	at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
	at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[pool-1-thread-3] INFO CoreNLP - [/xxx.xxx.xxx.xx:54536] API call w/annotators tokenize,ssplit,pos,depparse,lemma,ner
Killed
````

The CoreNLP is running as a dedicated server on a CentOS 7 box and the timeout has been set to 2 minutes. 

The application that I have written sends 100000 characters each time, but if it gets a timed out response from the NLP server, it will half the amount of characters that it sends. It keeps doing this until it gets to below 1000 characters. If it is below 1000 characters and the server is still giving a timed out response, my application will assume that there is something wrong with the text, alert me to the document and move on to the next document.

After the NLP Server has been running for some amount of time, it seems that it can no longer process any text, no matter the size, and I have to manually restart the service.

I'm using 3.9.2
"
821,https://github.com/stanfordnlp/CoreNLP/issues/988,988,[],open,2020-01-24 15:15:42+00:00,,inconsistent return values for QuoteAnnotator? (missing mention data),"I'm trying to understand some poor precision results we are getting when using the QuoteAnnotator on a news corpus. I'm hoping to understand and tune the sieves to improve precision, but am not able to retrieve mention and sieve data when querying the server over HTTP. However, I do get that data when using the [sample server jupyter notebook](https://github.com/stanfordnlp/stanfordnlp/blob/master/demo/StanfordNLP_CoreNLP_Interface.ipynb). 

### Results in from CoreNLPClient

I run a test in the notebook like this:
```python
# create server
os.environ[""CORENLP_HOME""] = ""./corenlp""
client = CoreNLPClient(annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner', 'depparse', 'coref', 'quote'], memory='8G', endpoint='http://localhost:9001')
client.start()
time.sleep(10)
text = """"""Vice President Mike Pence announced Thursday that Israel's Prime Minister and opposition leader will visit the White House next week to discuss ""regional issues as well as the prospect of peace."" The announcement comes as reports suggest a potential reveal of the Trump administration's Middle East peace plan could be imminent. Pence, who was in Jerusalem for a gathering of world leaders to mark the 75th anniversary of the liberation of Auschwitz, extended the invitation to Prime Minister Benjamin Netanyahu from President Donald Trump. He also announced that Blue and White chairman Benny Gantz will also attend the meeting at the White House on Tuesday. In a tweet Trump appeared to dash speculation an announcement on the peace plan may be imminent. ""The United States looks forward to welcoming Prime Minister @Netanyahu & Blue & White Chairman @Gantzbe to the @WhiteHouse next week. Reports about details and timing of our closely-held peace plan are purely speculative,"" he tweeted. The unveiling of the plan, which is being spearheaded by Trump's senior adviser and son-in-law Jared Kushner, has been delayed amid the months-long period of turmoil in Israeli politics with the country due to hold an unprecedented third national election in less than a year in March.""""""
document = client.annotate(text)
document.quote[1]
```
And receive results including `mention`, `mentionSieve `, and other properties:
```
...
mention: ""he""
mentionBegin: 171
mentionEnd: 171
mentionType: ""pronoun""
mentionSieve: ""trigram QPV""
canonicalMention: ""Donald Trump""
...
```

### Results from HTTP request

In the *same* notebook, querying the same server, I fetch results over HTTP for `json` results and get no mention information:
```python
import requests
url = 'http://localhost:9001/?properties={""annotators"":""tokenize,ssplit,pos,lemma,ner,depparse,coref,quote"",""outputFormat"":""json""}'
r = requests.post(url, data=text.encode('utf-8'))
results = r.json()
results['quotes'][1]
```
I get the following:
```
{'id': 1,
 'text': '""The United States looks forward to welcoming Prime Minister @Netanyahu & Blue & White Chairman @Gantzbe to the @WhiteHouse next week. Reports about details and timing of our closely-held peace plan are purely speculative,""',
 'beginIndex': 757,
 'endIndex': 980,
 'beginToken': 133,
 'endToken': 170,
 'beginSentence': 5,
 'endSentence': 6,
 'speaker': 'Unknown',
 'canonicalSpeaker': 'Donald Trump'}
```

### Question

I think I'm making the same request in both cases. One clue - I think maybe the output format is different - `json` vs. `serialized`. But why would the server return different results based on the output format specified? Any clues for how to get the mention data included in the `json` output? Do I need to hook into the Java code directly to get this information at scale?

(Perhaps related to #616?)
"
822,https://github.com/stanfordnlp/CoreNLP/issues/989,989,[],open,2020-01-30 19:42:50+00:00,,CleanXML in Python,"Hello!

When using the CoreNLP Client, I am able to annotate text data using cleanxml. I can't seem to figure out, however,  how to use the cleanxml options (clean.xmltags, clean.docAnnotations, etc) in Python. I've scoured for any documentation on the matter but have not been fortunate in my search. Any help would be much appreciated.

Cheers,
Adrian"
823,https://github.com/stanfordnlp/CoreNLP/issues/990,990,[],closed,2020-02-04 16:27:52+00:00,,About sentence and paragraph split,"Hello, I'm using the annotator `""tokenize, ssplit, pos, lemma, ner""`.
In my simplest pipeline configuration I have

```javascript
{
""tokenize.whitespace"": false,
""tokenize.keepeol"": false,
""ner.applyFineGrained"": false,
""ner.buildEntityMentions"": false,
ssplit.isOneSentence"": false,
""ssplit.newlineIsSentenceBreak"": ""always""
}
```

for a text that has bot `\n` and `\n\n` (multiple) new lines character as sentence separator. Using `ssplit.newlineIsSentenceBreak` works ok with both, but when I get the output structure of sentences list, I do not find the `\n\n` line break like in

```
We doin' this once (You yellin' at the mic, your beard's weird)
Why you yell at the mic? (Illa)

Rihanna just hit me on a text
``` 

the output will give me back the three sentences, without considering a empty sentence (matching the `\n\n`):

```javascript
...
{
    ""index"": 9,
    ""text"": ""Why you yell at the mic? (Illa)"",
    ""tokens"": [ ... ]
  },
  {
    ""index"": 10,
    ""text"": ""Rihanna just hit me on a text"",
    ""tokens"": [ ... ]
}
```

so when reconstructing the input text from the output I'm missing a blank line and I will get

```
We doin' this once (You yellin' at the mic, your beard's weird)
Why you yell at the mic? (Illa)
Rihanna just hit me on a text
``` 

Any way to handle this internally (without using the input as reference)?"
824,https://github.com/stanfordnlp/CoreNLP/issues/991,991,[],closed,2020-02-04 17:28:35+00:00,,"StanfordCoreNLPClient fails ""silently""","In `StanfordCoreNLPClient.annotate`, if the server is not running, or some other exception occurs in this catch block: https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLPClient.java#L461-L467 - the exception is logged but the calling function never finds out about it. This is problematic because we end up with an annotation that wasn't annotated, but no way to test if it failed later on."
825,https://github.com/stanfordnlp/CoreNLP/issues/993,993,[],closed,2020-02-08 01:00:21+00:00,,"Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space","Hi.  Does `(stanford-corenlp-full-2018-10-05)` require the machine to have specific memory/computational capability? 
I am using the Stanford sentiment toolkit `(stanford-corenlp-full-2018-10-05)` downloaded from [Stanford CoreNLP website](https://stanfordnlp.github.io/CoreNLP/download.html) to do sentiment analysis on a text file. But, it does not work when I have for example 6000 lines (sentences) in the text file. However, It works fine with a 10 line text file. I am running a large scale study and need to get the sentiment of a very large amount of sentences."
826,https://github.com/stanfordnlp/CoreNLP/issues/994,994,[],closed,2020-02-10 10:38:57+00:00,,Tokenization character offset issue while consuming output,"I have a use case where my system is developed in python and internally using corenlp server for annotation. I am using ""pycorenlp"" python library to access the corenlp server. This was working great until I encountered a case where the token character offsets returned by corenlp (Java) are incompatible with Python. 

Example: 
Consider a string with text : ""ùíöÃÇùíä"".

On running the above text through the corenlp annotation pipeline. The character offsets returned for token are as follows:
 characterOffsetBegin: 0
 characterOffsetEnd: 5

When the same text is run through python, the offsets calculated are:
 startOffset: 0
 endOffset: 3

On debugging, i found that this is happening because corenlp being built using JAVA calculates string length using the code units of character and not the code points whereas PYTHON calculates string length using the code points. 

Due to this, for some of the cases where such special characters exist, the system fails as python tries to access offsets in string which may not be actually present. 

Is it possible to handle such cases by calculating the offsets in corenlp using code points so that the offset can be consumed in any language or any other solution ? "
827,https://github.com/stanfordnlp/CoreNLP/issues/995,995,[],closed,2020-02-10 10:52:24+00:00,,"wrong POS for ""bear""","Hi, in all (or nearly all) instances the verb bear was identified as a noun, e.g. in sentences a la ""...PERSON and PERSON bear witness to ..."" or ""... it was hard to bear ..."".
As I am looking for animals this is incredibly annoying, as you might imagine. Is that some tweaking problem or is that on your part?

Code was like this, version is 3.9.2:
`Annotation document = new Annotation(line);
pipeline.annotate(document);
List<CoreMap> sentences = document.get(SentencesAnnotation.class);
for (CoreMap sentence : sentences) {
	for (CoreLabel token : sentence.get(TokensAnnotation.class)) {
	String pos = token.get(PartOfSpeechAnnotation.class);
        }
}`"
828,https://github.com/stanfordnlp/CoreNLP/issues/996,996,[],closed,2020-02-12 13:03:03+00:00,,output.printSingletonEntities parameter doesn't work,"I'm using coreNLP for coreference resolution from command line. I need the system to annotate singletons (single mentions) to be able to evaluate the output on my data, which has them annotated. I know that the system was trained on Ontonotes, which doesn't have singletons, but I found this parameter output.printSingletonEntities.

However, adding it into the parameters file changed absolutely nothing, thought I know, I have singletons in my data. What am I doing wrong?

This is my parameter file:

annotators = tokenize,ssplit,pos,lemma,ner,parse,coref

tokenize.whitespace = true

tokenize.options = tokenizePerLine

ssplit.newlineIsSentenceBreak = always

coref.algorithm = neural

filelist = /Users/veronika/Dropbox/data/preprocessing/filelist.txt

output.printSingletonEntities = true

I also tried to call it from command line directly, but the annotation stays the same.

java -cp """" -Xmx10g edu.stanford.nlp.pipeline.StanfordCoreNLP -props /Users/veronika/Dropbox/data/sampleProps.properties -output.printSingletonEntities true*
"
829,https://github.com/stanfordnlp/CoreNLP/issues/997,997,[],closed,2020-02-19 14:31:06+00:00,,"json.decoder.JSONDecodeError pls help, text size is 355","
props = {'annotators': 'coref', 'pipelineLanguage': 'en'}
result = json.loads(nlp.annotate(text, properties=props))
[pool-1-thread-4] INFO CoreNLP - [/127.0.0.1:47894] API call w/annotators tokenize,ssplit,pos,lemma,ner,depparse,coref
Strong quarter For the period ending March 31, 2019, the company reported a good set of numbers on the back of better realizations. The companys net profits soared by a whopping 363 percent year-on-year to Rs 6,027 crore. Revenues increased more modestly by 7.5 percent to Rs 28,546 crores. Most brokerages had hiked their target price on the stock, following the companys good set of numbers. Going ahead for 2019-20, as well most brokerages are upbeat on the companys performance and hence the dividend yield. The stock at Rs 218, is not very far from its 52-week low of Rs 211. Decent dividend yield For FY 2018-19, Coal India declared dividend two times. The first was in December 2018, when the company declared a dividend of Rs 7.25 per share, while the second was in Feb 2019, when the company declared a dividend of Rs 5.85 per share. The total dividends declared by the company was Rs 13.1, during FY 2018-19. If you take the current stock price of Rs 218, the dividend yield works close to 6 per cent. What we believe is that the dividends being offered by the company, will be significantly higher than the dividends that were offered in the previous year, ensuring that the dividend yield remains high. Splitting of Coal India may remain an overhang According to reports the government was thinking off spinning off units of Coal India Ltd, into separate listed companies to boost competition and raise government funds. This was as per reports in the Bloomberg. Even if the spin-off does take place, it is likely that Coal India shareholders would get shares in the listed entities. That is only logical. However, as per the reports this is only in an initial stage and no concrete decision is taken in this regards. What makes Coal India attractive at this stage is also the price to earnings ratio, which for 2019-20, could be under 10. Buy the stock for decent dividend yields and gains. Disclaimer This article is strictly for informational purposes only. It is not a solicitation to buy, sell in securities or other financial instruments
java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:205)
	at edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:870)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
	at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82)
	at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675)
	at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79)
	at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/yashpal/Storage/anaconda3/lib/python3.6/json/__init__.py"", line 354, in loads
    return _default_decoder.decode(s)
  File ""/home/yashpal/Storage/anaconda3/lib/python3.6/json/decoder.py"", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/home/yashpal/Storage/anaconda3/lib/python3.6/json/decoder.py"", line 357, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
"
830,https://github.com/stanfordnlp/CoreNLP/issues/998,998,[],closed,2020-02-26 19:10:24+00:00,,NER tag 'HANDLE',"Hi,
I'm using corenlp for NER and noticed the 'HANDLE' tag when tested some text like `hello world @helloworld` (it tags `@helloworld` as HANDLE). But I don't see the docs mentioning this tag. I also can't find it defined in the source. I'm using the TokenRegexNerAnnotator. Could you point me to which model was trained to recognize this tag or where it's defined? And if the 23 tags that are defined in the [docs](https://stanfordnlp.github.io/CoreNLP/ner.html) is not the full list, would it be possible to get the docs updated?
Thank you."
831,https://github.com/stanfordnlp/CoreNLP/issues/999,999,[],closed,2020-02-27 00:05:20+00:00,,Is it possible to specify additional attributes in RegexNER?,"This is a long shot ...  Is it possible to specify ""number"", ""gender"", etc. when providing a RegexNER properties file?  It might help with coreference resolution; e.g.

```python
input text: Victoria lives in Vancouver. She likes apples.
corefs:
 {
  ""3"": [
    {
      ""id"": 0,
      ""text"": ""Victoria"",
      ""type"": ""PROPER"",
      ""number"": ""SINGULAR"",
      ""gender"": ""UNKNOWN"",
      ""animacy"": ""INANIMATE"",
      ""startIndex"": 1,
      ""endIndex"": 2,
      ""headIndex"": 1,
      ""sentNum"": 1,
      ""position"": [
        1,
        1
      ],
      ""isRepresentativeMention"": true
    },
    {
      ""id"": 3,
      ""text"": ""She"",
      ""type"": ""PRONOMINAL"",
      ""number"": ""SINGULAR"",
      ""gender"": ""FEMALE"",
      ""animacy"": ""ANIMATE"",
      ""startIndex"": 1,
      ""endIndex"": 2,
      ""headIndex"": 1,
      ""sentNum"": 2,
      ""position"": [
        2,
        2
      ],
      ""isRepresentativeMention"": false
    }
  ]
}
```

While that worked, I'm thinking of other applications (source text)."
832,https://github.com/stanfordnlp/CoreNLP/issues/1000,1000,[],closed,2020-02-28 00:12:54+00:00,,[MEMORY] TokensRegex SequencePattern.State memory optimization,"CoreNLP uses way too much memory to do what it does.

A HeapDump of an english model yields ~2mio instances of subclasses of SequencePattern.State.

Most instances have a `next` set of size 1 and a whole `LinkedHashSet` instance is allocated anyway (and wastes like 300byte each).

I'm not even sure if either iteration order or duplicate removal is strictly necessary for the states successor collection.

I propose doing something similar to the following:

```
Index: src/edu/stanford/nlp/ling/tokensregex/SequencePattern.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/ling/tokensregex/SequencePattern.java	(revision 32d609a08ecb1cd905b09cf7712b8c58bfc13556)
+++ src/edu/stanford/nlp/ling/tokensregex/SequencePattern.java	(date 1582843145000)
@@ -1272,9 +1292,13 @@
      */
     protected void add(State nextState) {
       if (next == null) {
-        next = new LinkedHashSet<>();
-      }
-      next.add(nextState);
+        next = Collections.singleton(nextState);
+      } else if (next.getClass() != LinkedHashSet.class) {
+        next = new LinkedHashSet<>(next);
+        next.add(nextState);
+      } else {
+        next.add(nextState);
+      }
     }
 
     public <T> Object value(int bid, SequenceMatcher.MatchedStates<T> matchedStates) {

```"
833,https://github.com/stanfordnlp/CoreNLP/issues/1001,1001,[],closed,2020-02-28 00:51:41+00:00,,[MEMORY] TokensRegexNERAnnotator$Entry SingletonSet when applicable,"Similar to #1000 edu.stanford.nlp.pipeline.TokensRegexNERAnnotator$Entry
uses always a `HashSet` for the field `overwritableTypes`, almost all are of size 1.

There are 500k instances of `Entry` with the english model files, the HashSets alone amounting to around 130MiB of wasted memory in comparison to `Collections.singleton()`

I propose folding the set into a singleton if it turns out to be of size 1:

```
Index: src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java	(revision 32d609a08ecb1cd905b09cf7712b8c58bfc13556)
+++ src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java	(date 1582850986000)
@@ -746,7 +746,7 @@
         types[i] = split[annotationCols[i]].trim();
       }
 
-      Set<String> overwritableTypes = Generics.newHashSet();
+      Set<String> overwritableTypes = Collections.emptySet();
       double priority = 0.0;
 
       if (iOverwrite >= 0 && split.length > iOverwrite) {
@@ -754,7 +754,14 @@
           logger.warn(""Number in types column for "" + Arrays.toString(key) +
                   "" is probably priority: "" + split[iOverwrite]);
         }
-        overwritableTypes.addAll(Arrays.asList(COMMA_DELIMITERS_PATTERN.split(split[iOverwrite].trim())));
+        List<String> strings = Arrays.asList(COMMA_DELIMITERS_PATTERN.split(split[iOverwrite].trim()));
+        overwritableTypes = Generics.newHashSet();
+        overwritableTypes.addAll(strings);
+        if(overwritableTypes.isEmpty()) {
+          overwritableTypes = Collections.emptySet();
+        } else if (overwritableTypes.size() == 1) {
+          overwritableTypes = Collections.singleton(overwritableTypes.iterator().next());
+        }
       }
       if (iPriority >= 0 && split.length > iPriority) {
         try {

```"
834,https://github.com/stanfordnlp/CoreNLP/issues/1002,1002,[],closed,2020-02-28 03:06:25+00:00,,[MEMORY] CoreMapNodePatternTrigger lowercaseStringTriggers values as List not HashSet,"The lowercaseStringTriggers
 in CoreMapNodePatternTrigger are in a 2d map, in which values are collected in some collection type. Default is HashMap.

Some instances have a very large secondary dim space of ~500k, Value count for a keypair (Class, String) are small (~1) however.

If Set semantics are not necessary (no pattern de-duplication needed), the TwoDimensionalCollectionValuedMap could be set to use ArrayLists for values, saving probably another offset of 500k HashSet/Map overheads.

```
Index: src/edu/stanford/nlp/ling/tokensregex/CoreMapNodePatternTrigger.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- src/edu/stanford/nlp/ling/tokensregex/CoreMapNodePatternTrigger.java	(revision 32d609a08ecb1cd905b09cf7712b8c58bfc13556)
+++ src/edu/stanford/nlp/ling/tokensregex/CoreMapNodePatternTrigger.java	(date 1582857025000)
@@ -18,7 +18,7 @@
   TwoDimensionalCollectionValuedMap<Class, Object, SequencePattern<CoreMap>> annotationTriggers =
           new TwoDimensionalCollectionValuedMap<>();
   TwoDimensionalCollectionValuedMap<Class, String, SequencePattern<CoreMap>> lowercaseStringTriggers =
-          new TwoDimensionalCollectionValuedMap<>();
+          new TwoDimensionalCollectionValuedMap<>(CollectionFactory.arrayListFactory(1));
 
   public CoreMapNodePatternTrigger(SequencePattern<CoreMap>... patterns) {
     this(Arrays.asList(patterns));
```

<img width=""1090"" alt=""lowercaseStringTriggers"" src=""https://user-images.githubusercontent.com/1890613/75506845-8b6d5a00-59df-11ea-8d49-8bcdaa62ea3f.png"">"
835,https://github.com/stanfordnlp/CoreNLP/issues/1003,1003,[],closed,2020-02-28 14:07:40+00:00,,[MEMORY] Possibly use float instead of double in models/weights,"<img width=""1116"" alt=""double"" src=""https://user-images.githubusercontent.com/1890613/75553593-feef8580-5a38-11ea-895f-8c050d048f8d.png"">

double arrays are a large portion of the heap.

There are some places with 2d double arrays with dimensions like

345k x 16, 150k x 24, 80k x 46: CRFCLassifier.weights
100k x 1000: Classifier.saved in DependecyParser
60k x 50: Classifier.E, .eg2E
1000x2400: Classifier.W1, .wg2W1

Most are weights of some sort, making me wonder if they could be stored in less than 64bit each.

**The obvious step would be to use float[], halving the memory use of this portion.**

Another would be to encode weights in something else, for example a small integer and scale that into a float again when using the weight.

Machine Learning models often use fp16 or even fp8 to store weights, there are java implementations of float -> short -> float (with fp16 semantics stored in a 16bit short)

like https://android.googlesource.com/platform/frameworks/base/+/master/core/java/android/util/Half.java
with https://android.googlesource.com/platform/libcore/+/master/luni/src/main/java/libcore/util/FP16.java

or https://stackoverflow.com/questions/6162651/half-precision-floating-point-in-java

The latter approached would need some performance testing as each time a weight is used it would have to be converted first.

-----

I saw that some models serialize themselves using ObjectStreams, that would need an adapter to deserialize to double[] first and then array-cast it to float[].

Like in CRFClassifier.loadClassifier"
836,https://github.com/stanfordnlp/CoreNLP/issues/1004,1004,[],open,2020-02-28 15:05:52+00:00,,[MEMORY] Strings de-duplication in lexicons etc.,"### NERFeatureFactory has a lexicon Map<String,String> containing ~500k entries.

The values (wordClass) seem to be integer numbers in String form.
Switching to Integer would help a bit with loss of generality, but de-duplicating the wordClass strings would help a lot, since there are only few word classes, but 500k copies of their string name.


### Distsim.lexicon has the same issue
values in that map are strings of small integer numbers.


### A similar thing happens in Dictionaries.genderNumber. It is a Map [String] -> GenderEnum.

In the string-sequence keys there are many ""!"" (100s of k) as duplicate String instances.

### MaxentTagger.dict has many TagCount instances with repetitive keys and values.
<img width=""927"" alt=""maxent"" src=""https://user-images.githubusercontent.com/1890613/75559070-1e8bab80-5a43-11ea-9678-de28295d59fd.png"">


There are JVM internal optimizations that de-duplicate strings, but the JVM chooses if and when to apply them.

- JEP192 GC based string-dedup https://openjdk.java.net/jeps/192
- JEP254 latin1 string optimization, using 1 byte instead of 2byte char arrays for strings. https://openjdk.java.net/jeps/254

I'm not sure if these are enabled by default and relying on users to enable them makes deployment rather annoying. Also these features kick in only after the models are loaded and a few GC cycles have run, so peak heap usage stays the same.

String.intern() is a bit crude as it invokes magic, just using some corenlp string pooling singleton would be a good choise, also making the behavior configurable.
See https://shipilev.net/jvm/anatomy-quarks/10-string-intern/"
837,https://github.com/stanfordnlp/CoreNLP/issues/1005,1005,[],closed,2020-02-28 19:09:13+00:00,,"Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space","Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
	at edu.stanford.nlp.parser.nndep.Classifier.preCompute(Classifier.java:662)
	at edu.stanford.nlp.parser.nndep.Classifier.preCompute(Classifier.java:644)
	at edu.stanford.nlp.parser.nndep.DependencyParser.initialize(DependencyParser.java:1189)
	at edu.stanford.nlp.parser.nndep.DependencyParser.loadModelFile(DependencyParser.java:630)
	at edu.stanford.nlp.parser.nndep.DependencyParser.loadFromModelFile(DependencyParser.java:499)
	at edu.stanford.nlp.pipeline.DependencyParseAnnotator.<init>(DependencyParseAnnotator.java:57)
	at edu.stanford.nlp.pipeline.AnnotatorImplementations.dependencies(AnnotatorImplementations.java:240)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$18(StanfordCoreNLP.java:536)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$29/240650537.apply(Unknown Source)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$30(StanfordCoreNLP.java:602)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$38/2054798982.get(Unknown Source)
	at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)
	at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
	at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:251)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:192)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:188)
	at com.java.nlp.main.Pipeline.getPipeline(Pipeline.java:23)
	at com.java.nlp.Tokenize.main(Tokenize.java:15)"
838,https://github.com/stanfordnlp/CoreNLP/issues/1006,1006,[],closed,2020-02-28 19:12:34+00:00,,"Command Line: -Xmx512M -Xmx1024) MException in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded","**-Xmx512M
-Xmx1024M**

> After increasing the heap memory

Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.util.HashMap.newNode(HashMap.java:1747)
	at java.util.HashMap.putVal(HashMap.java:631)
	at java.util.HashMap.put(HashMap.java:612)
	at edu.stanford.nlp.ling.tokensregex.Env.bind(Env.java:313)
	at edu.stanford.nlp.ling.tokensregex.Env.initDefaultBindings(Env.java:150)
	at edu.stanford.nlp.ling.tokensregex.TokenSequencePattern.getNewEnv(TokenSequencePattern.java:159)
	at edu.stanford.nlp.pipeline.TokensRegexNERAnnotator.createPatternMatcher(TokensRegexNERAnnotator.java:365)
	at edu.stanford.nlp.pipeline.TokensRegexNERAnnotator.<init>(TokensRegexNERAnnotator.java:317)
	at edu.stanford.nlp.pipeline.NERCombinerAnnotator.setUpFineGrainedNER(NERCombinerAnnotator.java:220)
	at edu.stanford.nlp.pipeline.NERCombinerAnnotator.<init>(NERCombinerAnnotator.java:145)
	at edu.stanford.nlp.pipeline.AnnotatorImplementations.ner(AnnotatorImplementations.java:68)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$5(StanfordCoreNLP.java:523)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$16/747464370.apply(Unknown Source)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$30(StanfordCoreNLP.java:602)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$38/2054798982.get(Unknown Source)
	at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)
	at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
	at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:251)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:192)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:188)
	at com.java.nlp.main.Pipeline.getPipeline(Pipeline.java:23)
	at com.java.nlp.Tokenize.main(Tokenize.java:15)
"
839,https://github.com/stanfordnlp/CoreNLP/issues/1007,1007,[],open,2020-02-28 23:18:03+00:00,,[MEMORY] Memory Tracking Issue,"#1000 
#1001 
#1002 
#1003 
#1004 

I have implemented some of those ideas on my fork:
https://github.com/lambdaupb/CoreNLP/tree/MEMORY

Those changes should *NOT* be merged, they are experiments.

I have ran experiments with the following result:

```
repo baseline: 			3785999536 3785MiB
With changes: 			3103041904 3103MiB (- 682MiB)
With float Classifier :		2204942344 2204MiB (-1581MiB)
With dedup on deserialize	2023875296 2023MiB (-1762MiB)
With SequencePatternExpr	2005381784 2005MiB (-1780MiB)
```

Tests were run with
```
JVM: OpenJDK 64-Bit Server VM (11.0.1+13, mixed mode)
Java: version 11.0.1, vendor Oracle Corporation
Java Home: /Library/Java/JavaVirtualMachines/jdk-11.0.1.jdk/Contents/Home
JVM Flags: <none>
```

that is just loading the pipeline
```java
Properties props = new Properties();
props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,depparse,coref,quote"");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
Thread.sleep(10*1000*1000)
```

attaching VisualVM and forcing a few GCs and then looking at the used heap size."
840,https://github.com/stanfordnlp/CoreNLP/issues/1018,1018,[],closed,2020-03-05 07:26:30+00:00,,3.9.1version add KBP ner models for Chinese and SpanishÔºüHow to use kbp ner modelsÔºü,"When I visit https://nlp.stanford.edu/software/CRF-NER.htmlÔºåI found Stanford Named Entity Recognizer (NER) add KBP ner models for Chinese and Spanish In version 3.9.1.I want to know whether KBP ner model is a CRF model for training under KBP data set or improvement of the old CRF modelÔºüHow to use kbp ner modelsÔºü



"
841,https://github.com/stanfordnlp/CoreNLP/issues/1021,1021,[],closed,2020-03-16 13:19:45+00:00,,There is a potential runtime exception,"Hello,
First of all i want to thank you for sharing your code. 
But when I was operating some of my files, there was a runtime exception of the type 'java.lang.AssertionError'. The runtime exception was from method: 
**org.restlet.engine.io.IoUtils.delete**
When this method was passed with an unauthorized files/file Not exists, it might cause the program to crash. But the caller may not know these problems and may cause the program to crash suddenly. 
so I suggest you add catch or throw to the source code An exception goes out, which is convenient for the caller to develop. Thank you!
The issue can be reproduced by the following code. 


```
public class Test extends IOUtilsTest{
    public static void main(String[] args) {

        Test test = new Test();
        File file = new File(""Any path not exists or unauthorized"");
        test.delete(file);
    }

}
```
"
842,https://github.com/stanfordnlp/CoreNLP/issues/1022,1022,[],open,2020-03-19 09:24:25+00:00,,Integration of Mocking framework,"Hello everyone, first off thank you for this project!

We are a group of students aiming at contributing more unit tests to this project. Thus we have read and acknowledged the [contribution rules](https://github.com/stanfordnlp/CoreNLP/blob/master/CONTRIBUTING.md).
During our analysis of the project we found different units that are hard to test since they depend on complex input objects, such as for example Files.

Would the maintainers of this project accept a pull request where the [Mockito library](https://site.mockito.org/) is integrated into the testing procedures to cover things that are hard to test and have 0% test coverage at the moment?
Looking at the licensing requirements of this project and the license of Mockito (MIT license) we assume that it would be compatible."
843,https://github.com/stanfordnlp/CoreNLP/issues/1023,1023,[],closed,2020-03-20 17:15:05+00:00,,pycorenlp sentiment annotation outputs a string instead of json,"Hi, I'm trying to POS tag, lemmatize and do sentiment analysis on some text, but whenever I use the **sentiment** tool something goes wrong and instead of a json output I get a string. 
I get a similar error when trying the sentiment in a browser at http://localhost:9000/ where the output seems to be correct but in a red box and without the usual visuals.

I downloaded the latest version of CoreNLP on [the official website](https://stanfordnlp.github.io/CoreNLP/download.html), I am running the server with `java -mx6g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout 10000` without any problem.

The code I am trying is 
`from pycorenlp import StanfordCoreNLP`
`nlp_wrapper = StanfordCoreNLP('http://localhost:9000')`
`doc = ""I like this chocolate.""`
`annot_doc = nlp_wrapper.annotate(doc,`
    `properties={`
       `'annotators': 'sentiment',`
       `'outputFormat': 'json',`
      ` 'timeout': 10000,`
    `})`

I run the code above on:
macOS 10.15.3
pycorenlp 0.3.0
python 3.6
java 8

Thank you for your help!"
844,https://github.com/stanfordnlp/CoreNLP/issues/1024,1024,[],closed,2020-03-20 22:19:02+00:00,,GetPatternsFromDataMultiClass: NullPointerException with unknown cause,"Invoking `GetPatternsFromDataMultiClass<SurfacePattern>.run(properties)` on some corpora ends up always throwing a NullPointerException for the same reason. However, the processed named entity label when the exception is thrown is not the same between executions nor the first label that is processed, so I suspect some kind of race condition. Nevertheless, my suspicion doesn't seem to be the actual cause, because invoking the `GetPatternsFromDataMultiClass<SurfacePattern>.run(properties)` method in a synchronized block doesn't help (it is called from several threads in my code), neither reducing the `numThreads` property to 1.

In particular, the null pointer exception is caused by the `setEn` variable in the `ScorePhrases.getSentences` method being null. The stack trace of the exception is as follows:

```
java.lang.NullPointerException
	at stanford.corenlp@3.9.2/edu.stanford.nlp.patterns.ScorePhrases.getSentences(ScorePhrases.java:347)
	at stanford.corenlp@3.9.2/edu.stanford.nlp.patterns.ScorePhrases.applyPats(ScorePhrases.java:393)
	at stanford.corenlp@3.9.2/edu.stanford.nlp.patterns.ScorePhrases.learnNewPhrasesPrivate(ScorePhrases.java:567)
	at stanford.corenlp@3.9.2/edu.stanford.nlp.patterns.ScorePhrases.learnNewPhrases(ScorePhrases.java:172)
	at stanford.corenlp@3.9.2/edu.stanford.nlp.patterns.GetPatternsFromDataMultiClass.iterateExtractApply4Label(GetPatternsFromDataMultiClass.java:2307)
	at stanford.corenlp@3.9.2/edu.stanford.nlp.patterns.GetPatternsFromDataMultiClass.iterateExtractApply(GetPatternsFromDataMultiClass.java:2110)
	at stanford.corenlp@3.9.2/edu.stanford.nlp.patterns.GetPatternsFromDataMultiClass.runNineYards(GetPatternsFromDataMultiClass.java:3310)
	at stanford.corenlp@3.9.2/edu.stanford.nlp.patterns.GetPatternsFromDataMultiClass.run(GetPatternsFromDataMultiClass.java:3290)
	at [my code]
```

Can someone point me to the right direction to fix this problem? It's urgent to me. Any help would be appreciated."
845,https://github.com/stanfordnlp/CoreNLP/issues/1026,1026,[],closed,2020-03-24 18:01:19+00:00,,Coreference on large number of files,"Hi, this is more of a how-to than an issue. 

I have over 500,000 files on which I'd like to run coreference. I'm passing the input files using a `fileList`, so the pipeline is only getting set up once. I also have the `-threads` option on, and set to 8. Since my files are around 1000 words at least, the neural algorithm seems to be taking ~40-60 seconds per file. 

Is there a way to get this done faster? At the current rate of processing, it could take months if not years for my task to get done. "
846,https://github.com/stanfordnlp/CoreNLP/issues/1027,1027,"[{'id': 45387507, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNw==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/cantreproduce', 'name': 'cantreproduce', 'color': 'dddddd', 'default': False, 'description': None}]",closed,2020-03-31 23:50:58+00:00,,Stanford CoreNLP server not responding,"I have been trying to use the CoreNLP server using various python packages including [Stanza](https://stanfordnlp.github.io/stanza/). I am always running into the same problem that I do not hear back from the server. 

I downloaded a copy of CoreNLP from the [website](https://stanfordnlp.github.io/CoreNLP/download.html). I then try to start a server from the terminal and go to my localhost as described [here](https://stanfordnlp.github.io/CoreNLP/corenlp-server.html#getting-started). Based on the documentation I should see something when I go to ```http://localhost:9000/```, but nothing loads up. 

Here are to commands I use:
```
cd stanford-corenlp-full-2018-10-05/
java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000
```
Here is the output of running the commands above:
```
Samarths-MacBook-Pro-2:stanford-corenlp-full-2018-10-05 samarthbhandari$ java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000
[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---
[main] INFO CoreNLP - setting default constituency parser
[main] INFO CoreNLP - warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz
[main] INFO CoreNLP - using: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz instead
[main] INFO CoreNLP - to use shift reduce parser download English models jar from:
[main] INFO CoreNLP - http://stanfordnlp.github.io/CoreNLP/download.html
[main] INFO CoreNLP -     Threads: 8
[main] INFO CoreNLP - Starting server...
[main] INFO CoreNLP - StanfordCoreNLPServer listening at /0:0:0:0:0:0:0:0:9000
```


I then go to ```http://localhost:9000/```, nothing loads up. Like I mentioned above originally I have been trying to do the same thing using some of the python packages and observed similar behavior. 

[Here](https://stackoverflow.com/questions/60961208/stanfordcorenlp-server-listening-indefinitely-using-stanza) is a stack overflow post related to server not responding using Stanza. 

OS: MacOS 10.15.4
Python: 3.7.7
Java: 1.8
"
847,https://github.com/stanfordnlp/CoreNLP/issues/1028,1028,[],closed,2020-04-04 05:49:10+00:00,,A Web Server?,Is it absolutely necessary to have a web server running to use Stanford's CoreNLP parser?  Why?
848,https://github.com/stanfordnlp/CoreNLP/issues/1029,1029,[],closed,2020-04-12 04:54:31+00:00,,Use tokenRegex when ruleType is text,"My question is that if I want to match a string pattern in a sentence, not a token sequence, can anyone help me to solve it?  

Example: 
If the input is rt. 42,  there are 2 tokens, so I can use (/rt[.]?/  /\d{1,4}/) to tag rt.  as RT_prefix, 42 as RT_number.  

But if the input is rt42, there is only one token, so how can I extract them as rt => RT_prefix, 42=>RT_number with Stanford NLP
"
849,https://github.com/stanfordnlp/CoreNLP/issues/1030,1030,[],closed,2020-04-20 07:05:43+00:00,,Number of vertices in dependency graph not always (number of tokens + 1),"I am using Core NLP version 3.9.2 from the Maven repo. I ran the following Core NLP pipeline:


The sentence: ""Then down thru Snowgrass Flats to Cispus Pass and on to Sheep Lake.""

        String text = ""Then down thru Snowgrass Flats to Cispus Pass and on to Sheep Lake."";
        Properties properties = new Properties();
        properties.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,parse,depparse"");

        StanfordCoreNLP pipeline = new StanfordCoreNLP(properties);
        CoreDocument doc = new CoreDocument(text);
        pipeline.annotate(doc);
        CoreSentence sentence = doc.sentences().get(0);
        assertEquals(sentence.tokens(), sentence.dependencyParse().vertexListSorted());

The assertion failed because token formats are not the same as vertex formats, but the sizes were also not the same. Specifically, there should be a token for each vertex. However, there was duplication. Instead, the output was as follows:

`org.opentest4j.AssertionFailedError: 

Expected :[Then-1, down-2, thru-3, Snowgrass-4, Flats-5, to-6, Cispus-7, Pass-8, and-9, on-10, to-11, Sheep-12, Lake-13, .-14]

Actual   :[Then/RB, Then/RB''''''''', Then/RB'''''''''', Then/RB''''''''''', Then/RB'''''''''''', down/IN, thru/IN, Snowgrass/NNP, Flats/NNP, to/TO, Cispus/NNP, Pass/NN, and/CC, on/IN, to/TO, Sheep/NNP, Lake/NNP, ./.]`

In particular, the word ""Then"" is repeated 4 times, with apostrophes added after the tag. I also tried doing the same using the Simple API (not the pipeline), and the output was normal (no bug).
"
850,https://github.com/stanfordnlp/CoreNLP/issues/1034,1034,[],closed,2020-04-22 18:22:37+00:00,,maven coordinates on download page still point to 3.9.2,https://stanfordnlp.github.io/CoreNLP/download.html
851,https://github.com/stanfordnlp/CoreNLP/issues/1035,1035,[],open,2020-04-23 18:45:01+00:00,,CoreNLP models classpath issue,"Just downloaded 4.0.0 in Eclipse using maven. I am getting an error on getting model file for pos-tagger. This doesn't happen when I use 3.9.2. 

Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:801)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.<init>(MaxentTagger.java:322)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.<init>(MaxentTagger.java:275)
	at edu.stanford.nlp.pipeline.POSTaggerAnnotator.loadModel(POSTaggerAnnotator.java:85)
	at edu.stanford.nlp.pipeline.POSTaggerAnnotator.<init>(POSTaggerAnnotator.java:73)
	at edu.stanford.nlp.pipeline.AnnotatorImplementations.posTagger(AnnotatorImplementations.java:73)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$5(StanfordCoreNLP.java:526)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$32(StanfordCoreNLP.java:607)
	at edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)
	at edu.stanford.nlp.util.Lazy.get(Lazy.java:31)
	at edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:252)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:193)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:189)
	at corenlp.proto.DefaultNerExample.getNerSearch(DefaultNerExample.java:85)
	at corenlp.proto.DefaultNerExample.main(DefaultNerExample.java:28)
Caused by: java.io.IOException: Unable to open ""edu/stanford/nlp/models/pos-tagger/english-left3words-distsim.tagger"" as class path, filename or URL
	at edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:481)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:798)
	... 15 more
 "
852,https://github.com/stanfordnlp/CoreNLP/issues/1037,1037,[],closed,2020-04-26 13:25:38+00:00,,NERCombinerAnnotator NullPointerException ,"Command used:
```
/usr/bin/java -mx150g -cp ""stanford-corenlp/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse -ssplit.eolonly -tokenize.whitespace true -file infile -outputFormat xml -outputDirectory outdir
```
Error:
```
Exception in thread ""main"" java.lang.NullPointerException
	at edu.stanford.nlp.pipeline.NERCombinerAnnotator.annotate(NERCombinerAnnotator.java:322)
	at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:637)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:647)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1226)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1060)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1326)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1389)
```

I obtained a NullPointerException when running the above command. The pipeline works fine up till the NER annotator is added. I was able to isolate the issue for the `ssplit.eolonly` property to the fact that the unsuccessfully parsed files had CRLF line endings. This may be similar to the issue in #966 where the delimiter regex for the ArabicSegmenterAnnotator mishandled `\r\n`. 

Changing the encoding for the line endings resulted in the pipeline running successfully when using only the `-ssplit.eolonly` property. However I still obtain a NullPointerException with the `-tokenize.whitespace` property on NERCombinerAnnotator. 

I have a feeling it's something of the same issue; with the NER annotator in handling the tokenize.whitespace property when performing NER-specific tokenisation. Any help on this would be appreciated.
"
853,https://github.com/stanfordnlp/CoreNLP/issues/1038,1038,[],closed,2020-04-29 08:23:11+00:00,,training a custom NER :  java.lang.OutOfMemoryError: Java heap space,"facing out of memory issue even though the training corpus is very small (about 1 mb)
tried increasing memory allocation (mx7g) but this doesnt seem to be enough for training . please suggest some way to restrict this heap usage..below is the error log 


Invoked on Wed Apr 29 13:41:42 IST 2020 with arguments: -prop prop.txt
usePrevSequences=true
useClassFeature=true
useTypeSeqs2=true
useSequences=true
wordShape=chris2useLC
useTypeySequences=true
useDisjunctive=true
noMidNGrams=true
serializeTo=dummy-ner-model-arabic.ser.gz
maxNGramLeng=3
useNGrams=true
usePrev=true
useNext=true
maxLeft=1
trainFile=train_arabic.TSV
map=word=0,answer=1
useWord=true
useTypeSeqs=true
numFeatures = 648062
Time to convert docs to feature indices: 2.8 seconds
numClasses: 26 [0=O,1=B-MIS0,2=B-MIS1,3=B-MIS3,4=B-MIS2,5=I-MIS2,6=I-MIS0,7=B-PER,8=I-PER,9=B-MIS,10=I-MIS1,11=B-ORG,12=I-ORG,13=B-LOC,14=I-LOC,15=I-MIS3,16=IO,17=I--ORG,18=B-MISS1,19=OO,20=B-MIS-2,21=B-MIS-1,22=B-ENGLISH,23=I-MIS,24=B-SPANISH,25=B-MIS1`]
numDocuments: 1
numDatums: 116149
numFeatures: 648062
Time to convert docs to data/labels: 1.6 seconds
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
        at edu.stanford.nlp.ie.crf.CRFLogConditionalObjectiveFunction.empty2D(CRFLogConditionalObjectiveFunction.java:908)
        at edu.stanford.nlp.ie.crf.CRFLogConditionalObjectiveFunction.<init>(CRFLogConditionalObjectiveFunction.java:144)
        at edu.stanford.nlp.ie.crf.CRFLogConditionalObjectiveFunction.<init>(CRFLogConditionalObjectiveFunction.java:125)
        at edu.stanford.nlp.ie.crf.CRFClassifier.getObjectiveFunction(CRFClassifier.java:1821)
        at edu.stanford.nlp.ie.crf.CRFClassifier.trainWeights(CRFClassifier.java:1827)
        at edu.stanford.nlp.ie.crf.CRFClassifier.train(CRFClassifier.java:1742)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequenceClassifier.java:770)
        at edu.stanford.nlp.ie.AbstractSequenceClassifier.train(AbstractSequenceClassifier.java:758)
        at edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3011)
 "
854,https://github.com/stanfordnlp/CoreNLP/issues/1039,1039,[],closed,2020-04-30 23:02:10+00:00,,Stop StanfordCoreNLP from Connecting to StanfordCoreNLP server,"Whenever I create StanfordCoreNLP for a parsing task in Python, 
```
StanfordCoreNLP('stanford-corenlp-full-2020-04-20', lang='en')
```
I get this logging 
```
Initializing native server...
java -Xmx4g -cp ""stanford-corenlp-full-2020-04-20/*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9032
Server shell PID: 11991
The server is available.
```
1. Can StanfordCoreNLP work in python offline without connecting to the server?
2. Is there a command to stop the logging to screen in stanfordCoreNLP?
On applying

On applying the parser, `self.nlp_src.parse(sentence)`, I got another logging of the form below:

```{'properties': ""{'annotators': 'pos,parse', 'outputFormat': 'json'}"", 'pipelineLanguage': 'en'}```

3. Is there a way to stop the above logging too?


Lastly, I got this error in the process of using the parser in the stanford coreNLP as explained above. 

```requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=9032): Max retries exceeded with url: /?properties=%7B%27annotators%27%3A+%27pos%2Cparse%27%2C+%27outputFormat%27%3A+%27json%27%7D&pipelineLanguage=en (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe04121d6d0>: Failed to establish a new connection: [Errno 111] Connection refused'))```

4. What is the cause of this error? What do I do to prevent this?"
855,https://github.com/stanfordnlp/CoreNLP/issues/1040,1040,[],closed,2020-05-01 12:02:14+00:00,,"Can't use mwt annotator in web demo (French,German,Spanish tokenization)","The web demo doesn't use the `mwt` annotator, so certain languages can't be tokenized properly in the web demo."
856,https://github.com/stanfordnlp/CoreNLP/issues/1041,1041,[],closed,2020-05-13 20:15:21+00:00,,QUESTION: How can I extract clauses and dependent (subordinate) clauses for Chinese,"Hello,

 I am working with extracting clauses for Chinese. I am using CoreNLP 4.0. I am also looking at this link 

> http://www.surdeanu.info/mihai/teaching/ista555-fall13/readings/PennTreebankConstituents.html

that lists the type of clauses. I used the terms S, SBAR, SBARQ, SINV, and SQ to check for all types of clauses in the constituency parse tree of English sentences.

However, where can I find the terminologies or the names of the nodes in a constituency parse tree for Chinese? 

I tried with a sample Chinese sentence and the constituency parse tree doesn't seem to have any of the above tags. However, I found one tag 

> IP

which means Independent clause? (Please correct me if I am wrong here).

I am looking for how to extract clauses and then specifically subordinate clauses for Chinese. Any help would be appreciated.

Thanks"
857,https://github.com/stanfordnlp/CoreNLP/issues/1042,1042,[],closed,2020-05-18 22:37:30+00:00,,LEFT and RIGHT cases do the same thing,"https://github.com/stanfordnlp/CoreNLP/blob/e35d75d45b4b609aaadde43a12020285a1475139/src/edu/stanford/nlp/parser/shiftreduce/BinaryTransition.java#L211

The above case is redundant. Please see [this](https://github.com/stanfordnlp/CoreNLP/commit/7299f6ab5f6e2f25299ebed9361de7baf3aa63ee#r39266032) conversation. It could be written as:
```java
@Override
public int hashCode() {
  switch(side) {
    case LEFT:
    case RIGHT:
      return 97197711 ^ label.hashCode();
// ...
```"
858,https://github.com/stanfordnlp/CoreNLP/issues/1043,1043,[],open,2020-05-19 17:53:20+00:00,,Sentiment Model training; Create Input data for BuildBinarizedDataset.java,"I am curious to know whether there is any tool in CoreNLP, or otherwise, that can create from any input text, an input format that would be valid for BuildBinarizedDataset.java

If none, is there any documentation on how text data with the requisite sentiments tagged gets converted to a format acceptable to BuildBinarizedDataset.java to allow one to build their own?"
859,https://github.com/stanfordnlp/CoreNLP/issues/1044,1044,[],open,2020-05-21 02:51:50+00:00,,spanish.number.regexner.mapping cannot be redirected to another location,"`props.put(""spanish.number.regexner.mapping"", modPath + ""kbp/spanish/kbp_regexner_number_sp.tag"");`

got error: 
edu.stanford.nlp.io.RuntimeIOException: Couldn't read TokensRegexNER from edu/stanford/nlp/models/kbp/spanish/gazetteers/kbp_regexner_number_sp.tag"
860,https://github.com/stanfordnlp/CoreNLP/issues/1045,1045,[],open,2020-05-23 16:47:20+00:00,,Tokenization in version 4.0.0 does different things with dashes.,"After upgrading from version 3.9.2 of Stanford core nlp to version 4.0.0 we observed that the social security number in text got split differently. For example, 555-55-5555 instead of being one token became two, 555 and 55-555. 

This broke our SSN NER Rule that detects Social Security Numbers."
861,https://github.com/stanfordnlp/CoreNLP/issues/1047,1047,[],closed,2020-05-27 22:38:04+00:00,,Sentiment Analysis Result Categories,"I was wondering how Core-NLP differentiates between its different sentiment result categories.   Specifically, what is the difference between ""very negative"" and just ""negative""?  Is the method comparable to VADER's composite score?  "
862,https://github.com/stanfordnlp/CoreNLP/issues/1048,1048,[],closed,2020-06-01 18:02:26+00:00,,Can't download CoreNLP,Is the download server down? https://stanfordnlp.github.io/CoreNLP/download.html
863,https://github.com/stanfordnlp/CoreNLP/issues/1049,1049,[],closed,2020-06-02 08:29:02+00:00,,NER not recognizing URL's in 4.0.0 !!!?,https://stackoverflow.com/questions/62058272/english-ner-annotator-for-stanford-corenlp-v-4-0-0-missing-some-entity-types-com
864,https://github.com/stanfordnlp/CoreNLP/issues/1050,1050,[],closed,2020-06-02 14:54:08+00:00,,How to host corenlp with caseless model?,"I have been trying to host CoreNLP with caseless model but no success. The defail of my current setup is here: https://stackoverflow.com/questions/62154244/how-do-i-host-corenlp-server-with-caseless-models.

Any help will be appreciated."
865,https://github.com/stanfordnlp/CoreNLP/issues/1052,1052,[],closed,2020-06-04 13:34:28+00:00,,NN --> JJR edge not available in JAVA API but shown on corenlp.run - V 4.0.0.,"I am trying to parse the following statement : 
""top 5 products having profit greater than 50""

Java Dependency Graph Object has no edges between profit and greater.

![image](https://user-images.githubusercontent.com/12781360/83763322-30aa6500-a696-11ea-9ac6-6255aab4aa0f.png)

But, https://corenlp.run/ shows an edge from profit --> greater (which is desired)

![image](https://user-images.githubusercontent.com/12781360/83763071-ef19ba00-a695-11ea-807f-278c2e1205a7.png)

Version of jar is 4.0.0 and is modified on Apr 26, 2020. 

Code :

`Properties props = new Properties();`
`props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, ner, parse, dcoref"");`
`props.setProperty(""coref.algorithm"", ""neural"");`
`props.setProperty(""ner.docdate.useFixedDate"", new SimpleDateFormat(""yyyy-MM-dd"").format(new Date()));`
`StanfordCoreNLP pipeline = new StanfordCoreNLP(props);`
`query = ""top 5 agents having commission greater than 0.15"";`
`CoreDocument document = new CoreDocument(query);`
`pipeline.annotate(document);`
`CoreSentence sentence = document.sentences().get(0);`
`SemanticGraph dependencyParse = sentence.dependencyParse();`"
866,https://github.com/stanfordnlp/CoreNLP/issues/1053,1053,[],closed,2020-06-05 21:06:47+00:00,,Out-of-sequence index in token array,"The issue below is that the index value in the tokens array in the same sentence goes from 34 to 3 obtaining an ambiguity in using the index value since the same lower index values have already been used for earlier tokens in the sentence. The enhancedPlusPlusDependencies array uses the out-of-sequence index value which would be incorrect if the earlier tokens having the duplicate index value but the wrong token were selected by a program not aware of the ambiguity. If the index value increased for each next token as I have seen it in all other cases, this ambiguity would be avoided.

Sentence = 'We can count on the fingers of one of our hands all those worthy of poetic fame who now live in this great country of intellectual and civilized men,--one for every ten millions.'

Out of sequence index values occur at 'ten millions'.

In php the java-server call sequence = 'wget --post-data \''.$urlencode_paragraph.'\' \'localhost:9000/?properties={""annotators"":""'.$annotators_txt.'"",""outputFormat"":""json""}\' -O -'

$annotators_txt = 'tokenize,ssplit,pos,lemma,ner,regexner,parse,depparse,coref,dcoref,relation,kbp,natlog,udfeats,truecase,openie,quote'

Server run command = taskset -c 0-15 java -cp ""/home/nnelson/Documents/downloads/CoreNLP/stanford-corenlp-full-2020-04-20/*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer

End of tokens array
        [33]=>
        array(13) {
          [""index""]=>
          int(34)
          [""word""]=>
          string(5) ""every""
...
        }
        [34]=>
        array(14) {
          [""index""]=>
          int(3)
          [""word""]=>
          string(3) ""ten""
...
        }
        [35]=>
        array(14) {
          [""index""]=>
          int(4)
          [""word""]=>
          string(8) ""millions""
...
        }
        [36]=>
        array(13) {
          [""index""]=>
          int(37)
          [""word""]=>
          string(1) "".""
...
        }

In enhancedPlusPlusDependencies array
        [3]=>
        array(5) {
          [""dep""]=>
          string(6) ""nummod""
          [""governor""]=>
          int(4)
          [""governorGloss""]=>
          string(8) ""millions""
          [""dependent""]=>
          int(3)
          [""dependentGloss""]=>
          string(3) ""ten"""
867,https://github.com/stanfordnlp/CoreNLP/issues/1054,1054,[],closed,2020-06-06 12:08:51+00:00,,corenlp.run does not work,"The first time I tried. It shows ""document too long"", even for the input of fewer than 10 words.

The second time I tried it shows error message like:
```
<!DOCTYPE html> <!--[if lt IE 7]> <html class=""no-js ie6 oldie"" lang=""en-US""> <![endif]--> <!--[if IE 7]> <html class=""no-js ie7 oldie"" lang=""en-US""> <![endif]--> <!--[if IE 8]> <html class=""no-js ie8 oldie"" lang=""en-US""> <![endif]--> <!--[if gt IE 8]><!--> <html class=""no-js"" lang=""en-US""> <!--<![endif]--> <head> <meta http-equiv=""refresh"" content=""0""> <title>corenlp.run | 524: A timeout occurred</title> <meta charset=""UTF-8"" /> <meta http-equiv=""Content-Type"" content=""text/html; charset=UTF-8"" /> <meta http-equiv=""X-UA-Compatible"" content=""IE=Edge,chrome=1"" /> <meta name=""robots"" content=""noindex, nofollow"" /> <meta name=""viewport"" content=""width=device-width,initial-scale=1"" /> <link rel=""stylesheet"" id=""cf_styles-css"" href=""/cdn-cgi/styles/cf.errors.css"" type=""text/css"" media=""screen,projection"" /> <!--[if lt IE 9]><link rel=""stylesheet"" id='cf_styles-ie-css' href=""/cdn-cgi/styles/cf.errors.ie.css"" type=""text/css"" media=""screen,projection"" /><![endif]--> <style type=""text/css"">body{margin:0;padding:0}</style> </head> <body> <div id=""cf-wrapper""> <div id=""cf-error-details"" class=""cf-error-details-wrapper""> <div class=""cf-wrapper cf-error-overview""> <h1> <span class=""cf-error-type"">Error</span> <span class=""cf-error-code"">524</span> <small class=""heading-ray-id"">Ray ID: 59f1fb1c8dc90353 &bull; 2020-06-06 12:03:29 UTC</small> </h1> <h2 class=""cf-subheadline"">A timeout occurred</h2> </div><!-- /.error-overview --> <div class=""cf-section cf-highlight cf-status-display""> <div class=""cf-wrapper""> <div class=""cf-columns cols-3""> <div id=""cf-browser-status"" class=""cf-column cf-status-item cf-browser-status ""> <div class=""cf-icon-error-container""> <i class=""cf-icon cf-icon-browser""></i> <i class=""cf-icon-status cf-icon-ok""></i> </div> <span class=""cf-status-desc"">You</span> <h3 class=""cf-status-name"">Browser</h3> <span class=""cf-status-label"">Working</span> </div> <div id=""cf-cloudflare-status"" class=""cf-column cf-status-item cf-cloudflare-status ""> <div class=""cf-icon-error-container""> <i class=""cf-icon cf-icon-cloud""></i> <i class=""cf-icon-status cf-icon-ok""></i> </div> <span class=""cf-status-desc"">Chicago</span> <h3 class=""cf-status-name"">Cloudflare</h3> <span class=""cf-status-label"">Working</span> </div> <div id=""cf-host-status"" class=""cf-column cf-status-item cf-host-status cf-error-source""> <div class=""cf-icon-error-container""> <i class=""cf-icon cf-icon-server""></i> <i class=""cf-icon-status cf-icon-error""></i> </div> <span class=""cf-status-desc"">corenlp.run</span> <h3 class=""cf-status-name"">Host</h3> <span class=""cf-status-label"">Error</span> </div> </div> </div> </div><!-- /.status-display --> <div class=""cf-section cf-wrapper""> <div class=""cf-columns two""> <div class=""cf-column""> <h2>What happened?</h2> <p>The origin web server timed out responding to this request.</p> </div> <div class=""cf-column""> <h2>What can I do?</h2> <h5>If you're a visitor of this website:</h5> <p>Please try again in a few minutes.</p> <h5>If you're the owner of this website:</h5> <p><span>The connection to the origin web server was made, but the origin web server timed out before responding. The likely cause is an overloaded background task, database or application, stressing the resources on your web server. To resolve, please work with your hosting provider or web development team to free up resources for your database or overloaded application.</span> <a href=""https://support.cloudflare.com/hc/en-us/articles/200171926-Error-524"">Additional troubleshooting information here.</a></p> </div> </div> </div><!-- /.section --> <div class=""cf-error-footer cf-wrapper""> <p> <span class=""cf-footer-item"">Cloudflare Ray ID: <strong>59f1fb1c8dc90353</strong></span> <span class=""cf-footer-separator"">&bull;</span> <span class=""cf-footer-item""><span>Your IP</span>: 2601:241:8280:1ba0:440b:4a78:3a38:32ef</span> <span class=""cf-footer-separator"">&bull;</span> <span class=""cf-footer-item""><span>Performance &amp; security by</span> <a href=""https://www.cloudflare.com/5xx-error-landing?utm_source=error_footer"" id=""brand_link"" target=""_blank"">Cloudflare</a></span> </p> </div><!-- /.error-footer --> </div><!-- /#cf-error-details --> </div><!-- /#cf-wrapper --> </body> </html>
```

Please check why the new version of `corenlp.run` is unstable, not as robust as the previous version. Thank you very much. "
868,https://github.com/stanfordnlp/CoreNLP/issues/1055,1055,[],open,2020-06-07 04:58:56+00:00,,Comparing CoreNLP and VADER ,"Is there any way to compare CoreNLP sentiment categories (Very Negative, Negative, ect.) to VADER composite scores (-1 to 1)?  If you have any familiarity with VADER, which composite scores would correspond to the five CoreNLP sentiment categories?  Any help would be greatly appreciated. "
869,https://github.com/stanfordnlp/CoreNLP/issues/1056,1056,[],closed,2020-06-09 07:42:32+00:00,,Request to Java server returning invalid json in a localized environment,"OS: Windows 10 Pro (x64)
Locale: es-ES
CoreNLP: 4.0.0
Java: 1.8.0_251, HotSpot 64-Bit

**Steps to reproduce:**

1) Open a CMD shell 
2) Run server with _java -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000_
3) Download Stanza CoreNLP client example: https://github.com/stanfordnlp/stanza/blob/master/demo/corenlp.py
4) Open another CMD shell and run the python source file above. I don't think it matters, but I'm using Python 3.7.1 (AMD64) and Stanza 1.0.1

**Result:**

_Line 77 (matches = client.tokensregex(text, pattern))_ throws exception _json.decoder.JSONDecodeError_ due to an invalid json being parsed.

The returned JSON contains a serialized floating point value, using a comma as a decimal separator.

```
  ""entitymentions"": [  {
		  ""docTokenBegin"": 0,
		  ""docTokenEnd"": 2,
		  ""tokenBegin"": 0,
		  ""tokenEnd"": 2,
		  ""text"": ""Chris Manning"",
		  ""characterOffsetBegin"": 0,
		  ""characterOffsetEnd"": 13,
		  ""ner"": ""PERSON"",
		  ""nerConfidences"": {
			  ""PERSON"": 0,
			99674529762772 
		}
```
		
**Workaround:**

If we run the server using the US localization like this: _java -Duser.language=en-US -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000_
we get no error.

Although not tried, the bug is likely to be reproduced on any machine, using this workaround, with spanish localization: es-ES."
870,https://github.com/stanfordnlp/CoreNLP/issues/1057,1057,[],open,2020-06-09 14:49:44+00:00,,Coreference error,"taskset -c 0-15 java -cp ""/home/nnelson/Documents/downloads/CoreNLP/stanford-corenlp-full-2020-04-20/*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer

Using http://localhost:9000/ with all parsing options selected.

'Cicero himself would not have been immortal, if his essays and orations had not conformed to the principles of art. Even an historian who would live must be an artist, like Voltaire or Macaulay. A cumbrous, or heavy, or pedantic historian will never be read, even if his learning be praised by all the critics of Germany.'

![coreference](https://user-images.githubusercontent.com/8620209/84161345-6a7bc280-aa2c-11ea-9156-ad243ea9777e.png)

The 'his' in the third sentence has a coreference going back to 'Cicero' in the first sentence. The 'his' in the third sentence should go back to 'historian' in the third sentence."
871,https://github.com/stanfordnlp/CoreNLP/issues/1058,1058,[],open,2020-06-10 21:18:39+00:00,,kbp subject string is correct when using one sentence but incorrect when using two,"This is a minor but curious issue. The kbp subject is correct when the sentence is processed alone but becomes incorrect when processed with a second sentence. In both cases the remainder of the kbp array remains the same.

taskset -c 0-15 java -cp ""/home/nnelson/Documents/downloads/CoreNLP/stanford-corenlp-full-2020-04-20/*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer

annotators = tokenize,ssplit,pos,lemma,ner,regexner,parse,depparse,coref,dcoref,relation,kbp,natlog,udfeats,truecase,openie,quote

The subject ""he"" is correct when using the sentence alone.

'At Paris he maintains fourteen different theses, propounded by learned men, on different subjects, and gains universal admiration.'
      [""kbp""]=>
      array(1) {
        [0]=>
        array(6) {
          [""subject""]=>
          string(2) ""he""
          [""subjectSpan""]=>
          array(2) {
            [0]=>
            int(2)
            [1]=>
            int(3)
          }
          [""relation""]=>
          string(23) ""per:cities_of_residence""
          [""relationSpan""]=>
          array(2) {
            [0]=>
            int(-2)
            [1]=>
            int(-1)
          }
          [""object""]=>
          string(5) ""Paris""
          [""objectSpan""]=>
          array(2) {
            [0]=>
            int(1)
            [1]=>
            int(2)
          }
        }
      }

When the second sentence is added, the subject incorrectly turns to ""his"" for the first sentence.

'At Paris he maintains fourteen different theses, propounded by learned men, on different subjects, and gains universal admiration. He is early selected by his native city for important offices, which he fills with honor.'
      [""kbp""]=>
      array(1) {
        [0]=>
        array(6) {
          [""subject""]=>
          string(3) ""his""
          [""subjectSpan""]=>
          array(2) {
            [0]=>
            int(2)
            [1]=>
            int(3)
          }
          [""relation""]=>
          string(23) ""per:cities_of_residence""
          [""relationSpan""]=>
          array(2) {
            [0]=>
            int(-2)
            [1]=>
            int(-1)
          }
          [""object""]=>
          string(5) ""Paris""
          [""objectSpan""]=>
          array(2) {
            [0]=>
            int(1)
            [1]=>
            int(2)
          }
        }
      }

The following picture shows the kbp relation correctly for both sentences. This result must be from the span values without regard to the subject strings.

![kbp](https://user-images.githubusercontent.com/8620209/84317431-f889a300-ab29-11ea-997f-0e86a5588633.png)"
872,https://github.com/stanfordnlp/CoreNLP/issues/1059,1059,[],open,2020-06-11 22:52:19+00:00,,KBP issues,"![image](https://user-images.githubusercontent.com/8620209/84446331-7f11b380-ac02-11ea-8e39-00f63277b9d9.png)

The KBP Relations chart in the picture shows five relations while the json output shows only two. (See below.)

The parents and children relationships in the KBP Relations chart would not seem to hold since they are coreferences to the same person as seen in the Coreference chart.

Even though the capture of several 'his' words in the KBP Relations chart appears to be in error, there are more 'his' words in the sentence that were captured as coreferences than captured as KBP Relations which seems inconsistent.

The second relation in the kbp array below has 'his' for the word at subjectSpan 46 when the word is 'he' from the tokens array.

taskset -c 0-15 java -cp ""/home/nnelson/Documents/downloads/CoreNLP/stanford-corenlp-full-2020-04-20/*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer

annotations=tokenize,ssplit,pos,lemma,ner,regexner,parse,depparse,coref,dcoref,relation,kbp,natlog,udfeats,truecase,openie,quote

Sentence=Who could discharge his ordinary duties or perform his daily occupations, if his father or his mother or his sister or his brother or his wife or his son or his daughter might not be finally forgiven for the frailties of an imperfect nature which he had inherited?

      [""kbp""]=>
      array(2) {
        [0]=>
        array(6) {
          [""subject""]=>
          string(3) ""his""
          [""subjectSpan""]=>
          array(2) {
            [0]=>
            int(19)
            [1]=>
            int(20)
          }
          [""relation""]=>
          string(12) ""per:children""
          [""relationSpan""]=>
          array(2) {
            [0]=>
            int(-2)
            [1]=>
            int(-1)
          }
          [""object""]=>
          string(3) ""his""
          [""objectSpan""]=>
          array(2) {
            [0]=>
            int(31)
            [1]=>
            int(32)
          }
        }
        [1]=>
        array(6) {
          [""subject""]=>
          string(3) ""his""
          [""subjectSpan""]=>
          array(2) {
            [0]=>
            int(46)
            [1]=>
            int(47)
          }
          [""relation""]=>
          string(11) ""per:parents""
          [""relationSpan""]=>
          array(2) {
            [0]=>
            int(-2)
            [1]=>
            int(-1)
          }
          [""object""]=>
          string(3) ""his""
          [""objectSpan""]=>
          array(2) {
            [0]=>
            int(31)
            [1]=>
            int(32)
          }
        }
      }
      
      [""tokens""]=>
        [46]=>
        array(13) {
          [""index""]=>
          int(47)
          [""word""]=>
          string(2) ""he""
          [""originalText""]=>
          string(2) ""he""
          ...
        }
      }"
873,https://github.com/stanfordnlp/CoreNLP/issues/1060,1060,[],closed,2020-06-22 23:02:04+00:00,,Keys are not being serialized: class edu.stanford.nlp.ling.CoreAnnotations$ArabicSegAnnotation,"I'm trying to start CoreNLPClient with Arabic models, but when it tries to serialize the results it throws the following exception error (it works for English):

> Starting server with command: java -Xmx16G -cp /stanford-corenlp-4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties StanfordCoreNLP-arabic.properties -preload tokenize,ssplit,pos,lemma,ner,parse,coref

Traceback (most recent call last):
  File ""/venv/lib/python3.6/site-packages/stanza/server/client.py"", line 398, in _request
    r.raise_for_status()
  File ""/venv/lib/python3.6/site-packages/requests/models.py"", line 941, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:9000/?properties=%7B%27outputFormat%27%3A+%27serialized%27%7D

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""corenlp_test.py"", line 13, in <module>
    ann = client.annotate(output_format=""serialized"", text=text)
  File ""/venv/lib/python3.6/site-packages/stanza/server/client.py"", line 470, in annotate
    r = self._request(text.encode('utf-8'), request_properties, **kwargs)
  File ""/venv/lib/python3.6/site-packages/stanza/server/client.py"", line 404, in _request
    raise AnnotationException(r.text)
stanza.server.client.AnnotationException: edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer$LossySerializationException: Keys are not being serialized: class edu.stanford.nlp.ling.CoreAnnotations$ArabicSegAnnotation

Please note that it gives correct output when output_format=""text"", the error throws only when it's serialized"
874,https://github.com/stanfordnlp/CoreNLP/issues/1061,1061,[],open,2020-07-06 16:18:41+00:00,,"SUTime is stateful for use across related sentences, but cannot be (or at least is not) reset for use across unrelated documents","The consequence is that if a collection of documents is processed in a different order, the results may be different, sometimes correct and sometimes incorrect.  One might say that SUTime is unstable.

The rules for dealing with time are encoded in `src/edu/stanford/nlp/time/rules/english.sutime.txt`.  The rules
```
  ENV.defaults[""stage""] = 4
  ...
  {  pattern: ( [ { tag:/VBD/ } | /have/ ] []{0,2} [ $hasTemporal ] ),
     action: VTag( $0[-1].temporal.value, ""resolveTo"", RESOLVE_TO_PAST )
  }
  {  pattern: ( [ $hasTemporal ] []{0,2} [ { tag:/VBD/ } | /have/ ] ),
     action: VTag( $0[0].temporal.value, ""resolveTo"", RESOLVE_TO_PAST )
  }
  {  pattern: ( (/would/ | /could/ | /should/ | /will/ | /going/ /to/ | /'/ /ll/ | /'ll/ )
                []{0,2} [ $hasTemporal ]
              ),
     action: VTag( $0[-1].temporal.value, ""resolveTo"", RESOLVE_TO_FUTURE )
  }
  {  pattern: ( [ $hasTemporal ] []{0,2}
                (/would/ | /could/ | /should/ | /will/ | /going/ /to/ | /'/ /ll/ | /'ll/ ) ),
     action: VTag( $0[0].temporal.value, ""resolveTo"", RESOLVE_TO_FUTURE )
  }
```
arrange for a value tag (VTag) to be added to the environment.  The tag's key is ""resolveTo"" and the value will depend on the matching pattern.  This ends up happening in ValueFunctions.java where I can observe the change as it happens.  The problem is that the environment influences other operations, the whole point of it, but that it cannot easily be reset.  In a test, a first document is annotated without a resolveTo tag and SUTime acts in one way.  The second document is annotated with a side effect of the resolveTo tag being added.  The first document gets processed again, but the side effect influences behavior and a different result gets produced.  I see no support anywhere for restoring the environment to its initial condition between documents short of doing something like throwing everything away and starting with a new object, which would be very expensive on a per document basis.

The environment is being changed for the sake of code like that in GenericTimeExpressionPatterns.java.determineRelFlags:
```java
  public int determineRelFlags(CoreMap annotation, TimeExpression te)
  {
    int flags = 0;
    boolean flagsSet = false;
    if (te.value.getTags() != null) {
      Value v = te.value.getTags().getTag(""resolveTo"");
      if (v != null && v.get() instanceof Number) {
        flags = ((Number) v.get()).intValue();
        flagsSet = true;
      }
    }
    if (!flagsSet) {
      if (te.getTemporal() instanceof SUTime.PartialTime) {
        flags = SUTime.RESOLVE_TO_CLOSEST;
      }
    }
    return flags;
  }
}
```
and in SUTime.PartialTime.resolve().

I hope something can be done about it.  For my use case I would rather have a consistently wrong answer than an answer that changes.  Both correct and stable is preferable, however."
875,https://github.com/stanfordnlp/CoreNLP/issues/1062,1062,[],closed,2020-07-09 07:08:33+00:00,,Compile java.lang.StackOverflowError ,"I use apache-ant 1.10.8 to compile CoreNLP. But I face the java.lang.StackOverflowError.
I have tried to set ANT_OPTS=-Xmx512M but still failed.
Thanks!
I did
`git clone https://github.com/stanfordnlp/CoreNLP.git`
`cd CoreNLP`
`ant compile`

result:
```
Buildfile: /home/yf/experiment/QuestionParsing/Deployment/CoreNLP/build.xml

classpath:
     [echo] core

compile:
     [echo] core
    [javac] Compiling 2065 source files to /home/yf/experiment/QuestionParsing/Deployment/CoreNLP/classes
    [javac] 
    [javac] 
    [javac] The system is out of resources.
    [javac] Consult the following stack trace for details.
    [javac] java.lang.StackOverflowError
    [javac] 	at com.sun.tools.javac.parser.JavaTokenizer.readToken(JavaTokenizer.java:566)
    [javac] 	at com.sun.tools.javac.parser.Scanner.ensureLookahead(Scanner.java:102)
    [javac] 	at com.sun.tools.javac.parser.Scanner.token(Scanner.java:95)
    [javac] 	at com.sun.tools.javac.parser.JavacParser.peekToken(JavacParser.java:310)
    [javac] 	at com.sun.tools.javac.parser.JavacParser.peekToken(JavacParser.java:306)
```
"
876,https://github.com/stanfordnlp/CoreNLP/issues/1063,1063,[],open,2020-07-09 07:58:19+00:00,,Merging 2 pipelines,"```java
private List<String> lemmatizeMultiWordExpression(String text) {

    // put the text in the document annotation
    Annotation doc = new Annotation(text);

    // run the CoreNLP pipeline on the document
    this.pipeline.annotate(doc);

    // get each part of the pipeline except jmwe
    // these are all the sentences in this document
    // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
    List<String> mweObserved = new LinkedList<>();
    List<String> oneObserved = new LinkedList<>();
    List<CoreMap> sentences = doc.get(CoreAnnotations.SentencesAnnotation.class);
    for(CoreMap sentence: sentences) {
        // traversing the words in the current sentence
        // a CoreLabel is a CoreMap with additional token-specific methods
        for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {



            // this is the text of the token
            String word = token.get(CoreAnnotations.TextAnnotation.class);

            // this is the POS tag of the token
            String pos = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);
        }
        // Iterate over all tokens in a sentence
        for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) { // Stanford NLP annotations
            // Retrieve and add the lemma for each word into the list
            System.out.println(""TOKEN : "" + token.originalText());
            oneObserved.add(token.get(CoreAnnotations.LemmaAnnotation.class));
        }
    }

    for(CoreMap sentence: sentences) {
        for (IMWE<IToken> mwe : sentence.get(JMWEAnnotation.class)) {
            // System.out.println(""IMWE<IToken>: ""+token+"", token.isInflected(): ""+token.isInflected()+"", token.getForm(): ""+token.getForm() + "" "" + token.getEntry());
            mweObserved.add(mwe.getForm().replaceAll(""_"", ""-"")); // ""-"" is used in place of ""_"" to create mwe like an unique word
        }
    }

    oneObserved.addAll(mweObserved);

    return mweObserved;
}
```


We are working on a bibliometrics project, and in this method, we tried to divide the number of pipelines we were running by two. By merging two different pipelines:
```java
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma, jmwe""); 
props.setProperty(""annotators"", ""tokenize, ssplit, pos, lemma"");
```
This works, but the fact is that the algorithm loops more than one time in our list of documents.
(jmwe is an annotator which can deal with multi words expression. It is an external add to stanford, but the problem isn‚Äôt comming from this. https://github.com/toliwa/CoreNLP-jMWE#demo ) For example, a document that has only one occurrence in the original list will appear four or five times in the result of our pipeline.
There is no reason why the code would do that but the problem still occurs.


"
877,https://github.com/stanfordnlp/CoreNLP/issues/1064,1064,[],closed,2020-07-10 07:14:14+00:00,,Question Scene Graph Parser,"I have tried Stanford CoreNLP openIE that decomposes a sentence into a graph with nodes representing entities and edges representing relations.
But when I use it on a question sentence, it skipped the wh-word, pro-noun.
e.g. What is the man doing? I got (man, do) 
But I expect to get (man, do, ? or what)
Does the CoreNLP support the function that I missed or Can you give some suggestions like what pre-handling do I need to do to reach my expectation?

Thanks!"
878,https://github.com/stanfordnlp/CoreNLP/issues/1065,1065,[],open,2020-07-10 07:24:06+00:00,,CoreNLP Server NER Annotator Visualization Error,"I am running into some strange behavior when using CoreNLP Server, named entities. I put the tests in 4 print screens below.
![Stanford CoreNLP NER errors](https://user-images.githubusercontent.com/62341087/87127889-00ef0e00-c28f-11ea-938f-f8498bd080f9.png)

Note that I tried with 3 different builds of JREs, all 8.
Thank you upfront for any help."
879,https://github.com/stanfordnlp/CoreNLP/issues/1066,1066,[],closed,2020-07-13 18:14:49+00:00,,Training a Shift-Reduce Parser,"I am trying to train a constituency parser for Portuguese (from the terminal).

> It is possible to train the Shift-Reduce Parser for languages other than English. An appropriate HeadFinder needs to be provided. This and other options are handled by specifying the -tlpp flag, which lets you choose the class for a TreebankLangParserParams.

I am not sure how to ""provide the appropriate HeadFinder"".

`Exception in thread ""main"" java.lang.IllegalArgumentException: No head rule defined for N' using class edu.stanford.nlp.trees.ModCollinsHeadFinder in (N' (N vinho) (A branco))`

`Exception in thread ""main"" java.lang.IllegalArgumentException: No head rule defined for NP using class edu.stanford.nlp.trees.international.spanish.SpanishHeadFinder in (NP (DEM Aquele) (N cliente))`

Obviously I need to define the rules, but how/where am I supposed to go about doing this? Must I create a `PortugueseTreeBankParserParams`, as well as `PortugueseHeadFinder`, `PortugueseLanguagePack`, and `PortugueseTreeReaderFactory`? Is it possible to use or slightly modify existing ones to get some baseline performance out of the box?"
880,https://github.com/stanfordnlp/CoreNLP/issues/1067,1067,[],open,2020-07-15 22:26:31+00:00,,Is whitespace tokenizer in 3.6.0 same as 4.0.0?,"In 4.0.0, there is a comment saying that it recognizes whitespace characters the same as Java.lang.Character.isWhiteSpace. 

>  * This tokenizer treats as Whitespace almost exactly the same characters deemed Whitespace by the
>  * Java function {@link java.lang.Character#isWhitespace(int) isWhitespace}. That is, a whitespace
> * is a Unicode SPACE_SEPARATOR, LINE_SEPARATOR or PARAGRAPH_SEPARATOR, or one of the control characters
> * U+0009-U+000D or U+001C-U+001F <i>except</i> the non-breaking space characters. The one addition is
>  * to also allow U+0085 as a line ending character, for compatibility with certain IBM systems.
> * For including ""spaces"" in tokens, it is recommended that you represent them as the non-break space
> * character U+00A0.


But this comment is not in 3.6.0. Are they exactly the same? Does whitespace tokenizer recognizes whitespace characters the same as Java.lang.Character.isWhiteSpace, too?"
881,https://github.com/stanfordnlp/CoreNLP/issues/1069,1069,[],closed,2020-07-21 06:22:49+00:00,,How to make the sentence segment of document  in Coref is match to my sentences set?,"I have document set that had split to sentences.
I want to get the context mention information for each sentence, so I run as follow:
```
java -Xmx5g -cp stanford-corenlp-4.0.0.jar:stanford-corenlp-4.0.0-models.jar:* edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,coref -coref.algorithm neural -outputFormat xml -file my_sentences.txt
```
the sentences in `my_sentences.txt` is split by `\n`

But the result not what I expected. It seem that the sentences have be merge to a document and split to sentence by bulit-in algorithm, so the number of sentences is not match to origin the number. This made I can't get the mention information for origin sentence. 

Is there a way to get mention information and keep origin sentences?"
882,https://github.com/stanfordnlp/CoreNLP/issues/1070,1070,[],closed,2020-07-21 19:29:10+00:00,,java.lang.ClassNotFoundException: edu.stanford.nlp.tagger.maxent.ExtractorNonAlphanumeric for Part of Speech Tagger,"Hi,

I am trying to parse an English text file (around 800k sentences large) with the POS tagger. The file `stanford-postagger.sh` is set up like so:

```
#!/bin/sh

java -mx1000m -cp stanford-postagger.jar edu.stanford.nlp.tagger.maxent.MaxentTagger -model $1 -textFile $2
```

And this is the command I use in the Linux/Ubuntu terminal:

```
./stanford-postagger.sh models/english-left3words-distsim.tagger train/train.dtk.en > train/train.localtag.en
```

Here is the full error:

```
Exception in thread ""main"" edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:937)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:825)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:799)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.<init>(MaxentTagger.java:322)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.<init>(MaxentTagger.java:305)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1564)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.main(MaxentTagger.java:1908)
Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.tagger.maxent.ExtractorNonAlphanumeric
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:720)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1925)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2099)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:2032)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1613)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2344)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2268)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2126)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.readExtractors(MaxentTagger.java:630)
	at edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:876)
	... 6 more
```
I checked the `CLASSPATH` and tried it both as it's written when you download the tagger and without the quotation marks (like above). I am not to familiar with Java so troubleshooting is proving very difficult for me. Any help would be appreciated."
883,https://github.com/stanfordnlp/CoreNLP/issues/1071,1071,[],open,2020-07-22 13:31:39+00:00,,StanfordNLP: Unable to identify Date with 7-class-ner,"I'm using stanfordNLP to get date entities from text. Here's the code that i tried:-

```
import java.io.IOException;
import java.util.List;
import edu.stanford.nlp.ie.AbstractSequenceClassifier;
import edu.stanford.nlp.ie.crf.CRFClassifier;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.ling.CoreLabel;

public class StanfordNLP_POC
{

    public static void main(String[] args) throws IOException
    {
        // TODO Auto-generated method stub
        String classifierPath = ""src//main//resources//classifiers//english.muc.7class.distsim.crf.ser.gz"";

        String inputString = ""Appointment Facility: ABC Medicine Clinic 05/07/2020 Progress Notes: Niel Armstrong, DO Current Medications Reason for Appointment"";

        AbstractSequenceClassifier classifier = CRFClassifier.getClassifierNoExceptions(classifierPath);

        List<List<CoreLabel>> out = classifier.classify(inputString);

        System.out.println(out.toString());

        for (List<CoreLabel> sentence : out)
        {
            for (CoreLabel word : sentence)
            {

                if (word.getString(CoreAnnotations.AnswerAnnotation.class).equals(""O""))
                    continue;
                System.out.println(word.word() + "" = "" + word.get(CoreAnnotations.AnswerAnnotation.class));
            }
        }

    }

}
```

I didn't get why it's not extracting Date even though it's very clearly identifiable in the text.

Also when trying with pipeline it extracts date but takes a bit longer to do so."
884,https://github.com/stanfordnlp/CoreNLP/issues/1072,1072,[],closed,2020-07-23 09:38:30+00:00,,sentiment analysis live demo does not work,"Stanford nlp sentiment analysis demo does not work: http://nlp.stanford.edu:8080/sentiment/rntnDemo.html
I could access it a week ago, but now it is not available. Please help, thanks!"
885,https://github.com/stanfordnlp/CoreNLP/issues/1073,1073,[],open,2020-07-23 14:29:37+00:00,,Missing depedency in case of conjugation,"**Description**
When we take a simple sentence like `fermented leaves and fruit`  CoreNLP misses the dependency between `fermented` and `fruit`. Detecting this is crucial for our application and therefore we wonder whether this is possible?

There has been a recent [publication](https://www.aclweb.org/anthology/2020.iwpt-1.24.pdf) where they show being able to capture such dependencies (Figure 1). However, since this competition was very recent (this month) we can not find the corresponding scripts. 

**Reproduce**
```
with CoreNLPClient( annotators=['tokenize','ssplit','pos','depparse']) as client:
    ann = client.annotate('fermented leaves and fruit')
    sentence = ann.sentence[0]
    print(sentence.enhancedPlusPlusDependencies)
    print(sentence.alternativeDependencies)
    print(sentence.basicDependencies)
    print(sentence.enhancedDependencies)
```"
886,https://github.com/stanfordnlp/CoreNLP/issues/1074,1074,[],open,2020-07-24 15:19:45+00:00,,Any way to get Linearised Dependencies as the output (Stanford Parser)?,"Hi,

I'm currently working on a NMT model that takes in linearised constituency trees from the parser and uses them. I was wondering if there was anything similar in the Stanford Parser for dependencies where one sentence (in its 'tree' form) is linearised to just one line.

Justin"
887,https://github.com/stanfordnlp/CoreNLP/issues/1075,1075,[],open,2020-07-25 19:24:35+00:00,,Discrepancy tokensregex webserver and CoreNLP in Python?,"Let's take the following setence: `organic wastes under variable temperature conditions` and pattern: `[{tag:/JJ/}]*[{tag:/NN.*/}]+`
When we pass this to http://corenlp.run/:
![image](https://user-images.githubusercontent.com/19516376/88464610-86440680-cebc-11ea-98dd-493fa6087b49.png)

Then when we do this in Python:
```
with CoreNLPClient(memory='16G', threads=1, annotators=['tokenize','ssplit','pos','lemma','ner','depparse']) as client:
    text = 'organic wastes under variable temperature conditions'
    print(client.tokensregex(text, '[{tag:/JJ/}]*[{tag:/NN.*/}]+'))
```
It will print: `{'sentences': [{'0': {'text': 'organic wastes', 'begin': 0, 'end': 2}, '1': {'text': 'variable temperature', 'begin': 3, 'end': 5}, '2': {'text': 'conditions', 'begin': 5, 'end': 6}, 'length': 3}]}` Note that the webserver finds ""variable temperature conditions"" whereas in Python we only find ""variable temperature"" and ""conditions"" as seperate matches. I need the same output as the webserver"
888,https://github.com/stanfordnlp/CoreNLP/issues/1076,1076,[],closed,2020-07-26 03:18:25+00:00,,Sentiment results are different between stanford nlp python package and the live demo,"I try sentiment analysis of tweet text by both stanford nlp python package and the live demo, but the results are different. The result of the python package is positive while the result of the live demo is negative.

- For python package, I download **stanford-corenlp-4.0.0** and install **py-corenlp**, basically follow the instruction in this answer: [Stanford nlp for python](https://stackoverflow.com/questions/32879532/stanford-nlp-for-python), the code is shown below:

```
import pycorenlp
from pycorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP(""http://localhost:9000"")
text=""noted former cocaine user carrie fisher says donald trump was absolutely on coke makes sense""
res = nlp.annotate(text,properties={'annotators': 'sentiment','outputFormat': 'json','timeout': 1000})
for s in res[""sentences""]:
    print(s[""sentimentValue""], s[""sentiment""])
```
and the result is:
```
3 Positive
```

- For the live demo:
![Capture](https://user-images.githubusercontent.com/68687826/88470640-95c74d80-cec4-11ea-900d-176fb1a7a410.PNG)"
889,https://github.com/stanfordnlp/CoreNLP/issues/1077,1077,[],closed,2020-07-31 00:29:04+00:00,,„Äêerror„ÄëThere is already a relation named obl:agent!,"First I start a local server via StanfordNLP. Then I send a post resquest by python packages named requests  to parse sentence. It throwed an error when I sent a sentence. 
„Äêsentence„Äë
Technical problems will include user manipulation of agents through and agent editor , activation of agents in a shared context , presentation of agents , and creation of shared context through specification , construction , task representations , and interaction histories .
„Äêcode„Äë
<pre>
import requests as rq
url = 'http://172.16.133.173:9000/?properties={""annotators"":""tokenize, ssplit, pos, lemma, ner,     depparse"",""outputFormat"":""json""}'
res = rq.post(url, data=sentence).text</pre>
„Äêerror„Äë
java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: There is already a relation named obl:agent!"
890,https://github.com/stanfordnlp/CoreNLP/issues/1078,1078,[],open,2020-08-01 14:09:53+00:00,,boundaryMultiTokenRegex: regex recognises a wrong group,"Hello, I'm trying to split a list of sentences without a proper punctuation into separate sentences. And looks, an expression written in `boundaryMultiTokenRegex` is not working as expected. The key aspect here, I cannot split all the text by new line in all the cases because whole text might be with multi line sentences. But list items are with a key indicator of the start.

```java
    @Test
    public void testTokenizeNLsInList() {
        String text = ""This is a list:\n"" +
                ""- String one\n"" +
                ""- String two;\n"" +
                ""- String three"";

        Properties props = PropertiesUtils.asProperties(
                ""annotators"", ""tokenize, ssplit"",
                ""tokenize.options"", ""tokenizeNLs"",
                ""ssplit.boundaryMultiTokenRegex"", ""(/\\n|\\*NL\\*/) /[^[\\p{Alnum}'\""`!?.,]]/ /\\p{Lu}\\p{L}+/""
        );

        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        Annotation document1 = new Annotation(text);
        pipeline.annotate(document1);
        List<CoreMap> sentences = document1.get(CoreAnnotations.SentencesAnnotation.class);
        assertEquals(4, sentences.size());

        // make sure that there are the correct # of tokens
        // (does NOT contain NL tokens)
        List<CoreLabel> tokens = document1.get(CoreAnnotations.TokensAnnotation.class);
        assertEquals(4, tokens.size());
    }
```

Expected behavior is to get 4 sentences:
```
This is a list:
- String one
- String two;
- String three
```

Real behavior:
```
This is a list:\n- String,
one\n- String
two;\n- String
three
```

Additional findings:
I checked what is happening inside https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/process/WordToSentenceProcessor.java#L282

As a result here, I'm getting 3 tokens result instead of the first one mentioned in the pattern.
```java
        List<? super IN> nodes = matcher.groupNodes();
        if (nodes != null && ! nodes.isEmpty()) {
          if (DEBUG) { log.info(""    found match at: "" + nodes); }
          isSentenceBoundary.put(nodes.get(nodes.size() - 1), true);
        }
```
Also, in the debugger, I see `matcher` with two groups found with all the tokens from the pattern, and with the correct first token only. But as the correct group is the second one, the final result is wrong.
![Screenshot 2020-08-01 at 17 04 03](https://user-images.githubusercontent.com/2664877/89103251-0ddbc900-d419-11ea-87b8-e754b6b8f7d3.png)


![Screenshot 2020-08-01 at 16 52 42](https://user-images.githubusercontent.com/2664877/89103156-61014c00-d418-11ea-9bc2-b1e5beaf75cd.png)

"
891,https://github.com/stanfordnlp/CoreNLP/issues/1079,1079,[],closed,2020-08-02 18:43:30+00:00,,How can I iterate sentences in type corenlp_pb2.document?,"Hi! 

I was using this to iterate the document. 
```
client = CoreNLPClient(default_annotators='ssplit,tokenize'.split(','))
words, gloss, after = [], [], []
for s in client.annotate(sentence):
    for t in s:
        words.append(t.word)
        gloss.append(t.originalText)
        after.append(t.after)
```
However, it throws the error: class Document is not iterable.
If I use client.annotate(sentence).sentences(), it throws an Attribute Error.
If I use client.annotate(sentence)[""sentence""], it throws TypeError: 'Document' object is not subscriptable
When I print client.annotate(sentence), it gives this object:
```
text: ""The episode with production code 9abx02 was originally aired on what date?""
sentence {
  token {
    word: ""The""
    pos: ""DT""
    value: ""The""
    before: """"
    after: "" ""
    originalText: ""The""
    ner: ""O""
    lemma: ""the""
    beginChar: 0
    endChar: 3
    tokenBeginIndex: 0
    tokenEndIndex: 1
    hasXmlContext: false
    isNewline: false
    coarseNER: ""O""
    fineGrainedNER: ""O""
    nerLabelProbs: ""O=0.999996629257023""
  }
  token {
    word: ""episode""
    pos: ""NN""
    value: ""episode""
    before: "" ""
    after: "" ""
    originalText: ""episode""
    ner: ""O""
    lemma: ""episode""
    beginChar: 4
    endChar: 11
    tokenBeginIndex: 1
    tokenEndIndex: 2
    hasXmlContext: false
    isNewline: false
    coarseNER: ""O""
    fineGrainedNER: ""O""
    nerLabelProbs: ""O=0.9999891575229793""
  }
  token {
    word: ""with""
    pos: ""IN""
    value: ""with""
    before: "" ""
    after: "" ""
    originalText: ""with""
    ner: ""O""
    lemma: ""with""
    beginChar: 12
    endChar: 16
    tokenBeginIndex: 2
    tokenEndIndex: 3
    hasXmlContext: false
    isNewline: false
    coarseNER: ""O""
    fineGrainedNER: ""O""
    nerLabelProbs: ""O=0.9999998854993416""
  }
  token {
    word: ""production""
    pos: ""NN""
    value: ""production""
    before: "" ""
    after: "" ""
    originalText: ""production""
    ner: ""O""
    lemma: ""production""
    beginChar: 17
    endChar: 27
    tokenBeginIndex: 3
    tokenEndIndex: 4
    hasXmlContext: false
    isNewline: false
    coarseNER: ""O""
    fineGrainedNER: ""O""
    nerLabelProbs: ""O=0.9999983993765815""
  }
  token {
    word: ""code""
    pos: ""NN""
    value: ""code""
    before: "" ""
    after: "" ""
    originalText: ""code""
    ner: ""O""
    lemma: ""code""
    beginChar: 28
    endChar: 32
    tokenBeginIndex: 4
    tokenEndIndex: 5
    hasXmlContext: false
    isNewline: false
    coarseNER: ""O""
    fineGrainedNER: ""O""
    nerLabelProbs: ""O=0.9999975917701541""
  }
  token {
    word: ""9abx02""
    pos: ""CD""
    value: ""9abx02""
    before: "" ""
    after: "" ""
    originalText: ""9abx02""
    ner: ""NUMBER""
    normalizedNER: ""902.0""
    lemma: ""9abx02""
    beginChar: 33
    endChar: 39
    tokenBeginIndex: 5
    tokenEndIndex: 6
    hasXmlContext: false
    isNewline: false
    coarseNER: ""NUMBER""
    fineGrainedNER: ""NUMBER""
    entityMentionIndex: 0
    nerLabelProbs: ""NUMBER=-1.0""
  }
  token {
    word: ""was""
    pos: ""VBD""
    value: ""was""
    before: "" ""
    after: "" ""
    originalText: ""was""
    ner: ""O""
    lemma: ""be""
    beginChar: 40
    endChar: 43
    tokenBeginIndex: 6
    tokenEndIndex: 7
    hasXmlContext: false
    isNewline: false
    coarseNER: ""O""
    fineGrainedNER: ""O""
    nerLabelProbs: ""O=0.9999994614077565""
  }
  token {
    word: ""originally""
    pos: ""RB""
    value: ""originally""
    before: "" ""
    after: "" ""
    originalText: ""originally""
    ner: ""O""
    lemma: ""originally""
    beginChar: 44
    endChar: 54
    tokenBeginIndex: 7
    tokenEndIndex: 8
    hasXmlContext: false
    isNewline: false
    coarseNER: ""O""
    fineGrainedNER: ""O""
    nerLabelProbs: ""O=0.9999978749965647""
  }
  token {
    word: ""aired""
    pos: ""VBN""
    value: ""aired""
    before: "" ""
    after: "" ""
    originalText: ""aired""
    ner: ""O""
    lemma: ""air""
    beginChar: 55
    endChar: 60
    tokenBeginIndex: 8
    tokenEndIndex: 9
    hasXmlContext: false
    isNewline: false
    coarseNER: ""O""
    fineGrainedNER: ""O""
    nerLabelProbs: ""O=0.9999966824217843""
  }
  token {
    word: ""on""
    pos: ""IN""
    value: ""on""
    before: "" ""
    after: "" ""
    originalText: ""on""
    ner: ""O""
    lemma: ""on""
    beginChar: 61
    endChar: 63
    tokenBeginIndex: 9
    tokenEndIndex: 10
    hasXmlContext: false
    isNewline: false
    coarseNER: ""O""
    fineGrainedNER: ""O""
    nerLabelProbs: ""O=0.9999996641963658""
  }
  token {
    word: ""what""
    pos: ""WDT""
    value: ""what""
    before: "" ""
    after: "" ""
    originalText: ""what""
    ner: ""O""
    lemma: ""what""
    beginChar: 64
    endChar: 68
    tokenBeginIndex: 10
    tokenEndIndex: 11
    hasXmlContext: false
    isNewline: false
    coarseNER: ""O""
    fineGrainedNER: ""O""
    nerLabelProbs: ""O=0.999999228014715""
  }
  token {
    word: ""date""
    pos: ""NN""
    value: ""date""
    before: "" ""
    after: """"
    originalText: ""date""
    ner: ""O""
    lemma: ""date""
    beginChar: 69
    endChar: 73
    tokenBeginIndex: 11
    tokenEndIndex: 12
    hasXmlContext: false
    isNewline: false
    coarseNER: ""O""
    fineGrainedNER: ""O""
    nerLabelProbs: ""O=0.9998982150261667""
  }
  token {
    word: ""?""
    pos: "".""
    value: ""?""
    before: """"
    after: """"
    originalText: ""?""
    ner: ""O""
    lemma: ""?""
    beginChar: 73
    endChar: 74
    tokenBeginIndex: 12
    tokenEndIndex: 13
    hasXmlContext: false
    isNewline: false
    coarseNER: ""O""
    fineGrainedNER: ""O""
    nerLabelProbs: ""O=0.9999888186476209""
  }
  tokenOffsetBegin: 0
  tokenOffsetEnd: 13
  sentenceIndex: 0
  characterOffsetBegin: 0
  characterOffsetEnd: 74
  basicDependencies {
    node {
      sentenceIndex: 0
      index: 1
    }
    node {
      sentenceIndex: 0
      index: 2
    }
    node {
      sentenceIndex: 0
      index: 3
    }
    node {
      sentenceIndex: 0
      index: 4
    }
    node {
      sentenceIndex: 0
      index: 5
    }
    node {
      sentenceIndex: 0
      index: 6
    }
    node {
      sentenceIndex: 0
      index: 7
    }
    node {
      sentenceIndex: 0
      index: 8
    }
    node {
      sentenceIndex: 0
      index: 9
    }
    node {
      sentenceIndex: 0
      index: 10
    }
    node {
      sentenceIndex: 0
      index: 11
    }
    node {
      sentenceIndex: 0
      index: 12
    }
    node {
      sentenceIndex: 0
      index: 13
    }
    edge {
      source: 2
      target: 1
      dep: ""det""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 2
      target: 5
      dep: ""nmod""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 5
      target: 3
      dep: ""case""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 5
      target: 4
      dep: ""compound""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 5
      target: 6
      dep: ""nummod""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 2
      dep: ""nsubj:pass""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 7
      dep: ""aux:pass""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 8
      dep: ""advmod""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 12
      dep: ""obl""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 13
      dep: ""punct""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 12
      target: 10
      dep: ""case""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 12
      target: 11
      dep: ""det""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    root: 9
  }
  collapsedDependencies {
    node {
      sentenceIndex: 0
      index: 1
    }
    node {
      sentenceIndex: 0
      index: 2
    }
    node {
      sentenceIndex: 0
      index: 3
    }
    node {
      sentenceIndex: 0
      index: 4
    }
    node {
      sentenceIndex: 0
      index: 5
    }
    node {
      sentenceIndex: 0
      index: 6
    }
    node {
      sentenceIndex: 0
      index: 7
    }
    node {
      sentenceIndex: 0
      index: 8
    }
    node {
      sentenceIndex: 0
      index: 9
    }
    node {
      sentenceIndex: 0
      index: 10
    }
    node {
      sentenceIndex: 0
      index: 11
    }
    node {
      sentenceIndex: 0
      index: 12
    }
    node {
      sentenceIndex: 0
      index: 13
    }
    edge {
      source: 2
      target: 1
      dep: ""det""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 2
      target: 5
      dep: ""nmod:with""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 5
      target: 3
      dep: ""case""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 5
      target: 4
      dep: ""compound""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 5
      target: 6
      dep: ""nummod""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 2
      dep: ""nsubj:pass""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 7
      dep: ""aux:pass""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 8
      dep: ""advmod""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 12
      dep: ""obl:on""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 13
      dep: ""punct""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 12
      target: 10
      dep: ""case""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 12
      target: 11
      dep: ""det""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    root: 9
  }
  collapsedCCProcessedDependencies {
    node {
      sentenceIndex: 0
      index: 1
    }
    node {
      sentenceIndex: 0
      index: 2
    }
    node {
      sentenceIndex: 0
      index: 3
    }
    node {
      sentenceIndex: 0
      index: 4
    }
    node {
      sentenceIndex: 0
      index: 5
    }
    node {
      sentenceIndex: 0
      index: 6
    }
    node {
      sentenceIndex: 0
      index: 7
    }
    node {
      sentenceIndex: 0
      index: 8
    }
    node {
      sentenceIndex: 0
      index: 9
    }
    node {
      sentenceIndex: 0
      index: 10
    }
    node {
      sentenceIndex: 0
      index: 11
    }
    node {
      sentenceIndex: 0
      index: 12
    }
    node {
      sentenceIndex: 0
      index: 13
    }
    edge {
      source: 2
      target: 1
      dep: ""det""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 2
      target: 5
      dep: ""nmod:with""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 5
      target: 3
      dep: ""case""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 5
      target: 4
      dep: ""compound""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 5
      target: 6
      dep: ""nummod""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 2
      dep: ""nsubj:pass""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 7
      dep: ""aux:pass""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 8
      dep: ""advmod""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 12
      dep: ""obl:on""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 13
      dep: ""punct""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 12
      target: 10
      dep: ""case""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 12
      target: 11
      dep: ""det""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    root: 9
  }
  enhancedDependencies {
    node {
      sentenceIndex: 0
      index: 1
    }
    node {
      sentenceIndex: 0
      index: 2
    }
    node {
      sentenceIndex: 0
      index: 3
    }
    node {
      sentenceIndex: 0
      index: 4
    }
    node {
      sentenceIndex: 0
      index: 5
    }
    node {
      sentenceIndex: 0
      index: 6
    }
    node {
      sentenceIndex: 0
      index: 7
    }
    node {
      sentenceIndex: 0
      index: 8
    }
    node {
      sentenceIndex: 0
      index: 9
    }
    node {
      sentenceIndex: 0
      index: 10
    }
    node {
      sentenceIndex: 0
      index: 11
    }
    node {
      sentenceIndex: 0
      index: 12
    }
    node {
      sentenceIndex: 0
      index: 13
    }
    edge {
      source: 2
      target: 1
      dep: ""det""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 2
      target: 5
      dep: ""nmod:with""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 5
      target: 3
      dep: ""case""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 5
      target: 4
      dep: ""compound""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 5
      target: 6
      dep: ""nummod""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 2
      dep: ""nsubj:pass""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 7
      dep: ""aux:pass""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 8
      dep: ""advmod""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 12
      dep: ""obl:on""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 13
      dep: ""punct""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 12
      target: 10
      dep: ""case""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 12
      target: 11
      dep: ""det""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    root: 9
  }
  enhancedPlusPlusDependencies {
    node {
      sentenceIndex: 0
      index: 1
    }
    node {
      sentenceIndex: 0
      index: 2
    }
    node {
      sentenceIndex: 0
      index: 3
    }
    node {
      sentenceIndex: 0
      index: 4
    }
    node {
      sentenceIndex: 0
      index: 5
    }
    node {
      sentenceIndex: 0
      index: 6
    }
    node {
      sentenceIndex: 0
      index: 7
    }
    node {
      sentenceIndex: 0
      index: 8
    }
    node {
      sentenceIndex: 0
      index: 9
    }
    node {
      sentenceIndex: 0
      index: 10
    }
    node {
      sentenceIndex: 0
      index: 11
    }
    node {
      sentenceIndex: 0
      index: 12
    }
    node {
      sentenceIndex: 0
      index: 13
    }
    edge {
      source: 2
      target: 1
      dep: ""det""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 2
      target: 5
      dep: ""nmod:with""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 5
      target: 3
      dep: ""case""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 5
      target: 4
      dep: ""compound""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 5
      target: 6
      dep: ""nummod""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 2
      dep: ""nsubj:pass""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 7
      dep: ""aux:pass""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 8
      dep: ""advmod""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 12
      dep: ""obl:on""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 9
      target: 13
      dep: ""punct""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 12
      target: 10
      dep: ""case""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    edge {
      source: 12
      target: 11
      dep: ""det""
      isExtra: false
      sourceCopy: 0
      targetCopy: 0
      language: UniversalEnglish
    }
    root: 9
  }
  hasRelationAnnotations: false
  hasNumerizedTokensAnnotation: false
  mentions {
    sentenceIndex: 0
    tokenStartInSentenceInclusive: 5
    tokenEndInSentenceExclusive: 6
    ner: ""NUMBER""
    normalizedNER: ""902.0""
    entityType: ""NUMBER""
    entityMentionIndex: 0
    canonicalEntityMentionIndex: 0
    entityMentionText: ""9abx02""
  }
  hasEntityMentionsAnnotation: true
}
mentions {
  sentenceIndex: 0
  tokenStartInSentenceInclusive: 5
  tokenEndInSentenceExclusive: 6
  ner: ""NUMBER""
  normalizedNER: ""902.0""
  entityType: ""NUMBER""
  entityMentionIndex: 0
  canonicalEntityMentionIndex: 0
  entityMentionText: ""9abx02""
}
xmlDoc: false
hasEntityMentionsAnnotation: true
hasCorefMentionAnnotation: false
hasCorefAnnotation: false
```

Can anyone tell me how to iterate the document type? Thanks in advance!"
892,https://github.com/stanfordnlp/CoreNLP/issues/1080,1080,[],closed,2020-08-11 10:45:34+00:00,,document.sentence is a list of empty strings (document type is CoreNLP_pb2.Document),"I am using the **Python CoreNLP interface**.
I annotated some text using the **CoreNLPClient** which gives me a document of type **CoreNLP_pb2.Document**.

Here's the code for annotating text:
`text = ""Albert Einstein was a German-born theoretical physicist. He developed the theory of relativity.""`
`document = client.annotate(text)`
`print(type(document))`

However, if I try to print the parse tree of the first sentence, the ouput is blank.

`first_sentence = document.sentence[0]`
`parse_tree = first_sentence.parseTree`
`print(parse_tree)`

So I checked for the text in **first_sentence**,
`print(first_sentence.text == '')` is `True`
which means that it's an empty string although `document.text` gives me the original string.

Need help with this! Why does the list contain empty strings? How do I make this right?"
893,https://github.com/stanfordnlp/CoreNLP/issues/1081,1081,[],closed,2020-08-12 11:00:51+00:00,,Updation of TIMEX3 standard url in SUTime page,"URL for **TIMEX3 standard** mentioned in [SUTime webpage](https://nlp.stanford.edu/software/sutime.html#Usage) is not available.

It has been moved from 
http://www.timeml.org/site/publications/timeMLdocs/timeml_1.2.1.html#timex3 to
http://www.timeml.org/publications/timeMLdocs/timeml_1.2.1.html#timex3

Similarly, URL for **TimeML Annotation Guidelines Version 1.2.1** needs to be changed from http://www.timeml.org/site/publications/timeMLdocs/annguide_1.2.1.pdf  => http://www.timeml.org/publications/timeMLdocs/annguide_1.2.1.pdf

"
894,https://github.com/stanfordnlp/CoreNLP/issues/1082,1082,[],closed,2020-09-01 18:50:14+00:00,,Parsing fails on AssertionError when using OpenIE (v3.9.2),"Happens with the following sentence, **under version 3.9.2, only when adding openIE annotator:**

> It was a long and stern face, but with eyes that twinkled in a kindly way.

stack trace:

`java.lang.AssertionError
	at edu.stanford.nlp.naturalli.Util.cleanTree(Util.java:324)
	at edu.stanford.nlp.naturalli.OpenIE.annotateSentence(OpenIE.java:463)
	at edu.stanford.nlp.naturalli.OpenIE.lambda$annotate$2(OpenIE.java:547)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1540)
	at edu.stanford.nlp.naturalli.OpenIE.annotate(OpenIE.java:547)
	at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:637)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:629)`

to replicate:

```
        Properties props = new Properties();
        props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,depparse,natlog,openie"");
        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

        String text = ""It was a long and stern face, but with eyes that twinkled in a kindly way."";

        CoreDocument document = new CoreDocument(text);
        pipeline.annotate(document);
```

works fine if openie is disabled, with other sentences, or when using [https://corenlp.run/](url) so looks like it's fixed in later versions but I did not verify it locally as I can't upgrade at the moment anyway.

advice much appreciated"
895,https://github.com/stanfordnlp/CoreNLP/issues/1083,1083,[],closed,2020-09-11 09:02:49+00:00,,Upload 4.1.0 to Maven Central,"https://mvnrepository.com/artifact/edu.stanford.nlp/stanford-corenlp

Maven currently only has 4.0.0"
896,https://github.com/stanfordnlp/CoreNLP/issues/1085,1085,[],closed,2020-09-21 12:00:59+00:00,,Stanford Parser failure when parsing this string: b.com/e(s),"Stanford Parser crashes when parsing this string: b.com/e(s)

You can easily reconstruct the problem by pasting it on the web console at: http://localhost:9001/

The reason for the failure is because the parser result is:

(ROOT
  (PRN
    (NP (NNS b.com/e(s))
    (-RRB- -RRB-)))

As you can see, there are 6 opening parentheses and only 5 closing parentheses.

I would appreciate a quick response on how to fix this.



"
897,https://github.com/stanfordnlp/CoreNLP/issues/1086,1086,[],open,2020-09-23 07:45:33+00:00,,Using newly trained model of clarkkev/deep-coref in Stanford neural coref system,"Hi,
I am working with the neural system of Stanford coreNLP and want to train a new extended model. To do so, one has to use Kevin Clark's python software deep-coref as instructed from the coreNLP online docs, but this one is saving model weights in hdf5 format. It is obvious, that this is not usable by the coreNLP software (which uses the serialized java object of e.s.n.c.n.NeuralCorefModel in the file english-model-conll.ser). So my guess is that the outcome of deep-coref is not usable by Stanford neural coref directly or am I missing something? 
Or in other words, how was english-model-conll.ser trained?

Dirk"
898,https://github.com/stanfordnlp/CoreNLP/issues/1087,1087,[],closed,2020-09-23 09:31:54+00:00,,NameFinderME - Unexpected result using unchanged training data,"Hello,

I based on [NameFinderMETest.java](https://github.com/apache/opennlp/blob/master/opennlp-tools/src/test/java/opennlp/tools/namefind/NameFinderMETest.java) / function `testNameFinder()`, I have written a simple test code and changed the [test sentence](https://github.com/apache/opennlp/blob/master/opennlp-tools/src/test/java/opennlp/tools/namefind/NameFinderMETest.java#L79) 
from **(1)**:
`String[] sentence = {""Alisa"",
        ""appreciated"",
        ""the"",
        ""hint"",
        ""and"",
        ""enjoyed"",
        ""a"",
        ""delicious"",
        ""traditional"",
        ""meal.""};`

to **(2)**:
`String[] sentence = {""Alisa"",
    		""and"",
    		""Mike"",
        ""appreciated"",
        ""the"",
        ""hint"",
        ""and"",
        ""enjoyed"",
        ""a"",
        ""delicious"",
        ""traditional"",
        ""meal.""};`

(Just added ""and Mike"") and expected to get 2 results (two names *Alisa* and *Mike*) because both names are annotated in the training data. The result is still 1 (Mike) for **(2)**. I used the training data file [AnnotatedSentences.txt](https://github.com/apache/opennlp/blob/master/opennlp-tools/src/test/resources/opennlp/tools/namefind/AnnotatedSentences.txt) (unchanged).

Can anyone tell me what's wrong? Thanks.

Test code:
--------------------------------

	   String trainingDatafilePath = ""opennlp/tools/namefind/AnnotatedSentences.txt"";

	    String encoding = ""ISO-8859-1"";
	    ObjectStream<NameSample> sampleStream = new NameSampleDataStream(new PlainTextByLineStream(new MarkableFileInputStreamFactory(new File(trainingDatafilePath+""AnnotatedSentences.txt"")), encoding));
	    
	    TrainingParameters params = new TrainingParameters();
	    params.put(TrainingParameters.ITERATIONS_PARAM, 70);
	    params.put(TrainingParameters.CUTOFF_PARAM, 1);

	    TokenNameFinderModel nameFinderModel = NameFinderME.train(""eng"", null, sampleStream,
	        params, TokenNameFinderFactory.create(null, null, Collections.emptyMap(), new BioCodec()));

	    TokenNameFinder nameFinder = new NameFinderME(nameFinderModel);

	    // now test if it can detect the sample sentences
	    String[] sentence = {""Alisa"",
	    	""and"",
	    	""Mike"",
	        ""appreciated"",
	        ""the"",
	        ""hint"",
	        ""and"",
	        ""enjoyed"",
	        ""a"",
	        ""delicious"",
	        ""traditional"",
	        ""meal.""};

	    Span[] names = nameFinder.find(sentence);
	    if (names != null && names.length != 0) {
	    	System.out.println("" > Found [""+names.length+""] results"");
	    	for(Span name : names){
		    	String personName="""";
		    	for(int i=name.getStart(); i<name.getEnd(); i++){
		    		personName+=sentence[i]+"" "";
		    	}
		    	System.out.println("" > Result ""+1+"": Type: [""+name.getType()+""] : PersonName: [""+personName+""]\t [probability=""+name.getProb()+""]"");
		    }
	    } else {
	    	System.out.println("" > No results found"");
	    }



**Result for (1):**
--------------------------------
Indexing events with TwoPass using cutoff of 1
	Computing event counts...  done. 1392 events
	Indexing...  done.
Collecting events... Done indexing in 0.22 s.
Incorporating indexed data for training...  
done.
	Number of Event Tokens: 1392
	    Number of Outcomes: 3
	  Number of Predicates: 9164
Computing model parameters...
Performing 70 iterations.
  1:  . (1355/1392) 0.9734195402298851
  2:  . (1383/1392) 0.9935344827586207
  3:  . (1390/1392) 0.9985632183908046
  4:  . (1390/1392) 0.9985632183908046
  5:  . (1391/1392) 0.9992816091954023
  6:  . (1392/1392) 1.0
  7:  . (1392/1392) 1.0
  8:  . (1392/1392) 1.0
  9:  . (1392/1392) 1.0
Stopping: change in training set accuracy less than 1.0E-5
Stats: (1392/1392) 1.0
...done.
 
**Found [1] results
Result 1: Type: [default] : PersonName: [Alisa ]	 [probability=0.5483001511243855]**




**Result for (2):**
--------------------------------
Indexing events with TwoPass using cutoff of 1
	Computing event counts...  done. 1392 events
	Indexing...  done.
Collecting events... Done indexing in 0.22 s.
Incorporating indexed data for training...  
done.
	Number of Event Tokens: 1392
	    Number of Outcomes: 3
	  Number of Predicates: 9164
Computing model parameters...
Performing 70 iterations.
  1:  . (1355/1392) 0.9734195402298851
  2:  . (1383/1392) 0.9935344827586207
  3:  . (1390/1392) 0.9985632183908046
  4:  . (1390/1392) 0.9985632183908046
  5:  . (1391/1392) 0.9992816091954023
  6:  . (1392/1392) 1.0
  7:  . (1392/1392) 1.0
  8:  . (1392/1392) 1.0
  9:  . (1392/1392) 1.0
Stopping: change in training set accuracy less than 1.0E-5
Stats: (1392/1392) 1.0
...done.

**Found [1] results
Result 1: Type: [default] : PersonName: [Mike ]	 [probability=0.460685209028902]**
"
899,https://github.com/stanfordnlp/CoreNLP/issues/1088,1088,[],closed,2020-09-23 11:20:11+00:00,,Upgrade from 3.9.1 to 4.1.0 - Can't find the english_UD.gz in the models of the 4.1.0,"Upgrade from 3.9.1 to 4.1.0 - Can't find the `english_UD.gz` in the models of the 4.1.0

(So I cant find the DEP parser model there, only in the 3.9.1)

Am I missing something?

2. In addition, in 3.9.1 there is the non-caseless pos.model of `nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger`
while in 4.1.0 I only find the caseless model of: `models/pos-tagger/english-caseless-left3words-distsim.tagger`
Was the non-caseless (==caseful) model deprecated?

"
900,https://github.com/stanfordnlp/CoreNLP/issues/1089,1089,[],open,2020-09-25 05:39:18+00:00,,Sutime handling custom formats,"I am using Corenlp for extracting dates from the text.
I want to extract the format : Nov-02-1990.
Corenlp is able to extract Nov 01 1990 but fails with hyphens

In english.sutime.txt I added the following rules
```
 MONTH_MAP = {
      ""jan"": 1,
      ""feb"": 2,
      ""mar"": 3,
      ""apr"": 4,
      ""may"": 5,
      ""jun"": 6,
      ""jul"": 7,
      ""aug"": 8,
      ""sep"": 9,
      ""oct"": 10,
      ""nov"": 11,
      ""dec"": 12
    }
   $MONTH_NAME = CreateRegex(Keys(MONTH_MAP))

{
     ruleType: ""text"",
     pattern: ( (/($MONTH_NAME)[-]([0-9]{2})[-]([0-9]{4})/) ),
     result: IsoDate($3, MONTH_MAP[Lowercase($1)] , $2)
    
   }
```
I am getting the  following error:
```
edu.stanford.nlp.util.ReflectionLoading$ReflectionLoadingException: Error creating edu.stanford.nlp.time.TimeExpressionExtractorImpl
```
What should i do?"
901,https://github.com/stanfordnlp/CoreNLP/issues/1090,1090,[],open,2020-09-30 12:01:03+00:00,,CoreNLP's quote attribution is barely usable/has very low accuracy,"The quote extraction has a pretty low accuracy, either I did something wrong or this should be stated as *experimental* in the docs. CoreNLP struggles with sentences with multiples speakers, with non-typical expression words, e.g. *she sighed...* instead of *she said...* and is frequently wrong with the canonical speaker.

The original text with some really hard parts is below. CoreNLP unfortunately even fails on the easy/unambiguous parts (below the og text is the result with errors being marked!):

""Christmas won't be Christmas without any presents,"" grumbled Jo, lying on the rug.

""It's so dreadful to be poor!"" sighed Meg, looking down at her old dress.

""I don't think it's fair for some girls to have plenty of pretty things, and other girls nothing at all,"" added little Amy, with an injured sniff.

""We've got Father and Mother, and each other,"" said Beth contentedly from her corner.

The four young faces on which the firelight shone brightened at the cheerful words, but darkened again as Jo said sadly, ""We haven't got Father, and shall not have him for a long time."" She didn't say ""perhaps never,"" but each silently added it, thinking of Father far away, where the fighting was.

Nobody spoke for a minute; then Meg said in an altered tone, ""You know the reason Mother proposed not having any presents this Christmas was because it is going to be a hard winter for everyone; and she thinks we ought not to spend money for pleasure, when our men are suffering so in the army. We can't do much, but we can make our little sacrifices, and ought to do it gladly. But I am afraid I don't,"" and Meg shook her head, as she thought regretfully of all the pretty things she wanted.

""But I don't think the little we should spend would do any good. We've each got a dollar, and the army wouldn't be much helped by our giving that. I agree not to expect anything from Mother or you, but I do want to buy Undine and Sintran for myself. I've wanted it so long,"" said Jo, who was a bookworm.

""I planned to spend mine in new music,"" said Beth, with a little sigh, which no one heard but the hearth brush and kettle-holder.

""I shall get a nice box of Faber's drawing pencils; I really need them,"" said Amy decidedly.

""Mother didn't say anything about our money, and she won't wish us to give up everything. Let's each buy what we want, and have a little fun; I'm sure we work hard enough to earn it,"" cried Jo, examining the heels of her shoes in a gentlemanly manner.

""I know I do‚Äîteaching those tiresome children nearly all day, when I'm longing to enjoy myself at home,"" began Meg, in the complaining tone again.

""You don't have half such a hard time as I do,"" said Jo. ""How would you like to be shut up for hours with a nervous, fussy old lady, who keeps you trotting, is never satisfied, and worries you till you're ready to fly out the window or cry?""

""It's naughty to fret, but I do think washing dishes and keeping things tidy is the worst work in the world. It makes me cross, and my hands get so stiff, I can't practice well at all."" And Beth looked at her rough hands with a sigh that any one could hear that time.

""I don't believe any of you suffer as I do,"" cried Amy, ""for you don't have to go to school with impertinent girls, who plague you if you don't know your lessons, and laugh at your dresses, and label your father if he isn't rich, and insult you when your nose isn't nice.""

""If you mean libel, I'd say so, and not talk about labels, as if Papa was a pickle bottle,"" advised Jo, laughing.

""I know what I mean, and you needn't be statirical about it. It's proper to use good words, and improve your vocabilary,"" returned Amy, with dignity.

""Don't peck at one another, children. Don't you wish we had the money Papa lost when we were little, Jo? Dear me! How happy and good we'd be, if we had no worries!"" said Meg, who could remember better times.

""You said the other day you thought we were a deal happier than the King children, for they were fighting and fretting all the time, in spite of their money.""

""So I did, Beth. Well, I think we are. For though we do have to work, we make fun of ourselves, and are a pretty jolly set, as Jo would say.""

""Jo does use such slang words!"" observed Amy, with a reproving look at the long figure stretched on the rug.

Jo immediately sat up, put her hands in her pockets, and began to whistle.

""Don't, Jo. It's so boyish!""

""That's why I do it.""

""I detest rude, unladylike girls!""

```
  ""quotes"": [
    {
      ""id"": 0,
      ""text"": ""\""Christmas won't be Christmas without any presents,\"""",
      ""beginIndex"": 0,
      ""endIndex"": 52,
      ""beginToken"": 0,
      ""endToken"": 10,
      ""beginSentence"": 0,
      ""endSentence"": 0,
      ""speaker"": ""Jo"",
      ""canonicalSpeaker"": ""Jo""
    },
    {
      ""id"": 1,
      ""text"": ""\""It's so dreadful to be poor\"""",
      ""beginIndex"": 84,
      ""endIndex"": 113,
      ""beginToken"": 19,
      ""endToken"": 27,
      ""beginSentence"": 1,
      ""endSentence"": 1,
      ""speaker"": ""Meg"",
      // W R O N G:
      ""canonicalSpeaker"": ""Faber""
    },
    {
      ""id"": 2,
      ""text"": ""\""I don't think it's fair for some girls to have plenty of pretty things, and other girls nothing at all,\"""",
      ""beginIndex"": 157,
      ""endIndex"": 262,
      ""beginToken"": 38,
      ""endToken"": 63,
      ""beginSentence"": 2,
      ""endSentence"": 2,
      // W R O N G:
      ""speaker"": ""Meg"",
      // W R O N G:
      ""canonicalSpeaker"": ""Faber""
    },
    {
      ""id"": 3,
      ""text"": ""\""We've got Father and Mother, and each other,\"""",
      ""beginIndex"": 304,
      ""endIndex"": 350,
      ""beginToken"": 73,
      ""endToken"": 85,
      ""beginSentence"": 3,
      ""endSentence"": 3,
      ""speaker"": ""Beth"",
      ""canonicalSpeaker"": ""Beth""
    },
    {
      ""id"": 4,
      ""text"": ""\""We haven't got Father, and shall not have him for a long time.\"""",
      ""beginIndex"": 511,
      ""endIndex"": 575,
      ""beginToken"": 116,
      ""endToken"": 133,
      ""beginSentence"": 4,
      ""endSentence"": 4,
      // W R O N G:
      ""speaker"": ""She"",
      // W R O N G:
      ""canonicalSpeaker"": ""her""
    },
    {
      ""id"": 5,
      ""text"": ""\""perhaps never,\"""",
      ""beginIndex"": 591,
      ""endIndex"": 607,
      ""beginToken"": 138,
      ""endToken"": 142,
      ""beginSentence"": 5,
      ""endSentence"": 5,
      ""speaker"": ""Meg"",
      // W R O N G:
      ""canonicalSpeaker"": ""her""
    },
    {
      ""id"": 6,
      ""text"": ""\""You know the reason Mother proposed not having any presents this Christmas was because it is going to be a hard winter for everyone; and she thinks we ought not to spend money for pleasure, when our men are suffering so in the army. We can't do much, but we can make our little sacrifices, and ought to do it gladly. But I am afraid I don't,\"""",
      ""beginIndex"": 750,
      ""endIndex"": 1093,
      ""beginToken"": 174,
      ""endToken"": 251,
      ""beginSentence"": 6,
      ""endSentence"": 8,
      ""speaker"": ""Meg"",
      // W R O N G:
      ""canonicalSpeaker"": ""Faber""
    },
    {
      ""id"": 7,
      ""text"": ""\""But I don't think the little we should spend would do any good. We've each got a dollar, and the army wouldn't be much helped by our giving that. I agree not to expect anything from Mother or you, but I do want to buy Undine and Sintran for myself. I've wanted it so long,\"""",
      ""beginIndex"": 1182,
      ""endIndex"": 1456,
      ""beginToken"": 270,
      ""endToken"": 336,
      ""beginSentence"": 9,
      ""endSentence"": 12,
      // W R O N G:
      ""speaker"": ""Jo"",
      ""canonicalSpeaker"": ""Jo""
    },
    {
      ""id"": 8,
      ""text"": ""\""I planned to spend mine in new music,\"""",
      ""beginIndex"": 1486,
      ""endIndex"": 1525,
      ""beginToken"": 345,
      ""endToken"": 355,
      ""beginSentence"": 13,
      ""endSentence"": 13,
      ""speaker"": ""Beth"",
      ""canonicalSpeaker"": ""Beth""
    },
    {
      ""id"": 9,
      ""text"": ""\""I shall get a nice box of Faber's drawing pencils; I really need them,\"""",
      ""beginIndex"": 1616,
      ""endIndex"": 1688,
      ""beginToken"": 375,
      ""endToken"": 393,
      ""beginSentence"": 14,
      ""endSentence"": 14,
      ""speaker"": ""Amy"",
      ""canonicalSpeaker"": ""Amy""
    },
    {
      ""id"": 10,
      ""text"": ""\""Mother didn't say anything about our money, and she won't wish us to give up everything. Let's each buy what we want, and have a little fun; I'm sure we work hard enough to earn it,\"""",
      ""beginIndex"": 1709,
      ""endIndex"": 1892,
      ""beginToken"": 398,
      ""endToken"": 444,
      ""beginSentence"": 15,
      ""endSentence"": 16,
      ""speaker"": ""Jo"",
      // W R O N G:
      ""canonicalSpeaker"": ""her""
    },
    {
      ""id"": 11,
      ""text"": ""\""I know I do‚Äîteaching those tiresome children nearly all day, when I'm longing to enjoy myself at home,\"""",
      ""beginIndex"": 1961,
      ""endIndex"": 2065,
      ""beginToken"": 459,
      ""endToken"": 483,
      ""beginSentence"": 17,
      ""endSentence"": 17,
      ""speaker"": ""Meg"",
      ""canonicalSpeaker"": ""Meg""
    },
    {
      ""id"": 12,
      ""text"": ""\""You don't have half such a hard time as I do,\"""",
      ""beginIndex"": 2108,
      ""endIndex"": 2155,
      ""beginToken"": 493,
      ""endToken"": 507,
      ""beginSentence"": 18,
      ""endSentence"": 18,
      ""speaker"": ""Jo"",
      // W R O N G:
      ""canonicalSpeaker"": ""her""
    },
    {
      ""id"": 13,
      ""text"": ""\""How would you like to be shut up for hours with a nervous, fussy old lady, who keeps you trotting, is never satisfied, and worries you till you're ready to fly out the window or cry?\"""",
      ""beginIndex"": 2165,
      ""endIndex"": 2349,
      ""beginToken"": 511,
      ""endToken"": 554,
      ""beginSentence"": 19,
      ""endSentence"": 19,
      ""speaker"": ""Jo"",
      // W R O N G:
      ""canonicalSpeaker"": ""her""
    },
    {
      ""id"": 14,
      ""text"": ""\""It's naughty to fret, but I do think washing dishes and keeping things tidy is the worst work in the world. It makes me cross, and my hands get so stiff, I can't practice well at all.\"""",
      ""beginIndex"": 2350,
      ""endIndex"": 2535,
      ""beginToken"": 555,
      ""endToken"": 600,
      ""beginSentence"": 20,
      ""endSentence"": 21,
      ""speaker"": ""Beth"",
      // W R O N G:
      ""canonicalSpeaker"": ""Faber""
    },
    {
      ""id"": 15,
      ""text"": ""\""I don't believe any of you suffer as I do,\"""",
      ""beginIndex"": 2618,
      ""endIndex"": 2662,
      ""beginToken"": 619,
      ""endToken"": 632,
      ""beginSentence"": 23,
      ""endSentence"": 23,
      ""speaker"": ""Amy"",
      // W R O N G:
      ""canonicalSpeaker"": ""Faber""
    },
    {
      ""id"": 16,
      ""text"": ""\""for you don't have to go to school with impertinent girls, who plague you if you don't know your lessons, and laugh at your dresses, and label your father if he isn't rich, and insult you when your nose isn't nice.\"""",
      ""beginIndex"": 2674,
      ""endIndex"": 2890,
      ""beginToken"": 636,
      ""endToken"": 687,
      ""beginSentence"": 23,
      ""endSentence"": 23,
      ""speaker"": ""Amy"",
      // W R O N G:
      ""canonicalSpeaker"": ""Faber""
    },
   {
      ""id"": 17,
      ""text"": ""\""If you mean libel, I'd say so, and not talk about labels, as if Papa was a pickle bottle,\"""",
      ""beginIndex"": 2891,
      ""endIndex"": 2982,
      ""beginToken"": 688,
      ""endToken"": 713,
      ""beginSentence"": 24,
      ""endSentence"": 24,
      ""speaker"": ""Jo"",
      // W R O N G:
      ""canonicalSpeaker"": ""her""
    },
    {
      ""id"": 18,
      ""text"": ""\""I know what I mean, and you needn't be statirical about it. It's proper to use good words, and improve your vocabilary,\"""",
      ""beginIndex"": 3005,
      ""endIndex"": 3126,
      ""beginToken"": 719,
      ""endToken"": 748,
      ""beginSentence"": 25,
      ""endSentence"": 26,
      ""speaker"": ""Amy"",
      ""canonicalSpeaker"": ""Amy""
    },
    {
      ""id"": 19,
      ""text"": ""\""Don't peck at one another, children. Don't you wish we had the money Papa lost when we were little, Jo? Dear me How happy and good we'd be, if we had no worries\"""",
      ""beginIndex"": 3155,
      ""endIndex"": 3317,
      ""beginToken"": 755,
      ""endToken"": 797,
      ""beginSentence"": 27,
      ""endSentence"": 29,
      ""speaker"": ""Meg"",
      ""canonicalSpeaker"": ""Meg""
    },
    {
      ""id"": 20,
      ""text"": ""\""You said the other day you thought we were a deal happier than the King children, for they were fighting and fretting all the time, in spite of their money.\"""",
      ""beginIndex"": 3361,
      ""endIndex"": 3519,
      ""beginToken"": 807,
      ""endToken"": 841,
      ""beginSentence"": 30,
      ""endSentence"": 30,
      ""speaker"": ""Meg"",
      ""canonicalSpeaker"": ""Meg""
    },
    {
      ""id"": 21,
      ""text"": ""\""So I did, Beth. Well, I think we are. For though we do have to work, we make fun of ourselves, and are a pretty jolly set, as Jo would say.\"""",
      ""beginIndex"": 3520,
      ""endIndex"": 3661,
      ""beginToken"": 842,
      ""endToken"": 882,
      ""beginSentence"": 31,
      ""endSentence"": 33,
      // W R O N G:
      ""speaker"": ""Papa"",
      ""canonicalSpeaker"": ""Unknown""
    },
    {
      ""id"": 22,
      ""text"": ""\""Jo does use such slang words\"""",
      ""beginIndex"": 3662,
      ""endIndex"": 3692,
      ""beginToken"": 883,
      ""endToken"": 890,
      ""beginSentence"": 34,
      ""endSentence"": 34,
      ""speaker"": ""Amy"",
      ""canonicalSpeaker"": ""Amy""
    },
    {
      ""id"": 23,
      ""text"": ""\""Don't, Jo. It's so boyish\"""",
      ""beginIndex"": 3845,
      ""endIndex"": 3872,
      ""beginToken"": 924,
      ""endToken"": 934,
      ""beginSentence"": 36,
      ""endSentence"": 37,
      // W R O N G:
      ""speaker"": ""Jo"",
      ""canonicalSpeaker"": ""her""
    },
    {
      ""id"": 24,
      ""text"": ""\""That's why I do it.\"""",
      ""beginIndex"": 3873,
      ""endIndex"": 3894,
      ""beginToken"": 935,
      ""endToken"": 943,
      ""beginSentence"": 37,
      ""endSentence"": 37,
      ""speaker"": ""Jo"",
      ""canonicalSpeaker"": ""her""
    },
    {
      ""id"": 25,
      ""text"": ""\""I detest rude, unladylike girls\"""",
      ""beginIndex"": 3895,
      ""endIndex"": 3928,
      ""beginToken"": 944,
      ""endToken"": 951,
      ""beginSentence"": 38,
      ""endSentence"": 38,
      // W R O N G:
      ""speaker"": ""her"",
      ""canonicalSpeaker"": ""Unknown""
    }
  ]
}
```

The command to get it run:
```
wget -q --post-data ""\""Christmas won't be Christmas without any presents,\"" grumbled Jo, lying on the rug. \""It's so dreadful to be poor\"" sighed Meg, looking down at her old dress. \""I don't think it's fair for some girls to have plenty of pretty things, and other girls nothing at all,\"" added little Amy, with an injured sniff. \""We've got Father and Mother, and each other,\"" said Beth contentedly from her corner. The four young faces on which the firelight shone brightened at the cheerful words, but darkened again as Jo said sadly, \""We haven't got Father, and shall not have him for a long time.\"" She didn't say \""perhaps never,\"" but each silently added it, thinking of Father far away, where the fighting was. Nobody spoke for a minute; then Meg said in an altered tone, \""You know the reason Mother proposed not having any presents this Christmas was because it is going to be a hard winter for everyone; and she thinks we ought not to spend money for pleasure, when our men are suffering so in the army. We can't do much, but we can make our little sacrifices, and ought to do it gladly. But I am afraid I don't,\"" and Meg shook her head, as she thought regretfully of all the pretty things she wanted. \""But I don't think the little we should spend would do any good. We've each got a dollar, and the army wouldn't be much helped by our giving that. I agree not to expect anything from Mother or you, but I do want to buy Undine and Sintran for myself. I've wanted it so long,\"" said Jo, who was a bookworm. \""I planned to spend mine in new music,\"" said Beth, with a little sigh, which no one heard but the hearth brush and kettle-holder. \""I shall get a nice box of Faber's drawing pencils; I really need them,\"" said Amy decidedly. \""Mother didn't say anything about our money, and she won't wish us to give up everything. Let's each buy what we want, and have a little fun; I'm sure we work hard enough to earn it,\"" cried Jo, examining the heels of her shoes in a gentlemanly manner. \""I know I do‚Äîteaching those tiresome children nearly all day, when I'm longing to enjoy myself at home,\"" began Meg, in the complaining tone again. \""You don't have half such a hard time as I do,\"" said Jo. \""How would you like to be shut up for hours with a nervous, fussy old lady, who keeps you trotting, is never satisfied, and worries you till you're ready to fly out the window or cry?\"" \""It's naughty to fret, but I do think washing dishes and keeping things tidy is the worst work in the world. It makes me cross, and my hands get so stiff, I can't practice well at all.\"" And Beth looked at her rough hands with a sigh that any one could hear that time. \""I don't believe any of you suffer as I do,\"" cried Amy, \""for you don't have to go to school with impertinent girls, who plague you if you don't know your lessons, and laugh at your dresses, and label your father if he isn't rich, and insult you when your nose isn't nice.\"" \""If you mean libel, I'd say so, and not talk about labels, as if Papa was a pickle bottle,\"" advised Jo, laughing. \""I know what I mean, and you needn't be statirical about it. It's proper to use good words, and improve your vocabilary,\"" returned Amy, with dignity. \""Don't peck at one another, children. Don't you wish we had the money Papa lost when we were little, Jo? Dear me How happy and good we'd be, if we had no worries\"" said Meg, who could remember better times. \""You said the other day you thought we were a deal happier than the King children, for they were fighting and fretting all the time, in spite of their money.\"" \""So I did, Beth. Well, I think we are. For though we do have to work, we make fun of ourselves, and are a pretty jolly set, as Jo would say.\"" \""Jo does use such slang words\"" observed Amy, with a reproving look at the long figure stretched on the rug. Jo immediately sat up, put her hands in her pockets, and began to whistle. \""Don't, Jo. It's so boyish\"" \""That's why I do it.\"" \""I detest rude, unladylike girls\"""" '0.0.0.0:9000/?properties={""annotators"":""tokenize,ssplit,pos,lemma,ner,depparse,coref,quote"",""outputFormat"":""json""}' -O -
```




"
902,https://github.com/stanfordnlp/CoreNLP/issues/1091,1091,[],closed,2020-10-01 15:16:14+00:00,,Is there any way to speed up CoreNLP's `Sentence.sentiment()` method?,"I'm pretty new to using this java wrapper so I'll try my best to describe what's going on.

In my `build.gradle`, I have the following:

```
""edu.stanford.nlp:stanford-corenlp:4.0.0"",
""edu.stanford.nlp:stanford-corenlp:4.0.0:models""
```

We're using this wrapper library to get the sentiment of a large number of phrases. Each phrase will always have 4 words. We start out by using `Sentence.java` to initialize a `Sentence` for each phrase like so:

`new Sentence(phrase);`

Then we attempt to validate sentiment of each phrase with the following:

`boolean isValid = !sentence.sentiment().isNegative() && !sentence.sentiment().isExtreme();`

This line of code is our culprit in that its execution can range anywhere from 17 milliseconds to over 100 milliseconds. With larger data sets, this could be problematic for execution time even with multithreading.

We've noticed that initializing via `new Sentence(phrase);` gives it the following default properties defined in `Sentence.java`:

```
static Properties SINGLE_SENTENCE_DOCUMENT = PropertiesUtils.asProperties(
          ""language"", ""english"",
          ""ssplit.isOneSentence"", ""true"",
          ""tokenize.class"", ""PTBTokenizer"",
          ""tokenize.language"", ""en"",
          ""mention.type"", ""dep"",
          ""coref.mode"", ""statistical"",  // Use the new coref
          ""coref.md.type"", ""dep""
  );
```

We've also noticed running `Sentence.sentiment()` eventually runs the `ParserAnnotator(String annotatorName, Properties props)` constructor in `ParserAnnotator.java` to do the following:

`String model = props.getProperty(annotatorName + "".model"", LexicalizedParser.DEFAULT_PARSER_LOC);`

Where `DEFAULT_PARSER_LOC` is `edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz`. By the time the line of code above is executed, the `props` has one extra property `parse.binaryTrees` being `true`.

I've read that there's another parser called `englishSR.ser.gz`. Would this be a faster alternative in my case? If so, how would I be able to override the default text parser `englishPCFG.ser.gz`? Any other tips on how I might be able to optimize the execution time of getting the sentiment of large data sets of phrases?

"
903,https://github.com/stanfordnlp/CoreNLP/issues/1092,1092,[],closed,2020-10-07 17:00:05+00:00,,OpenIE doesn't work in Spanish,"I'm trying to use OpenIE for relation extraction in Spanish but it doesn't give any output.

It works well in English:
<img width=""1267"" alt=""Screen Shot 2020-10-07 at 11 53 00"" src=""https://user-images.githubusercontent.com/44789253/95362792-37d7be80-0894-11eb-8c3a-f4f94f81e65a.png"">

But not in Spanish:
<img width=""1277"" alt=""Screen Shot 2020-10-07 at 11 53 21"" src=""https://user-images.githubusercontent.com/44789253/95362846-4faf4280-0894-11eb-833c-c4efcacdc2df.png"">

What do you suggest?
Thanks"
904,https://github.com/stanfordnlp/CoreNLP/issues/1093,1093,[],closed,2020-10-15 14:31:41+00:00,,Scene graph in json form,"when I want print the scene graph in JSON form, I use the System.out.println(sg.toJSON()) command, but get an error:
error: method toJSON in class SceneGraph cannot be applied to given types;
        System.out.println(sg.toJSON());
                             ^
  required: int,String,String
  found: no arguments
  reason: actual and formal argument lists differ in length
1 error
what's wrong ???"
905,https://github.com/stanfordnlp/CoreNLP/issues/1095,1095,[],closed,2020-10-18 03:47:32+00:00,,Enhanced++ dependency of adverb modifying a comparative adverb,"In the noun phrase ""a much more important factor,"" the word ""much"" modifies the comparative adverb ""more,"" which in turn modifies ""important.""

If I put this phrase into the CoreNLP server and look at the resulting diagram of enhanced++ dependencies, rendered by brat, this is exactly what I see.  An advmod relation from ""much"" to ""more,"" and another advmod relation from ""more"" to ""important.""

However when I do the same thing in my code, I get a pair of SemanticGraphEdge that each have ""important"" as the governor, with ""much"" and ""more"" as dependents, respectively.  Am I misinterpreting the meaning of this data?  How does it get displayed correctly by brat?"
906,https://github.com/stanfordnlp/CoreNLP/issues/1098,1098,[],closed,2020-10-21 06:34:49+00:00,,"„ÄêCoreNLPServer„ÄëUnable to open ""edu/stanford/nlp/models/segmenter/chinese/ctb.gz"" as class path, filename or URL","# I'm trying to use CoreNLPServer with Chinese.

## Linux | ubuntu | Java1.8 | stanford-corenlp-4.1.0

First,I zipped the **stanford-corenlp-latest.zip**,then went into the directory **stanford-corenlp-4.1.0**.
I copied the **stanford-corenlp-4.1.0-models-chinese.jar** and **StanfordCoreNLP-chinese.properties** to this directory **stanford-corenlp-4.1.0**,like the pictue:
![Êñá‰ª∂ÁªìÊûÑ](https://user-images.githubusercontent.com/25652450/96680910-f4f40b80-13a8-11eb-9612-aed31ea7caa2.png)

Then put all the jar into the CLASSPATH,by this command:
```
for file in `find . -name ""*.jar""`; do export
CLASSPATH=""$CLASSPATH:`realpath $file`""; done
```
Then,started the Server,by this command:
```
java -Xmx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-chinese.properties -port 9000 -timeout 15000
```
So far,no problem,but when I sending a request to the Server,an Exception occurred:

**Unable to open ""edu/stanford/nlp/models/segmenter/chinese/ctb.gz"" as class path, filename or URL**
![image](https://user-images.githubusercontent.com/25652450/96680974-0f2de980-13a9-11eb-8e48-8443a1384767.png)

It seems that the program couldn't find the files in the models-chinese JAR.
But I don't know how to deal with it.
Plz help me,thx~"
907,https://github.com/stanfordnlp/CoreNLP/issues/1099,1099,[],closed,2020-10-22 19:47:17+00:00,,How do I get `models-current.jar` from Maven Central?,"Hi,

How would I be able to download the http://nlp.stanford.edu/software/stanford-english-corenlp-models-current.jar from maven central?"
908,https://github.com/stanfordnlp/CoreNLP/issues/1100,1100,[],open,2020-10-23 06:40:52+00:00,,Question: English NER model performance scores,"Hi, may i know the f1 score, precision score and recall score of the pretrained english model for ner? I'm using it on a project and i'm not sure how to obtain the precision and recall score, since the website only provide f1 score. 


thank you"
909,https://github.com/stanfordnlp/CoreNLP/issues/1101,1101,[],closed,2020-10-23 15:30:07+00:00,, edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file),"We have an Apache Spark program that leverages SCNLP 3.9.2 that we recently tried to upgrade to use the new SCNLP 4.x release. While the program ran well prior to the switch, we started getting the above error specifically for the model file: ""edu/stanford/nlp/models/pos-tagger/english-left3words-distsim.tagger"".  After confirming that we had the correct models jar as well as settings in the program we started digging down into the code. We were able to identify the changes in commit [b23221f012aeb9ab02479e0d5a03ddce22db777b](https://github.com/stanfordnlp/CoreNLP/commit/b23221f012aeb9ab02479e0d5a03ddce22db777b#diff-d250365debfb6d0aacce5da6bfdde12e7f99e0c7f7ad7aa915820b9bf6e71275) as the root cause of the problem. It basically switched to using the System ClassLoader to find the models instead of the Object's ClassLoader. By recompiling SCNLP after reverting the changes our migrated program successfully ran to completion.

When invoking Apache Spark you can specify a --jars option that copies the specified jars to the various worker machines and adds them to the classpath. My suspicion is that Spark introduces a ClassLoader (with access to the additional jar/classpath info) into the chain and by forcing the use of the System ClassLoader because of the change, you lose access to those additional jars.

I'm posting the issue here to make you aware of the breaking change as well as to leave a breadcrumb in case others encounter a similar issue (Most of the related web search results basically say just use the right model file)."
910,https://github.com/stanfordnlp/CoreNLP/issues/1102,1102,[],open,2020-10-29 06:25:00+00:00,,Is there an option to return the index of the quote speaker when using Quote Extraction?,"For example: I got 
Okumu:	""This  is  coming  during  the  growing  season ,  so  our  people  will  be  in  a  greatcrisis  in  three  months ,""	[index=0, charOffsetBegin=613] 
after running the command. Okumu here is the speaker and charOffsetBegin is the start location of the quote but I would like to also annotate the speaker in Brat. Is there a way to include an index for the speaker with the QuoteAnnotator?
Thank you."
911,https://github.com/stanfordnlp/CoreNLP/issues/1109,1109,[],closed,2020-11-12 08:11:44+00:00,,Annotate func will be very very slow if input a wrong propertity,"In my Case, if I input a wrong property paramater, the annotate func will be very slow. I think  a wrong param detection should be added

```python
import re
from stanfordcorenlp import StanfordCoreNLP
nlps = StanfordCoreNLP(
    r'./stanford-corenlp-full-2018-02-27')
text = 'golf and rugby union sevens get the nod for rio olympics in 2016 : ‚Ä¢ tiger woods says he hopes to compete in bra . .'
text = re.sub(r'\. ', ' .', text).strip()
text = re.sub(r' {2,}', ' ', text)
nlp_properties = {
    'annotatiors' : ""depparse"",   
# Right param is :   'annotators' : ""depparse"", 
    'tokenize.whitespace' : True,
    'ssplit.isOneSentence' : False,
    'outputFromat' : 'json'
}
nlps.annotate(text.strip(), nlp_properties)
```"
912,https://github.com/stanfordnlp/CoreNLP/issues/1111,1111,[],closed,2020-11-18 17:03:01+00:00,,Chinese Treebank version ?,"Hi,
I am wondering which Chinese Treebank version was used for training the constituency parsing model for Chinese? 
Is it v9.0?
Thanks."
913,https://github.com/stanfordnlp/CoreNLP/issues/1112,1112,[],closed,2020-11-20 04:01:51+00:00,,Possible to train classifier to a file?,"First, thanks for the great work. I feel lucky to have found the Softmax Classifier from this.

Here, i would like to ask, is there a choice to train the classifier 'To a File'? [Note, i am doing in code, not from CLI]

Currently, from understanding, here is my code:
`columnDataClassifier.trainClassifier(MY_TRAINING_FILE);`

After this line, the classifier is ready to be used. However, if i restart my program or android app, this line has to be called again, introducing additional overhead.

By any chance can this be saved to a file? So that it can be loaded subsequently without re-training?

Thanks in advance.

"
914,https://github.com/stanfordnlp/CoreNLP/issues/1113,1113,[],open,2020-11-21 14:32:03+00:00,,"I use 4.2.0 on Chinese and English copora with %,but it reported an error that ""json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)""","I use 4.2.0 on Chinese and English copora with ""%"", but it reported an error that ""json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)"".(normal on English corpora without ""%"".
When I use 4.0.0 or other versions, the error has been solved.

I wonder if someone has met with the same problem?"
915,https://github.com/stanfordnlp/CoreNLP/issues/1117,1117,[],open,2020-12-10 11:27:34+00:00,,'Title' Named Entity Recognition not working ,"I downloaded the Stanford NER model using instructions from https://nlp.stanford.edu/software/CRF-NER.html and http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford. It recognises organizations, locations, persons but no job titles. However, the demo online http://corenlp.run/ does work with titles. How do I resolve this issue?

"
916,https://github.com/stanfordnlp/CoreNLP/issues/1118,1118,[],closed,2020-12-16 06:25:00+00:00,,Quote Annotator IndexOutOfBoundsException,"Hello,

I came across a strange issue when trying to run the CoreNLP pipeline for quote annotation.

I am getting an IndexOutOfBoundsException: Index -1 out of bounds for length 8. when trying to run the pipeline on the following small sample text:

""Bob.""\n\n""What.""

The quotes are part of the input string. I was just wondering if there was something from my end that was wrong or if it is a library issue. I am invoking CoreNLP from the stanza python package with annotators = tokenize,ssplit,pos,lemma,ner,depparse,coref,quote,entitylink

This is with the latest version 4.2.0.

Any help would be appreciated! Thank you!

"
917,https://github.com/stanfordnlp/CoreNLP/issues/1124,1124,[],open,2020-12-30 22:27:56+00:00,,Week 53 in sutime,"Reported by Vijay Raj:

Test sentences executed on Dec 30th, on corenlp.run

1. I was working last week.
DATE NER: 2020-W52

2. I will be working next week.
DATE NER: NEXT P1W1

Note that the date is not resolved to 2020-W53."
918,https://github.com/stanfordnlp/CoreNLP/issues/1125,1125,[],closed,2021-01-02 22:42:28+00:00,,PTBLexer: Invalid options key in constructor: ptb3Dashes,"According to [the tokenizer documentation](https://nlp.stanford.edu/software/tokenizer.html) it should be possible to set `tokenize.options = ptb3Dashes=true` for the english model, but when I do that with version 4.2.0 of CoreNLP I get the error in the title.  Has this option been removed?  If so, probably it should be removed from the docs also."
919,https://github.com/stanfordnlp/CoreNLP/issues/1127,1127,[],open,2021-01-20 00:25:20+00:00,,No Title is being extracted for certain format of sentences.,"I've downloaded the core NLP and spun up the server locally according to the instructions [here](https://stanfordnlp.github.io/CoreNLP/corenlp-server.html). It works really well but for the certain format of sentences, there is no Title being extracted by `ner` annotator. A few of the examples are as follows:

- The man works as a maintenance man at a local
- The man works as a sales rep for a local
- The man works as a security officer at a private
- The man works as a landscaper and helps to

I have accumulated around ~60000 such examples where core NLP `ner` annotator failed to extract the title from the sentence. I would be happy to share all the examples if it helps make core NLP better at title extraction.

"
920,https://github.com/stanfordnlp/CoreNLP/issues/1128,1128,[],closed,2021-01-20 15:03:17+00:00,,SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation is Deprecated,"`SemanticGraph graph = sentence.get(SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation.class);
      out.println(graph.toString(SemanticGraph.OutputFormat.LIST));`

Using Eclipse IDE, When running the demo (public class StanfordCoreNlpDemo) I'm getting the error ""The type SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation is deprecated""

Looking at the documentation [here](https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/semgraph/SemanticGraphCoreAnnotations.html), which one do I replace it with?"
921,https://github.com/stanfordnlp/CoreNLP/issues/1131,1131,[],closed,2021-02-01 04:12:40+00:00,,can't download stanza for python3,"When I type in pip install stanza into the command line of python (im on Mac)  I keep getting a SyntaxError: invalid syntax error message. I am using python 3.8.2, so not sure why this is happening. I also have no experience with coding and just recently downloaded python 3 from pythons website. I was encouraged by your team (via email correspondence) to use java for my project, but that download failed several times, so here I am. I want to use stanza so I can first separate words (dealing with chinese) and then assign pos. 
"
922,https://github.com/stanfordnlp/CoreNLP/issues/1132,1132,[],closed,2021-02-03 22:38:19+00:00,,how can we invoke this code,"https://github.com/stanfordnlp/CoreNLP/blob/master/src/edu/stanford/nlp/trees/ud/UniversalEnhancer.java

> Can you give an example of how to invoke it via CoreNLP? It seems to need some kind of embeddings for the gapping component, but I couldn't find documentation explaining how to run it.

Reported in https://github.com/UniversalDependencies/UD_English-EWT/issues/127

"
923,https://github.com/stanfordnlp/CoreNLP/issues/1134,1134,[],closed,2021-02-08 18:20:06+00:00,,QuotationAttributionAnnotator accesses ArrayList out of bounds (causes IndexOutOfBoundsException),"**OS**: Linux Mint 20.1 Ulyssa (base: Ubuntu 20.04 focal)
**Java**: openjdk version ""11.0.9.1""
**CoreNLP**: 4.2.0 (also 4.1.0)

Command line: 
```
java -Xmx10g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,depparse,coref,quote -file input.txt -outputFormat text
```
Contents of input.txt:
```text
John Doe:""foo""
```
Note that the problem occurs when the first line of any document is of the above form.  A name (first [middle] last), punctuation (tested with colon and comma), and then a quote.

Exception thrown:
```
Exception in thread ""main"" java.lang.IndexOutOfBoundsException: Index -1 out of bounds for length 6
	at java.base/jdk.internal.util.Preconditions.outOfBounds(Preconditions.java:64)
	at java.base/jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Preconditions.java:70)
	at java.base/jdk.internal.util.Preconditions.checkIndex(Preconditions.java:248)
	at java.base/java.util.Objects.checkIndex(Objects.java:372)
	at java.base/java.util.ArrayList.get(ArrayList.java:459)
	at edu.stanford.nlp.quoteattribution.Sieves.QMSieves.TrigramSieve.trigramPatterns(TrigramSieve.java:71)
	at edu.stanford.nlp.quoteattribution.Sieves.QMSieves.TrigramSieve.doQuoteToMention(TrigramSieve.java:27)
	at edu.stanford.nlp.pipeline.QuoteAttributionAnnotator.annotate(QuoteAttributionAnnotator.java:251)
	at edu.stanford.nlp.pipeline.QuoteAnnotator.annotate(QuoteAnnotator.java:292)
	at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:653)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:663)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1261)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1095)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1361)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1430)
```"
924,https://github.com/stanfordnlp/CoreNLP/issues/1136,1136,[],open,2021-02-17 03:07:40+00:00,,Quotation attribution: gatherQuotes returns a set of quotes that have unexpected index numbers,"**OS**: Linux Mint 20.1 Ulyssa (base: Ubuntu 20.04 focal)
**Java**: openjdk version ""11.0.9.1""
**CoreNLP**: 4.2.0 (also 4.1.0, and the dev branch with commit 040b846a428a34373e4854bfee138c70f5d50a1d as the HEAD)

Command line:
```
java -Xmx10g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,depparse,coref,quote -file bug1input.txt -outputFormat text
```

Given the attached file as input [bug1input.txt](https://github.com/stanfordnlp/CoreNLP/files/5992875/bug1input.txt), the output results in this file: [bug1output.txt](https://github.com/stanfordnlp/CoreNLP/files/5992881/bug1output.txt)

Note the Extracted Quotes section near the bottom of the output file.

**Expected**: The index numbers of the quotes should begin at 0 and be a continuous range of integers without duplicates.
**Actual**: Index number 7 does not appear and Index number 8 is used twice, being in fact the same quote appearing two times. 
```
Extracted quotes:
...
Unknown:	‚ÄúI don't trust pharmaceuticals. I really don't. And it doesn't sound like it's going to be safe,‚Äù	[index=8, charOffsetBegin=2370]
Unknown:	‚ÄúI don't trust pharmaceuticals. I really don't. And it doesn't sound like it's going to be safe,‚Äù	[index=8, charOffsetBegin=2370]
...
```
I discovered this problem while storing attributed quotes after calling `gatherQuotes()`.  My assumption is that the quote index is unique to each quote within a given text and that each quote index should appear only once in the returned set.  Is this assumption correct, or have I misunderstood?  I've seen this happen in other instances as well, so it's not unique to this body of text.  The command line execution also generates the following warning:
```
WARNING: unmatched quote of type "" found at index 1905 in text segment: You‚Äôre going to need to get quite large proportions of the population vaccinated before you see a real effect.""
About 33.8 million Americans, or 10% o...
```
[edit: additional details follow]

## Some observations about the input text
The input text does not actually contain any embedded quotes.  However, it uses a mix of simple undirected quotes and `""` and directed quotes `‚Äú‚Äù`.  In particular, there is one quote that begins with a directed opening quote but ends with an undirected quote:
```
‚ÄúYou‚Äôre going to need to get quite large proportions of the population vaccinated before you see a real effect.""
```
... and one that begins with an undirected quote but ends with a directed closing quote:
```
""I feel like I have plenty of time before I get a chance to get (the vaccine) anyway, to find out if there are bad side effects and whether it‚Äôs even worth getting it,‚Äù
```
CoreNLP does not recognize this error (expects too much of mere humans :-) ), and so sees large areas of the text as a series of embedded quotes that are not actual quotes."
925,https://github.com/stanfordnlp/CoreNLP/issues/1137,1137,[],open,2021-02-18 20:39:33+00:00,,SUTime output differs with Java version and stanford-corenlp version,"Hi,

While running unit tests on some code that uses SUTime, we noticed that all tests passed with Java 1.8 but that one failed with Java 11.¬† In both cases we were using the same stanford-corenlp 3.9.2.¬† We tracked it down to a discrepancy in SUTime, which produces different output for the different Java versions.¬† The example¬†SUTimeDemo program from¬†https://nlp.stanford.edu/software/sutime.shtml produces for the sentence

```
The Food and Agriculture Organization of the United Nations (FAO), the United Nations Children's Fund (UNICEF) and the World Food Programme (WFP) stressed that while the deteriorating situation coincides with an unusually long and harsh annual lean season, when families have depleted their food stocks and new harvests are not expected until August, the level of food insecurity this year is unprecedented.
```

output with Java 8 that includes the word ""annual"".¬† Java 11 output is without it.

```
annual [from char offset 237 to 243] --> P1Y
August [from char offset 343 to 349] --> 2013-08
this year [from char offset 380 to 389] --> 2013
```

For what it is worth, if the same test is run using standord-corenlp 4.2.0, ""annual"" appears in the output of both runs.¬† However, we are reluctant to switch to the newer version because of other changes that it has.

Does anyone know the reason for this behavior?¬† Is there something we can backport to the older version so that we can get consistent output?

Thank you,"
926,https://github.com/stanfordnlp/CoreNLP/issues/1138,1138,[],open,2021-02-27 14:37:16+00:00,,Issue with implementing Stanford Corenlp 4.2.0,"I'm having trouble implementing the Stanford Corenlp 4.2.0 library into my android project.

I am using Android 4.2 Beta on a Windows 10.

On the first run attempt, this error shows:
`""More than one file was found with OS independent path 'edu/stanford/nlp/pipeline/demo/corenlp-brat.html'.""`

I'd like to do this for an offline mobile app.

Thank you in advance!"
927,https://github.com/stanfordnlp/CoreNLP/issues/1139,1139,[],open,2021-03-05 23:24:49+00:00,,Quotation Attribution - quotation extraction improvement: properly extract quotes that span paragraphs,"A common occurrence in English text around the world, especially in news articles, is use of the following convention (sourced from [wikipedia](https://en.wikipedia.org/wiki/Quotation_marks_in_English#Quotations_and_speech)):

> The convention in English is to give opening quotation marks to the first and each subsequent paragraph, using closing quotation marks only for the final paragraph of the quotation.

Here is just one example extracted from [here](https://montrealgazette.com/news/quebec/dube-arruda-say-variants-could-impede-easing-of-restrictions-after-march-break""):

> ‚ÄúWhat we are watching is with what speed can we control the variants,‚Äù Dub√© said. ‚ÄúWe have seen the conventional cases drop but the variants increase.
> 
> ‚ÄúUntil now, we have been able to stay within the 800 mark (of daily new cases, including variants), but it‚Äôs something we‚Äôre following.
> 
> ‚ÄúI ask you to follow the rules, because when we see the risks associated with March break and the risks associated with variants, I would not want to have to back up in the coming weeks. I think it‚Äôs important that we come out of March break the right way.‚Äù

Note that the second quotation begins part way through the first paragraph and does not end until the end of the third paragraph. Therefore, the second quotation in the above text is:

> We have seen the conventional cases drop but the variants increase.
> 
> Until now, we have been able to stay within the 800 mark (of daily new cases, including variants), but it‚Äôs something we‚Äôre following.
> 
> I ask you to follow the rules, because when we see the risks associated with March break and the risks associated with variants, I would not want to have to back up in the coming weeks. I think it‚Äôs important that we come out of March break the right way.

I have also seen examples where the paragraph-spanning quote closes part way through it's last paragraph rather than only at the end.

Ideally, CoreNLP would extract the entire unbroken quotation as shown above.  Currently, CoreNLP does not extract the part of the quotation that starts in the first paragraph, nor does it recognize the second paragraph as being part of a quote or even a quote on its own.  These are not even recorded in `UnclosedQuotationsAnnotation` when `quote.extractUnclosedQuotes` is set to `true`.  The final paragraph is, as one would expect given the opening and closing quotation marks, successfully extracted as a quote, but only on its own.  The earlier parts of the quote are not annotated as quotes or part thereof whatsoever.

Therefore, to sum up: when annotating quotations (at least those in the English language), CoreNLP should recognize paragraph-spanning quotations as such.  That is, those quotations that end a paragraph without a closing quotation mark and continue onto the next paragraph with an opening quotation mark, and so on until a closing quotation mark is found."
928,https://github.com/stanfordnlp/CoreNLP/issues/1141,1141,[],closed,2021-03-10 23:51:06+00:00,,POS tag errors,"Hello,
  I am using the maximum entropy tagger combined with a the lexical parser and I noticed that some proper nouns such as cities as ""bombay"" or ""rome""  are not recognized by the tagger or are assigned noun tags. Also there are times where nouns ( f.e (""food"") or adjectives (such as ""spanish"") are tagged as proper nouns. An example is a phrase like ""spanish food""  where both these were assigned a proper noun tag ( when using the ""english-left3words-distsim.tagger"" model I only get ""spanish"" as proper noun). In another instance the city ""madrid"" was assigned an adjective tag. When I tried to use the lexical parser without pos tags generated beforehand, I get similar errors but f.e ""madrid"" is assigned an adverb tag instead (the sentence was ""in madrid please"").

I started by using the POS tag model provided in the ""english-bidirectional-distsim.tagger""  file and I am running the tagger in Visual Studio through the .NET NuGet package provided. The dataset I am using is the The (6) dialog bAbI tasks dataset.  When I am using the ""english-left3words-distsim.tagger"" I do not get unknown word tags but still get proper nouns like ""bombay"" and ""madrid"" erroneously assigned as nouns etc.

 Are these errors expected or is it something that might be wrong with my implementation that causes this?

Thank you."
929,https://github.com/stanfordnlp/CoreNLP/issues/1143,1143,[],closed,2021-03-21 05:14:30+00:00,,StanfordScene GraphParser: Error: There is already a relation named det:qmod!,"Hello,

I have Java11. I downloaded:
1. CoreNLP4.2.0/4.1.0/3.9.2 + English models from https://stanfordnlp.github.io/CoreNLP/history.html
2. SceneGraphParser from https://nlp.stanford.edu/software/scenegraph-parser.shtml

In the directory which contains all jar files, I run command: java -mx2g -cp ""*"" edu.stanford.nlp.scenegraph.RuleBasedParser
Next, I input sentence:
A brown fox chases a white rabbit.

However, I receive the below output (providing the full trace so that it is easy to understand):

[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse
[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.5 sec].
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma
[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.1 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].
[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.6 sec].
[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.
[main] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 580704 unique entries out of 581863 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_caseless.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 4869 unique entries out of 4869 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_cased.tab, 0 TokensRegex patterns.
[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 585573 unique entries from 2 files
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at edu.stanford.nlp.scenegraph.RuleBasedParser.parse(RuleBasedParser.java:84)
	at edu.stanford.nlp.scenegraph.AbstractSceneGraphParser.parse(AbstractSceneGraphParser.java:20)
	at edu.stanford.nlp.scenegraph.AbstractSceneGraphParser.parse(AbstractSceneGraphParser.java:48)
	at edu.stanford.nlp.scenegraph.RuleBasedParser.main(RuleBasedParser.java:254)
Caused by: java.lang.IllegalArgumentException: There is already a relation named det:qmod!
	at edu.stanford.nlp.trees.GrammaticalRelation.<init>(GrammaticalRelation.java:322)
	at edu.stanford.nlp.trees.GrammaticalRelation.<init>(GrammaticalRelation.java:349)
	at edu.stanford.nlp.scenegraph.SemanticGraphEnhancer.<clinit>(SemanticGraphEnhancer.java:48)
	... 4 more

I tried it with CoreNLP4.2.0/4.1.0/3.9.2. Please suggest. Thank you in advance."
930,https://github.com/stanfordnlp/CoreNLP/issues/1145,1145,[],closed,2021-03-24 06:41:01+00:00,,ssplit.tokenPatternsToDiscard causes AnnotationException,"Hi,

To resolve coreference across sentences, I joint my sentence with separators.
Then I call CoreNLPClient while specifying ssplit.boundariesToDiscard, as
```
with CoreNLPClient(timeout=30000, memory='16G', endpoint='http://localhost:8000',
                   properties={'annotators': 'tokenize,ssplit,pos,lemma,coref',
                    'ssplit.boundariesToDiscard': '<title>,</title>,</sent>' 
                                       }
                   ) as client: 
    Text = '<title> ed wood film </title> ed wood is 1994 american biographical period comedydrama film directed and produced by tim burton and starring johnny depp as cult filmmaker ed wood </sent> film concerns period in woods life when he made his bestknown films as well as his relationship with actor bela lugosi played by martin landau </sent> sarah jessica parker patricia arquette jeffrey jones lisa marie and bill murray are among supporting cast </sent>'
    
    ann = client.annotate(Text)
```

It prints out the following errors:

> HTTPError                                 Traceback (most recent call last)
> /usr/local/lib/python3.7/dist-packages/stanza/server/client.py in _request(self, buf, properties, reset_default, **kwargs)
>     451                               timeout=(self.timeout*2)/1000, **kwargs)
> --> 452             r.raise_for_status()
>     453             return r
> 
> 3 frames
> HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:8000/?properties=%7B%27outputFormat%27%3A+%27serialized%27%7D&resetDefault=false
> 
> During handling of the above exception, another exception occurred:
> 
> AnnotationException                       Traceback (most recent call last)
> /usr/local/lib/python3.7/dist-packages/stanza/server/client.py in _request(self, buf, properties, reset_default, **kwargs)
>     456                 raise TimeoutException(r.text)
>     457             else:
> --> 458                 raise AnnotationException(r.text)
>     459 
>     460     def annotate(self, text, annotators=None, output_format=None, properties=None, reset_default=None, **kwargs):
> 
> AnnotationException: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index 68 out of bounds for length 68

Thank you"
931,https://github.com/stanfordnlp/CoreNLP/issues/1146,1146,[],closed,2021-03-30 03:42:59+00:00,,Blocked loading mixed active content ‚Äúhttp://cdnjs.cloudflare.com/ajax/libs/dagre-d3/0.4.17/dagre-d3.min.js‚Äù,"We're running CoreNLP using Docker. The issue we're having is that the constituency parse is not showing up. It seems that the `<script>` include for dagre-d3 requests the http not the https version and the browser blocks the mixed active content. I'm not sure if this is unique to our usage or not so filing this issue.

I mention the following in case it's relevant. We recently updated our version to 4.2.0 from the CoreNLP website to see if this had been fixed, but our instance says its on version 4.1.0 (not 4.2.0) on the landing page. Perhaps we've made a mistake in updating?

Regards"
932,https://github.com/stanfordnlp/CoreNLP/issues/1147,1147,[],closed,2021-04-05 20:41:32+00:00,,Quote Annotation - AnnotationException StringIndexOutOfBoundsException,"Hello,

I had a situation with text that had this: `""""=`

It seems to throw an error when I try running the pipeline with quote annotation on this small fragment. Just wanted to verify that it was an issue.

Thank you."
933,https://github.com/stanfordnlp/CoreNLP/issues/1149,1149,[],open,2021-04-07 01:25:27+00:00,,Issue when using StanfordCorenlp for entity link,"Hi, when i use Stanford Corenlp for entity link, the program is stuck, In other words, the program is running all the time without results. I use python's library stanfordcorenlp, here is my code:

import stanfordcorenlp
import json
from stanfordcorenlp import StanfordCoreNLP
nlp = StanfordCoreNLP(r'D:\stanford-corenlp-full-2018-10-05')#Êñá‰ª∂ÁöÑ‰ΩçÁΩÆ
sentence = 'I want to go to beijing.'
output = nlp.annotate(sentence, properties={
    'annotators': 'tokenize,ssplit,pos,lemma,ner,entitylink',
    'outputFormat': 'json'
    })
data = json.loads(output)
print(data)

How does this happen? Thanks."
934,https://github.com/stanfordnlp/CoreNLP/issues/1152,1152,[],closed,2021-04-09 21:11:38+00:00,,CoreNLP not working for Chinese,"English and German both worked fine, but Chinese did not work, was given numbers instead of Chinese characters. Even the segmenter did not even work. Seems there is an issue with the pipeline for Chinese. 
Have tried several times and even on two different machines, still not working. "
935,https://github.com/stanfordnlp/CoreNLP/issues/1153,1153,[],open,2021-04-12 16:45:19+00:00,,Using 'coref' annotator using corenlp from Stanza,"Hello, 
     Not sure if this is the place for it, but I've been using 'coref' from the Stanza wrapper for corenlp. I was wondering which coref model is referenced from within 'annotators'. I noticed that you can use either 'coref' or 'dcoref', so I know it's not the deterministic model. So that leaves either the neural or statistical. If 'coref' is referring to the statistical model, is there any way to access the neural model from Stanza?

- Thanks! "
936,https://github.com/stanfordnlp/CoreNLP/issues/1155,1155,[],open,2021-05-05 06:59:04+00:00,,build error with maven in Win10,"When use `maven package`  to build the source code of CoreNLP 4.2.0 with maven 3.8.1 and jdk 16.0.1, I get this errorÔºö


```
Running edu.stanford.nlp.util.TimingTest
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.162 sec <<< FAILURE!
testTiming(edu.stanford.nlp.util.TimingTest)  Time elapsed: 0.162 sec  <<< FAILURE!
junit.framework.ComparisonFailure: Wrong formatted time expected:<0.[1]> but was:<0.[2]>
        at junit.framework.Assert.assertEquals(Assert.java:100)
        at junit.framework.TestCase.assertEquals(TestCase.java:253)
        at edu.stanford.nlp.util.TimingTest.testTiming(TimingTest.java:40)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:567)
        at junit.framework.TestCase.runTest(TestCase.java:177)
        at junit.framework.TestCase.runBare(TestCase.java:142)
        at junit.framework.TestResult$1.protect(TestResult.java:122)
        at junit.framework.TestResult.runProtected(TestResult.java:142)
        at junit.framework.TestResult.run(TestResult.java:125)
        at junit.framework.TestCase.run(TestCase.java:130)
        at junit.framework.TestSuite.runTest(TestSuite.java:241)
        at junit.framework.TestSuite.run(TestSuite.java:236)
        at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:90)
        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:567)
        at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
        at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
        at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)

Running edu.stanford.nlp.util.TreeShapedStackTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec
Running edu.stanford.nlp.util.TwoDimensionalMapTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec
Running edu.stanford.nlp.util.XMLUtilsTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 sec
Running edu.stanford.nlp.wordseg.ChineseStringUtilsTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.073 sec

Results :

Failed tests:   testTiming(edu.stanford.nlp.util.TimingTest): Wrong formatted time expected:<0.[1]> but was:<0.[2]>

Tests run: 1336, Failures: 1, Errors: 0, Skipped: 0

[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  03:22 min
[INFO] Finished at: 2021-05-05T14:42:10+08:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.12.4:test (default-test) on project stanford-corenlp: There are test failures.
[ERROR]
[ERROR] Please refer to C:\Users\123\Desktop\CoreNLP-4.2.0\target\surefire-reports for the individual test results.
[ERROR] -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.12.4:test (default-test) on project stanford-corenlp: There are test failures.

Please refer to C:\Users\123\Desktop\CoreNLP-4.2.0\target\surefire-reports for the individual test results.
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:957)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:289)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:193)
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:78)
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:567)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
Caused by: org.apache.maven.plugin.MojoFailureException: There are test failures.

Please refer to C:\Users\123\Desktop\CoreNLP-4.2.0\target\surefire-reports for the individual test results.
    at org.apache.maven.plugin.surefire.SurefireHelper.reportExecution (SurefireHelper.java:83)
    at org.apache.maven.plugin.surefire.SurefirePlugin.writeSummary (SurefirePlugin.java:176)
    at org.apache.maven.plugin.surefire.SurefirePlugin.handleSummary (SurefirePlugin.java:150)
    at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked (AbstractSurefireMojo.java:650)
    at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute (AbstractSurefireMojo.java:586)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:957)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:289)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:193)
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:78)
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:567)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
[ERROR]
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
```
"
937,https://github.com/stanfordnlp/CoreNLP/issues/1157,1157,[],open,2021-05-10 07:13:43+00:00,,Could not handle incoming annotation error,"When a string contains a % character, the pipeline is not able to annotate it with named entities. This is the same error as #206. With `quiet=False` and `logging_level=logging.DEBUG`, I get the following:

```
'Could not handle incoming annotation'
(<class 'json.decoder.JSONDecodeError'>, JSONDecodeError('Expecting value: line 1 column 1 (char 0)',), <traceback object at 0x00000218D876A548>)
```

Looking at the server, this prints out the following when it receives a sentence containing a %:

```
[pool-1-thread-9] INFO CoreNLP - [/0:0:0:0:0:0:0:1:50544] API call w/annotators tokenize,ssplit,pos,lemma,ner
[pool-1-thread-9] WARN CoreNLP - java.lang.IllegalArgumentException: URLDecoder: Illegal hex characters in escape (%) pattern - For input string: "" i""
  java.net.URLDecoder.decode(Unknown Source)
  edu.stanford.nlp.pipeline.StanfordCoreNLPServer.getDocument(StanfordCoreNLPServer.java:330)
  edu.stanford.nlp.pipeline.StanfordCoreNLPServer.access$600(StanfordCoreNLPServer.java:54)
  edu.stanford.nlp.pipeline.StanfordCoreNLPServer$CoreNLPHandler.handle(StanfordCoreNLPServer.java:898)
  com.sun.net.httpserver.Filter$Chain.doFilter(Unknown Source)
  sun.net.httpserver.AuthFilter.doFilter(Unknown Source)
  com.sun.net.httpserver.Filter$Chain.doFilter(Unknown Source)
  sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(Unknown Source)
  com.sun.net.httpserver.Filter$Chain.doFilter(Unknown Source)
  sun.net.httpserver.ServerImpl$Exchange.run(Unknown Source)
  java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  java.lang.Thread.run(Unknown Source)
```

For reference, the sentence was ""There was 60% inflation"". "
938,https://github.com/stanfordnlp/CoreNLP/issues/1160,1160,[],closed,2021-05-10 21:06:59+00:00,,NullPointer Exception Thrown when using NERClassifierCombiner to Combine RegexSequenceNERClassifier and CRFClassifier,"I am using NERClassifierCombiner in CoreNLP to combine RegexNERSequenceClassifier and CRFClassifier. (The same exception was thrown when I used the ClassifierCombiner).

```
import edu.stanford.nlp.ie.ClassifierCombiner;
import edu.stanford.nlp.ie.NERClassifierCombiner;
import edu.stanford.nlp.ie.crf.CRFClassifier;
import edu.stanford.nlp.ie.regexp.RegexNERSequenceClassifier;
import edu.stanford.nlp.io.IOUtils;
import java.io.IOException;

public class Main {

    public static void main(String[] args) throws IOException, ClassNotFoundException {

        String basePath = ""some_base_path"";
        String filename = basePath + ""input.txt"";
        String inputText = IOUtils.slurpFile(filename);

        String mucClassifierPath = basePath +  ""english.muc.7class.distsim.crf.ser.gz"";
        String connlClassifierPath = basePath + ""english.conll.4class.distsim.crf.ser.gz"";

        CRFClassifier classifier_muc = CRFClassifier.getClassifier(mucClassifierPath);
        CRFClassifier classifier_connl = CRFClassifier.getClassifier(connlClassifierPath);

        String mappingPath = basePath + ""regexner.rules"";
        RegexNERSequenceClassifier regexNERSequenceClassifier = new RegexNERSequenceClassifier(mappingPath, true, true, null);

        NERClassifierCombiner combinedNERClassifier = new NERClassifierCombiner(false, false, classifier_connl, classifier_muc, regexNERSequenceClassifier);

        String output = combinedNERClassifer.classifyToString(inputText);
        System.out.println(output);

        return;

    }
}
```

Here is the stack trace of the exception which is thrown when the classifyToText method is called.

```
Exception in thread ""main"" java.lang.NullPointerException
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.labels(AbstractSequenceClassifier.java:286)
    at edu.stanford.nlp.ie.ClassifierCombiner.mergeDocuments(ClassifierCombiner.java:334)
    at edu.stanford.nlp.ie.ClassifierCombiner.classify(ClassifierCombiner.java:481)
    at edu.stanford.nlp.ie.NERClassifierCombiner.classifyWithGlobalInformation(NERClassifierCombiner.java:273)
    at edu.stanford.nlp.ie.NERClassifierCombiner.classify(NERClassifierCombiner.java:268)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.classifyObjectBank(AbstractSequenceClassifier.java:481)
    at edu.stanford.nlp.ie.AbstractSequenceClassifier.classify(AbstractSequenceClassifier.java:441)
    at com.company.Main.main(Main.java:27)
```
PS: I can't use the CoreNLP Pipeline need to merge RegexNER with the existing CRFClassifiers in Production.
"
939,https://github.com/stanfordnlp/CoreNLP/issues/1161,1161,[],open,2021-05-26 17:29:39+00:00,,Tokenization of cliticized tokens different for single-word tokens versus in a sentence,"In English, when I tokenize text such as ""that'll"" with no other context, I get the following tokens:
* that
* '
* ll

However, when I tokenize text such as ""that'll be all folks."", I get the following tokens (which is my expected result for ""'ll""):
* that
* 'll
* be
* all
* folks
* .

I have the following set:
* tokenize.language = English
* tokenize.whitespace = false
* tokenize.keepeol = false
* tokenize.verbose = false
* tokenize.options = invertible=true,splitAssimilations=true,splitHyphenated=false,splitForwardSlash=true,untokenizable=firstDelete,strictTreebank3=true,normalizeSpace=false

Is there a way to make Stanford CoreNLP correctly tokenize a single contracted word with no other context? I don't see any other options here: [Tokenization](https://stanfordnlp.github.io/CoreNLP/tokenize.html).

The same thing happens with ""wanna"", ""gonna"", ""shouldda"", etc. They don't get tokenized when in isolation."
940,https://github.com/stanfordnlp/CoreNLP/issues/1163,1163,[],closed,2021-05-28 12:14:54+00:00,,StanfordCoreNLP giving error for dependency parsing sentences with '%' in stanford-corenlp-4.2.2,"Hello, 
I have stanford-corenlp-4.2.2.
I was trying depedency parsing and it is working well but for any length of sentences containing '%', I am getting the error: 'Not able to parse' 
The lines of code to load and run the parser are following:
![image](https://user-images.githubusercontent.com/29333403/119981851-bb891b80-bfdb-11eb-8cb1-5a2474b78a0f.png)
`from pycorenlp import StanfordCoreNLP  '\n'
nlp = StanfordCoreNLP('http://localhost:9000')\n
res = nlp.annotate(text,
                       properties={
                           'annotators': 'depparse',
                           'outputFormat': 'json',
                           'timeout': 40000,
                       })       
`

This issue was not with previous versions.

Sorry I am new here and not know how to do it well"
941,https://github.com/stanfordnlp/CoreNLP/issues/1166,1166,"[{'id': 45387505, 'node_id': 'MDU6TGFiZWw0NTM4NzUwNQ==', 'url': 'https://api.github.com/repos/stanfordnlp/CoreNLP/labels/duplicate', 'name': 'duplicate', 'color': 'bbbbbb', 'default': True, 'description': None}]",closed,2021-06-09 10:42:55+00:00,,Problem when running stanfordcorenlp in multithreads,"I plan to preprocess a large corpus using stanfordcorenlp tool, however, when i try to type in this commandÔºå`java -Xmx14g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,depparse,coref,kbp -threads 4 -fileList sample-files.txt -outputFormat text` . It just stop working, how can i fix this?"
942,https://github.com/stanfordnlp/CoreNLP/issues/1167,1167,[],closed,2021-06-09 10:43:12+00:00,,Problem when running stanfordcorenlp in multithreads,"I plan to preprocess a large corpus using stanfordcorenlp tool, however, when i try to type in this commandÔºå`java -Xmx14g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,depparse,coref,kbp -threads 4 -fileList sample-files.txt -outputFormat text` . It just stop working, how can i fix this?"
943,https://github.com/stanfordnlp/CoreNLP/issues/1168,1168,[],closed,2021-06-21 00:31:12+00:00,,Constituent structure to dependency structure: No head rule defined for IP using class edu.stanford.nlp.trees.SemanticHeadFinder,"I am trying to convert constituent structure to dependency structure for old English and I was getting the following error 

""No head rule defined for IP using class edu.stanford.nlp.trees.SemanticHeadFinder""

Based on how I understood Parser FAQ, I should be using LeftHeadFinder instead of SemanticHeadFinder. I go at the trees and do this change at EnglishGramaticalStructure and GramaticalStructure. Still somehow it looks for the SemanticHeadFinder. Error point after my changes is edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineHead(AbstractCollinsHeadFinder.java:193)

Has someone used leftHeadFinder for converting constituent structure to dependency structure, if yes, can you please help me.
"
944,https://github.com/stanfordnlp/CoreNLP/issues/1169,1169,[],open,2021-07-12 03:09:23+00:00,,Issue when using corenlp for processing large corpus," I divided the large English corpus into several subsets and ran multiple CorenLp commands simultaneously, but the following error always occurs after a period of time:
""""""
        Exception in thread ""main"" java.lang.RuntimeException: Error making document
	at edu.stanford.nlp.coref.CorefSystem.annotate(CorefSystem.java:55)
	at edu.stanford.nlp.pipeline.CorefAnnotator.annotate(CorefAnnotator.java:160)
	at edu.stanford.nlp.pipeline.AnnotationPipeline.annotate(AnnotationPipeline.java:76)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:641)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.annotate(StanfordCoreNLP.java:651)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1249)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.processFiles(StanfordCoreNLP.java:1083)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.run(StanfordCoreNLP.java:1366)
	at edu.stanford.nlp.pipeline.StanfordCoreNLP.main(StanfordCoreNLP.java:1418)
Caused by: java.lang.IllegalArgumentException
	at edu.stanford.nlp.semgraph.SemanticGraph.parentPairs(SemanticGraph.java:730)
	at edu.stanford.nlp.semgraph.semgrex.GraphRelation$DEPENDENT$1.advance(GraphRelation.java:325)
	at edu.stanford.nlp.semgraph.semgrex.GraphRelation$SearchNodeIterator.initialize(GraphRelation.java:1103)
	at edu.stanford.nlp.semgraph.semgrex.GraphRelation$SearchNodeIterator.<init>(GraphRelation.java:1084)
	at edu.stanford.nlp.semgraph.semgrex.GraphRelation$DEPENDENT$1.<init>(GraphRelation.java:310)
	at edu.stanford.nlp.semgraph.semgrex.GraphRelation$DEPENDENT.searchNodeIterator(GraphRelation.java:310)
	at edu.stanford.nlp.semgraph.semgrex.NodePattern$NodeMatcher.resetChildIter(NodePattern.java:337)
	at edu.stanford.nlp.semgraph.semgrex.NodePattern$NodeMatcher.<init>(NodePattern.java:332)
	at edu.stanford.nlp.semgraph.semgrex.NodePattern.matcher(NodePattern.java:293)
	at edu.stanford.nlp.semgraph.semgrex.CoordinationPattern$CoordinationMatcher.<init>(CoordinationPattern.java:146)
	at edu.stanford.nlp.semgraph.semgrex.CoordinationPattern.matcher(CoordinationPattern.java:120)
	at edu.stanford.nlp.semgraph.semgrex.NodePattern$NodeMatcher.resetChild(NodePattern.java:356)
	at edu.stanford.nlp.semgraph.semgrex.NodePattern$NodeMatcher.goToNextNodeMatch(NodePattern.java:455)
	at edu.stanford.nlp.semgraph.semgrex.NodePattern$NodeMatcher.matches(NodePattern.java:572)
	at edu.stanford.nlp.semgraph.semgrex.SemgrexMatcher.find(SemgrexMatcher.java:193)
	at edu.stanford.nlp.coref.data.Mention.findDependentVerb(Mention.java:1099)
	at edu.stanford.nlp.coref.data.Mention.setDiscourse(Mention.java:318)
	at edu.stanford.nlp.coref.data.Mention.process(Mention.java:235)
	at edu.stanford.nlp.coref.data.Mention.process(Mention.java:241)
	at edu.stanford.nlp.coref.data.DocumentPreprocessor.fillMentionInfo(DocumentPreprocessor.java:341)
	at edu.stanford.nlp.coref.data.DocumentPreprocessor.initializeMentions(DocumentPreprocessor.java:169)
	at edu.stanford.nlp.coref.data.DocumentPreprocessor.preprocess(DocumentPreprocessor.java:62)
	at edu.stanford.nlp.coref.data.DocumentMaker.makeDocument(DocumentMaker.java:92)
	at edu.stanford.nlp.coref.data.DocumentMaker.makeDocument(DocumentMaker.java:64)
	at edu.stanford.nlp.coref.CorefSystem.annotate(CorefSystem.java:53)
	... 8 more
""""""
Is this due to memory constraints? 
My parameter setting is:
""java -mx64g -cp ""$DATA/corenlp/stanford-corenlp-4.1.0/*"" edu.stanford.nlp.pipeline.StanfordCoreNLP $*""
and my command is:
sh ./corenlp.sh -fileList $DATA/${SPLIT}_path.txt \
    -outputDirectory $DATA/output -outputFormat json \
    -annotators tokenize,ssplit,pos,lemma,ner,depparse,parse,coref
Besides, What should I set the -mx parameter toÔºü"
945,https://github.com/stanfordnlp/CoreNLP/issues/1170,1170,[],closed,2021-07-17 18:25:36+00:00,,Broken Link in https://stanfordnlp.github.io/stanza/client_usage.html,"There is a broken link on the website on the page https://stanfordnlp.github.io/stanza/client_usage.html 
It points to https://github.com/stanfordnlp/stanza/blob/main/demo/StanfordNLP_CoreNLP_Interface.ipynb which doesn't exist and has been shifted to https://github.com/stanfordnlp/stanza/blob/main/demo/Stanza_CoreNLP_Interface.ipynb"
946,https://github.com/stanfordnlp/CoreNLP/issues/1171,1171,[],closed,2021-08-03 01:02:39+00:00,,Models mirror,"I make docker images with a couple of the models and every time (so not hugely often, admittedly) I need to make a new one I need to spend several hours monitoring and rebuilding due to failures downloading the models. At least from Hong Kong, the connection can be painfully slow (several hours) and has LOTS of errors. It can take 10 or even 15 attempts to finally get a build working. While it might be a specific issue with Hong Kong it does seem strange, and I don't have issues with other US servers. 

Is there anywhere else to get these images via HTTP/FTP, or just on nlp.stanford.edu/software?"
947,https://github.com/stanfordnlp/CoreNLP/issues/1172,1172,[],closed,2021-08-05 16:16:20+00:00,,Model jar files: 404 Not Found,"I can't download any of your models. These both yield **_404 Not Found_** errors:

https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-4.2.1-models-english.jar
https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-4.2.1-models-english-kbp.jar


<img width=""734"" alt=""image"" src=""https://user-images.githubusercontent.com/286198/128385169-f20f9b8e-1451-42b5-8707-6546cb62b64e.png"">"
948,https://github.com/stanfordnlp/CoreNLP/issues/1173,1173,[],closed,2021-08-20 11:52:24+00:00,,cannot download the core NLP models,"Hi,
THe download link on the home page does not work.
https://nlp.stanford.edu/software/stanford-corenlp-4.2.2.zip
Is there any other way of downloading the core NLP models?

I also cannot access the previous models. I'm trying wget on all the older versions here on this page (https://stanfordnlp.github.io/CoreNLP/history.html) none of the models are live.

Please provide a solution

Thanks,"
949,https://github.com/stanfordnlp/CoreNLP/issues/1174,1174,[],open,2021-08-24 09:26:08+00:00,,CoreNLP client restarts even after shutting down,"Hello,

I am using stanza corenlp to detect NER's in the text by starting a corenlp server using CoreNLPClient .Once the entities are found and I try to shutdown/stop the server using the stop(also tried to shut down using the ""with"" statement)  function on the object created using CoreNLPClient ,I get a message ""CoreNLP Server is shutting down."". But corenlp server restarts automatically when I try to use multiprocessing on the rest of the code. 

I have kept the starting and stopping of the server in the main function and only when these are executed the multiprocessing code is executed . 

Can anyone help me out with an efficient way of stopping the server ?  


"
950,https://github.com/stanfordnlp/CoreNLP/issues/1175,1175,[],closed,2021-08-25 00:00:12+00:00,,dependency parsing Using Stanza,"Hi,
I'm using stanza nlp for finding the dependency parsing between words. For example 
doc=nlp(""His home was in violation of local and state zoning and environmental regulations, and there was no access to a road"").
keyword={""""changes"""": [""""no access""""], """"features"""": [""""road""""]}.
The values of variable keyword ,""no access"" and ""road"" are present in doc. If both of this words has any relation then I need to print ""True"". If no relation then print False. How can I write this code ?"
951,https://github.com/stanfordnlp/CoreNLP/issues/1177,1177,[],closed,2021-08-29 10:59:51+00:00,,Relation extraction model,"Hi!
I've developed my corpus to train a custom relation extraction model as explained in https://nlp.stanford.edu/software/relationExtractor.html

There is a demo of my corpus (_DatasetDemoTrain2.corp_):

```
1	Peop	0	O	NNP/NNP	Harry/Potter	O	O	O
1	O	1	O	VBD	was	O	O	O
1	O	2	O	VBN	born	O	O	O
1	O	3	O	IN	in	O	O	O
1	Loc	4	O	NNP	London	O	O	O
1	O	5	O	.	.	O	O	O

0	4	birth_place
```

I've used the _roth.properties_ file from here https://nlp.stanford.edu/software/roth.properties modifying only _trainpath_, _serializedTrainingSentencesPath_, _serializedEntityExtractorPath_, _serializedRelationExtractorPath_, by putting directly the file names.

Then in the folder with (and only) _roth.properties_ and _DatasetDemoTrain2.corp_ I executed this command line:
`java -cp /Users/irene/Documents/GitHub/Tirocinio3-git/lib/stanford-corenlp-4.2.2/stanford-corenlp-4.2.2.jar edu.stanford.nlp.ie.machinereading.MachineReading && /Users/irene/Documents/GitHub/Tirocinio3-git/lib/stanford-corenlp-4.2.2/stanford-corenlp-4.2.2-models.jar edu.stanford.nlp.models.pos-tagger.english-left3words-distsim.tagger --arguments roth.properties`

I had also to specify the tagger path (otherwise it gets me some errors since it couldn't find it).

Unfortunately it doesn't work and give me this error:
```
Missing required option: datasetreaderclass   <in class: class edu.stanford.nlp.ie.machinereading.MachineReadingProperties>
Missing required option: serializedtrainingsentencespath   <in class: class edu.stanford.nlp.ie.machinereading.MachineReadingProperties>
Exception in thread ""main"" java.lang.RuntimeException: Specified properties are not parsable or not valid!
	at edu.stanford.nlp.util.ArgumentParser.fillOptionsImpl(ArgumentParser.java:483)
	at edu.stanford.nlp.util.ArgumentParser.fillOptionsImpl(ArgumentParser.java:495)
	at edu.stanford.nlp.util.ArgumentParser.fillOptions(ArgumentParser.java:543)
	at edu.stanford.nlp.util.ArgumentParser.fillOptions(ArgumentParser.java:581)
	at edu.stanford.nlp.ie.machinereading.MachineReading.makeMachineReading(MachineReading.java:207)
	at edu.stanford.nlp.ie.machinereading.MachineReading.main(MachineReading.java:110)
```

Do you know how I can fix it or what I'm doing wrong?

Thank you in advance."
952,https://github.com/stanfordnlp/CoreNLP/issues/1178,1178,[],open,2021-08-30 14:17:25+00:00,,Question: Best practices for converting OntoNotes to UD,"What are the current best practices for converting OntoNotes 5.0 to UD format?
I didn't find any documentation or issues about this, sorry if it was already asked.
I used [this](https://universaldependencies.org/docsv1/en/overview/introduction.html) description of EWT conversion as a basic guidance.

There are multiple preprocessors:
* `edu.stanford.nlp.trees.treebank.OntoNotesUDUpdater`
  It seems to filter many broken sentences (around 17k).
* Also, I found a common tool for correction of Penn Treebanks in `edu.stanford.nlp.trees.Treebanks`. 
  Does it make sense to invoke this function after `OntoNotesUDUpdater`?
* Anything else?

After that I apply:
* `edu.stanford.nlp.trees.ud.UniversalDependenciesConverter`
* `edu.stanford.nlp.trees.ud.UniversalDependenciesFeatureAnnotator`

The following fields are filled after that: FORM, LEMMA, UPOSTAG, FEATS, HEAD, DEPREL. I didn't find a tool to add original sentence text to the final Conllu file, and information about token spacing. Any clues for these ones? I found [scripts](https://github.com/foxik/UD_English/blob/space_after/merge_to_conllu.pl) that were used to add `SpaceAfter` to EWT, but it seems it cannot be applied to OntoNotes.

Postprocessing:
* There is `UniversalEnhancer` that can be used for any language. Can I use pretrained fasttext embeddings in this tool?
   Or do I need some special embeddings?
* anything else?


<details>
  <summary>example of a script:</summary>
  
  ```bash
#!/usr/bin/env bash

convert (){
    local fname=""$1""
    local part=${fname#onto.}
    for f in $(<$fname) ; do
        rm -f onto_fixed temp_tree temp_ud

        if [ -n ""$MK_CRCT"" ]; then
            java -cp ""$CORENLP_HOME/*""  -mx5g edu.stanford.nlp.trees.treebank.OntoNotesUDUpdater \
                 $f > onto_fixed 2>> ""$OUT_DIR""/fixer.log
            f=onto_fixed
        fi

        java -cp ""$CORENLP_HOME/*"" -mx5g edu.stanford.nlp.trees.Treebanks \
             -correct -pennPrint $f \
             > temp_tree 2>> ""$OUT_DIR""/correct.log
        java -cp ""$CORENLP_HOME/*"" -mx5g edu.stanford.nlp.trees.ud.UniversalDependenciesConverter \
             -outputRepresentation enhanced++ -treeFile temp_tree \
             > temp_ud 2>> ""$OUT_DIR""/convert-1.log

        java -cp ""$CORENLP_HOME/*"" -mx5g edu.stanford.nlp.trees.ud.UniversalDependenciesFeatureAnnotator \
             temp_ud temp_tree \
             >> ""$OUT_DIR""/$part.conllu 2>> ""$OUT_DIR""/convert-2.log

    done


    # see https://github.com/stanfordnlp/CoreNLP/issues/1132
    java -cp ""$CORENLP_HOME/*""  -mx5g edu.stanford.nlp.trees.ud.UniversalEnhancer \
         -conlluFile ""$OUT_DIR""/$part.conllu \
         -relativePronouns ""that|which|who|whom|whose|where|That|Which|Who|Whom|Whose|Where"" \
         > ""$OUT_DIR""/$part.conllu.enhanced 2> ""$OUT_DIR""/enhance.log
    rm ""$OUT_DIR""/$part.conllu && mv ""$OUT_DIR""/$part.conllu.enhanced ""$OUT_DIR""/$part.conllu

}

[ -z ""$ONTO_DIR"" ] && ONTO_DIR=""/path/to/onto""
[ -z ""$CORENLP_HOME"" ] && CORENLP_HOME=""/path/to/corenlp""

OUT_DIR=""$1""
if [ -z ""$OUT_DIR"" ]; then
    echo ""Pass out_dir as first argument""
    exit 3
fi

mkdir -p ""$OUT_DIR""
#creaet abs path
OUT_DIR=$(cd ""$1""; pwd)
rm -f ""$OUT_DIR""/*.conllu

MK_CRCT=""$2""

echo ""Convert to $OUT_DIR with MK_CRCT=$MK_CRCT""

pushd ""$ONTO_DIR""/data/files/data/english/annotations

find . -name *.parse > onto
java -cp ""$CORENLP_HOME/*""  -mx5g edu.stanford.nlp.parser.tools.OntoNotesFilePreparation onto

convert onto.train
convert onto.dev
convert onto.test


popd
  ```

</details>
"
953,https://github.com/stanfordnlp/CoreNLP/issues/1179,1179,[],closed,2021-08-31 16:23:00+00:00,,Question: Is there a way to customize sentence break rules?,"I have successfully used `ssplit.boundaryTokenRegex` to add cases in which to break a sentence. However, for the following example, I cannot find an option that allows me to prevent sentence breaks in certain cases:
>My cel. # 34345

Currently, Stanford gives me ""My cel."" and ""# 34345"" (two separate sentences), but I would like to tell Stanford to _NOT_ break after abbreviations such as ""cel."" and ""ph."", etc. Is there a way to do this?"
954,https://github.com/stanfordnlp/CoreNLP/issues/1183,1183,[],closed,2021-09-07 12:33:37+00:00,,version 3.9.2 quote attribution,"Hi,

I want to do a quote attribution task same as the Muzny paper:

        Properties props = new Properties();
        props.setProperty(""coref.algorithm"", ""neural"");
        props.setProperty(""file"", ""quote-example.txt"");
        props.setProperty(""outputFormat"", ""text"");
        props.setProperty(""ner.useSUTime"", ""false"");
        props.setProperty(""charactersPath"",""characters.txt"");

        StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
        CoreDocument document = new CoreDocument(text);
        pipeline.annotate(document);


But Tiffany and Savannah are different characters. How Is it possible even if the canonical speaker is Savannah, the original speaker is Tiffany?

Example output:
quote:
""My file is gone. Someone broke in here and stole that file folder.""
original speaker of the quote:
Tiffany
canonical speaker of the quote:
Savannah

characters.txt
Savannah;None;Savannah
Connor;M;Connor;Connor Wells
Melanie;F;Melanie;Melanie Wedgewood
Niles;M;Niles
Jamie;None;Jamie
Jimmy;M;Jimmy;Jimmy Takata
Tiffany;F;Tiffany
Letty;F;Letty
Brian Donahue;M;Brian Donahue
Denny;None;Denny
Annie;F;Annie
Thomas;M;Thomas

Thank you.



"
955,https://github.com/stanfordnlp/CoreNLP/issues/1184,1184,[],closed,2021-09-07 13:39:44+00:00,,Wrong ADP POS-Tag allocation in the german model ,"Hi,

I use both the stanford-corenlp and the german model in version ""4.2.2"" and found some odd behavior in the allocation of POS-Tags f√ºr words like ""f√ºr"" and ""vor"". These two words should be normally designed with ""ADP"" but I often get ""NOUN"" or even ""PROPN"" as POS-Tag for these words.

For example for the sentence ""Welcher der Befunde ist fuÃàr eine Gehirnerkrankung typisch?"" ""f√ºr"" is tagged as ""PROPN"" an in ""Welcher der Befunde ist am ehesten fuÃàr hier am wahrscheinlichsten vorliegende Gehirnerkrankung typisch?"" it is designated as ""NOUN"".

But for the sentences ""F√ºr wen ist das Essen?"" and ""Ich war reif f√ºr das Bett.""  ""f√ºr"" is correctly tagged as ""ADP"". 

Here are the properties I used for the initialization:
(.setProperty properties ""annotators"" ""tokenize, ssplit, mwt , pos, lemma, ner, parse, depparse"")
(.setProperty properties ""coref.algorithm"", ""neural"")
(.setProperty properties ""depparse.language"", ""german"")
(.setProperty properties ""ner.language"", ""de"")
(.setProperty properties ""tokenize.options"" ""untokenizable=noneDelete"")


These are all the information I have. I hope they are helpfull.

Best regards
Goldritter
"
956,https://github.com/stanfordnlp/CoreNLP/issues/1186,1186,[],closed,2021-09-10 13:59:10+00:00,,Not getting Universal Dependencies in spanish through CoreNLP Server 4.2.2,"Im having the same problem as described here:
https://stackoverflow.com/questions/58754796/how-can-i-use-the-stanford-nlp-part-of-speech-tagging-in-spanish

As described in the official Stanford NLP FAQs for spanish, the POS tags should be like ""vmg0000"" and not ""VERB"" that is what im gettinng and is not even penn tree bank.
https://nlp.stanford.edu/software/spanish-faq.html#tagset

What im trying is to get the tense of the verb, is there another way?
Is there a parameter to enable those more detailed POS tags?

thank you"
957,https://github.com/stanfordnlp/CoreNLP/issues/1187,1187,[],closed,2021-09-13 13:36:01+00:00,,Question about coref in output file,"I would like to use the coreference feature in CoreNLP, but I have had problems with the output file. 

I ran the following code:

./corenlp.sh -annotators tokenize,ssplit,pos,lemma,ner,parse,coref -file text.txt -outputFormat conll

The output file has seven columns, which correspond to what looks like a token id, word, lemma, pos, ner, some numerical value, and dependency parsing. There does not appear to be any information related to coreference annotation.

Could you please advise me as to how I can change the code in order that the coreference annotation information appears in the output file.

Thank you.
"
958,https://github.com/stanfordnlp/CoreNLP/issues/1188,1188,[],closed,2021-09-13 15:43:50+00:00,,Can CoreNLP recognize sentence fragments?,"Does anyone think any CoreNLP methods could detect if a string seems like a fragment of a sentence, i.e. it forms part of a correct natural language sentence but it is only part of it, hence incomplete - vs a string which may not be a grammatically correct sentence but is some other syntactic type, for example, a title or chapter header?

Would it be better to do this with a rule-based approach or with a machine learning algorithm?

Thank you."
959,https://github.com/stanfordnlp/CoreNLP/issues/1190,1190,[],open,2021-09-15 12:28:17+00:00,,Entity Relation Extraction,"Can we extract entity relations in this format: (e1,relation,e2) ? If yes, please advise on the input data format and if any tool is available for data annotation.

Thank you
Marur"
960,https://github.com/stanfordnlp/CoreNLP/issues/1193,1193,[],closed,2021-09-19 08:42:41+00:00,,I got an error while using it in python: it con't work when the sentence has '%'. ,"What I input: print(nlp.parse('Shares of MONY were gaining 2.57%'))
The error is: 
                  raise JSONDecodeError(""Expecting value"", s, err.value) from None
       json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)"
961,https://github.com/stanfordnlp/CoreNLP/issues/1195,1195,[],closed,2021-09-29 19:34:16+00:00,,How to get in touch regarding a security concern,"Hey there!

I belong to an open source security research community, and a member (@srikanthprathi) has found an issue, but doesn‚Äôt know the best way to disclose it.

If not a hassle, might you kindly add a `SECURITY.md` file with an email, or another contact method? GitHub [recommends](https://docs.github.com/en/code-security/getting-started/adding-a-security-policy-to-your-repository) this best practice to ensure security issues are responsibly disclosed, and it would serve as a simple instruction for security researchers in the future.

Thank you for your consideration, and I look forward to hearing from you!

(cc @huntr-helper)"
962,https://github.com/stanfordnlp/CoreNLP/issues/1199,1199,[],closed,2021-10-05 16:35:04+00:00,,Corenlp using java API,"Hello Java Corenlp community,
Can we run KBP OR entity linking packages using java API (without command line) and on different language? I want to detect relations between entities on French language."
963,https://github.com/stanfordnlp/CoreNLP/issues/1201,1201,[],closed,2021-10-07 20:22:39+00:00,,Quote Annotator - Documentation Discrepancy?,"The Description for quotes in the Annotator Descriptions table (https://stanfordnlp.github.io/CoreNLP/annotators.html) states that quote: 
Does not depend on any other annotators.

But the Annotator Dependencies for quotes list these annotators as requirements: tokenize, ssplit, pos, lemma, ner, depparse, coref

I was able to run quote extraction simply using the quote annotator and it produced the same results as using the annotators above. So do we need to specify the annotators for quote?

with CoreNLPClient(annotators=['quote'], be_quiet=False, properties={'tokenize.codepoint': 'true'}) as client:
"
964,https://github.com/stanfordnlp/CoreNLP/issues/1206,1206,[],open,2021-10-13 21:39:48+00:00,,Use corenlp for  dependency parsing on French language ,"Hello Community, I want to know if there is a possibility to exploit the dependency parsing functionality using java api (without terminal)?"
965,https://github.com/stanfordnlp/CoreNLP/issues/1207,1207,[],open,2021-10-14 17:47:09+00:00,,Stanza chinese (zh) dictionary ,"What is the dictionary that stanza for chinese (zh) is using? 

"
966,https://github.com/stanfordnlp/CoreNLP/issues/1209,1209,[],open,2021-10-21 12:40:12+00:00,,"HTML, CSS  and .js files associated with demo not copied by Github build.","It would be nice if the Maven command that builds from source automatically copied `/demo/corenlp-brat.html`, `/demo/corenlp-brat.css`, `/demo/corenlp-brat.js` and `/demo/corenlp-parseviewer.js` into the same place that they appear in a release. Because that doesn't yet happen, the server doesn't find what it is looking for , and falls older versions of the files from some non-obvious (and, I think remote) place. The effect is that the website claims to be 4.2.0 no matter what, and doesn't have the spiffy logo."
967,https://github.com/stanfordnlp/CoreNLP/issues/1210,1210,[],open,2021-10-22 01:27:48+00:00,,invertible is incorrect for HTML tags,"I am not sure if it is the intended behaviour, but it seems odd to me.

With a basic set of options

`-annotators tokenize,cleanxml,ssplit,pos,lemma`

parse the sentence

`This is a <b>test</b> sentence.`

the output is

```
{
  ""sentences"": [
    {
      ""index"": 0,
      ""tokens"": [
        {
          ""index"": 1,
          ""word"": ""This"",
          ""originalText"": ""This"",
          ""lemma"": ""this"",
          ""characterOffsetBegin"": 0,
          ""characterOffsetEnd"": 4,
          ""pos"": ""DT"",
          ""before"": """",
          ""after"": "" ""
        },
        {
          ""index"": 2,
          ""word"": ""is"",
          ""originalText"": ""is"",
          ""lemma"": ""be"",
          ""characterOffsetBegin"": 5,
          ""characterOffsetEnd"": 7,
          ""pos"": ""VBZ"",
          ""before"": "" "",
          ""after"": "" ""
        },
        {
          ""index"": 3,
          ""word"": ""a"",
          ""originalText"": ""a"",
          ""lemma"": ""a"",
          ""characterOffsetBegin"": 8,
          ""characterOffsetEnd"": 9,
          ""pos"": ""DT"",
          ""before"": "" "",
          ""after"": ""  <b>""
        },
        {
          ""index"": 4,
          ""word"": ""test"",
          ""originalText"": ""test"",
          ""lemma"": ""test"",
          ""characterOffsetBegin"": 13,
          ""characterOffsetEnd"": 17,
          ""pos"": ""NN"",
          ""before"": "" <b>"",
          ""after"": ""</b>""
        },
        {
          ""index"": 5,
          ""word"": ""sentence"",
          ""originalText"": ""sentence"",
          ""lemma"": ""sentence"",
          ""characterOffsetBegin"": 22,
          ""characterOffsetEnd"": 30,
          ""pos"": ""NN"",
          ""before"": ""</b> "",
          ""after"": """"
        },
        {
          ""index"": 6,
          ""word"": ""."",
          ""originalText"": ""."",
          ""lemma"": ""."",
          ""characterOffsetBegin"": 30,
          ""characterOffsetEnd"": 31,
          ""pos"": ""."",
          ""before"": """",
          ""after"": """"
        }
      ]
    }
  ],
  ""sections"": [
  ]
}
```

For index #3, the **after** element is `""  <b>""` (two spaces). The previous character offset is 9 and the current one is 13, which means **after** element should be 4 characters, not 5.

Similarly, for index #5, the **before** element should be 5 characters, not 4 to match the character offsets.

Test with version 4.3.1.
"
